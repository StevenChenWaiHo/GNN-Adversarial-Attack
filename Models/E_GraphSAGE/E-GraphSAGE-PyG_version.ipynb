{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9fb9e1b2-1c5f-49e3-82b4-3bfe52cabad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import from_networkx, add_self_loops, degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "# import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Datasets.UNSW_NB15.UNSW_NB15_config import UNSW_NB15_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ef09a-d405-43b8-971e-fe9e6a592c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1199204/3841627832.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(os.path.join(project_root, \"Datasets\", \"UNSW_NB15/All/all_raw_downsampled.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "Normal            221876\n",
      "Generic           215481\n",
      "Exploits           44525\n",
      "Fuzzers            24246\n",
      "DoS                16353\n",
      "Reconnaissance     13987\n",
      "Analysis            2677\n",
      "Backdoor            1795\n",
      "Shellcode           1511\n",
      "Backdoors            534\n",
      "Worms                174\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(project_root, \"Datasets\", \"UNSW_NB15/All/all_raw_downsampled.csv\"))\n",
    "\n",
    "DATASET_NAME = \"UNSW_NB15\"\n",
    "\n",
    "SOURCE_IP_COL_NAME = UNSW_NB15_Config.SOURCE_IP_COL_NAME\n",
    "DESTINATION_IP_COL_NAME = UNSW_NB15_Config.DESTINATION_IP_COL_NAME\n",
    "SOURCE_PORT_COL_NAME = UNSW_NB15_Config.SOURCE_PORT_COL_NAME\n",
    "DESTINATION_PORT_COL_NAME = UNSW_NB15_Config.DESTINATION_PORT_COL_NAME\n",
    "\n",
    "ATTACK_CLASS_COL_NAME = UNSW_NB15_Config.ATTACK_CLASS_COL_NAME\n",
    "IS_ATTACK_COL_NAME = UNSW_NB15_Config.IS_ATTACK_COL_NAME\n",
    "\n",
    "BENIGN_CLASS_NAME = UNSW_NB15_Config.BENIGN_CLASS_NAME\n",
    "\n",
    "MULTICLASS = True\n",
    "label_col = ATTACK_CLASS_COL_NAME if MULTICLASS else IS_ATTACK_COL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a1af1-1d3d-4179-9628-7c2ec551ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=UNSW_NB15_Config.DROP_COLS,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb74f87-8df1-4c98-a849-e263e03a06f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    321283\n",
      "0    221876\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[ATTACK_CLASS_COL_NAME].value_counts())\n",
    "print(data[IS_ATTACK_COL_NAME].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2a2c690c-86a4-49f7-aa9c-58f94529547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME].apply(str)\n",
    "data[SOURCE_PORT_COL_NAME] = data[SOURCE_PORT_COL_NAME].apply(str)\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME].apply(str)\n",
    "data[DESTINATION_PORT_COL_NAME] = data[DESTINATION_PORT_COL_NAME].apply(str)\n",
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME] + ':' + data[SOURCE_PORT_COL_NAME]\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME] + ':' + data[DESTINATION_PORT_COL_NAME]\n",
    "data.drop(columns=[SOURCE_PORT_COL_NAME,DESTINATION_PORT_COL_NAME],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5651ef5b-0a9d-4641-aad7-5738092c46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                      srcip               dstip state        dur  sbytes  \\\n",
      "0             10.40.85.1:0         224.0.0.5:0   INT  50.004341     384   \n",
      "1          59.166.0.6:2142    149.171.126.4:53   CON   0.001134     132   \n",
      "2       175.45.176.0:13284   149.171.126.16:80   FIN   2.390390    1362   \n",
      "3         59.166.0.3:42587    149.171.126.8:25   FIN  34.077175   37358   \n",
      "4            10.40.170.2:0       10.40.170.2:0   INT   0.000000      46   \n",
      "...                    ...                 ...   ...        ...     ...   \n",
      "543154  175.45.176.0:47439   149.171.126.10:53   INT   0.000001     114   \n",
      "543155  175.45.176.0:47439   149.171.126.10:53   INT   0.000001     114   \n",
      "543156    59.166.0.5:53521    149.171.126.7:21   CON   1.086072    1940   \n",
      "543157  175.45.176.0:17293  149.171.126.17:110   CON   0.942984     574   \n",
      "543158  175.45.176.0:17293  149.171.126.17:110   CON   0.942984     574   \n",
      "\n",
      "        dbytes  sttl  dttl  sloss  dloss  ...  ct_ftp_cmd  ct_srv_src  \\\n",
      "0            0     1     0      0      0  ...         0.0           2   \n",
      "1          164    31    29      0      0  ...         0.0          12   \n",
      "2          268   254   252      6      1  ...         0.0           5   \n",
      "3         3380    31    29     18      8  ...         0.0           1   \n",
      "4            0     0     0      0      0  ...         0.0           2   \n",
      "...        ...   ...   ...    ...    ...  ...         ...         ...   \n",
      "543154       0   254     0      0      0  ...         NaN          15   \n",
      "543155       0   254     0      0      0  ...         NaN          15   \n",
      "543156    2404    31    29      8     10  ...         2.0           2   \n",
      "543157     676    62   252      5      6  ...         NaN           2   \n",
      "543158     676    62   252      5      6  ...         NaN           1   \n",
      "\n",
      "        ct_srv_dst  ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  \\\n",
      "0                4           4           2                 2   \n",
      "1                7           1           2                 2   \n",
      "2                2           2           1                 1   \n",
      "3                1          12          10                 1   \n",
      "4                2           2           2                 2   \n",
      "...            ...         ...         ...               ...   \n",
      "543154          15          15          15                15   \n",
      "543155          15          15          15                15   \n",
      "543156           2           3           3                 2   \n",
      "543157           1           2           4                 2   \n",
      "543158           1           2           4                 2   \n",
      "\n",
      "        ct_dst_sport_ltm  ct_dst_src_ltm      attack_cat  label  \n",
      "0                      4               2          Normal      0  \n",
      "1                      1               1          Normal      0  \n",
      "2                      1               1  Reconnaissance      1  \n",
      "3                      1               2          Normal      0  \n",
      "4                      2               2          Normal      0  \n",
      "...                  ...             ...             ...    ...  \n",
      "543154                15              15         Generic      1  \n",
      "543155                15              15         Generic      1  \n",
      "543156                 2               3          Normal      0  \n",
      "543157                 2               2        Exploits      1  \n",
      "543158                 2               2        Exploits      1  \n",
      "\n",
      "[543159 rows x 45 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7fb458-ca34-42ca-a8af-f8e1609aff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns = UNSW_NB15_Config.CATEGORICAL_COLS) # One Hot Encoding for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2d96115-31f9-48cb-b3e6-7853d2d253cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                      srcip               dstip        dur  sbytes  dbytes  \\\n",
      "0             10.40.85.1:0         224.0.0.5:0  50.004341     384       0   \n",
      "1          59.166.0.6:2142    149.171.126.4:53   0.001134     132     164   \n",
      "2       175.45.176.0:13284   149.171.126.16:80   2.390390    1362     268   \n",
      "3         59.166.0.3:42587    149.171.126.8:25  34.077175   37358    3380   \n",
      "4            10.40.170.2:0       10.40.170.2:0   0.000000      46       0   \n",
      "...                    ...                 ...        ...     ...     ...   \n",
      "543154  175.45.176.0:47439   149.171.126.10:53   0.000001     114       0   \n",
      "543155  175.45.176.0:47439   149.171.126.10:53   0.000001     114       0   \n",
      "543156    59.166.0.5:53521    149.171.126.7:21   1.086072    1940    2404   \n",
      "543157  175.45.176.0:17293  149.171.126.17:110   0.942984     574     676   \n",
      "543158  175.45.176.0:17293  149.171.126.17:110   0.942984     574     676   \n",
      "\n",
      "        sttl  dttl  sloss  dloss         Sload  ...  state_ECO  state_FIN  \\\n",
      "0          1     0      0      0  5.119556e+01  ...      False      False   \n",
      "1         31    29      0      0  4.656085e+05  ...      False      False   \n",
      "2        254   252      6      1  4.233619e+03  ...      False       True   \n",
      "3         31    29     18      8  8.601652e+03  ...      False       True   \n",
      "4          0     0      0      0  0.000000e+00  ...      False      False   \n",
      "...      ...   ...    ...    ...           ...  ...        ...        ...   \n",
      "543154   254     0      0      0  4.560000e+08  ...      False      False   \n",
      "543155   254     0      0      0  4.560000e+08  ...      False      False   \n",
      "543156    31    29      8     10  1.387017e+04  ...      False      False   \n",
      "543157    62   252      5      6  4.470914e+03  ...      False      False   \n",
      "543158    62   252      5      6  4.470914e+03  ...      False      False   \n",
      "\n",
      "        state_INT  state_MAS  state_PAR  state_REQ  state_RST  state_TST  \\\n",
      "0            True      False      False      False      False      False   \n",
      "1           False      False      False      False      False      False   \n",
      "2           False      False      False      False      False      False   \n",
      "3           False      False      False      False      False      False   \n",
      "4            True      False      False      False      False      False   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "543154       True      False      False      False      False      False   \n",
      "543155       True      False      False      False      False      False   \n",
      "543156      False      False      False      False      False      False   \n",
      "543157      False      False      False      False      False      False   \n",
      "543158      False      False      False      False      False      False   \n",
      "\n",
      "        state_TXD  state_URH  \n",
      "0           False      False  \n",
      "1           False      False  \n",
      "2           False      False  \n",
      "3           False      False  \n",
      "4           False      False  \n",
      "...           ...        ...  \n",
      "543154      False      False  \n",
      "543155      False      False  \n",
      "543156      False      False  \n",
      "543157      False      False  \n",
      "543158      False      False  \n",
      "\n",
      "[543159 rows x 57 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "data.drop(columns=['index'],inplace=True)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1df5c4c-70a2-4566-ae5e-ee3dcc6037a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 dur        sbytes        dbytes           sttl  \\\n",
      "count  543159.000000  5.431590e+05  5.431590e+05  543159.000000   \n",
      "mean        0.706760  5.136572e+03  1.936909e+04     157.197364   \n",
      "std        12.637229  1.202311e+05  1.390925e+05     108.452474   \n",
      "min         0.000000  0.000000e+00  0.000000e+00       0.000000   \n",
      "25%         0.000007  1.140000e+02  0.000000e+00      31.000000   \n",
      "50%         0.000011  2.000000e+02  0.000000e+00     254.000000   \n",
      "75%         0.072088  1.580000e+03  1.940000e+03     254.000000   \n",
      "max      8760.776367  1.435577e+07  1.465753e+07     255.000000   \n",
      "\n",
      "                dttl          sloss          dloss         Sload  \\\n",
      "count  543159.000000  543159.000000  543159.000000  5.431590e+05   \n",
      "mean       38.850764       3.800661       8.729770  6.877595e+07   \n",
      "std        77.034389      45.616565      50.136204  1.420534e+08   \n",
      "min         0.000000       0.000000       0.000000  0.000000e+00   \n",
      "25%         0.000000       0.000000       0.000000  3.705672e+05   \n",
      "50%         0.000000       0.000000       0.000000  4.560000e+07   \n",
      "75%        29.000000       3.000000       4.000000  8.888889e+07   \n",
      "max       254.000000    5319.000000    5507.000000  5.988000e+09   \n",
      "\n",
      "              Dload          Spkts  ...  ct_flw_http_mthd   is_ftp_login  \\\n",
      "count  5.431590e+05  543159.000000  ...     543159.000000  543159.000000   \n",
      "mean   1.148111e+06      20.369921  ...          0.088724       0.011490   \n",
      "std    3.127653e+06     101.923505  ...          0.566327       0.109623   \n",
      "min    0.000000e+00       0.000000  ...          0.000000       0.000000   \n",
      "25%    0.000000e+00       2.000000  ...          0.000000       0.000000   \n",
      "50%    0.000000e+00       2.000000  ...          0.000000       0.000000   \n",
      "75%    4.087816e+05      14.000000  ...          0.000000       0.000000   \n",
      "max    2.274587e+07   10646.000000  ...         36.000000       4.000000   \n",
      "\n",
      "          ct_ftp_cmd     ct_srv_src     ct_srv_dst     ct_dst_ltm  \\\n",
      "count  543159.000000  543159.000000  543159.000000  543159.000000   \n",
      "mean        0.013029      15.008556      14.833110      10.306825   \n",
      "std         0.141497      14.229735      14.305878      10.989668   \n",
      "min         0.000000       1.000000       1.000000       1.000000   \n",
      "25%         0.000000       3.000000       3.000000       2.000000   \n",
      "50%         0.000000       9.000000       8.000000       5.000000   \n",
      "75%         0.000000      26.000000      26.000000      17.000000   \n",
      "max         8.000000      67.000000      67.000000      67.000000   \n",
      "\n",
      "          ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \n",
      "count  543159.000000     543159.000000     543159.000000   543159.000000  \n",
      "mean       10.837838          9.343288          7.209839       13.766985  \n",
      "std        10.967546         11.391238          8.069018       14.972826  \n",
      "min         1.000000          1.000000          1.000000        1.000000  \n",
      "25%         2.000000          1.000000          1.000000        1.000000  \n",
      "50%         6.000000          2.000000          2.000000        5.000000  \n",
      "75%        17.000000         17.000000         15.000000       26.000000  \n",
      "max        67.000000         67.000000         60.000000       67.000000  \n",
      "\n",
      "[8 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_norm = UNSW_NB15_Config.COLS_TO_NORM\n",
    "print(data[cols_to_norm].describe()) # Check if there's any too large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea95177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numeric_issues(df, cols_to_norm):\n",
    "    for col in cols_to_norm:\n",
    "        try:\n",
    "            # Try to coerce to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Try to clip the column\n",
    "            df[col] = df[col].clip(lower=-1e9, upper=1e9)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Column '{col}' failed with error: {e}\")\n",
    "            print(f\"  - Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "            print(f\"  - Data type: {df[col].dtype}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n✅ All other columns processed successfully.\")\n",
    "\n",
    "check_numeric_issues(data, UNSW_NB15_Config.COLS_TO_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37256006-abc1-44aa-8e74-46d05dc6ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis' 'Backdoor' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic'\n",
      " 'Normal' 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "Attack label mapping: {'Analysis': 0, 'Backdoor': 1, 'Backdoors': 2, 'DoS': 3, 'Exploits': 4, 'Fuzzers': 5, 'Generic': 6, 'Normal': 7, 'Reconnaissance': 8, 'Shellcode': 9, 'Worms': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_classes = 2\n",
    "class_map = [0, 1]\n",
    "if MULTICLASS:\n",
    "    le = LabelEncoder()\n",
    "    attack_labels = le.fit_transform(data[ATTACK_CLASS_COL_NAME])\n",
    "    class_map = le.classes_\n",
    "    print(class_map)\n",
    "    print(\"Attack label mapping:\", dict(zip(class_map, range(len(class_map)))))\n",
    "    data[ATTACK_CLASS_COL_NAME] = attack_labels\n",
    "    num_classes = len(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f4cdd-2716-431f-af50-b34cc3d2d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325895\n",
      "217264\n",
      "attack_cat\n",
      "Normal            133125\n",
      "Generic           129289\n",
      "Exploits           26715\n",
      "Fuzzers            14548\n",
      "DoS                 9812\n",
      "Reconnaissance      8392\n",
      "Analysis            1606\n",
      "Backdoor            1077\n",
      "Shellcode            907\n",
      "Backdoors            320\n",
      "Worms                104\n",
      "Name: count, dtype: int64\n",
      "attack_cat\n",
      "Normal            88751\n",
      "Generic           86192\n",
      "Exploits          17810\n",
      "Fuzzers            9698\n",
      "DoS                6541\n",
      "Reconnaissance     5595\n",
      "Analysis           1071\n",
      "Backdoor            718\n",
      "Shellcode           604\n",
      "Backdoors           214\n",
      "Worms                70\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     data, data[label_col], test_size=0.3, random_state=42, stratify=data[label_col])\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0f83bd73-531f-42a0-9f73-e1fd98dce1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['h'] = X_train[ cols_to_norm ].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f9cdb-1316-461a-a927-8d67d90d6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 325895\n",
      "Number of node in G_pyg: 192101\n",
      "Shape of node in G_pyg: torch.Size([192101, 40])\n",
      "Shape of edge attr in G_pyg: torch.Size([325895, 40])\n",
      "Shape of edge class in G_pyg: torch.Size([325895])\n"
     ]
    }
   ],
   "source": [
    "# Convert NetworkX graph to PyG graph\n",
    "\n",
    "G_nx = nx.from_pandas_edgelist(X_train, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "G_pyg = from_networkx(G_nx)\n",
    "\n",
    "num_nodes = G_pyg.num_nodes\n",
    "num_edges = G_pyg.num_edges\n",
    "\n",
    "G_pyg.x = th.ones(num_nodes, len(X_train['h'].iloc[0])) \n",
    "\n",
    "edge_attr_list = []\n",
    "edge_label_list = []\n",
    "\n",
    "for u, v, key, data in G_nx.edges(keys=True, data=True):\n",
    "    edge_attr_list.append(data['h']) \n",
    "    edge_label_list.append(data[label_col]) \n",
    "\n",
    "G_pyg.edge_attr = th.tensor(edge_attr_list, dtype=th.float32)\n",
    "G_pyg.edge_label = th.tensor(edge_label_list, dtype=th.long)\n",
    "\n",
    "print(\"Number of edges in G_pyg:\", G_pyg.num_edges)\n",
    "print(\"Number of node in G_pyg:\", G_pyg.num_nodes)\n",
    "print(\"Shape of node in G_pyg:\", G_pyg.x.shape)\n",
    "print(\"Shape of edge attr in G_pyg:\", G_pyg.edge_attr.shape)\n",
    "print(\"Shape of edge label in G_pyg:\", G_pyg.edge_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41795339-6036-468f-9b9d-2bb68d78ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EGraphSAGEConv(MessagePassing):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, out_channels):\n",
    "        super(EGraphSAGEConv, self).__init__(aggr='mean')  # mean aggregation\n",
    "        self.lin_node = nn.Linear(node_in_channels, out_channels)\n",
    "        self.lin_edge = nn.Linear(edge_in_channels, out_channels)\n",
    "        self.lin_update = nn.Linear(node_in_channels + out_channels, out_channels) # out_channels * 2\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: Node features, edge_attr: Edge features, edge_index: Connectivity\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            if edge_attr.size(0) != edge_index.size(1):\n",
    "                loop_attr = th.zeros((edge_index.size(1) - edge_attr.size(0), edge_attr.size(1))).to(edge_attr.device)\n",
    "                edge_attr = th.cat([edge_attr, loop_attr], dim=0)\n",
    "        else:\n",
    "            print(\"edge_attr is unexist\")\n",
    "        \n",
    "        # Propagate and aggregate neighbor information\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j represents the adjacent nodes of x\n",
    "        # Compute messages by combining node and edge features\n",
    "        return self.lin_node(x_j) + self.lin_edge(edge_attr)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # Update node features after message passing\n",
    "        return self.lin_update(th.cat([x, aggr_out], dim=1))\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.lin = nn.Linear(in_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, data, z):\n",
    "        row, col = data.edge_index\n",
    "        # Concatenate the features of source and target nodes for each edge\n",
    "        edge_feat = th.cat([z[row], z[col]], dim=1)\n",
    "        return self.lin(edge_feat)\n",
    "\n",
    "class EGraphSAGE(nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels, out_channels):\n",
    "        super(EGraphSAGE, self).__init__()\n",
    "        self.conv1 = EGraphSAGEConv(node_in_channels, edge_in_channels, hidden_channels)\n",
    "        self.conv2 = EGraphSAGEConv(hidden_channels, edge_in_channels, hidden_channels)\n",
    "        self.mlp_predictor = MLPPredictor(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return self.mlp_predictor(data, x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bca25fef-29d9-40cf-8910-16b24d530693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cccdc850-b98d-4836-b82b-67aa4b9e1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89157faf-e24b-49d6-9c90-6f71dae515b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EGraphSAGE(node_in_channels=G_pyg.num_node_features, \n",
    "                   edge_in_channels=G_pyg.num_edge_features,\n",
    "                   hidden_channels=128, \n",
    "                   out_channels=num_classes).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "labels = G_pyg.edge_label.cpu().numpy()\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=num_classes,\n",
    "                                                  y=labels)\n",
    "\n",
    "class_weights = th.FloatTensor(class_weights).cuda()\n",
    "print(\"Class weights:\", class_weights)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d37f0-713b-4abc-8d7a-3e768ae9a2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "Epoch 0, Loss: 0.7250, Accuracy: 0.6076\n",
      "torch.Size([361903])\n",
      "epoch : 1\n",
      "Epoch 1, Loss: 0.7162, Accuracy: 0.6582\n",
      "torch.Size([361903])\n",
      "epoch : 2\n",
      "Epoch 2, Loss: 0.7039, Accuracy: 0.6064\n",
      "torch.Size([361903])\n",
      "epoch : 3\n",
      "Epoch 3, Loss: 0.7159, Accuracy: 0.6851\n",
      "torch.Size([361903])\n",
      "epoch : 4\n",
      "Epoch 4, Loss: 0.7101, Accuracy: 0.6290\n",
      "torch.Size([361903])\n",
      "epoch : 5\n",
      "Epoch 5, Loss: 0.7064, Accuracy: 0.6851\n",
      "torch.Size([361903])\n",
      "epoch : 6\n",
      "Epoch 6, Loss: 0.7151, Accuracy: 0.6845\n",
      "torch.Size([361903])\n",
      "epoch : 7\n",
      "Epoch 7, Loss: 0.7085, Accuracy: 0.6080\n",
      "torch.Size([361903])\n",
      "epoch : 8\n",
      "Epoch 8, Loss: 0.7047, Accuracy: 0.6843\n",
      "torch.Size([361903])\n",
      "epoch : 9\n",
      "Epoch 9, Loss: 0.7010, Accuracy: 0.6672\n",
      "torch.Size([361903])\n",
      "epoch : 10\n",
      "Epoch 10, Loss: 0.7049, Accuracy: 0.6845\n",
      "torch.Size([361903])\n",
      "epoch : 11\n",
      "Epoch 11, Loss: 0.7044, Accuracy: 0.6078\n",
      "torch.Size([361903])\n",
      "epoch : 12\n",
      "Epoch 12, Loss: 0.7128, Accuracy: 0.6193\n",
      "torch.Size([361903])\n",
      "epoch : 13\n",
      "Epoch 13, Loss: 0.7057, Accuracy: 0.6081\n",
      "torch.Size([361903])\n",
      "epoch : 14\n",
      "Epoch 14, Loss: 0.7066, Accuracy: 0.6855\n",
      "torch.Size([361903])\n",
      "epoch : 15\n",
      "Epoch 15, Loss: 0.7104, Accuracy: 0.6441\n",
      "torch.Size([361903])\n",
      "epoch : 16\n",
      "Epoch 16, Loss: 0.7079, Accuracy: 0.6849\n",
      "torch.Size([361903])\n",
      "epoch : 17\n",
      "Epoch 17, Loss: 0.7154, Accuracy: 0.7087\n",
      "torch.Size([361903])\n",
      "epoch : 18\n",
      "Epoch 18, Loss: 0.7084, Accuracy: 0.6294\n",
      "torch.Size([361903])\n",
      "epoch : 19\n",
      "Epoch 19, Loss: 0.7086, Accuracy: 0.6074\n",
      "torch.Size([361903])\n",
      "epoch : 20\n",
      "Epoch 20, Loss: 0.7060, Accuracy: 0.7084\n",
      "torch.Size([361903])\n",
      "epoch : 21\n",
      "Epoch 21, Loss: 0.7006, Accuracy: 0.6082\n",
      "torch.Size([361903])\n",
      "epoch : 22\n",
      "Epoch 22, Loss: 0.7084, Accuracy: 0.6848\n",
      "torch.Size([361903])\n",
      "epoch : 23\n",
      "Epoch 23, Loss: 0.7147, Accuracy: 0.6081\n",
      "torch.Size([361903])\n",
      "epoch : 24\n",
      "Epoch 24, Loss: 0.7002, Accuracy: 0.7092\n",
      "torch.Size([361903])\n",
      "epoch : 25\n",
      "Epoch 25, Loss: 0.7022, Accuracy: 0.6844\n",
      "torch.Size([361903])\n",
      "epoch : 26\n",
      "Epoch 26, Loss: 0.7007, Accuracy: 0.7086\n",
      "torch.Size([361903])\n",
      "epoch : 27\n",
      "Epoch 27, Loss: 0.7003, Accuracy: 0.6846\n",
      "torch.Size([361903])\n",
      "epoch : 28\n",
      "Epoch 28, Loss: 0.7001, Accuracy: 0.6085\n",
      "torch.Size([361903])\n",
      "epoch : 29\n",
      "Epoch 29, Loss: 0.7009, Accuracy: 0.7086\n",
      "torch.Size([361903])\n",
      "epoch : 30\n",
      "Epoch 30, Loss: 0.6961, Accuracy: 0.6857\n",
      "torch.Size([361903])\n",
      "epoch : 31\n",
      "Epoch 31, Loss: 0.7021, Accuracy: 0.6848\n",
      "torch.Size([361903])\n",
      "epoch : 32\n",
      "Epoch 32, Loss: 0.6969, Accuracy: 0.7093\n",
      "torch.Size([361903])\n",
      "epoch : 33\n",
      "Epoch 33, Loss: 0.6962, Accuracy: 0.6842\n",
      "torch.Size([361903])\n",
      "epoch : 34\n",
      "Epoch 34, Loss: 0.6904, Accuracy: 0.7093\n",
      "torch.Size([361903])\n",
      "epoch : 35\n",
      "Epoch 35, Loss: 0.6866, Accuracy: 0.7079\n",
      "torch.Size([361903])\n",
      "epoch : 36\n",
      "Epoch 36, Loss: 0.6935, Accuracy: 0.6681\n",
      "torch.Size([361903])\n",
      "epoch : 37\n",
      "Epoch 37, Loss: 0.6906, Accuracy: 0.7093\n",
      "torch.Size([361903])\n",
      "epoch : 38\n",
      "Epoch 38, Loss: 0.6934, Accuracy: 0.6847\n",
      "torch.Size([361903])\n",
      "epoch : 39\n",
      "Epoch 39, Loss: 0.6881, Accuracy: 0.7092\n",
      "torch.Size([361903])\n",
      "epoch : 40\n",
      "Epoch 40, Loss: 0.6960, Accuracy: 0.7086\n",
      "torch.Size([361903])\n",
      "epoch : 41\n",
      "Epoch 41, Loss: 0.6921, Accuracy: 0.7094\n",
      "torch.Size([361903])\n",
      "epoch : 42\n",
      "Epoch 42, Loss: 0.6917, Accuracy: 0.6848\n",
      "torch.Size([361903])\n",
      "epoch : 43\n",
      "Epoch 43, Loss: 0.6856, Accuracy: 0.6675\n",
      "torch.Size([361903])\n",
      "epoch : 44\n",
      "Epoch 44, Loss: 0.6856, Accuracy: 0.7310\n",
      "torch.Size([361903])\n",
      "epoch : 45\n",
      "Epoch 45, Loss: 0.6891, Accuracy: 0.6853\n",
      "torch.Size([361903])\n",
      "epoch : 46\n",
      "Epoch 46, Loss: 0.6861, Accuracy: 0.7094\n",
      "torch.Size([361903])\n",
      "epoch : 47\n",
      "Epoch 47, Loss: 0.6924, Accuracy: 0.7088\n",
      "torch.Size([361903])\n",
      "epoch : 48\n",
      "Epoch 48, Loss: 0.6855, Accuracy: 0.7095\n",
      "torch.Size([361903])\n",
      "epoch : 49\n",
      "Epoch 49, Loss: 0.6846, Accuracy: 0.7091\n",
      "torch.Size([361903])\n",
      "epoch : 50\n",
      "Epoch 50, Loss: 0.6920, Accuracy: 0.7086\n",
      "torch.Size([361903])\n",
      "epoch : 51\n",
      "Epoch 51, Loss: 0.6793, Accuracy: 0.7091\n",
      "torch.Size([361903])\n",
      "epoch : 52\n",
      "Epoch 52, Loss: 0.6830, Accuracy: 0.7088\n",
      "torch.Size([361903])\n",
      "epoch : 53\n",
      "Epoch 53, Loss: 0.6872, Accuracy: 0.6683\n",
      "torch.Size([361903])\n",
      "epoch : 54\n",
      "Epoch 54, Loss: 0.6863, Accuracy: 0.7082\n",
      "torch.Size([361903])\n",
      "epoch : 55\n",
      "Epoch 55, Loss: 0.6798, Accuracy: 0.7094\n",
      "torch.Size([361903])\n",
      "epoch : 56\n",
      "Epoch 56, Loss: 0.6777, Accuracy: 0.7085\n",
      "torch.Size([361903])\n",
      "epoch : 57\n",
      "Epoch 57, Loss: 0.6762, Accuracy: 0.7520\n",
      "torch.Size([361903])\n",
      "epoch : 58\n",
      "Epoch 58, Loss: 0.6850, Accuracy: 0.7084\n",
      "torch.Size([361903])\n",
      "epoch : 59\n",
      "Epoch 59, Loss: 0.6859, Accuracy: 0.7095\n",
      "torch.Size([361903])\n",
      "epoch : 60\n",
      "Epoch 60, Loss: 0.6796, Accuracy: 0.7094\n",
      "torch.Size([361903])\n",
      "epoch : 61\n",
      "Epoch 61, Loss: 0.6814, Accuracy: 0.6862\n",
      "torch.Size([361903])\n",
      "epoch : 62\n",
      "Epoch 62, Loss: 0.6790, Accuracy: 0.7079\n",
      "torch.Size([361903])\n",
      "epoch : 63\n",
      "Epoch 63, Loss: 0.6793, Accuracy: 0.6684\n",
      "torch.Size([361903])\n",
      "epoch : 64\n",
      "Epoch 64, Loss: 0.6815, Accuracy: 0.7086\n",
      "torch.Size([361903])\n",
      "epoch : 65\n",
      "Epoch 65, Loss: 0.6797, Accuracy: 0.7096\n",
      "torch.Size([361903])\n",
      "epoch : 66\n",
      "Epoch 66, Loss: 0.6890, Accuracy: 0.7085\n",
      "torch.Size([361903])\n",
      "epoch : 67\n",
      "Epoch 67, Loss: 0.6787, Accuracy: 0.6684\n",
      "torch.Size([361903])\n",
      "epoch : 68\n",
      "Epoch 68, Loss: 0.6834, Accuracy: 0.7088\n",
      "torch.Size([361903])\n",
      "epoch : 69\n",
      "Epoch 69, Loss: 0.6845, Accuracy: 0.7096\n",
      "torch.Size([361903])\n",
      "epoch : 70\n",
      "Epoch 70, Loss: 0.6821, Accuracy: 0.7088\n",
      "torch.Size([361903])\n",
      "epoch : 71\n",
      "Epoch 71, Loss: 0.6824, Accuracy: 0.7095\n",
      "torch.Size([361903])\n",
      "epoch : 72\n",
      "Epoch 72, Loss: 0.6821, Accuracy: 0.7088\n",
      "torch.Size([361903])\n",
      "epoch : 73\n",
      "Epoch 73, Loss: 0.6742, Accuracy: 0.6685\n",
      "torch.Size([361903])\n",
      "epoch : 74\n",
      "Epoch 74, Loss: 0.6834, Accuracy: 0.7085\n",
      "torch.Size([361903])\n",
      "epoch : 75\n",
      "Epoch 75, Loss: 0.6701, Accuracy: 0.7521\n",
      "torch.Size([361903])\n",
      "epoch : 76\n",
      "Epoch 76, Loss: 0.6829, Accuracy: 0.7304\n",
      "torch.Size([361903])\n",
      "epoch : 77\n",
      "Epoch 77, Loss: 0.6773, Accuracy: 0.7096\n",
      "torch.Size([361903])\n",
      "epoch : 78\n",
      "Epoch 78, Loss: 0.6772, Accuracy: 0.7087\n",
      "torch.Size([361903])\n",
      "epoch : 79\n",
      "Epoch 79, Loss: 0.6727, Accuracy: 0.6326\n",
      "torch.Size([361903])\n",
      "epoch : 80\n",
      "Epoch 80, Loss: 0.6801, Accuracy: 0.7096\n",
      "torch.Size([361903])\n",
      "epoch : 81\n",
      "Epoch 81, Loss: 0.6808, Accuracy: 0.7098\n",
      "torch.Size([361903])\n",
      "epoch : 82\n",
      "Epoch 82, Loss: 0.6659, Accuracy: 0.7088\n",
      "torch.Size([361903])\n",
      "epoch : 83\n",
      "Epoch 83, Loss: 0.6656, Accuracy: 0.8290\n",
      "torch.Size([361903])\n",
      "epoch : 84\n",
      "Epoch 84, Loss: 0.6727, Accuracy: 0.7310\n",
      "torch.Size([361903])\n",
      "epoch : 85\n",
      "Epoch 85, Loss: 0.6692, Accuracy: 0.7098\n",
      "torch.Size([361903])\n",
      "epoch : 86\n",
      "Epoch 86, Loss: 0.6646, Accuracy: 0.7088\n",
      "torch.Size([361903])\n",
      "epoch : 87\n",
      "Epoch 87, Loss: 0.6774, Accuracy: 0.7307\n",
      "torch.Size([361903])\n",
      "epoch : 88\n",
      "Epoch 88, Loss: 0.6829, Accuracy: 0.7522\n",
      "torch.Size([361903])\n",
      "epoch : 89\n",
      "Epoch 89, Loss: 0.6805, Accuracy: 0.7307\n",
      "torch.Size([361903])\n",
      "epoch : 90\n",
      "Epoch 90, Loss: 0.6731, Accuracy: 0.6327\n",
      "torch.Size([361903])\n",
      "epoch : 91\n",
      "Epoch 91, Loss: 0.6772, Accuracy: 0.7318\n",
      "torch.Size([361903])\n",
      "epoch : 92\n",
      "Epoch 92, Loss: 0.6794, Accuracy: 0.7089\n",
      "torch.Size([361903])\n",
      "epoch : 93\n",
      "Epoch 93, Loss: 0.6737, Accuracy: 0.7316\n",
      "torch.Size([361903])\n",
      "epoch : 94\n",
      "Epoch 94, Loss: 0.6698, Accuracy: 0.7308\n",
      "torch.Size([361903])\n",
      "epoch : 95\n",
      "Epoch 95, Loss: 0.6723, Accuracy: 0.7316\n",
      "torch.Size([361903])\n",
      "epoch : 96\n",
      "Epoch 96, Loss: 0.6743, Accuracy: 0.7306\n",
      "torch.Size([361903])\n",
      "epoch : 97\n",
      "Epoch 97, Loss: 0.6632, Accuracy: 0.8283\n",
      "torch.Size([361903])\n",
      "epoch : 98\n",
      "Epoch 98, Loss: 0.6716, Accuracy: 0.7516\n",
      "torch.Size([361903])\n",
      "epoch : 99\n",
      "Epoch 99, Loss: 0.6645, Accuracy: 0.6616\n",
      "torch.Size([361903])\n",
      "epoch : 100\n",
      "Epoch 100, Loss: 0.6693, Accuracy: 0.7289\n",
      "torch.Size([361903])\n",
      "epoch : 101\n",
      "Epoch 101, Loss: 0.6702, Accuracy: 0.8296\n",
      "torch.Size([361903])\n",
      "epoch : 102\n",
      "Epoch 102, Loss: 0.6820, Accuracy: 0.7091\n",
      "torch.Size([361903])\n",
      "epoch : 103\n",
      "Epoch 103, Loss: 0.6698, Accuracy: 0.7314\n",
      "torch.Size([361903])\n",
      "epoch : 104\n",
      "Epoch 104, Loss: 0.6661, Accuracy: 0.7305\n",
      "torch.Size([361903])\n",
      "epoch : 105\n",
      "Epoch 105, Loss: 0.6775, Accuracy: 0.7598\n",
      "torch.Size([361903])\n",
      "epoch : 106\n",
      "Epoch 106, Loss: 0.6668, Accuracy: 0.8295\n",
      "torch.Size([361903])\n",
      "epoch : 107\n",
      "Epoch 107, Loss: 0.6701, Accuracy: 0.7098\n",
      "torch.Size([361903])\n",
      "epoch : 108\n",
      "Epoch 108, Loss: 0.6704, Accuracy: 0.7093\n",
      "torch.Size([361903])\n",
      "epoch : 109\n",
      "Epoch 109, Loss: 0.6658, Accuracy: 0.8291\n",
      "torch.Size([361903])\n",
      "epoch : 110\n",
      "Epoch 110, Loss: 0.6658, Accuracy: 0.7318\n",
      "torch.Size([361903])\n",
      "epoch : 111\n",
      "Epoch 111, Loss: 0.6694, Accuracy: 0.7090\n",
      "torch.Size([361903])\n",
      "epoch : 112\n",
      "Epoch 112, Loss: 0.6690, Accuracy: 0.8294\n",
      "torch.Size([361903])\n",
      "epoch : 113\n",
      "Epoch 113, Loss: 0.6680, Accuracy: 0.7305\n",
      "torch.Size([361903])\n",
      "epoch : 114\n",
      "Epoch 114, Loss: 0.6711, Accuracy: 0.6904\n",
      "torch.Size([361903])\n",
      "epoch : 115\n",
      "Epoch 115, Loss: 0.6680, Accuracy: 0.7317\n",
      "torch.Size([361903])\n",
      "epoch : 116\n",
      "Epoch 116, Loss: 0.6693, Accuracy: 0.7100\n",
      "torch.Size([361903])\n",
      "epoch : 117\n",
      "Epoch 117, Loss: 0.6703, Accuracy: 0.7101\n",
      "torch.Size([361903])\n",
      "epoch : 118\n",
      "Epoch 118, Loss: 0.6585, Accuracy: 0.7088\n",
      "torch.Size([361903])\n",
      "epoch : 119\n",
      "Epoch 119, Loss: 0.6611, Accuracy: 0.8296\n",
      "torch.Size([361903])\n",
      "epoch : 120\n",
      "Epoch 120, Loss: 0.6681, Accuracy: 0.7309\n",
      "torch.Size([361903])\n",
      "epoch : 121\n",
      "Epoch 121, Loss: 0.6660, Accuracy: 0.7321\n",
      "torch.Size([361903])\n",
      "epoch : 122\n",
      "Epoch 122, Loss: 0.6662, Accuracy: 0.7306\n",
      "torch.Size([361903])\n",
      "epoch : 123\n",
      "Epoch 123, Loss: 0.6661, Accuracy: 0.7320\n",
      "torch.Size([361903])\n",
      "epoch : 124\n",
      "Epoch 124, Loss: 0.6697, Accuracy: 0.7593\n",
      "torch.Size([361903])\n",
      "epoch : 125\n",
      "Epoch 125, Loss: 0.6664, Accuracy: 0.7090\n",
      "torch.Size([361903])\n",
      "epoch : 126\n",
      "Epoch 126, Loss: 0.6688, Accuracy: 0.7525\n",
      "torch.Size([361903])\n",
      "epoch : 127\n",
      "Epoch 127, Loss: 0.6695, Accuracy: 0.8296\n",
      "torch.Size([361903])\n",
      "epoch : 128\n",
      "Epoch 128, Loss: 0.6737, Accuracy: 0.7094\n",
      "torch.Size([361903])\n",
      "epoch : 129\n",
      "Epoch 129, Loss: 0.6599, Accuracy: 0.7098\n",
      "torch.Size([361903])\n",
      "epoch : 130\n",
      "Epoch 130, Loss: 0.6653, Accuracy: 0.8300\n",
      "torch.Size([361903])\n",
      "epoch : 131\n",
      "Epoch 131, Loss: 0.6700, Accuracy: 0.7312\n",
      "torch.Size([361903])\n",
      "epoch : 132\n",
      "Epoch 132, Loss: 0.6645, Accuracy: 0.7318\n",
      "torch.Size([361903])\n",
      "epoch : 133\n",
      "Epoch 133, Loss: 0.6523, Accuracy: 0.7507\n",
      "torch.Size([361903])\n",
      "epoch : 134\n",
      "Epoch 134, Loss: 0.6620, Accuracy: 0.7283\n",
      "torch.Size([361903])\n",
      "epoch : 135\n",
      "Epoch 135, Loss: 0.6646, Accuracy: 0.7522\n",
      "torch.Size([361903])\n",
      "epoch : 136\n",
      "Epoch 136, Loss: 0.6636, Accuracy: 0.7322\n",
      "torch.Size([361903])\n",
      "epoch : 137\n",
      "Epoch 137, Loss: 0.6732, Accuracy: 0.7095\n",
      "torch.Size([361903])\n",
      "epoch : 138\n",
      "Epoch 138, Loss: 0.6646, Accuracy: 0.8295\n",
      "torch.Size([361903])\n",
      "epoch : 139\n",
      "Epoch 139, Loss: 0.6637, Accuracy: 0.7102\n",
      "torch.Size([361903])\n",
      "epoch : 140\n",
      "Epoch 140, Loss: 0.6658, Accuracy: 0.7315\n",
      "torch.Size([361903])\n",
      "epoch : 141\n",
      "Epoch 141, Loss: 0.6622, Accuracy: 0.7321\n",
      "torch.Size([361903])\n",
      "epoch : 142\n",
      "Epoch 142, Loss: 0.6628, Accuracy: 0.7313\n",
      "torch.Size([361903])\n",
      "epoch : 143\n",
      "Epoch 143, Loss: 0.6613, Accuracy: 0.6908\n",
      "torch.Size([361903])\n",
      "epoch : 144\n",
      "Epoch 144, Loss: 0.6645, Accuracy: 0.7313\n",
      "torch.Size([361903])\n",
      "epoch : 145\n",
      "Epoch 145, Loss: 0.6610, Accuracy: 0.7528\n",
      "torch.Size([361903])\n",
      "epoch : 146\n",
      "Epoch 146, Loss: 0.6610, Accuracy: 0.7314\n",
      "torch.Size([361903])\n",
      "epoch : 147\n",
      "Epoch 147, Loss: 0.6692, Accuracy: 0.6538\n",
      "torch.Size([361903])\n",
      "epoch : 148\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "G_pyg.edge_label = G_pyg.edge_label.to(device)\n",
    "G_pyg.edge_attr = G_pyg.edge_attr.to(device)\n",
    "\n",
    "def generate_edge_based_batches_with_node_expansion(graph, batch_size, min_nodes):\n",
    "    num_edges = graph.edge_index.size(1)  # Get total number of edges\n",
    "    edge_indices = th.arange(num_edges)   # Create list of edge indices\n",
    "    num_edges_processed = 0\n",
    "    \n",
    "    while num_edges_processed < num_edges:\n",
    "        # Select a batch of edges\n",
    "        batch_edge_indices = edge_indices[num_edges_processed : min(num_edges_processed + batch_size, num_edges)]\n",
    "        edge_index = graph.edge_index[:, batch_edge_indices]\n",
    "        \n",
    "        # Update the number of edges processed\n",
    "        num_edges_processed += batch_size\n",
    "        \n",
    "        # Get the unique nodes associated with these edges\n",
    "        batch_nodes = th.cat([edge_index[0], edge_index[1]]).unique()\n",
    "\n",
    "        # Check if the batch has enough unique nodes\n",
    "        while batch_nodes.size(0) < min_nodes:\n",
    "            # Sample additional neighboring nodes to ensure diversity\n",
    "            additional_edges = int(batch_size / 8)  # Ensure additional_edges is an integer\n",
    "            batch_edge_indices = th.cat([batch_edge_indices, edge_indices[num_edges_processed : min(num_edges_processed + additional_edges, num_edges)]])\n",
    "            edge_index = graph.edge_index[:, batch_edge_indices]\n",
    "            batch_nodes = th.cat([edge_index[0], edge_index[1]]).unique()\n",
    "            num_edges_processed += additional_edges\n",
    "\n",
    "            # Avoid potential infinite loops by breaking if no more edges can be added\n",
    "            if num_edges_processed >= num_edges:\n",
    "                break\n",
    "\n",
    "        # Create subgraph from the selected nodes and edges\n",
    "        edge_index, _, edge_mask = subgraph(batch_nodes, graph.edge_index, relabel_nodes=True, return_edge_mask=True)\n",
    "\n",
    "        # Use edge_mask to select edge attributes and labels\n",
    "        edge_attr = graph.edge_attr[edge_mask]\n",
    "        edge_label = graph.edge_label[edge_mask]\n",
    "\n",
    "        yield batch_nodes, edge_index, edge_attr, edge_label\n",
    "\n",
    "batch_size = 64\n",
    "for epoch in range(200):\n",
    "    print(f'epoch : {epoch}')\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    try:\n",
    "        for batch_idx, (batch_nodes, edge_index, edge_attr, edge_label) in enumerate(generate_edge_based_batches_with_node_expansion(G_pyg, batch_size, 20)):\n",
    "            # print(f\"Processing epoch {epoch}, batch {batch_idx} with {batch_nodes.size(0)} nodes and {edge_index.size(1)} edges\")\n",
    "            batch = Data(x=G_pyg.x[batch_nodes], edge_index=edge_index, edge_attr=edge_attr, edge_label=edge_label)\n",
    "            \n",
    "            if batch.edge_index.size(1) == 0 or batch.edge_label.size(0) == 0:\n",
    "                print(f\"Warning: Empty batch at batch {batch_idx}\")\n",
    "                continue\n",
    "                \n",
    "            if batch is None or batch.num_nodes == 0:\n",
    "                print(f\"Warning: Empty batch at Batch {batch_idx}\")\n",
    "                continue \n",
    "    \n",
    "            if th.isnan(batch.x).any() or th.isinf(batch.x).any() or th.isnan(batch.edge_attr).any() or th.isinf(batch.edge_attr).any():\n",
    "                print(f\"Warning: batch x and edge_attr contains NaN or Inf at Batch {batch_idx}\")\n",
    "                continue \n",
    "                \n",
    "            try:\n",
    "                batch = batch.to(device)\n",
    "            except Exception as batch_error:\n",
    "                print(f\"Error moving batch to device at Batch {batch_idx}: {batch_error}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                out = model(batch)\n",
    "    \n",
    "                if th.isnan(out).any() or th.isinf(out).any():\n",
    "                    print(f\"Warning: out contains NaN or Inf at Batch {batch_idx}\")\n",
    "                    continue \n",
    "                all_preds.append(out.argmax(dim=1))\n",
    "                all_labels.append(batch.edge_label)\n",
    "    \n",
    "                loss = criterion(out, batch.edge_label)\n",
    "                if th.isnan(loss):\n",
    "                    print(f\"loss: {loss}\")\n",
    "                    print(f\"out: {out}\")\n",
    "                    print(f\"edge_labels: {batch.edge_label}\")\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            except Exception as forward_error:\n",
    "                print(f\"Error during forward/backward pass at Epoch {epoch}, Batch {batch_idx}: {forward_error}\")\n",
    "                continue\n",
    "        \n",
    "        all_preds = th.cat(all_preds)\n",
    "        all_labels = th.cat(all_labels)\n",
    "        \n",
    "        epoch_accuracy = compute_accuracy(all_preds, all_labels)\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        print(all_labels.shape)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred at epoch {epoch}, batch {batch_idx}: {str(e)}\")\n",
    "print(\"Training is over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63f9fc-02a2-4e16-94c6-bef7bf80ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.save(model.state_dict(), f\"./Weights/GNN_model_weights_{DATASET_NAME}_subset_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8db72-643a-476a-8c99-5c711869c1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg_test: 217264\n",
      "Number of node in G_pyg_test: 132746\n",
      "Shape of node in G_pyg_test: torch.Size([132746, 40])\n",
      "Shape of edge attr in G_pyg_test: torch.Size([217264, 40])\n",
      "Shape of edge label in G_pyg_test: torch.Size([217264])\n",
      "Shape of edge class in G_pyg_test: torch.Size([217264])\n"
     ]
    }
   ],
   "source": [
    "X_test['h'] = X_test[ cols_to_norm ].values.tolist()\n",
    "\n",
    "G_nx_test = nx.from_pandas_edgelist(X_test, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "\n",
    "G_pyg_test = from_networkx(G_nx_test)\n",
    "\n",
    "test_num_nodes = G_pyg_test.num_nodes\n",
    "test_num_edges = G_pyg_test.num_edges\n",
    "\n",
    "G_pyg_test.x = th.ones(test_num_nodes, len(X_test['h'].iloc[0]))\n",
    "\n",
    "test_edge_attr_list = []\n",
    "test_edge_label_list = []\n",
    "\n",
    "for u, v, key, data in G_nx_test.edges(keys=True, data=True):\n",
    "    test_edge_attr_list.append(data['h']) \n",
    "    test_edge_label_list.append(data[label_col]) \n",
    "\n",
    "G_pyg_test.edge_attr = th.tensor(test_edge_attr_list, dtype=th.float32)\n",
    "G_pyg_test.edge_label = th.tensor(test_edge_label_list, dtype=th.long)\n",
    "\n",
    "print(\"Number of edges in G_pyg_test:\", G_pyg_test.num_edges)\n",
    "print(\"Number of node in G_pyg_test:\", G_pyg_test.num_nodes)\n",
    "print(\"Shape of node in G_pyg_test:\", G_pyg_test.x.shape)\n",
    "print(\"Shape of edge attr in G_pyg_test:\", G_pyg_test.edge_attr.shape)\n",
    "print(\"Shape of edge label in G_pyg_test:\", G_pyg_test.edge_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f271a-612b-4cd6-a85a-e4236dec9d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference done\n",
      "Test Accuracy: 0.4756\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "new_model_2 = EGraphSAGE(node_in_channels=G_pyg_test.num_node_features, \n",
    "                       edge_in_channels=G_pyg_test.num_edge_features,\n",
    "                       hidden_channels=128, \n",
    "                       out_channels=num_classes).to(device)\n",
    "\n",
    "new_model_2.load_state_dict(th.load(f\"./Weights/GNN_model_weights_{DATASET_NAME}_subset_2.pth\", weights_only=True))\n",
    "\n",
    "new_model_2.eval()\n",
    "\n",
    "all_test_preds = []\n",
    "all_test_labels = []\n",
    "all_test_classes = []\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "print(\"inference start\")\n",
    "with th.no_grad():\n",
    "    for batch_idx, (batch_nodes, edge_index, edge_attr, edge_label) in enumerate(generate_edge_based_batches_with_node_expansion(G_pyg_test, batch_size, 20)):\n",
    "        # print(f\"Processing batch {batch_idx} with {batch_nodes.size(0)} nodes and {edge_index.size(1)} edges\")\n",
    "        batch = Data(x=G_pyg_test.x[batch_nodes], edge_index=edge_index, edge_attr=edge_attr, edge_label=edge_label)\n",
    "        \n",
    "        if batch.edge_index.size(1) == 0 or batch.edge_label.size(0) == 0:\n",
    "            print(f\"Warning: Empty batch at batch {batch_idx}\")\n",
    "            continue\n",
    "            \n",
    "        if batch is None or batch.num_nodes == 0:\n",
    "            print(f\"Warning: Empty batch at Batch {batch_idx}\")\n",
    "            continue\n",
    "\n",
    "        if th.isnan(batch.x).any() or th.isinf(batch.x).any() or th.isnan(batch.edge_attr).any() or th.isinf(batch.edge_attr).any():\n",
    "            print(f\"Warning: batch x and edge_attr contains NaN or Inf at Batch {batch_idx}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            batch = batch.to(device)\n",
    "        except Exception as batch_error:\n",
    "            print(f\"Error moving batch to device at Batch {batch_idx}: {batch_error}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            out = new_model_2(batch)\n",
    "\n",
    "            if th.isnan(out).any() or th.isinf(out).any():\n",
    "                print(f\"Warning: out contains NaN or Inf at Batch {batch_idx}\")\n",
    "                continue \n",
    "            \n",
    "            all_test_preds.append(out.argmax(dim=1))\n",
    "            all_test_labels.append(edge_label)\n",
    "\n",
    "        except Exception as forward_error:\n",
    "            print(f\"Error during forward/backward pass at Batch {batch_idx}: {forward_error}\")\n",
    "            continue\n",
    "\n",
    "print(\"inference done\")\n",
    "all_test_preds = th.cat(all_test_preds).to(device)\n",
    "all_test_labels = th.cat(all_test_labels).to(device)\n",
    "\n",
    "test_accuracy = compute_accuracy(all_test_preds, all_test_labels)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bf70a-3dbc-48d7-9df0-5ff66e99ac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  206     0     0     0    58   495     0     0   617     1     1]\n",
      " [    3    11     0     0    21   388     0     0   527    13     7]\n",
      " [    2     0     0     0     2   199     0     0    17     0     0]\n",
      " [   88    21     0    16   891  3098     0     9  4620   102    55]\n",
      " [  362    40     8    11  8632  4582     0    12  6427   366   467]\n",
      " [   21     0    82     0   448  8104     0     7  1377   157    17]\n",
      " [   41     1     0     9   861 62296     0 27887   751    80    82]\n",
      " [  299     3    12     2   123  1037     0 89859   110   110     0]\n",
      " [   14     1     3     2   284  1349     0     0  3671   507   229]\n",
      " [    0     0     1     0     1   173     0     0   201   228     0]\n",
      " [    4     0     0     0    30    12     0     0     1     1    22]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.1981    0.1495    0.1704      1378\n",
      "      Backdoor     0.1429    0.0113    0.0210       970\n",
      "     Backdoors     0.0000    0.0000    0.0000       220\n",
      "           DoS     0.4000    0.0018    0.0036      8900\n",
      "      Exploits     0.7605    0.4129    0.5352     20907\n",
      "       Fuzzers     0.0992    0.7935    0.1763     10213\n",
      "       Generic     0.0000    0.0000    0.0000     92008\n",
      "        Normal     0.7630    0.9815    0.8585     91555\n",
      "Reconnaissance     0.2004    0.6058    0.3012      6060\n",
      "     Shellcode     0.1457    0.3775    0.2102       604\n",
      "         Worms     0.0250    0.3143    0.0463        70\n",
      "\n",
      "      accuracy                         0.4756    232885\n",
      "     macro avg     0.2486    0.3316    0.2112    232885\n",
      "  weighted avg     0.3952    0.4756    0.4029    232885\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred_labels = all_test_preds.cpu()\n",
    "all_test_labels = all_test_labels.cpu()\n",
    "\n",
    "cm = confusion_matrix(all_test_labels, pred_labels)\n",
    "print(cm)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(all_test_labels, pred_labels, target_names=class_map, digits=4)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
