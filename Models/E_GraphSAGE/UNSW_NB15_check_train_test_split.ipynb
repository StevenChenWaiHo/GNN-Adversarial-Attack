{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb9e1b2-1c5f-49e3-82b4-3bfe52cabad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "=====Experiment=====\n",
    "Dataset: UNSW-NB15 dataset\n",
    "\n",
    "Check Train Test Dataset Coverage for\n",
    "- 1. Random Split\n",
    "- 2. Stratified Split\n",
    "'''\n",
    "\n",
    "from torch_geometric.utils import from_networkx, add_self_loops, degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "# import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Datasets.UNSW_NB15.UNSW_NB15_config import UNSW_NB15_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9ef09a-d405-43b8-971e-fe9e6a592c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "Normal            2218764\n",
      "Generic            215481\n",
      "Exploits            44525\n",
      "Fuzzers             24246\n",
      "DoS                 16353\n",
      "Reconnaissance      13987\n",
      "Analysis             2677\n",
      "Backdoors            2329\n",
      "Shellcode            1511\n",
      "Worms                 174\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    2218764\n",
      "1     321283\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "csv_file_name = \"all_raw\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n",
    "\n",
    "DATASET_NAME = \"UNSW_NB15\"\n",
    "EXPERIMENT_NAME = \"check_train_test_split\"\n",
    "\n",
    "SOURCE_FILE_ID_COL_NAME = UNSW_NB15_Config.SOURCE_FILE_ID_COL_NAME\n",
    "\n",
    "SOURCE_IP_COL_NAME = UNSW_NB15_Config.SOURCE_IP_COL_NAME\n",
    "DESTINATION_IP_COL_NAME = UNSW_NB15_Config.DESTINATION_IP_COL_NAME\n",
    "SOURCE_PORT_COL_NAME = UNSW_NB15_Config.SOURCE_PORT_COL_NAME\n",
    "DESTINATION_PORT_COL_NAME = UNSW_NB15_Config.DESTINATION_PORT_COL_NAME\n",
    "\n",
    "ATTACK_CLASS_COL_NAME = UNSW_NB15_Config.ATTACK_CLASS_COL_NAME\n",
    "IS_ATTACK_COL_NAME = UNSW_NB15_Config.IS_ATTACK_COL_NAME\n",
    "\n",
    "BENIGN_CLASS_NAME = UNSW_NB15_Config.BENIGN_CLASS_NAME\n",
    "\n",
    "TIME_COLS = UNSW_NB15_Config.TIME_COL_NAMES\n",
    "\n",
    "print(data[ATTACK_CLASS_COL_NAME].value_counts())\n",
    "print(data[IS_ATTACK_COL_NAME].value_counts())\n",
    "\n",
    "MULTICLASS = True\n",
    "\n",
    "if MULTICLASS:\n",
    "    label_col = ATTACK_CLASS_COL_NAME\n",
    "    data.drop(columns=[IS_ATTACK_COL_NAME], inplace=True)\n",
    "else:\n",
    "    label_col = IS_ATTACK_COL_NAME\n",
    "    data.drop(columns=[ATTACK_CLASS_COL_NAME], inplace=True)\n",
    "\n",
    "\n",
    "saves_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, EXPERIMENT_NAME)\n",
    "\n",
    "checkpoint_path = os.path.join(saves_path, f\"checkpoints_{csv_file_name}.pth\")\n",
    "best_model_path = os.path.join(saves_path, f\"best_model_{csv_file_name}.pth\")\n",
    "\n",
    "os.makedirs(saves_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449a1af1-1d3d-4179-9628-7c2ec551ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['srcip', 'sport', 'dstip', 'dsport', 'state', 'dur', 'sbytes', 'dbytes',\n",
      "       'sttl', 'dttl', 'sloss', 'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts',\n",
      "       'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth',\n",
      "       'res_bdy_len', 'Sjit', 'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt',\n",
      "       'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl',\n",
      "       'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src',\n",
      "       'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm',\n",
      "       'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat', 'source_file_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data.drop(columns=UNSW_NB15_Config.DROP_COLS,inplace=True)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2c690c-86a4-49f7-aa9c-58f94529547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME].apply(str)\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME].apply(str)\n",
    "\n",
    "# # Combine Port and IP\n",
    "data[SOURCE_PORT_COL_NAME] = data[SOURCE_PORT_COL_NAME].apply(str)\n",
    "data[DESTINATION_PORT_COL_NAME] = data[DESTINATION_PORT_COL_NAME].apply(str)\n",
    "\n",
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME] + ':' + data[SOURCE_PORT_COL_NAME]\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME] + ':' + data[DESTINATION_PORT_COL_NAME]\n",
    "data.drop(columns=[SOURCE_PORT_COL_NAME,DESTINATION_PORT_COL_NAME],inplace=True)\n",
    "\n",
    "data = pd.get_dummies(data, columns = UNSW_NB15_Config.CATEGORICAL_COLS) # One Hot Encoding for categorical data\n",
    "converted_categorical_cols = [col for col in data.columns if col.startswith(tuple(UNSW_NB15_Config.CATEGORICAL_COLS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5651ef5b-0a9d-4641-aad7-5738092c46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                          srcip                    dstip        dur  sbytes  \\\n",
      "0              10.40.182.1_0:0            224.0.0.5_0:0  50.004337     384   \n",
      "1               10.40.85.1_0:0            224.0.0.5_0:0  50.004341     384   \n",
      "2              10.40.182.1_0:0            224.0.0.5_0:0  50.004337     384   \n",
      "3               10.40.85.1_0:0            224.0.0.5_0:0  50.004341     384   \n",
      "4        192.168.241.243_0:259  192.168.241.243_0:49320   0.000000    1780   \n",
      "...                        ...                      ...        ...     ...   \n",
      "2540042      59.166.0.0_3:2111       149.171.126.5_3:53   0.001035     146   \n",
      "2540043     59.166.0.5_3:49044    149.171.126.3_3:30639   0.220630     424   \n",
      "2540044     59.166.0.6_3:37717    149.171.126.7_3:35667   0.031576    2646   \n",
      "2540045      59.166.0.2_3:1768    149.171.126.7_3:64122   0.096835    4862   \n",
      "2540046      59.166.0.9_3:7045       149.171.126.7_3:25   0.201886   37552   \n",
      "\n",
      "         dbytes  sttl  dttl  sloss  dloss         Sload  ...  state_INT  \\\n",
      "0             0     1     0      0      0  5.119556e+01  ...       True   \n",
      "1             0     1     0      0      0  5.119556e+01  ...       True   \n",
      "2             0     1     0      0      0  5.119556e+01  ...       True   \n",
      "3             0     1     0      0      0  5.119556e+01  ...       True   \n",
      "4             0    64     0      0      0  1.964095e+02  ...      False   \n",
      "...         ...   ...   ...    ...    ...           ...  ...        ...   \n",
      "2540042     178    31    29      0      0  5.642512e+05  ...      False   \n",
      "2540043    8824    31    29      1      4  1.345239e+04  ...      False   \n",
      "2540044   25564    31    29      7     15  6.544211e+05  ...      False   \n",
      "2540045   82782    31    29      7     36  3.969639e+05  ...      False   \n",
      "2540046    3380    31    29     18      8  1.459438e+06  ...      False   \n",
      "\n",
      "         state_MAS  state_PAR  state_REQ  state_RST  state_TST  state_TXD  \\\n",
      "0            False      False      False      False      False      False   \n",
      "1            False      False      False      False      False      False   \n",
      "2            False      False      False      False      False      False   \n",
      "3            False      False      False      False      False      False   \n",
      "4            False      False      False      False      False      False   \n",
      "...            ...        ...        ...        ...        ...        ...   \n",
      "2540042      False      False      False      False      False      False   \n",
      "2540043      False      False      False      False      False      False   \n",
      "2540044      False      False      False      False      False      False   \n",
      "2540045      False      False      False      False      False      False   \n",
      "2540046      False      False      False      False      False      False   \n",
      "\n",
      "         state_URH  state_URN  state_no  \n",
      "0            False      False     False  \n",
      "1            False      False     False  \n",
      "2            False      False     False  \n",
      "3            False      False     False  \n",
      "4             True      False     False  \n",
      "...            ...        ...       ...  \n",
      "2540042      False      False     False  \n",
      "2540043      False      False     False  \n",
      "2540044      False      False     False  \n",
      "2540045      False      False     False  \n",
      "2540046      False      False     False  \n",
      "\n",
      "[2540047 rows x 60 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2d96115-31f9-48cb-b3e6-7853d2d253cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                          srcip                    dstip        dur  sbytes  \\\n",
      "0              10.40.182.1_0:0            224.0.0.5_0:0  50.004337     384   \n",
      "1               10.40.85.1_0:0            224.0.0.5_0:0  50.004341     384   \n",
      "2              10.40.182.1_0:0            224.0.0.5_0:0  50.004337     384   \n",
      "3               10.40.85.1_0:0            224.0.0.5_0:0  50.004341     384   \n",
      "4        192.168.241.243_0:259  192.168.241.243_0:49320   0.000000    1780   \n",
      "...                        ...                      ...        ...     ...   \n",
      "2540042      59.166.0.0_3:2111       149.171.126.5_3:53   0.001035     146   \n",
      "2540043     59.166.0.5_3:49044    149.171.126.3_3:30639   0.220630     424   \n",
      "2540044     59.166.0.6_3:37717    149.171.126.7_3:35667   0.031576    2646   \n",
      "2540045      59.166.0.2_3:1768    149.171.126.7_3:64122   0.096835    4862   \n",
      "2540046      59.166.0.9_3:7045       149.171.126.7_3:25   0.201886   37552   \n",
      "\n",
      "         dbytes  sttl  dttl  sloss  dloss         Sload  ...  state_INT  \\\n",
      "0             0     1     0      0      0  5.119556e+01  ...       True   \n",
      "1             0     1     0      0      0  5.119556e+01  ...       True   \n",
      "2             0     1     0      0      0  5.119556e+01  ...       True   \n",
      "3             0     1     0      0      0  5.119556e+01  ...       True   \n",
      "4             0    64     0      0      0  1.964095e+02  ...      False   \n",
      "...         ...   ...   ...    ...    ...           ...  ...        ...   \n",
      "2540042     178    31    29      0      0  5.642512e+05  ...      False   \n",
      "2540043    8824    31    29      1      4  1.345239e+04  ...      False   \n",
      "2540044   25564    31    29      7     15  6.544211e+05  ...      False   \n",
      "2540045   82782    31    29      7     36  3.969639e+05  ...      False   \n",
      "2540046    3380    31    29     18      8  1.459438e+06  ...      False   \n",
      "\n",
      "         state_MAS  state_PAR  state_REQ  state_RST  state_TST  state_TXD  \\\n",
      "0            False      False      False      False      False      False   \n",
      "1            False      False      False      False      False      False   \n",
      "2            False      False      False      False      False      False   \n",
      "3            False      False      False      False      False      False   \n",
      "4            False      False      False      False      False      False   \n",
      "...            ...        ...        ...        ...        ...        ...   \n",
      "2540042      False      False      False      False      False      False   \n",
      "2540043      False      False      False      False      False      False   \n",
      "2540044      False      False      False      False      False      False   \n",
      "2540045      False      False      False      False      False      False   \n",
      "2540046      False      False      False      False      False      False   \n",
      "\n",
      "         state_URH  state_URN  state_no  \n",
      "0            False      False     False  \n",
      "1            False      False     False  \n",
      "2            False      False     False  \n",
      "3            False      False     False  \n",
      "4             True      False     False  \n",
      "...            ...        ...       ...  \n",
      "2540042      False      False     False  \n",
      "2540043      False      False     False  \n",
      "2540044      False      False     False  \n",
      "2540045      False      False     False  \n",
      "2540046      False      False     False  \n",
      "\n",
      "[2540047 rows x 60 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "data.drop(columns=['index'],inplace=True)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1df5c4c-70a2-4566-ae5e-ee3dcc6037a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                dur        sbytes        dbytes          sttl          dttl  \\\n",
      "count  2.540047e+06  2.540047e+06  2.540047e+06  2.540047e+06  2.540047e+06   \n",
      "mean   6.587916e-01  4.339600e+03  3.642759e+04  6.278197e+01  3.076681e+01   \n",
      "std    1.392493e+01  5.640599e+04  1.610960e+05  7.462277e+01  4.285089e+01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    1.037000e-03  2.000000e+02  1.780000e+02  3.100000e+01  2.900000e+01   \n",
      "50%    1.586100e-02  1.470000e+03  1.820000e+03  3.100000e+01  2.900000e+01   \n",
      "75%    2.145545e-01  3.182000e+03  1.489400e+04  3.100000e+01  2.900000e+01   \n",
      "max    8.786638e+03  1.435577e+07  1.465753e+07  2.550000e+02  2.540000e+02   \n",
      "\n",
      "              sloss         dloss         Sload         Dload         Spkts  \\\n",
      "count  2.540047e+06  2.540047e+06  2.540047e+06  2.540047e+06  2.540047e+06   \n",
      "mean   5.163921e+00  1.632944e+01  3.695645e+07  2.450861e+06  3.328884e+01   \n",
      "std    2.251707e+01  5.659474e+01  1.186043e+08  4.224863e+06  7.628388e+01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00  0.000000e+00  1.353963e+05  1.191594e+04  2.000000e+00   \n",
      "50%    3.000000e+00  4.000000e+00  5.893038e+05  5.893179e+05  1.200000e+01   \n",
      "75%    7.000000e+00  1.400000e+01  2.039923e+06  2.925974e+06  4.400000e+01   \n",
      "max    5.319000e+03  5.507000e+03  5.988000e+09  1.287619e+08  1.064600e+04   \n",
      "\n",
      "       ...  ct_flw_http_mthd  is_ftp_login    ct_ftp_cmd    ct_srv_src  \\\n",
      "count  ...      2.540047e+06  2.540047e+06  2.540047e+06  2.540047e+06   \n",
      "mean   ...      1.100779e-01  1.735125e-02  9.320300e-03  9.206988e+00   \n",
      "std    ...      5.564195e-01  1.334571e-01  9.997025e-02  1.083676e+01   \n",
      "min    ...      0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00   \n",
      "25%    ...      0.000000e+00  0.000000e+00  0.000000e+00  2.000000e+00   \n",
      "50%    ...      0.000000e+00  0.000000e+00  0.000000e+00  5.000000e+00   \n",
      "75%    ...      0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+01   \n",
      "max    ...      3.600000e+01  4.000000e+00  4.000000e+00  6.700000e+01   \n",
      "\n",
      "         ct_srv_dst    ct_dst_ltm    ct_src_ltm  ct_src_dport_ltm  \\\n",
      "count  2.540047e+06  2.540047e+06  2.540047e+06      2.540047e+06   \n",
      "mean   8.988958e+00  6.439103e+00  6.900986e+00      4.642139e+00   \n",
      "std    1.082249e+01  8.162034e+00  8.205062e+00      8.477579e+00   \n",
      "min    1.000000e+00  1.000000e+00  1.000000e+00      1.000000e+00   \n",
      "25%    2.000000e+00  2.000000e+00  2.000000e+00      1.000000e+00   \n",
      "50%    5.000000e+00  3.000000e+00  4.000000e+00      1.000000e+00   \n",
      "75%    1.000000e+01  6.000000e+00  7.000000e+00      2.000000e+00   \n",
      "max    6.700000e+01  6.700000e+01  6.700000e+01      6.700000e+01   \n",
      "\n",
      "       ct_dst_sport_ltm  ct_dst_src_ltm  \n",
      "count      2.540047e+06    2.540047e+06  \n",
      "mean       3.592729e+00    6.845886e+00  \n",
      "std        6.174445e+00    1.125828e+01  \n",
      "min        1.000000e+00    1.000000e+00  \n",
      "25%        1.000000e+00    1.000000e+00  \n",
      "50%        1.000000e+00    2.000000e+00  \n",
      "75%        1.000000e+00    5.000000e+00  \n",
      "max        6.000000e+01    6.700000e+01  \n",
      "\n",
      "[8 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_norm = UNSW_NB15_Config.COLS_TO_NORM\n",
    "print(data[cols_to_norm].describe()) # Check if there's any too large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea95177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All other columns processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_numeric_issues(df, cols_to_norm):\n",
    "    for col in cols_to_norm:\n",
    "        try:\n",
    "            # Try to coerce to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Try to clip the column\n",
    "            df[col] = df[col].clip(lower=-1e9, upper=1e9)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Column '{col}' failed with error: {e}\")\n",
    "            print(f\"  - Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "            print(f\"  - Data type: {df[col].dtype}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n✅ All other columns processed successfully.\")\n",
    "\n",
    "check_numeric_issues(data, UNSW_NB15_Config.COLS_TO_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37256006-abc1-44aa-8e74-46d05dc6ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c6e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "Attack label mapping: {'Analysis': 0, 'Backdoors': 1, 'DoS': 2, 'Exploits': 3, 'Fuzzers': 4, 'Generic': 5, 'Normal': 6, 'Reconnaissance': 7, 'Shellcode': 8, 'Worms': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_classes = 2\n",
    "class_map = [0, 1]\n",
    "if MULTICLASS:\n",
    "    le = LabelEncoder()\n",
    "    attack_labels = le.fit_transform(data[ATTACK_CLASS_COL_NAME])\n",
    "    class_map = le.classes_\n",
    "    print(class_map)\n",
    "    print(\"Attack label mapping:\", dict(zip(class_map, range(len(class_map)))))\n",
    "    data[ATTACK_CLASS_COL_NAME] = attack_labels\n",
    "    num_classes = len(class_map)\n",
    "    class_dict = {le.inverse_transform([i])[0]: i for i in range(len(le.classes_))}\n",
    "\n",
    "BENIGN_CLASS_LABEL = le.transform([BENIGN_CLASS_NAME])[0] if MULTICLASS else 0\n",
    "ADVERSARIAL_CLASS_LABEL = len(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d35f4cdd-2716-431f-af50-b34cc3d2d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'state_ACC', 'state_CLO', 'state_CON', 'state_ECO', 'state_ECR', 'state_FIN', 'state_INT', 'state_MAS', 'state_PAR', 'state_REQ', 'state_RST', 'state_TST', 'state_TXD', 'state_URH', 'state_URN', 'state_no']\n",
      "Number of Features: 54\n"
     ]
    }
   ],
   "source": [
    "# # Maintain the order of the rows in the original dataframe\n",
    "\n",
    "feature_cols = cols_to_norm + converted_categorical_cols\n",
    "\n",
    "print('Feature Columns:', feature_cols)\n",
    "num_features = len(feature_cols)\n",
    "print('Number of Features:', num_features)\n",
    "\n",
    "data['h'] = data[ feature_cols ].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "743e7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df):\n",
    "\n",
    "    G_nx = nx.from_pandas_edgelist(df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "    G_pyg = from_networkx(G_nx)\n",
    "\n",
    "    num_nodes = G_pyg.num_nodes\n",
    "    num_edges = G_pyg.num_edges\n",
    "\n",
    "    G_pyg.x = th.ones(num_nodes, len(df['h'].iloc[0])) \n",
    "\n",
    "    edge_attr_list = []\n",
    "    edge_label_list = []\n",
    "\n",
    "    for u, v, key, data in G_nx.edges(keys=True, data=True):\n",
    "        edge_attr_list.append(data['h']) \n",
    "        edge_label_list.append(data[label_col]) \n",
    "\n",
    "    G_pyg.edge_attr = th.tensor(edge_attr_list, dtype=th.float32)\n",
    "    G_pyg.edge_label = th.tensor(edge_label_list, dtype=th.long)\n",
    "\n",
    "    return G_pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac36567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "import tqdm\n",
    "\n",
    "class Downsampler:\n",
    "    def __init__(self, downsample_classes=[BENIGN_CLASS_LABEL], downsample_ratios=[0.1]):\n",
    "        \"\"\"\n",
    "        downsample_classes: list of class names to downsample\n",
    "        downsample_ratio: keep no more than this ratio for each class\n",
    "        \"\"\"\n",
    "        assert len(downsample_classes) == len(downsample_ratios)\n",
    "        self.downsample_classes = downsample_classes\n",
    "        self.downsample_ratio = downsample_ratios\n",
    "\n",
    "    def downsample(self, label_counts_list, X, y):\n",
    "        total_counts = defaultdict(int)\n",
    "\n",
    "        class_heaps = defaultdict(list)\n",
    "        for i, lc in enumerate(label_counts_list):\n",
    "            for cls in self.downsample_classes:\n",
    "                class_label_count = lc.get(cls, 0)\n",
    "                total_counts[cls] += class_label_count\n",
    "                heapq.heappush(class_heaps[cls], (-class_label_count, i))\n",
    "\n",
    "        class_target = {\n",
    "            cls: total_counts[cls] * self.downsample_ratio[i] for i, cls in enumerate(self.downsample_classes)\n",
    "        }\n",
    "                \n",
    "        indices_to_remove = set()\n",
    "        class_counts = total_counts\n",
    "\n",
    "        # 3. For each class, remove top contributing samples until threshold reached\n",
    "        for cls in self.downsample_classes:\n",
    "            target = class_target[cls]\n",
    "            heap = class_heaps[cls]\n",
    "\n",
    "            pbar = tqdm(desc=f\"Downsampling '{cls}'\", total=len(heap))\n",
    "            while class_counts[cls] > target and heap:\n",
    "                _, idx = heapq.heappop(heap)\n",
    "                if idx in indices_to_remove:\n",
    "                    continue\n",
    "                # For each class in this sample, if it's a downsample class, decrement the count\n",
    "                for sample_cls, count in label_counts_list[idx].items():\n",
    "                    if sample_cls in self.downsample_classes:\n",
    "                        class_counts[sample_cls] -= count\n",
    "                indices_to_remove.add(idx)\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(class_label=cls, remaining=class_counts[cls], target=target)\n",
    "            pbar.close()\n",
    "\n",
    "        # 4. Apply filter\n",
    "        keep_mask = [i for i in range(len(X)) if i not in indices_to_remove]\n",
    "        X_new = [X[i] for i in keep_mask]\n",
    "        y_new = [y[i] for i in keep_mask]\n",
    "\n",
    "        return X_new, y_new\n",
    "\n",
    "downsampler = Downsampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e650028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "class RandomSplitGraphDataset:\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.total_count = len(self.y)\n",
    "\n",
    "        # Compute class weights\n",
    "        labels = []\n",
    "\n",
    "        for graph in self.X:\n",
    "            labels.append(graph.edge_label.tolist())\n",
    "\n",
    "        labels = np.concatenate(labels)\n",
    "\n",
    "        self.class_counts = Counter(labels)\n",
    "\n",
    "        # Compute the class weights\n",
    "        self.class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(labels),\n",
    "            y=labels\n",
    "        )\n",
    "\n",
    "    def graph_train_test_split(self, test_ratio: float = 0.15, random_state: int = 42):\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            np.arange(len(self.X)), \n",
    "            test_size=test_ratio, \n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        X_train = [self.X[i] for i in train_idx]\n",
    "        X_test = [self.X[i] for i in test_idx]\n",
    "\n",
    "        y_train = [self.y[i] for i in train_idx]\n",
    "        y_test = [self.y[i] for i in test_idx]\n",
    "\n",
    "        return RandomSplitGraphDataset(X_train, y_train), RandomSplitGraphDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "class StratifiedGraphDataset:\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.total_count = len(self.y)\n",
    "\n",
    "        # Compute class weights\n",
    "        labels = []\n",
    "\n",
    "        for graph in self.X:\n",
    "            labels.append(graph.edge_label.tolist())\n",
    "\n",
    "        labels = np.concatenate(labels)\n",
    "\n",
    "        self.class_counts = Counter(labels)\n",
    "\n",
    "        # Compute the class weights\n",
    "        self.class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(labels),\n",
    "            y=labels\n",
    "        )\n",
    "\n",
    "    def k_fold_split(self, k: int = 5, test_ratio: float = 0.15, random_state: int = 42):\n",
    "        cv = MultilabelStratifiedShuffleSplit(test_size=test_ratio, random_state=random_state, n_splits=k)\n",
    "\n",
    "        mlb = MultiLabelBinarizer()\n",
    "\n",
    "        y_binary = mlb.fit_transform(self.y)\n",
    "\n",
    "        return cv.split(np.zeros(len(self.X)), y_binary)\n",
    "\n",
    "    def graph_train_test_split(self, test_ratio: float = 0.15, random_state: int = 42):\n",
    "        train_idx, test_idx = next(self.k_fold_split(k = 1, test_ratio = test_ratio, random_state = random_state))\n",
    "        \n",
    "        X_train = [self.X[i] for i in train_idx]\n",
    "        X_test = [self.X[i] for i in test_idx]\n",
    "\n",
    "        y_train = [self.y[i] for i in train_idx]\n",
    "        y_test = [self.y[i] for i in test_idx]\n",
    "\n",
    "        return StratifiedGraphDataset(X_train, y_train), StratifiedGraphDataset(X_test, y_test)\n",
    "    \n",
    "    def print_class_distribution_and_weights(self):\n",
    "        # Use the label encoder to inverse transform the class labels\n",
    "        class_counts_named = {cls: count for cls, count in self.class_counts.items()}\n",
    "        class_weights_named = {cls: weight for cls, weight in enumerate(self.class_weights)}\n",
    "        print(\"Class Counts and Weights:\")\n",
    "        for cls_label in class_counts_named.keys():\n",
    "            count = class_counts_named[cls_label]\n",
    "            weight = class_weights_named[cls_label]\n",
    "            print(f\"{cls_label:<2}  {le.inverse_transform([cls_label])[0]:<15}: Count = {count:<10}, Weight = {weight:<10.4f}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_count\n",
    "\n",
    "    def __iter__(self):\n",
    "        for g in self.X:\n",
    "            yield g\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            return self.X[idx], self.y[idx]\n",
    "        elif isinstance(idx, slice):\n",
    "            return [self.X[i] for i in range(len(self.X))][idx], [self.y[i] for i in range(len(self.y))][idx]\n",
    "        else:\n",
    "            raise TypeError(\"Index must be an integer or a slice.\")\n",
    "\n",
    "def generate_graph_datasets(\n",
    "    df: pd.DataFrame, \n",
    "    window_size: int = 200, \n",
    "    # overlap_ratio: float = 0, \n",
    "    feature_cols=feature_cols,\n",
    "    ordering_cols=[SOURCE_FILE_ID_COL_NAME] + TIME_COLS + [ATTACK_CLASS_COL_NAME], \n",
    "    label_col=label_col,\n",
    "    build_graph_func=create_graph,\n",
    "    downsampler=downsampler\n",
    "    ):\n",
    "\n",
    "    print(\"All Columns: \", df.columns)\n",
    "    print(\"Ordering Columns: \", ordering_cols)\n",
    "    assert all(col in df.columns for col in ordering_cols), \"All timestamp columns are required\"\n",
    "    assert label_col in df.columns, \"Edge label column 'label' is required\"\n",
    "    \n",
    "    df = df.sort_values(ordering_cols).reset_index(drop=True)\n",
    "    window_size = int(window_size)\n",
    "    \n",
    "    df.drop(columns=set(df.columns) - set(feature_cols) - set(label_col))\n",
    "\n",
    "    print(\"Final Columns: \", df.columns)\n",
    "    \n",
    "    label_counts_list = []\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    progress_bar = tqdm(range(0, len(df), window_size), desc=f\"Generating graphs\")\n",
    "    for start in progress_bar:\n",
    "        window_df = df[start: min(start + window_size, len(df))]\n",
    "        contains_label = window_df[label_col].unique()\n",
    "\n",
    "        G_pyg = build_graph_func(window_df)\n",
    "\n",
    "        label_counts = window_df[label_col].value_counts()\n",
    "\n",
    "        label_counts_list.append(label_counts)\n",
    "        X.append(G_pyg)\n",
    "        y.append(contains_label.tolist())\n",
    "\n",
    "    X, y = downsampler.downsample(label_counts_list, X, y)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e7421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Columns:  Index(['srcip', 'dstip', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss',\n",
      "       'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb',\n",
      "       'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit',\n",
      "       'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack',\n",
      "       'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm',\n",
      "       'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
      "       'attack_cat', 'source_file_id', 'state_ACC', 'state_CLO', 'state_CON',\n",
      "       'state_ECO', 'state_ECR', 'state_FIN', 'state_INT', 'state_MAS',\n",
      "       'state_PAR', 'state_REQ', 'state_RST', 'state_TST', 'state_TXD',\n",
      "       'state_URH', 'state_URN', 'state_no', 'h'],\n",
      "      dtype='object')\n",
      "Ordering Columns:  ['source_file_id', 'Stime', 'Ltime', 'attack_cat']\n",
      "Final Columns:  Index(['srcip', 'dstip', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss',\n",
      "       'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb',\n",
      "       'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit',\n",
      "       'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack',\n",
      "       'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm',\n",
      "       'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
      "       'attack_cat', 'source_file_id', 'state_ACC', 'state_CLO', 'state_CON',\n",
      "       'state_ECO', 'state_ECR', 'state_FIN', 'state_INT', 'state_MAS',\n",
      "       'state_PAR', 'state_REQ', 'state_RST', 'state_TST', 'state_TXD',\n",
      "       'state_URH', 'state_URN', 'state_no', 'h'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating graphs: 100%|██████████| 12701/12701 [02:01<00:00, 104.84it/s]\n",
      "Downsampling '6':  85%|████████▌ | 10853/12701 [00:07<00:01, 1498.50it/s, class_label=6, remaining=221852, target=2.22e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified Train Dataset Size: Counter({np.int64(6): 188281, np.int64(5): 89484, np.int64(3): 15147, np.int64(2): 10218, np.int64(4): 4292, np.int64(7): 2911, np.int64(0): 1752, np.int64(1): 1531, np.int64(8): 196, np.int64(9): 35})\n",
      "Stratified Train Dataset Size: Counter({np.int64(6): 188277, np.int64(5): 89392, np.int64(3): 15334, np.int64(2): 10014, np.int64(4): 4362, np.int64(7): 2897, np.int64(0): 1778, np.int64(1): 1561, np.int64(8): 196, np.int64(9): 36})\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_graph_datasets(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee16656",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stratified_graph_daataset = StratifiedGraphDataset(X, y)\n",
    "random_split_graph_dataset = RandomSplitGraphDataset(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fcea448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified Split Jensen-Shannon Divergence: 1.546106727616257e-06\n",
      "Random Split Jensen-Shannon Divergence: 3.0448183797230593e-05\n"
     ]
    }
   ],
   "source": [
    "num_tests = 10\n",
    "for i in range(num_tests):\n",
    "\n",
    "    total_stratified_js = 0\n",
    "    total_random_js = 0\n",
    "\n",
    "    stratified_train_graph_dataset, stratified_test_graph_dataset = stratified_graph_daataset.graph_train_test_split(test_ratio=0.15, random_state=i)\n",
    "    random_train_graph_dataset, random_test_graph_dataset = random_split_graph_dataset.graph_train_test_split(test_ratio=0.15, random_state=i)\n",
    "\n",
    "    # Compute the Jensen-Shannon Divergence of train and test datasets\n",
    "    def compute_js_divergence(counter_p, counter_q):\n",
    "        \"\"\"\n",
    "        Compute the Jensen-Shannon Divergence between two class count dictionaries (Counter).\n",
    "        \"\"\"\n",
    "        # Get the union of all class labels\n",
    "        all_keys = sorted(set(counter_p.keys()).union(set(counter_q.keys())))\n",
    "        # Convert to arrays in the same order\n",
    "        p = np.array([counter_p.get(k, 0) for k in all_keys], dtype=np.float64)\n",
    "        q = np.array([counter_q.get(k, 0) for k in all_keys], dtype=np.float64)\n",
    "\n",
    "        # Normalize the distributions\n",
    "        p = p / np.sum(p) if np.sum(p) > 0 else p\n",
    "        q = q / np.sum(q) if np.sum(q) > 0 else q\n",
    "\n",
    "        # Compute the average distribution\n",
    "        m = 0.5 * (p + q)\n",
    "\n",
    "        # Compute the Kullback-Leibler divergence\n",
    "        kl_p_m = np.sum(np.where(p != 0, p * np.log(p / m), 0))\n",
    "        kl_q_m = np.sum(np.where(q != 0, q * np.log(q / m), 0))\n",
    "\n",
    "        # Return the Jensen-Shannon Divergence\n",
    "        jsd = 0.5 * (kl_p_m + kl_q_m)\n",
    "        return jsd\n",
    "\n",
    "\n",
    "    def KL_divergence(counter_p, counter_q):\n",
    "        \"\"\"\n",
    "        Compute the Kullback-Leibler Divergence between two distributions.\n",
    "        \"\"\"\n",
    "        # Get the union of all class labels\n",
    "        all_keys = sorted(set(counter_p.keys()).union(set(counter_q.keys())))\n",
    "\n",
    "        p = np.array([counter_p.get(k, 0) for k in all_keys], dtype=np.float64)\n",
    "        q = np.array([counter_q.get(k, 0) for k in all_keys], dtype=np.float64)\n",
    "\n",
    "        # Normalize the distributions\n",
    "        p = p / np.sum(p) if np.sum(p) > 0 else p\n",
    "        q = q / np.sum(q) if np.sum(q) > 0 else q\n",
    "\n",
    "        print(\"KL-Divergence: \", np.sum(np.where(p != 0, p * np.log(p / q), 0))) \n",
    "\n",
    "    total_stratified_js += compute_js_divergence(\n",
    "        stratified_train_graph_dataset.class_counts,\n",
    "        stratified_test_graph_dataset.class_counts\n",
    "    )\n",
    "\n",
    "    total_random_js += compute_js_divergence(\n",
    "        random_train_graph_dataset.class_counts,\n",
    "        random_test_graph_dataset.class_counts\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Stratified Split Jensen-Shannon Divergence: {total_stratified_js / num_tests}\")\n",
    "print(f\"Random Split Jensen-Shannon Divergence: {total_random_js / num_tests}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
