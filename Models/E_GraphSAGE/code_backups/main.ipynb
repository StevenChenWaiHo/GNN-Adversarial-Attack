{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb9e1b2-1c5f-49e3-82b4-3bfe52cabad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import from_networkx, add_self_loops, degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "# import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Datasets.UNSW_NB15.UNSW_NB15_config import UNSW_NB15_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9ef09a-d405-43b8-971e-fe9e6a592c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2503128/1548955270.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "Normal            221876\n",
      "Generic           215481\n",
      "Exploits           44525\n",
      "Fuzzers            24246\n",
      "DoS                16353\n",
      "Reconnaissance     13987\n",
      "Analysis            2677\n",
      "Backdoor            1795\n",
      "Shellcode           1511\n",
      "Backdoors            534\n",
      "Worms                174\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    321283\n",
      "0    221876\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "csv_file_name = \"all_raw_downsampled\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n",
    "\n",
    "DATASET_NAME = \"UNSW_NB15\"\n",
    "\n",
    "SOURCE_IP_COL_NAME = UNSW_NB15_Config.SOURCE_IP_COL_NAME\n",
    "DESTINATION_IP_COL_NAME = UNSW_NB15_Config.DESTINATION_IP_COL_NAME\n",
    "SOURCE_PORT_COL_NAME = UNSW_NB15_Config.SOURCE_PORT_COL_NAME\n",
    "DESTINATION_PORT_COL_NAME = UNSW_NB15_Config.DESTINATION_PORT_COL_NAME\n",
    "\n",
    "ATTACK_CLASS_COL_NAME = UNSW_NB15_Config.ATTACK_CLASS_COL_NAME\n",
    "IS_ATTACK_COL_NAME = UNSW_NB15_Config.IS_ATTACK_COL_NAME\n",
    "\n",
    "BENIGN_CLASS_NAME = UNSW_NB15_Config.BENIGN_CLASS_NAME\n",
    "\n",
    "TIME_COLS = UNSW_NB15_Config.TIME_COL_NAMES\n",
    "\n",
    "MULTICLASS = True\n",
    "label_col = ATTACK_CLASS_COL_NAME if MULTICLASS else IS_ATTACK_COL_NAME\n",
    "\n",
    "print(data[ATTACK_CLASS_COL_NAME].value_counts())\n",
    "print(data[IS_ATTACK_COL_NAME].value_counts())\n",
    "\n",
    "if MULTICLASS:\n",
    "    data.drop(columns=[IS_ATTACK_COL_NAME], inplace=True)\n",
    "else:\n",
    "    data.drop(columns=[ATTACK_CLASS_COL_NAME], inplace=True)\n",
    "\n",
    "checkpoint_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, f\"checkpoints_{csv_file_name}.pth\")\n",
    "best_model_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, f\"best_model_{csv_file_name}.pth\")\n",
    "final_epoch_model_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, f\"final_epoch{csv_file_name}.pth\")\n",
    "\n",
    "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(best_model_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449a1af1-1d3d-4179-9628-7c2ec551ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=UNSW_NB15_Config.DROP_COLS,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb74f87-8df1-4c98-a849-e263e03a06f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2c690c-86a4-49f7-aa9c-58f94529547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME].apply(str)\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME].apply(str)\n",
    "\n",
    "# # Combine Port and IP\n",
    "data[SOURCE_PORT_COL_NAME] = data[SOURCE_PORT_COL_NAME].apply(str)\n",
    "data[DESTINATION_PORT_COL_NAME] = data[DESTINATION_PORT_COL_NAME].apply(str)\n",
    "\n",
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME] + ':' + data[SOURCE_PORT_COL_NAME]\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME] + ':' + data[DESTINATION_PORT_COL_NAME]\n",
    "data.drop(columns=[SOURCE_PORT_COL_NAME,DESTINATION_PORT_COL_NAME],inplace=True)\n",
    "\n",
    "# data[SOURCE_PORT_COL_NAME] = pd.to_numeric(data[SOURCE_PORT_COL_NAME], errors='coerce').fillna(0).astype(int)\n",
    "# data[DESTINATION_PORT_COL_NAME] = pd.to_numeric(data[DESTINATION_PORT_COL_NAME], errors='coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5651ef5b-0a9d-4641-aad7-5738092c46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                      srcip               dstip state        dur  sbytes  \\\n",
      "0             10.40.85.1:0         224.0.0.5:0   INT  50.004341     384   \n",
      "1          59.166.0.6:2142    149.171.126.4:53   CON   0.001134     132   \n",
      "2       175.45.176.0:13284   149.171.126.16:80   FIN   2.390390    1362   \n",
      "3         59.166.0.3:42587    149.171.126.8:25   FIN  34.077175   37358   \n",
      "4            10.40.170.2:0       10.40.170.2:0   INT   0.000000      46   \n",
      "...                    ...                 ...   ...        ...     ...   \n",
      "543154  175.45.176.0:47439   149.171.126.10:53   INT   0.000001     114   \n",
      "543155  175.45.176.0:47439   149.171.126.10:53   INT   0.000001     114   \n",
      "543156    59.166.0.5:53521    149.171.126.7:21   CON   1.086072    1940   \n",
      "543157  175.45.176.0:17293  149.171.126.17:110   CON   0.942984     574   \n",
      "543158  175.45.176.0:17293  149.171.126.17:110   CON   0.942984     574   \n",
      "\n",
      "        dbytes  sttl  dttl  sloss  dloss  ...  is_ftp_login  ct_ftp_cmd  \\\n",
      "0            0     1     0      0      0  ...           0.0         0.0   \n",
      "1          164    31    29      0      0  ...           0.0         0.0   \n",
      "2          268   254   252      6      1  ...           0.0         0.0   \n",
      "3         3380    31    29     18      8  ...           0.0         0.0   \n",
      "4            0     0     0      0      0  ...           0.0         0.0   \n",
      "...        ...   ...   ...    ...    ...  ...           ...         ...   \n",
      "543154       0   254     0      0      0  ...           0.0         NaN   \n",
      "543155       0   254     0      0      0  ...           0.0         NaN   \n",
      "543156    2404    31    29      8     10  ...           2.0         2.0   \n",
      "543157     676    62   252      5      6  ...           0.0         NaN   \n",
      "543158     676    62   252      5      6  ...           0.0         NaN   \n",
      "\n",
      "        ct_srv_src  ct_srv_dst  ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  \\\n",
      "0                2           4           4           2                 2   \n",
      "1               12           7           1           2                 2   \n",
      "2                5           2           2           1                 1   \n",
      "3                1           1          12          10                 1   \n",
      "4                2           2           2           2                 2   \n",
      "...            ...         ...         ...         ...               ...   \n",
      "543154          15          15          15          15                15   \n",
      "543155          15          15          15          15                15   \n",
      "543156           2           2           3           3                 2   \n",
      "543157           2           1           2           4                 2   \n",
      "543158           1           1           2           4                 2   \n",
      "\n",
      "        ct_dst_sport_ltm  ct_dst_src_ltm      attack_cat  \n",
      "0                      4               2          Normal  \n",
      "1                      1               1          Normal  \n",
      "2                      1               1  Reconnaissance  \n",
      "3                      1               2          Normal  \n",
      "4                      2               2          Normal  \n",
      "...                  ...             ...             ...  \n",
      "543154                15              15         Generic  \n",
      "543155                15              15         Generic  \n",
      "543156                 2               3          Normal  \n",
      "543157                 2               2        Exploits  \n",
      "543158                 2               2        Exploits  \n",
      "\n",
      "[543159 rows x 44 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7fb458-ca34-42ca-a8af-f8e1609aff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns = UNSW_NB15_Config.CATEGORICAL_COLS) # One Hot Encoding for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d96115-31f9-48cb-b3e6-7853d2d253cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                      srcip               dstip        dur  sbytes  dbytes  \\\n",
      "0             10.40.85.1:0         224.0.0.5:0  50.004341     384       0   \n",
      "1          59.166.0.6:2142    149.171.126.4:53   0.001134     132     164   \n",
      "2       175.45.176.0:13284   149.171.126.16:80   2.390390    1362     268   \n",
      "3         59.166.0.3:42587    149.171.126.8:25  34.077175   37358    3380   \n",
      "4            10.40.170.2:0       10.40.170.2:0   0.000000      46       0   \n",
      "...                    ...                 ...        ...     ...     ...   \n",
      "543154  175.45.176.0:47439   149.171.126.10:53   0.000001     114       0   \n",
      "543155  175.45.176.0:47439   149.171.126.10:53   0.000001     114       0   \n",
      "543156    59.166.0.5:53521    149.171.126.7:21   1.086072    1940    2404   \n",
      "543157  175.45.176.0:17293  149.171.126.17:110   0.942984     574     676   \n",
      "543158  175.45.176.0:17293  149.171.126.17:110   0.942984     574     676   \n",
      "\n",
      "        sttl  dttl  sloss  dloss         Sload  ...  state_ECO  state_FIN  \\\n",
      "0          1     0      0      0  5.119556e+01  ...      False      False   \n",
      "1         31    29      0      0  4.656085e+05  ...      False      False   \n",
      "2        254   252      6      1  4.233619e+03  ...      False       True   \n",
      "3         31    29     18      8  8.601652e+03  ...      False       True   \n",
      "4          0     0      0      0  0.000000e+00  ...      False      False   \n",
      "...      ...   ...    ...    ...           ...  ...        ...        ...   \n",
      "543154   254     0      0      0  4.560000e+08  ...      False      False   \n",
      "543155   254     0      0      0  4.560000e+08  ...      False      False   \n",
      "543156    31    29      8     10  1.387017e+04  ...      False      False   \n",
      "543157    62   252      5      6  4.470914e+03  ...      False      False   \n",
      "543158    62   252      5      6  4.470914e+03  ...      False      False   \n",
      "\n",
      "        state_INT  state_MAS  state_PAR  state_REQ  state_RST  state_TST  \\\n",
      "0            True      False      False      False      False      False   \n",
      "1           False      False      False      False      False      False   \n",
      "2           False      False      False      False      False      False   \n",
      "3           False      False      False      False      False      False   \n",
      "4            True      False      False      False      False      False   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "543154       True      False      False      False      False      False   \n",
      "543155       True      False      False      False      False      False   \n",
      "543156      False      False      False      False      False      False   \n",
      "543157      False      False      False      False      False      False   \n",
      "543158      False      False      False      False      False      False   \n",
      "\n",
      "        state_TXD  state_URH  \n",
      "0           False      False  \n",
      "1           False      False  \n",
      "2           False      False  \n",
      "3           False      False  \n",
      "4           False      False  \n",
      "...           ...        ...  \n",
      "543154      False      False  \n",
      "543155      False      False  \n",
      "543156      False      False  \n",
      "543157      False      False  \n",
      "543158      False      False  \n",
      "\n",
      "[543159 rows x 56 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "data.drop(columns=['index'],inplace=True)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1df5c4c-70a2-4566-ae5e-ee3dcc6037a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 dur        sbytes        dbytes           sttl  \\\n",
      "count  543159.000000  5.431590e+05  5.431590e+05  543159.000000   \n",
      "mean        0.706760  5.136572e+03  1.936909e+04     157.197364   \n",
      "std        12.637229  1.202311e+05  1.390925e+05     108.452474   \n",
      "min         0.000000  0.000000e+00  0.000000e+00       0.000000   \n",
      "25%         0.000007  1.140000e+02  0.000000e+00      31.000000   \n",
      "50%         0.000011  2.000000e+02  0.000000e+00     254.000000   \n",
      "75%         0.072088  1.580000e+03  1.940000e+03     254.000000   \n",
      "max      8760.776367  1.435577e+07  1.465753e+07     255.000000   \n",
      "\n",
      "                dttl          sloss          dloss         Sload  \\\n",
      "count  543159.000000  543159.000000  543159.000000  5.431590e+05   \n",
      "mean       38.850764       3.800661       8.729770  6.877595e+07   \n",
      "std        77.034389      45.616565      50.136204  1.420534e+08   \n",
      "min         0.000000       0.000000       0.000000  0.000000e+00   \n",
      "25%         0.000000       0.000000       0.000000  3.705672e+05   \n",
      "50%         0.000000       0.000000       0.000000  4.560000e+07   \n",
      "75%        29.000000       3.000000       4.000000  8.888889e+07   \n",
      "max       254.000000    5319.000000    5507.000000  5.988000e+09   \n",
      "\n",
      "              Dload          Spkts  ...  ct_flw_http_mthd   is_ftp_login  \\\n",
      "count  5.431590e+05  543159.000000  ...     543159.000000  543159.000000   \n",
      "mean   1.148111e+06      20.369921  ...          0.088724       0.011490   \n",
      "std    3.127653e+06     101.923505  ...          0.566327       0.109623   \n",
      "min    0.000000e+00       0.000000  ...          0.000000       0.000000   \n",
      "25%    0.000000e+00       2.000000  ...          0.000000       0.000000   \n",
      "50%    0.000000e+00       2.000000  ...          0.000000       0.000000   \n",
      "75%    4.087816e+05      14.000000  ...          0.000000       0.000000   \n",
      "max    2.274587e+07   10646.000000  ...         36.000000       4.000000   \n",
      "\n",
      "          ct_ftp_cmd     ct_srv_src     ct_srv_dst     ct_dst_ltm  \\\n",
      "count  543159.000000  543159.000000  543159.000000  543159.000000   \n",
      "mean        0.013029      15.008556      14.833110      10.306825   \n",
      "std         0.141497      14.229735      14.305878      10.989668   \n",
      "min         0.000000       1.000000       1.000000       1.000000   \n",
      "25%         0.000000       3.000000       3.000000       2.000000   \n",
      "50%         0.000000       9.000000       8.000000       5.000000   \n",
      "75%         0.000000      26.000000      26.000000      17.000000   \n",
      "max         8.000000      67.000000      67.000000      67.000000   \n",
      "\n",
      "          ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \n",
      "count  543159.000000     543159.000000     543159.000000   543159.000000  \n",
      "mean       10.837838          9.343288          7.209839       13.766985  \n",
      "std        10.967546         11.391238          8.069018       14.972826  \n",
      "min         1.000000          1.000000          1.000000        1.000000  \n",
      "25%         2.000000          1.000000          1.000000        1.000000  \n",
      "50%         6.000000          2.000000          2.000000        5.000000  \n",
      "75%        17.000000         17.000000         15.000000       26.000000  \n",
      "max        67.000000         67.000000         60.000000       67.000000  \n",
      "\n",
      "[8 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_norm = UNSW_NB15_Config.COLS_TO_NORM\n",
    "print(data[cols_to_norm].describe()) # Check if there's any too large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea95177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All other columns processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_numeric_issues(df, cols_to_norm):\n",
    "    for col in cols_to_norm:\n",
    "        try:\n",
    "            # Try to coerce to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Try to clip the column\n",
    "            df[col] = df[col].clip(lower=-1e9, upper=1e9)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Column '{col}' failed with error: {e}\")\n",
    "            print(f\"  - Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "            print(f\"  - Data type: {df[col].dtype}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n✅ All other columns processed successfully.\")\n",
    "\n",
    "check_numeric_issues(data, UNSW_NB15_Config.COLS_TO_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37256006-abc1-44aa-8e74-46d05dc6ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c6e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis' 'Backdoor' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic'\n",
      " 'Normal' 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "Attack label mapping: {'Analysis': 0, 'Backdoor': 1, 'Backdoors': 2, 'DoS': 3, 'Exploits': 4, 'Fuzzers': 5, 'Generic': 6, 'Normal': 7, 'Reconnaissance': 8, 'Shellcode': 9, 'Worms': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_classes = 2\n",
    "class_map = [0, 1]\n",
    "if MULTICLASS:\n",
    "    le = LabelEncoder()\n",
    "    attack_labels = le.fit_transform(data[ATTACK_CLASS_COL_NAME])\n",
    "    class_map = le.classes_\n",
    "    print(class_map)\n",
    "    print(\"Attack label mapping:\", dict(zip(class_map, range(len(class_map)))))\n",
    "    data[ATTACK_CLASS_COL_NAME] = attack_labels\n",
    "    num_classes = len(class_map)\n",
    "    class_dict = {le.inverse_transform([i])[0]: i for i in range(len(le.classes_))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d35f4cdd-2716-431f-af50-b34cc3d2d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'state_ACC', 'state_CLO', 'state_CON', 'state_ECO', 'state_FIN', 'state_INT', 'state_MAS', 'state_PAR', 'state_REQ', 'state_RST', 'state_TST', 'state_TXD', 'state_URH']\n",
      "Number of training samples: 380211\n",
      "attack_cat\n",
      "7     155313\n",
      "6     150837\n",
      "4      31167\n",
      "5      16972\n",
      "3      11447\n",
      "8       9791\n",
      "0       1874\n",
      "1       1256\n",
      "9       1058\n",
      "2        374\n",
      "10       122\n",
      "Name: count, dtype: int64\n",
      "Number of validation samples: 81474\n",
      "attack_cat\n",
      "7     33282\n",
      "6     32322\n",
      "4      6679\n",
      "5      3637\n",
      "3      2453\n",
      "8      2098\n",
      "0       402\n",
      "1       269\n",
      "9       226\n",
      "2        80\n",
      "10       26\n",
      "Name: count, dtype: int64\n",
      "Number of test samples: 81474\n",
      "attack_cat\n",
      "7     33282\n",
      "6     32322\n",
      "4      6679\n",
      "5      3637\n",
      "3      2453\n",
      "8      2098\n",
      "0       402\n",
      "1       269\n",
      "9       226\n",
      "2        80\n",
      "10       26\n",
      "Name: count, dtype: int64\n",
      "                     srcip                dstip       dur    sbytes    dbytes  \\\n",
      "353743      175.45.176.1:0     149.171.126.17:0 -0.055926 -0.041059 -0.139253   \n",
      "400917    59.166.0.9:52225     149.171.126.6:80  0.031109 -0.028716 -0.066151   \n",
      "154000   175.45.176.0:1043    149.171.126.12:53 -0.055926 -0.041774 -0.139253   \n",
      "488357  175.45.176.0:45254  149.171.126.14:5060  0.050908 -0.007341 -0.136708   \n",
      "500299      175.45.176.2:0     149.171.126.17:0 -0.055926 -0.041059 -0.139253   \n",
      "\n",
      "            sttl      dttl     sloss     dloss     Sload     Dload     Spkts  \\\n",
      "353743  0.892582 -0.504331 -0.083318 -0.174121  0.106433 -0.367084 -0.180233   \n",
      "400917 -1.163620 -0.127875 -0.017552 -0.074393 -0.555068 -0.344750 -0.062497   \n",
      "154000  0.892582 -0.504331 -0.083318 -0.174121  0.199056 -0.367084 -0.180233   \n",
      "488357  0.892582  2.766939 -0.017552 -0.154176 -0.554971 -0.366497 -0.082120   \n",
      "500299  0.892582 -0.504331 -0.083318 -0.174121  0.179944 -0.367084 -0.180233   \n",
      "\n",
      "           Dpkts      swin      dwin     stcpb     dtcpb   smeansz   dmeansz  \\\n",
      "353743 -0.216121 -0.768239 -0.767201 -0.721514 -0.721619 -0.065470 -0.526921   \n",
      "400917 -0.046980  1.301681  1.303441  0.299750  0.302046  0.063214  1.479209   \n",
      "154000 -0.216121 -0.768239 -0.767201 -0.721514 -0.721619 -0.342141 -0.526921   \n",
      "488357 -0.140947  1.301681  1.303441  1.482943 -0.361645  1.575254 -0.370691   \n",
      "500299 -0.216121 -0.768239 -0.767201 -0.721514 -0.721619 -0.065470 -0.526921   \n",
      "\n",
      "        trans_depth  res_bdy_len      Sjit      Djit  Stime  Ltime   Sintpkt  \\\n",
      "353743    -0.134353    -0.054182 -0.080649 -0.189023    0.0    0.0 -0.060832   \n",
      "400917     1.819878     0.035127  0.261390  2.929229    0.0    0.0 -0.017963   \n",
      "154000    -0.134353    -0.054182 -0.080649 -0.189023    0.0    0.0 -0.060834   \n",
      "488357    -0.134353    -0.054182  0.227610 -0.078170    0.0    0.0  0.000364   \n",
      "500299    -0.134353    -0.054182 -0.080649 -0.189023    0.0    0.0 -0.060832   \n",
      "\n",
      "         Dintpkt    tcprtt    synack    ackdat  is_sm_ips_ports  ct_state_ttl  \\\n",
      "353743 -0.055697 -0.293130 -0.270319 -0.288797        -0.027818      0.926467   \n",
      "400917  0.006760 -0.281998 -0.253781 -0.284558        -0.027818     -1.123447   \n",
      "154000 -0.055697 -0.293130 -0.270319 -0.288797        -0.027818      0.926467   \n",
      "488357  0.119245  3.651394  2.599703  4.420921        -0.027818     -0.098490   \n",
      "500299 -0.055697 -0.293130 -0.270319 -0.288797        -0.027818      0.926467   \n",
      "\n",
      "        ct_flw_http_mthd  is_ftp_login  ct_ftp_cmd  ct_srv_src  ct_srv_dst  \\\n",
      "353743         -0.156665     -0.104815   -0.092082   -0.492529   -0.477644   \n",
      "400917          1.609100     -0.104815   -0.092082   -0.843907   -0.897052   \n",
      "154000         -0.156665     -0.104815   -0.092082    0.702153    0.850483   \n",
      "488357         -0.156665     -0.104815   -0.092082   -0.773631   -0.897052   \n",
      "500299         -0.156665     -0.104815   -0.092082   -0.914182   -0.897052   \n",
      "\n",
      "        ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  \\\n",
      "353743   -0.664882   -0.714640         -0.644644         -0.645660   \n",
      "400917   -0.573887   -0.349927         -0.644644         -0.769591   \n",
      "154000    0.609043    0.561855          0.672159          0.097925   \n",
      "488357   -0.846871   -0.896996         -0.732431         -0.769591   \n",
      "500299   -0.846871   -0.805818         -0.732431         -0.769591   \n",
      "\n",
      "        ct_dst_src_ltm  attack_cat  state_ACC  state_CLO  state_CON  \\\n",
      "353743       -0.118013           4      False      False      False   \n",
      "400917       -0.785890           7      False      False      False   \n",
      "154000        0.750227           6      False      False      False   \n",
      "488357       -0.785890           4      False      False      False   \n",
      "500299       -0.785890           4      False      False      False   \n",
      "\n",
      "        state_ECO  state_FIN  state_INT  state_MAS  state_PAR  state_REQ  \\\n",
      "353743      False      False       True      False      False      False   \n",
      "400917      False       True      False      False      False      False   \n",
      "154000      False      False       True      False      False      False   \n",
      "488357      False       True      False      False      False      False   \n",
      "500299      False      False       True      False      False      False   \n",
      "\n",
      "        state_RST  state_TST  state_TXD  state_URH  \\\n",
      "353743      False      False      False      False   \n",
      "400917      False      False      False      False   \n",
      "154000      False      False      False      False   \n",
      "488357      False      False      False      False   \n",
      "500299      False      False      False      False   \n",
      "\n",
      "                                                        h  \n",
      "353743  [-0.055926079960930894, -0.041059064818655, -0...  \n",
      "400917  [0.03110866337143625, -0.028716155990272666, -...  \n",
      "154000  [-0.0559264756176422, -0.041774354683372567, -...  \n",
      "488357  [0.050907879124617354, -0.007340633288828872, ...  \n",
      "500299  [-0.05592615909227315, -0.041059064818655, -0....  \n"
     ]
    }
   ],
   "source": [
    "# 70% train, 15% validation, 15% test\n",
    "train_df, temp_df = train_test_split(\n",
    "     data, test_size=0.3, random_state=42, stratify=data[label_col])\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "     temp_df, test_size=0.5, random_state=42, stratify=temp_df[label_col])\n",
    "\n",
    "\n",
    "# Maintain the order of the rows in the original dataframe\n",
    "train_df = train_df.sort_values(by=UNSW_NB15_Config.TIME_COL_NAMES)\n",
    "val_df = val_df.sort_values(by=UNSW_NB15_Config.TIME_COL_NAMES)\n",
    "test_df = test_df.sort_values(by=UNSW_NB15_Config.TIME_COL_NAMES)\n",
    "\n",
    "feature_cols = [col for col in data.columns if col not in [label_col, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME]]\n",
    "\n",
    "print('Feature Columns:', feature_cols)\n",
    "\n",
    "train_df['h'] = train_df[ feature_cols ].values.tolist()\n",
    "val_df['h'] = val_df[ feature_cols ].values.tolist()\n",
    "test_df['h'] = test_df[ feature_cols ].values.tolist()\n",
    "\n",
    "# X_train = train_df.drop(columns=[label_col])\n",
    "# X_val = val_df.drop(columns=[label_col])\n",
    "# X_test = test_df.drop(columns=[label_col])\n",
    "\n",
    "y_train = train_df[label_col]\n",
    "y_val = test_df[label_col]\n",
    "y_test = test_df[label_col]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_df))\n",
    "print(y_train.value_counts())\n",
    "print(\"Number of validation samples:\", len(val_df))\n",
    "print(y_val.value_counts())\n",
    "print(\"Number of test samples:\", len(test_df))\n",
    "print(y_test.value_counts())\n",
    "\n",
    "print(train_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da147a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f83bd73-531f-42a0-9f73-e1fd98dce1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, source_ip_col, destination_ip_col, edge_attr, create_using=nx.MultiDiGraph(), **kwargs):\n",
    "    G_nx = nx.from_pandas_edgelist(df, source_ip_col, destination_ip_col, edge_attr, create_using=create_using, **kwargs)\n",
    "    G_pyg = from_networkx(G_nx)\n",
    "\n",
    "    num_nodes = G_pyg.num_nodes\n",
    "    num_edges = G_pyg.num_edges\n",
    "\n",
    "    G_pyg.x = th.ones(num_nodes, len(df['h'].iloc[0])) \n",
    "\n",
    "    edge_attr_list = []\n",
    "    edge_label_list = []\n",
    "\n",
    "    for u, v, key, data in G_nx.edges(keys=True, data=True):\n",
    "        edge_attr_list.append(data['h']) \n",
    "        edge_label_list.append(data[label_col]) \n",
    "\n",
    "    G_pyg.edge_attr = th.tensor(edge_attr_list, dtype=th.float32)\n",
    "    G_pyg.edge_label = th.tensor(edge_label_list, dtype=th.long)\n",
    "\n",
    "    print(\"Number of edges in G_pyg:\", num_edges)\n",
    "    print(\"Number of node in G_pyg:\", num_nodes)\n",
    "    print(\"Shape of node in G_pyg:\", G_pyg.x.shape)\n",
    "    print(\"Shape of edge attr in G_pyg:\", G_pyg.edge_attr.shape)\n",
    "    print(\"Shape of edge label in G_pyg:\", G_pyg.edge_label.shape)\n",
    "\n",
    "    return G_nx, G_pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8f9cdb-1316-461a-a927-8d67d90d6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 380211\n",
      "Number of node in G_pyg: 220487\n",
      "Shape of node in G_pyg: torch.Size([220487, 53])\n",
      "Shape of edge attr in G_pyg: torch.Size([380211, 53])\n",
      "Shape of edge label in G_pyg: torch.Size([380211])\n",
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 52568\n",
      "Shape of node in G_pyg: torch.Size([52568, 53])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 53])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41795339-6036-468f-9b9d-2bb68d78ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGraphSAGEConv(MessagePassing):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, out_channels):\n",
    "        super(EGraphSAGEConv, self).__init__(aggr='mean')  # mean aggregation\n",
    "        self.lin_node = nn.Linear(node_in_channels, out_channels)\n",
    "        self.lin_edge = nn.Linear(edge_in_channels, out_channels)\n",
    "        self.lin_update = nn.Linear(node_in_channels + out_channels, out_channels) # out_channels * 2\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: Node features, edge_attr: Edge features, edge_index: Connectivity\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            if edge_attr.size(0) != edge_index.size(1):\n",
    "                loop_attr = th.zeros((edge_index.size(1) - edge_attr.size(0), edge_attr.size(1))).to(edge_attr.device)\n",
    "                edge_attr = th.cat([edge_attr, loop_attr], dim=0)\n",
    "        else:\n",
    "            print(\"edge_attr is unexist\")\n",
    "        \n",
    "        # Propagate and aggregate neighbor information\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j represents the adjacent nodes of x\n",
    "        # Compute messages by combining node and edge features\n",
    "        return self.lin_node(x_j) + self.lin_edge(edge_attr)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # Update node features after message passing\n",
    "        return self.lin_update(th.cat([x, aggr_out], dim=1))\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.lin = nn.Linear(in_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, data, z):\n",
    "        row, col = data.edge_index\n",
    "        # Concatenate the features of source and target nodes for each edge\n",
    "        edge_feat = th.cat([z[row], z[col]], dim=1)\n",
    "        return self.lin(edge_feat)\n",
    "\n",
    "class EGraphSAGE(nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels, out_channels):\n",
    "        super(EGraphSAGE, self).__init__()\n",
    "        self.conv1 = EGraphSAGEConv(node_in_channels, edge_in_channels, hidden_channels)\n",
    "        self.conv2 = EGraphSAGEConv(hidden_channels, edge_in_channels, hidden_channels)\n",
    "        self.mlp_predictor = MLPPredictor(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return self.mlp_predictor(data, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bca25fef-29d9-40cf-8910-16b24d530693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cccdc850-b98d-4836-b82b-67aa4b9e1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89157faf-e24b-49d6-9c90-6f71dae515b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([4.3677e-01, 6.5168e-01, 2.1885e+00, 7.1504e-02, 2.6262e-02, 4.8227e-02,\n",
      "        5.4265e-03, 5.2701e-03, 8.3598e-02, 7.7364e-01, 6.7091e+00],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features, \n",
    "                   edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                   hidden_channels=128, \n",
    "                   out_channels=num_classes).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.unique(labels),\n",
    "                                                  y=labels)\n",
    "\n",
    "# Normalise to stabilise training\n",
    "class_weights = class_weights / np.mean(class_weights)\n",
    "\n",
    "class_weights = th.FloatTensor(class_weights).to(device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "385d37f0-713b-4abc-8d7a-3e768ae9a2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed training from epoch 200\n",
      "Training is over\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "def generate_edge_based_batches_with_node_expansion(graph, batch_size, min_nodes):\n",
    "    num_edges = graph.edge_index.size(1)  # Get total number of edges\n",
    "    edge_indices = th.arange(num_edges)   # Create list of edge indices\n",
    "    num_edges_processed = 0\n",
    "    \n",
    "    while num_edges_processed < num_edges:\n",
    "        # Select a batch of edges\n",
    "        batch_edge_indices = edge_indices[num_edges_processed : min(num_edges_processed + batch_size, num_edges)]\n",
    "        edge_index = graph.edge_index[:, batch_edge_indices]\n",
    "        \n",
    "        # Update the number of edges processed\n",
    "        num_edges_processed += batch_size\n",
    "        \n",
    "        # Get the unique nodes associated with these edges\n",
    "        batch_nodes = th.cat([edge_index[0], edge_index[1]]).unique()\n",
    "\n",
    "        # Check if the batch has enough unique nodes\n",
    "        while batch_nodes.size(0) < min_nodes:\n",
    "            # Sample additional neighboring nodes to ensure diversity\n",
    "            additional_edges = int(batch_size / 8)  # Ensure additional_edges is an integer\n",
    "            batch_edge_indices = th.cat([batch_edge_indices, edge_indices[num_edges_processed : min(num_edges_processed + additional_edges, num_edges)]])\n",
    "            edge_index = graph.edge_index[:, batch_edge_indices]\n",
    "            batch_nodes = th.cat([edge_index[0], edge_index[1]]).unique()\n",
    "            num_edges_processed += additional_edges\n",
    "\n",
    "            # Avoid potential infinite loops by breaking if no more edges can be added\n",
    "            if num_edges_processed >= num_edges:\n",
    "                break\n",
    "\n",
    "        # Create subgraph from the selected nodes and edges\n",
    "        edge_index, _, edge_mask = subgraph(batch_nodes, graph.edge_index, relabel_nodes=True, return_edge_mask=True)\n",
    "\n",
    "        # Use edge_mask to select edge attributes and labels\n",
    "        edge_attr = graph.edge_attr[edge_mask]\n",
    "        edge_label = graph.edge_label[edge_mask]\n",
    "\n",
    "        yield batch_nodes, edge_index, edge_attr, edge_label\n",
    "\n",
    "best_f1 = 0\n",
    "best_model_state = None\n",
    "\n",
    "# Load checkpoint if exists\n",
    "start_epoch = 0\n",
    "epochs = 200\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = th.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "    print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "batch_size = 256\n",
    "min_nodes = 20\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    print(f'epoch : {epoch}')\n",
    "    all_preds_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    train_loss = 0\n",
    "    train_num_batches = 0\n",
    "\n",
    "    val_loss = 0\n",
    "    val_num_batches = 0\n",
    "    \n",
    "    try:\n",
    "        model.train()\n",
    "        for batch_idx, (batch_nodes, edge_index, edge_attr, edge_label) in enumerate(generate_edge_based_batches_with_node_expansion(G_pyg_train, batch_size, min_nodes)):\n",
    "            # print(f\"Processing epoch {epoch}, batch {batch_idx} with {batch_nodes.size(0)} nodes and {edge_index.size(1)} edges\")\n",
    "            batch = Data(x=G_pyg_train.x[batch_nodes], edge_index=edge_index, edge_attr=edge_attr, edge_label=edge_label)\n",
    "            \n",
    "            if batch.edge_index.size(1) == 0 or batch.edge_label.size(0) == 0:\n",
    "                print(f\"Warning: Empty batch at batch {batch_idx}\")\n",
    "                continue\n",
    "                \n",
    "            if batch is None or batch.num_nodes == 0:\n",
    "                print(f\"Warning: Empty batch at Batch {batch_idx}\")\n",
    "                continue \n",
    "    \n",
    "            if th.isnan(batch.x).any() or th.isinf(batch.x).any() or th.isnan(batch.edge_attr).any() or th.isinf(batch.edge_attr).any():\n",
    "                print(f\"Warning: batch x and edge_attr contains NaN or Inf at Batch {batch_idx}\")\n",
    "                continue \n",
    "                \n",
    "            try:\n",
    "                batch = batch.to(device)\n",
    "            except Exception as batch_error:\n",
    "                print(f\"Error moving batch to device at Batch {batch_idx}: {batch_error}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                out = model(batch)\n",
    "    \n",
    "                if th.isnan(out).any() or th.isinf(out).any():\n",
    "                    print(f\"Warning: out contains NaN or Inf at Batch {batch_idx}\")\n",
    "                    continue \n",
    "                all_preds_logits.append(out)\n",
    "                all_labels.append(batch.edge_label)\n",
    "    \n",
    "                loss = criterion(out, batch.edge_label)\n",
    "                train_loss += loss.item()\n",
    "                train_num_batches += 1\n",
    "                if th.isnan(loss):\n",
    "                    print(f\"loss: {loss}\")\n",
    "                    print(f\"out: {out}\")\n",
    "                    print(f\"edge_labels: {batch.edge_label}\")\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            except Exception as forward_error:\n",
    "                print(f\"Error during forward/backward pass at Epoch {epoch}, Batch {batch_idx}: {forward_error}\")\n",
    "                continue\n",
    "        \n",
    "        all_preds_logits = th.cat(all_preds_logits)\n",
    "        all_labels = th.cat(all_labels)\n",
    "        \n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        with th.no_grad():\n",
    "            for batch_idx, (batch_nodes, edge_index, edge_attr, edge_label) in enumerate(generate_edge_based_batches_with_node_expansion(G_pyg_val, batch_size, min_nodes)):\n",
    "                # print(f\"Processing epoch {epoch}, batch {batch_idx} with {batch_nodes.size(0)} nodes and {edge_index.size(1)} edges\")\n",
    "                batch = Data(x=G_pyg_val.x[batch_nodes], edge_index=edge_index, edge_attr=edge_attr, edge_label=edge_label)\n",
    "\n",
    "                if batch.edge_index.size(1) == 0 or batch.edge_label.size(0) == 0:\n",
    "                    print(f\"Warning: Empty batch at batch {batch_idx}\")\n",
    "                    continue\n",
    "                    \n",
    "                if batch is None or batch.num_nodes == 0:\n",
    "                    print(f\"Warning: Empty batch at Batch {batch_idx}\")\n",
    "                    continue \n",
    "        \n",
    "                if th.isnan(batch.x).any() or th.isinf(batch.x).any() or th.isnan(batch.edge_attr).any() or th.isinf(batch.edge_attr).any():\n",
    "                    print(f\"Warning: batch x and edge_attr contains NaN or Inf at Batch {batch_idx}\")\n",
    "                    continue \n",
    "                    \n",
    "                try:\n",
    "                    batch = batch.to(device)\n",
    "                except Exception as batch_error:\n",
    "                    print(f\"Error moving batch to device at Batch {batch_idx}: {batch_error}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    out = model(batch)\n",
    "        \n",
    "                    if th.isnan(out).any() or th.isinf(out).any():\n",
    "                        print(f\"Warning: out contains NaN or Inf at Batch {batch_idx}\")\n",
    "                        continue \n",
    "                    val_preds.append(out.argmax(dim=1))\n",
    "                    val_labels.append(batch.edge_label)\n",
    "\n",
    "                    loss = criterion(out, batch.edge_label)\n",
    "                    val_loss += loss.item()\n",
    "                    val_num_batches += 1\n",
    "                    \n",
    "                except Exception as forward_error:\n",
    "                    print(f\"Error during validation at Epoch {epoch}, Batch {batch_idx}: {forward_error}\")\n",
    "                    continue\n",
    "\n",
    "        val_preds = th.cat(val_preds)\n",
    "        val_labels = th.cat(val_labels)\n",
    "        val_f1 = f1_score(val_labels.cpu(), val_preds.cpu(), average='weighted')\n",
    "\n",
    "        train_loss /= train_num_batches\n",
    "        val_loss /= val_num_batches\n",
    "\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "        # Save the best model\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_state = model.state_dict()\n",
    "            th.save(best_model_state, best_model_path)\n",
    "            print(\"Saved best model.\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        th.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_f1': best_f1\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred at epoch {epoch}, batch {batch_idx}: {str(e)}\")\n",
    "print(\"Training is over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac63f9fc-02a2-4e16-94c6-bef7bf80ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.save(model.state_dict(), final_epoch_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8db72-643a-476a-8c99-5c711869c1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f271a-612b-4cd6-a85a-e4236dec9d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg_test: 81474\n",
      "Number of node in G_pyg_test: 52280\n",
      "Shape of node in G_pyg_test: torch.Size([52280, 53])\n",
      "Shape of edge attr in G_pyg_test: torch.Size([81474, 53])\n",
      "Shape of edge label in G_pyg_test: torch.Size([81474])\n",
      "inference start\n",
      "inference done\n",
      "Test Accuracy: 0.7920\n",
      "[[   56     4     0     2    28   238     0     2   148     0     7]\n",
      " [    0     9     0     0     5   169     0     0   143    22     6]\n",
      " [    0     1     0     0     1    75     0     0     0     2     1]\n",
      " [   10    23     3    40   251  1450     1     0  1130   103    71]\n",
      " [   43    52     8    43  2683  1836     0     2  1621   397   575]\n",
      " [    6    11     1     7    50  2614     0     0   298   698    52]\n",
      " [   14     5     2    11   221  8103 36704     0   174    71   117]\n",
      " [   22     7     3     0    39   278     0 34948    13   127    11]\n",
      " [    1     7     0     4    15   287     0     0   571  1168   137]\n",
      " [    0     0     1     0     0     9     0     0     1   215     0]\n",
      " [    0     0     0     0     1     1     0     0     2     2    20]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.3684    0.1155    0.1758       485\n",
      "      Backdoor     0.0756    0.0254    0.0381       354\n",
      "     Backdoors     0.0000    0.0000    0.0000        80\n",
      "           DoS     0.3738    0.0130    0.0251      3082\n",
      "      Exploits     0.8145    0.3696    0.5084      7260\n",
      "       Fuzzers     0.1736    0.6995    0.2781      3737\n",
      "       Generic     1.0000    0.8081    0.8938     45422\n",
      "        Normal     0.9999    0.9859    0.9928     35448\n",
      "Reconnaissance     0.1392    0.2607    0.1815      2190\n",
      "     Shellcode     0.0766    0.9513    0.1419       226\n",
      "         Worms     0.0201    0.7692    0.0391        26\n",
      "\n",
      "      accuracy                         0.7920     98310\n",
      "     macro avg     0.3674    0.4544    0.2977     98310\n",
      "  weighted avg     0.9064    0.7920    0.8253     98310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "\n",
    "\n",
    "def eval(dataframe, adversarial=False):\n",
    "    G_nx_test = nx.from_pandas_edgelist(dataframe, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "\n",
    "    G_pyg_test = from_networkx(G_nx_test)\n",
    "\n",
    "    test_num_nodes = G_pyg_test.num_nodes\n",
    "    test_num_edges = G_pyg_test.num_edges\n",
    "\n",
    "    G_pyg_test.x = th.ones(test_num_nodes, len(test_df['h'].iloc[0]))\n",
    "\n",
    "    test_edge_attr_list = []\n",
    "    test_edge_label_list = []\n",
    "\n",
    "    for u, v, key, data in G_nx_test.edges(keys=True, data=True):\n",
    "        test_edge_attr_list.append(data['h']) \n",
    "        test_edge_label_list.append(data[label_col]) \n",
    "\n",
    "    G_pyg_test.edge_attr = th.tensor(test_edge_attr_list, dtype=th.float32)\n",
    "    G_pyg_test.edge_label = th.tensor(test_edge_label_list, dtype=th.long)\n",
    "\n",
    "    print(\"Number of edges in G_pyg_test:\", G_pyg_test.num_edges)\n",
    "    print(\"Number of node in G_pyg_test:\", G_pyg_test.num_nodes)\n",
    "    print(\"Shape of node in G_pyg_test:\", G_pyg_test.x.shape)\n",
    "    print(\"Shape of edge attr in G_pyg_test:\", G_pyg_test.edge_attr.shape)\n",
    "    print(\"Shape of edge label in G_pyg_test:\", G_pyg_test.edge_label.shape)\n",
    "    \n",
    "    new_model_2 = EGraphSAGE(node_in_channels=G_pyg_test.num_node_features, \n",
    "                       edge_in_channels=G_pyg_test.num_edge_features,\n",
    "                       hidden_channels=128, \n",
    "                       out_channels=num_classes).to(device)\n",
    "\n",
    "    new_model_2.load_state_dict(th.load(best_model_path, weights_only=True))\n",
    "\n",
    "    new_model_2.eval()\n",
    "\n",
    "    all_test_preds_logits = []\n",
    "    all_test_labels = []\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "    print(\"inference start\")\n",
    "    with th.no_grad():\n",
    "        for batch_idx, (batch_nodes, edge_index, edge_attr, edge_label) in enumerate(generate_edge_based_batches_with_node_expansion(G_pyg_test, batch_size, 20)):\n",
    "            # print(f\"Processing batch {batch_idx} with {batch_nodes.size(0)} nodes and {edge_index.size(1)} edges\")\n",
    "            batch = Data(x=G_pyg_test.x[batch_nodes], edge_index=edge_index, edge_attr=edge_attr, edge_label=edge_label)\n",
    "            \n",
    "            if batch.edge_index.size(1) == 0 or batch.edge_label.size(0) == 0:\n",
    "                print(f\"Warning: Empty batch at batch {batch_idx}\")\n",
    "                continue\n",
    "                \n",
    "            if batch is None or batch.num_nodes == 0:\n",
    "                print(f\"Warning: Empty batch at Batch {batch_idx}\")\n",
    "                continue\n",
    "\n",
    "            if th.isnan(batch.x).any() or th.isinf(batch.x).any() or th.isnan(batch.edge_attr).any() or th.isinf(batch.edge_attr).any():\n",
    "                print(f\"Warning: batch x and edge_attr contains NaN or Inf at Batch {batch_idx}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                batch = batch.to(device)\n",
    "            except Exception as batch_error:\n",
    "                print(f\"Error moving batch to device at Batch {batch_idx}: {batch_error}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                out = new_model_2(batch)\n",
    "\n",
    "                if th.isnan(out).any() or th.isinf(out).any():\n",
    "                    print(f\"Warning: out contains NaN or Inf at Batch {batch_idx}\")\n",
    "                    continue \n",
    "                \n",
    "                all_test_preds_logits.append(out)\n",
    "                all_test_labels.append(edge_label)\n",
    "\n",
    "            except Exception as forward_error:\n",
    "                print(f\"Error during forward/backward pass at Batch {batch_idx}: {forward_error}\")\n",
    "                continue\n",
    "\n",
    "    print(\"inference done\")\n",
    "    all_test_preds_logits = th.cat(all_test_preds_logits).to(device)\n",
    "    all_test_labels = th.cat(all_test_labels).to(device)\n",
    "\n",
    "    test_accuracy = compute_accuracy(all_test_preds_logits, all_test_labels)\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    pred_labels = all_test_preds_logits.argmax(dim=1).cpu()\n",
    "    all_test_labels = all_test_labels.cpu()\n",
    "\n",
    "    \n",
    "    global class_map\n",
    "    class_map_2 = class_map\n",
    "    if adversarial:\n",
    "        class_map_2 = np.append(class_map, \"Adversarial\")\n",
    "\n",
    "    # Generate a report\n",
    "    cm = confusion_matrix(all_test_labels, pred_labels, labels=range(len(class_map_2)))\n",
    "    print(cm)\n",
    "    report = classification_report(all_test_labels, pred_labels, target_names=class_map_2, digits=4)\n",
    "    print(report)\n",
    "\n",
    "eval(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f83fbbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg_test: 105916\n",
      "Number of node in G_pyg_test: 52285\n",
      "Shape of node in G_pyg_test: torch.Size([52285, 53])\n",
      "Shape of edge attr in G_pyg_test: torch.Size([105916, 53])\n",
      "Shape of edge label in G_pyg_test: torch.Size([105916])\n",
      "inference start\n",
      "inference done\n",
      "Test Accuracy: 0.6036\n",
      "[[   56     4     0     2    28   238     0     2   148     0     7     0]\n",
      " [    0     9     0     0     5   169     0     0   143    22     6     0]\n",
      " [    0     1     0     0     1    75     0     0     0     2     1     0]\n",
      " [   10    23     3    40   251  1450     1     0  1130   103    71     0]\n",
      " [   43    52     8    43  2683  1836     0     2  1621   397   575     0]\n",
      " [    6    11     1     7    50  2614     0     0   298   698    52     0]\n",
      " [   14     5     2    11   221  8103 36704     0   174    71   117     0]\n",
      " [   22     7     3     0    39   278     0 34948    13   127    11     0]\n",
      " [    1     7     0     4    15   287     0     0   571  1168   137     0]\n",
      " [    0     0     1     0     0     9     0     0     1   215     0     0]\n",
      " [    0     0     0     0     1     1     0     0     2     2    20     0]\n",
      " [   48    23     7    21  1766  8578 16721     2  1549  1575   389     0]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.2800    0.1155    0.1635       485\n",
      "      Backdoor     0.0634    0.0254    0.0363       354\n",
      "     Backdoors     0.0000    0.0000    0.0000        80\n",
      "           DoS     0.3125    0.0130    0.0249      3082\n",
      "      Exploits     0.5302    0.3696    0.4356      7260\n",
      "       Fuzzers     0.1106    0.6995    0.1910      3737\n",
      "       Generic     0.6870    0.8081    0.7426     45422\n",
      "        Normal     0.9998    0.9859    0.9928     35448\n",
      "Reconnaissance     0.1011    0.2607    0.1457      2190\n",
      "     Shellcode     0.0491    0.9513    0.0934       226\n",
      "         Worms     0.0144    0.7692    0.0283        26\n",
      "   Adversarial     0.0000    0.0000    0.0000     30679\n",
      "\n",
      "      accuracy                         0.6036    128989\n",
      "     macro avg     0.2623    0.4165    0.2378    128989\n",
      "  weighted avg     0.5602    0.6036    0.5683    128989\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def attack_attacker(dataframe, ratio, num_injected_nodes):\n",
    "    attack_eval = dataframe[dataframe[label_col] != class_dict[UNSW_NB15_Config.BENIGN_CLASS_NAME]]\n",
    "    num_injected = int(ratio * len(dataframe))\n",
    "\n",
    "    # Sample attack rows\n",
    "    sampled_attack_flows = attack_eval.sample(n=num_injected, random_state=42).copy().reset_index(drop=True)\n",
    "\n",
    "    injected_rows = sampled_attack_flows.copy()\n",
    "    node_ips = [f\"192.168.1.{i+1}\" for i in range(num_injected_nodes)]\n",
    "    injected_rows[UNSW_NB15_Config.DESTINATION_IP_COL_NAME] = injected_rows[UNSW_NB15_Config.SOURCE_IP_COL_NAME] # Target the Real Attacker Nodes\n",
    "    injected_rows[UNSW_NB15_Config.SOURCE_IP_COL_NAME] = [node_ips[i % len(node_ips)] for i in range(num_injected)]\n",
    "    # injected_rows['pkSeqID'] = [f'Injected-{i}' for i in range(num_injected)]\n",
    "    injected_rows[label_col] = len(class_map) # Assign a new class for injected samples\n",
    "\n",
    "    # Append and reorder\n",
    "    combined_df = pd.concat([dataframe, injected_rows], ignore_index=True)\n",
    "\n",
    "    # Sort using this datetime column\n",
    "    combined_df = combined_df.sort_values(by=UNSW_NB15_Config.TIME_COL_NAMES).reset_index(drop=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Inject adversarial samples\n",
    "attack_attacker_df = attack_attacker(test_df, 0.3, num_injected_nodes=5)\n",
    "eval(attack_attacker_df, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a6707d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Flows: 33282\n",
      "Attack Flows: 48192\n",
      "Number of edges in G_pyg_test: 105916\n",
      "Number of node in G_pyg_test: 52285\n",
      "Shape of node in G_pyg_test: torch.Size([52285, 53])\n",
      "Shape of edge attr in G_pyg_test: torch.Size([105916, 53])\n",
      "Shape of edge label in G_pyg_test: torch.Size([105916])\n",
      "inference start\n",
      "inference done\n",
      "Test Accuracy: 0.6036\n",
      "[[   56     4     0     2    28   238     0     2   148     0     7     0]\n",
      " [    0     9     0     0     5   169     0     0   143    22     6     0]\n",
      " [    0     1     0     0     1    75     0     0     0     2     1     0]\n",
      " [   10    23     3    40   251  1450     1     0  1130   103    71     0]\n",
      " [   43    52     8    43  2683  1836     0     2  1621   397   575     0]\n",
      " [    6    11     1     7    50  2614     0     0   298   698    52     0]\n",
      " [   14     5     2    11   221  8103 36704     0   174    71   117     0]\n",
      " [   22     7     3     0    39   278     0 34948    13   127    11     0]\n",
      " [    1     7     0     4    15   287     0     0   571  1168   137     0]\n",
      " [    0     0     1     0     0     9     0     0     1   215     0     0]\n",
      " [    0     0     0     0     1     1     0     0     2     2    20     0]\n",
      " [    5     4     1     0    10    36     0 30488     2   133     0     0]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.3567    0.1155    0.1745       485\n",
      "      Backdoor     0.0732    0.0254    0.0377       354\n",
      "     Backdoors     0.0000    0.0000    0.0000        80\n",
      "           DoS     0.3738    0.0130    0.0251      3082\n",
      "      Exploits     0.8120    0.3696    0.5080      7260\n",
      "       Fuzzers     0.1732    0.6995    0.2776      3737\n",
      "       Generic     1.0000    0.8081    0.8938     45422\n",
      "        Normal     0.5340    0.9859    0.6928     35448\n",
      "Reconnaissance     0.1392    0.2607    0.1815      2190\n",
      "     Shellcode     0.0732    0.9513    0.1359       226\n",
      "         Worms     0.0201    0.7692    0.0391        26\n",
      "   Adversarial     0.0000    0.0000    0.0000     30679\n",
      "\n",
      "      accuracy                         0.6036    128989\n",
      "     macro avg     0.2963    0.4165    0.2472    128989\n",
      "  weighted avg     0.5626    0.6036    0.5465    128989\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def normalise_attacker(dataframe, ratio, num_injected_nodes):\n",
    "\n",
    "    normal_eval = dataframe[dataframe[label_col] == class_dict[UNSW_NB15_Config.BENIGN_CLASS_NAME]]\n",
    "    attack_eval = dataframe[dataframe[label_col] != class_dict[UNSW_NB15_Config.BENIGN_CLASS_NAME]]\n",
    "    print(\"Normal Flows:\", len(normal_eval))\n",
    "    print(\"Attack Flows:\", len(attack_eval))\n",
    "    num_injected = int(ratio * len(dataframe))\n",
    "\n",
    "    sampled_normal_flows = normal_eval.sample(n=num_injected, random_state=42).copy().reset_index(drop=True)\n",
    "    sampled_attack_flows = attack_eval.sample(n=num_injected, random_state=42).copy().reset_index(drop=True)\n",
    "\n",
    "    injected_rows = sampled_normal_flows.copy()\n",
    "    node_ips = [f\"192.168.1.{i+1}\" for i in range(num_injected_nodes)]\n",
    "    injected_rows[UNSW_NB15_Config.DESTINATION_IP_COL_NAME] = sampled_attack_flows[UNSW_NB15_Config.SOURCE_IP_COL_NAME] # Direct BENGIN Traffic to the Real Attacker Nodes\n",
    "    injected_rows[UNSW_NB15_Config.SOURCE_IP_COL_NAME] = [node_ips[i % len(node_ips)] for i in range(num_injected)]\n",
    "    injected_rows[label_col] =len(class_map)\n",
    "\n",
    "    combined_df = pd.concat([dataframe, injected_rows], ignore_index=True)\n",
    "\n",
    "    # Sort using this datetime column\n",
    "    combined_df = combined_df.sort_values(by=UNSW_NB15_Config.TIME_COL_NAMES).reset_index(drop=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Inject adversarial samples\n",
    "attack_attacker_df = normalise_attacker(test_df, 0.3, 5)\n",
    "eval(attack_attacker_df, adversarial=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
