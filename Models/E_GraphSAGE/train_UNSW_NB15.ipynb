{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb9e1b2-1c5f-49e3-82b4-3bfe52cabad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import from_networkx, add_self_loops, degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "# import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Datasets.UNSW_NB15.UNSW_NB15_config import UNSW_NB15_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9ef09a-d405-43b8-971e-fe9e6a592c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_648477/4196508244.py:1: DtypeWarning: Columns (1,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(os.path.join(project_root, \"Datasets\", \"UNSW_NB15/All/all_raw.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "Normal            2218764\n",
      "Generic            215481\n",
      "Exploits            44525\n",
      "Fuzzers             24246\n",
      "DoS                 16353\n",
      "Reconnaissance      13987\n",
      "Analysis             2677\n",
      "Backdoor             1795\n",
      "Shellcode            1511\n",
      "Backdoors             534\n",
      "Worms                 174\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(project_root, \"Datasets\", \"UNSW_NB15/All/all_raw.csv\"))\n",
    "\n",
    "DATASET_NAME = \"UNSW_NB15\"\n",
    "\n",
    "SOURCE_IP_COL_NAME = UNSW_NB15_Config.SOURCE_IP_COL_NAME\n",
    "DESTINATION_IP_COL_NAME = UNSW_NB15_Config.DESTINATION_IP_COL_NAME\n",
    "SOURCE_PORT_COL_NAME = UNSW_NB15_Config.SOURCE_PORT_COL_NAME\n",
    "DESTINATION_PORT_COL_NAME = UNSW_NB15_Config.DESTINATION_PORT_COL_NAME\n",
    "\n",
    "ATTACK_CLASS_COL_NAME = UNSW_NB15_Config.ATTACK_CLASS_COL_NAME\n",
    "IS_ATTACK_COL_NAME = UNSW_NB15_Config.IS_ATTACK_COL_NAME\n",
    "\n",
    "BENIGN_CLASS_NAME = UNSW_NB15_Config.BENIGN_CLASS_NAME\n",
    "print(data[ATTACK_CLASS_COL_NAME].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449a1af1-1d3d-4179-9628-7c2ec551ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=UNSW_NB15_Config.DROP_COLS,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb74f87-8df1-4c98-a849-e263e03a06f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    2218764\n",
      "1     321283\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[IS_ATTACK_COL_NAME].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a2c690c-86a4-49f7-aa9c-58f94529547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME].apply(str)\n",
    "    data[SOURCE_PORT_COL_NAME] = data[SOURCE_PORT_COL_NAME].apply(str)\n",
    "    data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME].apply(str)\n",
    "    data[DESTINATION_PORT_COL_NAME] = data[DESTINATION_PORT_COL_NAME].apply(str)\n",
    "    data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME] + ':' + data[SOURCE_PORT_COL_NAME]\n",
    "    data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME] + ':' + data[DESTINATION_PORT_COL_NAME]\n",
    "    data.drop(columns=[SOURCE_PORT_COL_NAME,DESTINATION_PORT_COL_NAME],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5651ef5b-0a9d-4641-aad7-5738092c46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                       srcip                dstip state       dur  sbytes  \\\n",
      "0           59.166.0.0:1390     149.171.126.6:53   CON  0.001055     132   \n",
      "1          59.166.0.0:33661   149.171.126.9:1024   CON  0.036133     528   \n",
      "2           59.166.0.6:1464     149.171.126.7:53   CON  0.001119     146   \n",
      "3           59.166.0.5:3593     149.171.126.5:53   CON  0.001209     132   \n",
      "4          59.166.0.3:49664     149.171.126.0:53   CON  0.001169     146   \n",
      "...                     ...                  ...   ...       ...     ...   \n",
      "2540042    59.166.0.5:33094  149.171.126.7:43433   FIN  0.087306     320   \n",
      "2540043    59.166.0.7:20848     149.171.126.4:21   CON  0.365058     456   \n",
      "2540044    59.166.0.3:21511     149.171.126.9:21   CON  6.335154    1802   \n",
      "2540045    59.166.0.9:35433     149.171.126.0:80   CON  2.200934    3498   \n",
      "2540046  175.45.176.0:17293   149.171.126.17:110   CON  0.942984     574   \n",
      "\n",
      "         dbytes  sttl  dttl  sloss  dloss  ...  ct_ftp_cmd  ct_srv_src  \\\n",
      "0           164    31    29      0      0  ...         0.0           3   \n",
      "1           304    31    29      0      0  ...         0.0           2   \n",
      "2           178    31    29      0      0  ...         0.0          12   \n",
      "3           164    31    29      0      0  ...         0.0           6   \n",
      "4           178    31    29      0      0  ...         0.0           7   \n",
      "...         ...   ...   ...    ...    ...  ...         ...         ...   \n",
      "2540042    1828    31    29      1      2  ...         NaN           1   \n",
      "2540043     346    31    29      2      2  ...         2.0           2   \n",
      "2540044    2088    31    29      7      9  ...         2.0           2   \n",
      "2540045  166054    31    29      2     57  ...         NaN           1   \n",
      "2540046     676    62   252      5      6  ...         NaN           1   \n",
      "\n",
      "         ct_srv_dst  ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  \\\n",
      "0                 7           1           3                 1   \n",
      "1                 4           2           3                 1   \n",
      "2                 8           1           2                 2   \n",
      "3                 9           1           1                 1   \n",
      "4                 9           1           1                 1   \n",
      "...             ...         ...         ...               ...   \n",
      "2540042           2           3           3                 1   \n",
      "2540043           2           2           2                 2   \n",
      "2540044           2           4           2                 2   \n",
      "2540045           1           2           4                 2   \n",
      "2540046           1           2           4                 2   \n",
      "\n",
      "         ct_dst_sport_ltm  ct_dst_src_ltm  attack_cat  label  \n",
      "0                       1               1      Normal      0  \n",
      "1                       1               2      Normal      0  \n",
      "2                       1               1      Normal      0  \n",
      "3                       1               1      Normal      0  \n",
      "4                       1               1      Normal      0  \n",
      "...                   ...             ...         ...    ...  \n",
      "2540042                 1               3      Normal      0  \n",
      "2540043                 2               2      Normal      0  \n",
      "2540044                 2               2      Normal      0  \n",
      "2540045                 2               2      Normal      0  \n",
      "2540046                 2               2    Exploits      1  \n",
      "\n",
      "[2540047 rows x 45 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb7fb458-ca34-42ca-a8af-f8e1609aff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ground_truth = data[[SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ATTACK_CLASS_COL_NAME]]\n",
    "data = pd.get_dummies(data, columns = UNSW_NB15_Config.CATEGORICAL_COLS) # One Hot Encoding for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d96115-31f9-48cb-b3e6-7853d2d253cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                       srcip                dstip       dur  sbytes  dbytes  \\\n",
      "0           59.166.0.0:1390     149.171.126.6:53  0.001055     132     164   \n",
      "1          59.166.0.0:33661   149.171.126.9:1024  0.036133     528     304   \n",
      "2           59.166.0.6:1464     149.171.126.7:53  0.001119     146     178   \n",
      "3           59.166.0.5:3593     149.171.126.5:53  0.001209     132     164   \n",
      "4          59.166.0.3:49664     149.171.126.0:53  0.001169     146     178   \n",
      "...                     ...                  ...       ...     ...     ...   \n",
      "2540042    59.166.0.5:33094  149.171.126.7:43433  0.087306     320    1828   \n",
      "2540043    59.166.0.7:20848     149.171.126.4:21  0.365058     456     346   \n",
      "2540044    59.166.0.3:21511     149.171.126.9:21  6.335154    1802    2088   \n",
      "2540045    59.166.0.9:35433     149.171.126.0:80  2.200934    3498  166054   \n",
      "2540046  175.45.176.0:17293   149.171.126.17:110  0.942984     574     676   \n",
      "\n",
      "         sttl  dttl  sloss  dloss          Sload  ...  state_INT  state_MAS  \\\n",
      "0          31    29      0      0  500473.937500  ...      False      False   \n",
      "1          31    29      0      0   87676.085940  ...      False      False   \n",
      "2          31    29      0      0  521894.531300  ...      False      False   \n",
      "3          31    29      0      0  436724.562500  ...      False      False   \n",
      "4          31    29      0      0  499572.250000  ...      False      False   \n",
      "...       ...   ...    ...    ...            ...  ...        ...        ...   \n",
      "2540042    31    29      1      2   24465.671880  ...      False      False   \n",
      "2540043    31    29      2      2    8743.816406  ...      False      False   \n",
      "2540044    31    29      7      9    2204.839844  ...      False      False   \n",
      "2540045    31    29      2     57   12496.513670  ...      False      False   \n",
      "2540046    62   252      5      6    4470.913574  ...      False      False   \n",
      "\n",
      "         state_PAR  state_REQ  state_RST  state_TST  state_TXD  state_URH  \\\n",
      "0            False      False      False      False      False      False   \n",
      "1            False      False      False      False      False      False   \n",
      "2            False      False      False      False      False      False   \n",
      "3            False      False      False      False      False      False   \n",
      "4            False      False      False      False      False      False   \n",
      "...            ...        ...        ...        ...        ...        ...   \n",
      "2540042      False      False      False      False      False      False   \n",
      "2540043      False      False      False      False      False      False   \n",
      "2540044      False      False      False      False      False      False   \n",
      "2540045      False      False      False      False      False      False   \n",
      "2540046      False      False      False      False      False      False   \n",
      "\n",
      "         state_URN  state_no  \n",
      "0            False     False  \n",
      "1            False     False  \n",
      "2            False     False  \n",
      "3            False     False  \n",
      "4            False     False  \n",
      "...            ...       ...  \n",
      "2540042      False     False  \n",
      "2540043      False     False  \n",
      "2540044      False     False  \n",
      "2540045      False     False  \n",
      "2540046      False     False  \n",
      "\n",
      "[2540047 rows x 60 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "data.drop(columns=['index'],inplace=True)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1df5c4c-70a2-4566-ae5e-ee3dcc6037a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                dur        sbytes        dbytes          sttl          dttl  \\\n",
      "count  2.540047e+06  2.540047e+06  2.540047e+06  2.540047e+06  2.540047e+06   \n",
      "mean   6.587916e-01  4.339600e+03  3.642759e+04  6.278197e+01  3.076681e+01   \n",
      "std    1.392493e+01  5.640599e+04  1.610960e+05  7.462277e+01  4.285089e+01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    1.037000e-03  2.000000e+02  1.780000e+02  3.100000e+01  2.900000e+01   \n",
      "50%    1.586100e-02  1.470000e+03  1.820000e+03  3.100000e+01  2.900000e+01   \n",
      "75%    2.145545e-01  3.182000e+03  1.489400e+04  3.100000e+01  2.900000e+01   \n",
      "max    8.786638e+03  1.435577e+07  1.465753e+07  2.550000e+02  2.540000e+02   \n",
      "\n",
      "              sloss         dloss         Sload         Dload         Spkts  \\\n",
      "count  2.540047e+06  2.540047e+06  2.540047e+06  2.540047e+06  2.540047e+06   \n",
      "mean   5.163921e+00  1.632944e+01  3.695645e+07  2.450861e+06  3.328884e+01   \n",
      "std    2.251707e+01  5.659474e+01  1.186043e+08  4.224863e+06  7.628388e+01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00  0.000000e+00  1.353963e+05  1.191594e+04  2.000000e+00   \n",
      "50%    3.000000e+00  4.000000e+00  5.893038e+05  5.893179e+05  1.200000e+01   \n",
      "75%    7.000000e+00  1.400000e+01  2.039923e+06  2.925974e+06  4.400000e+01   \n",
      "max    5.319000e+03  5.507000e+03  5.988000e+09  1.287619e+08  1.064600e+04   \n",
      "\n",
      "       ...  ct_flw_http_mthd  is_ftp_login    ct_ftp_cmd    ct_srv_src  \\\n",
      "count  ...      2.540047e+06  2.540047e+06  2.540047e+06  2.540047e+06   \n",
      "mean   ...      1.100779e-01  1.735125e-02  2.055789e-02  9.206988e+00   \n",
      "std    ...      5.564195e-01  1.334571e-01  1.843620e-01  1.083676e+01   \n",
      "min    ...      0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00   \n",
      "25%    ...      0.000000e+00  0.000000e+00  0.000000e+00  2.000000e+00   \n",
      "50%    ...      0.000000e+00  0.000000e+00  0.000000e+00  5.000000e+00   \n",
      "75%    ...      0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+01   \n",
      "max    ...      3.600000e+01  4.000000e+00  8.000000e+00  6.700000e+01   \n",
      "\n",
      "         ct_srv_dst    ct_dst_ltm    ct_src_ltm  ct_src_dport_ltm  \\\n",
      "count  2.540047e+06  2.540047e+06  2.540047e+06      2.540047e+06   \n",
      "mean   8.988958e+00  6.439103e+00  6.900986e+00      4.642139e+00   \n",
      "std    1.082249e+01  8.162034e+00  8.205062e+00      8.477579e+00   \n",
      "min    1.000000e+00  1.000000e+00  1.000000e+00      1.000000e+00   \n",
      "25%    2.000000e+00  2.000000e+00  2.000000e+00      1.000000e+00   \n",
      "50%    5.000000e+00  3.000000e+00  4.000000e+00      1.000000e+00   \n",
      "75%    1.000000e+01  6.000000e+00  7.000000e+00      2.000000e+00   \n",
      "max    6.700000e+01  6.700000e+01  6.700000e+01      6.700000e+01   \n",
      "\n",
      "       ct_dst_sport_ltm  ct_dst_src_ltm  \n",
      "count      2.540047e+06    2.540047e+06  \n",
      "mean       3.592729e+00    6.845886e+00  \n",
      "std        6.174445e+00    1.125828e+01  \n",
      "min        1.000000e+00    1.000000e+00  \n",
      "25%        1.000000e+00    1.000000e+00  \n",
      "50%        1.000000e+00    2.000000e+00  \n",
      "75%        1.000000e+00    5.000000e+00  \n",
      "max        6.000000e+01    6.700000e+01  \n",
      "\n",
      "[8 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_norm = UNSW_NB15_Config.COLS_TO_NORM\n",
    "print(data[cols_to_norm].describe()) # Check if there's any too large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37256006-abc1-44aa-8e74-46d05dc6ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[cols_to_norm] = data[cols_to_norm].clip(lower=-1e9, upper=1e9)\n",
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61c6e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis' 'Backdoor' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic'\n",
      " 'Normal' 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "Attack label mapping: {'Analysis': 0, 'Backdoor': 1, 'Backdoors': 2, 'DoS': 3, 'Exploits': 4, 'Fuzzers': 5, 'Generic': 6, 'Normal': 7, 'Reconnaissance': 8, 'Shellcode': 9, 'Worms': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "attack_labels = le.fit_transform(data[ATTACK_CLASS_COL_NAME])\n",
    "class_map = le.classes_\n",
    "print(class_map)\n",
    "print(\"Attack label mapping:\", dict(zip(class_map, range(len(class_map)))))\n",
    "data[ATTACK_CLASS_COL_NAME] = attack_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d35f4cdd-2716-431f-af50-b34cc3d2d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1524028\n",
      "1016019\n",
      "attack_cat\n",
      "Normal            1331258\n",
      "Generic            129289\n",
      "Exploits            26715\n",
      "Fuzzers             14548\n",
      "DoS                  9812\n",
      "Reconnaissance       8392\n",
      "Analysis             1606\n",
      "Backdoor             1077\n",
      "Shellcode             907\n",
      "Backdoors             320\n",
      "Worms                 104\n",
      "Name: count, dtype: int64\n",
      "attack_cat\n",
      "Normal            887506\n",
      "Generic            86192\n",
      "Exploits           17810\n",
      "Fuzzers             9698\n",
      "DoS                 6541\n",
      "Reconnaissance      5595\n",
      "Analysis            1071\n",
      "Backdoor             718\n",
      "Shellcode            604\n",
      "Backdoors            214\n",
      "Worms                 70\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     data, class_ground_truth, test_size=0.4, random_state=42, stratify=class_ground_truth[UNSW_NB15_Config.ATTACK_CLASS_COL_NAME])\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(y_train[UNSW_NB15_Config.ATTACK_CLASS_COL_NAME].value_counts())\n",
    "print(y_test[UNSW_NB15_Config.ATTACK_CLASS_COL_NAME].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f83bd73-531f-42a0-9f73-e1fd98dce1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['h'] = X_train[ cols_to_norm ].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c8f9cdb-1316-461a-a927-8d67d90d6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 1524028\n",
      "Number of node in G_pyg: 890219\n",
      "Shape of node in G_pyg: torch.Size([890219, 40])\n",
      "Shape of edge attr in G_pyg: torch.Size([1524028, 40])\n",
      "Shape of edge class in G_pyg: torch.Size([1524028])\n"
     ]
    }
   ],
   "source": [
    "# Convert NetworkX graph to PyG graph\n",
    "G_nx = nx.from_pandas_edgelist(X_train, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', ATTACK_CLASS_COL_NAME], create_using=nx.MultiDiGraph())\n",
    "G_pyg = from_networkx(G_nx)\n",
    "\n",
    "num_nodes = G_pyg.num_nodes\n",
    "num_edges = G_pyg.num_edges\n",
    "\n",
    "G_pyg.x = th.ones(num_nodes, len(X_train['h'].iloc[0])) \n",
    "\n",
    "edge_attr_list = []\n",
    "edge_class_list = []\n",
    "\n",
    "for u, v, key, data in G_nx.edges(keys=True, data=True):\n",
    "    edge_attr_list.append(data['h']) \n",
    "    edge_class_list.append(data[ATTACK_CLASS_COL_NAME]) \n",
    "\n",
    "G_pyg.edge_attr = th.tensor(edge_attr_list, dtype=th.float32)\n",
    "G_pyg.edge_class = th.tensor(edge_class_list, dtype=th.long)\n",
    "\n",
    "print(\"Number of edges in G_pyg:\", G_pyg.num_edges)\n",
    "print(\"Number of node in G_pyg:\", G_pyg.num_nodes)\n",
    "print(\"Shape of node in G_pyg:\", G_pyg.x.shape)\n",
    "print(\"Shape of edge attr in G_pyg:\", G_pyg.edge_attr.shape)\n",
    "print(\"Shape of edge class in G_pyg:\", G_pyg.edge_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41795339-6036-468f-9b9d-2bb68d78ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EGraphSAGEConv(MessagePassing):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, out_channels):\n",
    "        super(EGraphSAGEConv, self).__init__(aggr='mean')  # mean aggregation\n",
    "        self.lin_node = nn.Linear(node_in_channels, out_channels)\n",
    "        self.lin_edge = nn.Linear(edge_in_channels, out_channels)\n",
    "        self.lin_update = nn.Linear(node_in_channels + out_channels, out_channels) # out_channels * 2\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: Node features, edge_attr: Edge features, edge_index: Connectivity\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        if edge_attr is not None:\n",
    "            if edge_attr.size(0) != edge_index.size(1):\n",
    "                loop_attr = th.zeros((edge_index.size(1) - edge_attr.size(0), edge_attr.size(1))).to(edge_attr.device)\n",
    "                edge_attr = th.cat([edge_attr, loop_attr], dim=0)\n",
    "        else:\n",
    "            print(\"edge_attr is unexist\")\n",
    "        \n",
    "        # Propagate and aggregate neighbor information\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j represents the adjacent nodes of x\n",
    "        # Compute messages by combining node and edge features\n",
    "        return self.lin_node(x_j) + self.lin_edge(edge_attr)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # Update node features after message passing\n",
    "        return self.lin_update(th.cat([x, aggr_out], dim=1))\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.lin = nn.Linear(in_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, data, z):\n",
    "        row, col = data.edge_index\n",
    "        # Concatenate the features of source and target nodes for each edge\n",
    "        edge_feat = th.cat([z[row], z[col]], dim=1)\n",
    "        return self.lin(edge_feat)\n",
    "\n",
    "class EGraphSAGE(nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels, out_channels):\n",
    "        super(EGraphSAGE, self).__init__()\n",
    "        self.conv1 = EGraphSAGEConv(node_in_channels, edge_in_channels, hidden_channels)\n",
    "        self.conv2 = EGraphSAGEConv(hidden_channels, edge_in_channels, hidden_channels)\n",
    "        self.mlp_predictor = MLPPredictor(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return self.mlp_predictor(data, x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bca25fef-29d9-40cf-8910-16b24d530693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cccdc850-b98d-4836-b82b-67aa4b9e1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89157faf-e24b-49d6-9c90-6f71dae515b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     13\u001b[39m labels = G_pyg.edge_class.cpu().numpy()\n\u001b[32m     14\u001b[39m class_weights = class_weight.compute_class_weight(\u001b[33m'\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m                                                   classes=np.unique(labels),\n\u001b[32m     16\u001b[39m                                                   y=labels)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m class_weights = \u001b[43mth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m criterion = nn.CrossEntropyLoss(weight = class_weights)\n\u001b[32m     20\u001b[39m optimizer = th.optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/shc20/FYP/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:293\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    292\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    297\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "model = EGraphSAGE(node_in_channels=G_pyg.num_node_features, \n",
    "                   edge_in_channels=G_pyg.num_edge_features,\n",
    "                   hidden_channels=128, \n",
    "                   out_channels=len(class_map)).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "labels = G_pyg.edge_class.cpu().numpy()\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.unique(labels),\n",
    "                                                  y=labels)\n",
    "\n",
    "class_weights = th.FloatTensor(class_weights).cuda()\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d37f0-713b-4abc-8d7a-3e768ae9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "G_pyg.edge_label = G_pyg.edge_label.to(device)\n",
    "G_pyg.edge_attr = G_pyg.edge_attr.to(device)\n",
    "\n",
    "def generate_edge_based_batches_with_node_expansion(graph, batch_size, min_nodes):\n",
    "    num_edges = graph.edge_index.size(1)  # Get total number of edges\n",
    "    edge_indices = th.arange(num_edges)   # Create list of edge indices\n",
    "    num_edges_processed = 0\n",
    "    \n",
    "    while num_edges_processed < num_edges:\n",
    "        # Select a batch of edges\n",
    "        batch_edge_indices = edge_indices[num_edges_processed : min(num_edges_processed + batch_size, num_edges)]\n",
    "        edge_index = graph.edge_index[:, batch_edge_indices]\n",
    "        \n",
    "        # Update the number of edges processed\n",
    "        num_edges_processed += batch_size\n",
    "        \n",
    "        # Get the unique nodes associated with these edges\n",
    "        batch_nodes = th.cat([edge_index[0], edge_index[1]]).unique()\n",
    "\n",
    "        # Check if the batch has enough unique nodes\n",
    "        while batch_nodes.size(0) < min_nodes:\n",
    "            # Sample additional neighboring nodes to ensure diversity\n",
    "            additional_edges = int(batch_size / 8)  # Ensure additional_edges is an integer\n",
    "            batch_edge_indices = th.cat([batch_edge_indices, edge_indices[num_edges_processed : min(num_edges_processed + additional_edges, num_edges)]])\n",
    "            edge_index = graph.edge_index[:, batch_edge_indices]\n",
    "            batch_nodes = th.cat([edge_index[0], edge_index[1]]).unique()\n",
    "            num_edges_processed += additional_edges\n",
    "\n",
    "            # Avoid potential infinite loops by breaking if no more edges can be added\n",
    "            if num_edges_processed >= num_edges:\n",
    "                break\n",
    "\n",
    "        # Create subgraph from the selected nodes and edges\n",
    "        edge_index, _, edge_mask = subgraph(batch_nodes, graph.edge_index, relabel_nodes=True, return_edge_mask=True)\n",
    "\n",
    "        # Use edge_mask to select edge attributes and labels\n",
    "        edge_attr = graph.edge_attr[edge_mask]\n",
    "        edge_label = graph.edge_label[edge_mask]\n",
    "\n",
    "        yield batch_nodes, edge_index, edge_attr, edge_label\n",
    "\n",
    "batch_size = 64\n",
    "for epoch in range(5):\n",
    "    print(f'epoch : {epoch}')\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    try:\n",
    "        for batch_idx, (batch_nodes, edge_index, edge_attr, edge_label) in enumerate(generate_edge_based_batches_with_node_expansion(G_pyg, batch_size, 20)):\n",
    "            # print(f\"Processing epoch {epoch}, batch {batch_idx} with {batch_nodes.size(0)} nodes and {edge_index.size(1)} edges\")\n",
    "            batch = Data(x=G_pyg.x[batch_nodes], edge_index=edge_index, edge_attr=edge_attr, edge_label=edge_label)\n",
    "            \n",
    "            if batch.edge_index.size(1) == 0 or batch.edge_label.size(0) == 0:\n",
    "                print(f\"Warning: Empty batch at batch {batch_idx}\")\n",
    "                continue\n",
    "                \n",
    "            if batch is None or batch.num_nodes == 0:\n",
    "                print(f\"Warning: Empty batch at Batch {batch_idx}\")\n",
    "                continue \n",
    "    \n",
    "            if th.isnan(batch.x).any() or th.isinf(batch.x).any() or th.isnan(batch.edge_attr).any() or th.isinf(batch.edge_attr).any():\n",
    "                print(f\"Warning: batch x and edge_attr contains NaN or Inf at Batch {batch_idx}\")\n",
    "                continue \n",
    "                \n",
    "            try:\n",
    "                batch = batch.to(device)\n",
    "            except Exception as batch_error:\n",
    "                print(f\"Error moving batch to device at Batch {batch_idx}: {batch_error}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                out = model(batch)\n",
    "    \n",
    "                if th.isnan(out).any() or th.isinf(out).any():\n",
    "                    print(f\"Warning: out contains NaN or Inf at Batch {batch_idx}\")\n",
    "                    continue \n",
    "                all_preds.append(out)\n",
    "                all_labels.append(batch.edge_label)\n",
    "    \n",
    "                loss = criterion(out, batch.edge_label)\n",
    "                if th.isnan(loss):\n",
    "                    print(f\"loss: {loss}\")\n",
    "                    print(f\"out: {out}\")\n",
    "                    print(f\"edge_labels: {batch.edge_label}\")\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            except Exception as forward_error:\n",
    "                print(f\"Error during forward/backward pass at Epoch {epoch}, Batch {batch_idx}: {forward_error}\")\n",
    "                continue\n",
    "        \n",
    "        all_preds = th.cat(all_preds)\n",
    "        all_labels = th.cat(all_labels)\n",
    "        \n",
    "        epoch_accuracy = compute_accuracy(all_preds, all_labels)\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        print(all_labels.shape)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred at epoch {epoch}, batch {batch_idx}: {str(e)}\")\n",
    "print(\"Training is over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63f9fc-02a2-4e16-94c6-bef7bf80ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.save(model.state_dict(), f\"./Weights/GNN_model_weights_{DATASET_NAME}_subset_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9742c0d1-0e23-4c5f-bcfc-c72a3f92f558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benign' 'Bot' 'Brute Force -Web' 'Brute Force -XSS' 'DDOS attack-HOIC'\n",
      " 'DDOS attack-LOIC-UDP' 'DDoS attacks-LOIC-HTTP' 'DoS attacks-GoldenEye'\n",
      " 'DoS attacks-Hulk' 'DoS attacks-SlowHTTPTest' 'DoS attacks-Slowloris'\n",
      " 'FTP-BruteForce' 'Infilteration' 'SQL Injection' 'SSH-Bruteforce']\n",
      "Attack label mapping: {'Benign': 0, 'Bot': 1, 'Brute Force -Web': 2, 'Brute Force -XSS': 3, 'DDOS attack-HOIC': 4, 'DDOS attack-LOIC-UDP': 5, 'DDoS attacks-LOIC-HTTP': 6, 'DoS attacks-GoldenEye': 7, 'DoS attacks-Hulk': 8, 'DoS attacks-SlowHTTPTest': 9, 'DoS attacks-Slowloris': 10, 'FTP-BruteForce': 11, 'Infilteration': 12, 'SQL Injection': 13, 'SSH-Bruteforce': 14}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "test_le = LabelEncoder()\n",
    "X_test[IS_ATTACK_COL_NAME] = test_le.fit_transform(X_test[IS_ATTACK_COL_NAME])\n",
    "test_class_map = test_le.classes_\n",
    "print(test_class_map)\n",
    "print(\"Attack label mapping:\", dict(zip(test_class_map, range(len(test_class_map)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8db72-643a-476a-8c99-5c711869c1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg_test: 1143400\n",
      "Number of node in G_pyg_test: 1080734\n",
      "Shape of node in G_pyg_test: torch.Size([1080734, 28])\n",
      "Shape of edge attr in G_pyg_test: torch.Size([1143400, 28])\n",
      "Shape of edge label in G_pyg_test: torch.Size([1143400])\n",
      "Shape of edge class in G_pyg_test: torch.Size([1143400])\n"
     ]
    }
   ],
   "source": [
    "X_test['h'] = X_test[ cols_to_norm ].values.tolist()\n",
    "\n",
    "G_nx_test = nx.from_pandas_edgelist(X_test, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', ATTACK_CLASS_COL_NAME, IS_ATTACK_COL_NAME], create_using=nx.MultiDiGraph())\n",
    "\n",
    "G_pyg_test = from_networkx(G_nx_test)\n",
    "\n",
    "test_num_nodes = G_pyg_test.num_nodes\n",
    "test_num_edges = G_pyg_test.num_edges\n",
    "\n",
    "G_pyg_test.x = th.ones(test_num_nodes, len(X_test['h'].iloc[0]))\n",
    "\n",
    "test_edge_attr_list = []\n",
    "test_edge_label_list = []\n",
    "test_edge_class_list = []\n",
    "\n",
    "for u, v, key, data in G_nx_test.edges(keys=True, data=True):\n",
    "    test_edge_attr_list.append(data['h']) \n",
    "    test_edge_label_list.append(data[ATTACK_CLASS_COL_NAME]) \n",
    "    test_edge_class_list.append(data[IS_ATTACK_COL_NAME])\n",
    "\n",
    "G_pyg_test.edge_attr = th.tensor(test_edge_attr_list, dtype=th.float32)\n",
    "G_pyg_test.edge_label = th.tensor(test_edge_label_list, dtype=th.long)\n",
    "G_pyg_test.edge_class = th.tensor(test_edge_class_list, dtype=th.long)\n",
    "\n",
    "print(\"Number of edges in G_pyg_test:\", G_pyg_test.num_edges)\n",
    "print(\"Number of node in G_pyg_test:\", G_pyg_test.num_nodes)\n",
    "print(\"Shape of node in G_pyg_test:\", G_pyg_test.x.shape)\n",
    "print(\"Shape of edge attr in G_pyg_test:\", G_pyg_test.edge_attr.shape)\n",
    "print(\"Shape of edge label in G_pyg_test:\", G_pyg_test.edge_label.shape)\n",
    "print(\"Shape of edge class in G_pyg_test:\", G_pyg_test.edge_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f271a-612b-4cd6-a85a-e4236dec9d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference start\n",
      "inference done\n",
      "torch.Size([1146184, 2])\n",
      "torch.Size([1146184])\n",
      "torch.Size([1146184])\n",
      "Test Accuracy: 0.5270\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "new_model_2 = EGraphSAGE(node_in_channels=G_pyg_test.num_node_features, \n",
    "                       edge_in_channels=G_pyg_test.num_edge_features,\n",
    "                       hidden_channels=128, \n",
    "                       out_channels=2).to(device)\n",
    "\n",
    "new_model_2.load_state_dict(th.load(f\"./Weights/GNN_model_weights_{DATASET_NAME}_subset_2.pth\", weights_only=True))\n",
    "\n",
    "def generate_edge_based_batches_with_node_expansion(graph, batch_size, min_nodes):\n",
    "    num_edges = graph.edge_index.size(1) \n",
    "    edge_indices = th.arange(num_edges)  \n",
    "    num_edges_processed = 0\n",
    "    \n",
    "    while num_edges_processed < num_edges:\n",
    "        # Select a batch of edges\n",
    "        batch_edge_indices = edge_indices[num_edges_processed : min(num_edges_processed + batch_size, num_edges)]\n",
    "        edge_index = graph.edge_index[:, batch_edge_indices]\n",
    "        \n",
    "        # Update the number of edges processed\n",
    "        num_edges_processed += batch_size\n",
    "        \n",
    "        # Get the unique nodes associated with these edges\n",
    "        batch_nodes = th.cat([edge_index[0], edge_index[1]]).unique()\n",
    "\n",
    "        # Check if the batch has enough unique nodes\n",
    "        while batch_nodes.size(0) < min_nodes:\n",
    "            # Sample additional neighboring nodes to ensure diversity\n",
    "            additional_edges = int(batch_size / 8)  # Ensure additional_edges is an integer\n",
    "            batch_edge_indices = th.cat([batch_edge_indices, edge_indices[num_edges_processed : min(num_edges_processed + additional_edges, num_edges)]])\n",
    "            edge_index = graph.edge_index[:, batch_edge_indices]\n",
    "            batch_nodes = th.cat([edge_index[0], edge_index[1]]).unique()\n",
    "            num_edges_processed += additional_edges\n",
    "\n",
    "            # Avoid potential infinite loops by breaking if no more edges can be added\n",
    "            if num_edges_processed >= num_edges:\n",
    "                break\n",
    "\n",
    "        # Create subgraph from the selected nodes and edges\n",
    "        edge_index, _, edge_mask = subgraph(batch_nodes, graph.edge_index, relabel_nodes=True, return_edge_mask=True)\n",
    "\n",
    "        # Use edge_mask to select edge attributes and labels\n",
    "        edge_attr = graph.edge_attr[edge_mask]\n",
    "        edge_label = graph.edge_label[edge_mask]\n",
    "        edge_class = graph.edge_class[edge_mask]\n",
    "\n",
    "        yield batch_nodes, edge_index, edge_attr, edge_label, edge_class\n",
    "\n",
    "\n",
    "new_model_2.eval()\n",
    "\n",
    "all_test_preds = []\n",
    "all_test_labels = []\n",
    "all_test_classes = []\n",
    "attack_class_performance = {attack_type: {'correct': 0, 'incorrect': 0} for attack_type in test_class_map}\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "print(\"inference start\")\n",
    "with th.no_grad():\n",
    "    for batch_idx, (batch_nodes, edge_index, edge_attr, edge_label, edge_class) in enumerate(generate_edge_based_batches_with_node_expansion(G_pyg_test, batch_size, 20)):\n",
    "        # print(f\"Processing batch {batch_idx} with {batch_nodes.size(0)} nodes and {edge_index.size(1)} edges\")\n",
    "        batch = Data(x=G_pyg_test.x[batch_nodes], edge_index=edge_index, edge_attr=edge_attr, edge_label=edge_label)\n",
    "        \n",
    "        if batch.edge_index.size(1) == 0 or batch.edge_label.size(0) == 0:\n",
    "            print(f\"Warning: Empty batch at batch {batch_idx}\")\n",
    "            continue\n",
    "            \n",
    "        if batch is None or batch.num_nodes == 0:\n",
    "            print(f\"Warning: Empty batch at Batch {batch_idx}\")\n",
    "            continue\n",
    "\n",
    "        if th.isnan(batch.x).any() or th.isinf(batch.x).any() or th.isnan(batch.edge_attr).any() or th.isinf(batch.edge_attr).any():\n",
    "            print(f\"Warning: batch x and edge_attr contains NaN or Inf at Batch {batch_idx}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            batch = batch.to(device)\n",
    "        except Exception as batch_error:\n",
    "            print(f\"Error moving batch to device at Batch {batch_idx}: {batch_error}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            out = new_model_2(batch)\n",
    "\n",
    "            if th.isnan(out).any() or th.isinf(out).any():\n",
    "                print(f\"Warning: out contains NaN or Inf at Batch {batch_idx}\")\n",
    "                continue \n",
    "            \n",
    "            all_test_preds.append(out)\n",
    "            all_test_labels.append(edge_label)\n",
    "            all_test_classes.append(edge_class)\n",
    "\n",
    "            pred = out.argmax(dim=1) \n",
    "\n",
    "            for i in range(len(pred)):\n",
    "                true_label = edge_label[i].item()\n",
    "                predicted_label = pred[i].item()\n",
    "                attack_type = test_le.inverse_transform([edge_class[i].item()])[0] \n",
    "\n",
    "                if true_label == 0 and attack_type != 'Benign':\n",
    "                    print('this sample is Benign but label is wrong')\n",
    "                \n",
    "                if true_label == predicted_label:\n",
    "                    attack_class_performance[attack_type]['correct'] += 1\n",
    "                else:\n",
    "                    attack_class_performance[attack_type]['incorrect'] += 1\n",
    "        except Exception as forward_error:\n",
    "            print(f\"Error during forward/backward pass at Batch {batch_idx}: {forward_error}\")\n",
    "            continue\n",
    "\n",
    "print(\"inference done\")\n",
    "all_test_preds = th.cat(all_test_preds).to(device)\n",
    "all_test_labels = th.cat(all_test_labels).to(device)\n",
    "all_test_classes = th.cat(all_test_classes).to(device)\n",
    "\n",
    "test_accuracy = compute_accuracy(all_test_preds, all_test_labels)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bf70a-3dbc-48d7-9df0-5ff66e99ac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 5623\n",
      "False Positives (FP): 409411\n",
      "True Negatives (TN): 598395\n",
      "False Negatives (FN): 132755\n",
      "Accuracy: 0.5270\n",
      "Precision: 0.0135\n",
      "Recall: 0.0406\n",
      "F1 Score: 0.0203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred_labels = all_test_preds.argmax(dim=1)\n",
    "\n",
    "pred_labels = pred_labels.cpu()\n",
    "all_test_labels = all_test_labels.cpu()\n",
    "\n",
    "cm = confusion_matrix(all_test_labels, pred_labels)\n",
    "\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "print(f'True Positives (TP): {TP}')\n",
    "print(f'False Positives (FP): {FP}')\n",
    "print(f'True Negatives (TN): {TN}')\n",
    "print(f'False Negatives (FN): {FN}')\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df046bd-b50b-481c-826d-2f4ce4569f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Type: Benign, Accuracy: 0.5938, Total Samples: 1007806, Correct Samples: 598395, Incorrect Samples: 409411\n",
      "Attack Type: Bot, Accuracy: 0.0002, Total Samples: 8772, Correct Samples: 2, Incorrect Samples: 8770\n",
      "Attack Type: Brute Force -Web, Accuracy: 0.0000, Total Samples: 149, Correct Samples: 0, Incorrect Samples: 149\n",
      "Attack Type: Brute Force -XSS, Accuracy: 0.0000, Total Samples: 51, Correct Samples: 0, Incorrect Samples: 51\n",
      "Attack Type: DDOS attack-HOIC, Accuracy: 0.0000, Total Samples: 65972, Correct Samples: 1, Incorrect Samples: 65971\n",
      "Attack Type: DDOS attack-LOIC-UDP, Accuracy: 0.0000, Total Samples: 135, Correct Samples: 0, Incorrect Samples: 135\n",
      "Attack Type: DDoS attacks-LOIC-HTTP, Accuracy: 0.1121, Total Samples: 18991, Correct Samples: 2129, Incorrect Samples: 16862\n",
      "Attack Type: DoS attacks-GoldenEye, Accuracy: 0.0369, Total Samples: 1706, Correct Samples: 63, Incorrect Samples: 1643\n",
      "Attack Type: DoS attacks-Hulk, Accuracy: 0.0110, Total Samples: 26815, Correct Samples: 295, Incorrect Samples: 26520\n",
      "Attack Type: DoS attacks-SlowHTTPTest, Accuracy: 0.0000, Total Samples: 848, Correct Samples: 0, Incorrect Samples: 848\n",
      "Attack Type: DoS attacks-Slowloris, Accuracy: 0.3936, Total Samples: 592, Correct Samples: 233, Incorrect Samples: 359\n",
      "Attack Type: FTP-BruteForce, Accuracy: 0.0000, Total Samples: 1574, Correct Samples: 0, Incorrect Samples: 1574\n",
      "Attack Type: Infilteration, Accuracy: 0.4123, Total Samples: 7003, Correct Samples: 2887, Incorrect Samples: 4116\n",
      "Attack Type: SQL Injection, Accuracy: 0.0000, Total Samples: 24, Correct Samples: 0, Incorrect Samples: 24\n",
      "Attack Type: SSH-Bruteforce, Accuracy: 0.0023, Total Samples: 5746, Correct Samples: 13, Incorrect Samples: 5733\n",
      "138378\n"
     ]
    }
   ],
   "source": [
    "for attack_type, performance in attack_class_performance.items():\n",
    "    total_samples = performance['correct'] + performance['incorrect']\n",
    "    if attack_type != BENIGN_CLASS_NAME:\n",
    "        sum += total_samples\n",
    "    accuracy = performance['correct'] / total_samples if total_samples > 0 else 0\n",
    "    print(f\"Attack Type: {attack_type}, Accuracy: {accuracy:.4f}, Total Samples: {total_samples}, Correct Samples: {performance['correct']}, Incorrect Samples: {performance['incorrect']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d9b00-3518-49c9-bac2-60551a6d12ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
