{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fb9e1b2-1c5f-49e3-82b4-3bfe52cabad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "=====Experiment=====\n",
    "Dataset: BoT-IoT dataset\n",
    "\n",
    "Check Train Test Dataset Coverage for\n",
    "- 1. Random Split\n",
    "- 2. Stratified Split\n",
    "'''\n",
    "\n",
    "from torch_geometric.utils import from_networkx, add_self_loops, degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "# import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Datasets.BoT_IoT.BoT_IoT_config import BoT_IoT_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d9ef09a-d405-43b8-971e-fe9e6a592c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "DDoS              1926624\n",
      "DoS               1650260\n",
      "Reconnaissance      91082\n",
      "Normal                477\n",
      "Theft                  79\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "csv_file_name = \"all_raw\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"BoT_IoT/All/{csv_file_name}.csv\"))\n",
    "\n",
    "DATASET_NAME = \"BoT_IoT\"\n",
    "EXPERIMENT_NAME = \"check_train_test_split\"\n",
    "WINDOW_SIZE = 2000\n",
    "\n",
    "SOURCE_IP_COL_NAME = BoT_IoT_Config.SOURCE_IP_COL_NAME\n",
    "DESTINATION_IP_COL_NAME = BoT_IoT_Config.DESTINATION_IP_COL_NAME\n",
    "SOURCE_PORT_COL_NAME = BoT_IoT_Config.SOURCE_PORT_COL_NAME\n",
    "DESTINATION_PORT_COL_NAME = BoT_IoT_Config.DESTINATION_PORT_COL_NAME\n",
    "\n",
    "ATTACK_CLASS_COL_NAME = BoT_IoT_Config.ATTACK_CLASS_COL_NAME\n",
    "\n",
    "BENIGN_CLASS_NAME = BoT_IoT_Config.BENIGN_CLASS_NAME\n",
    "\n",
    "TIME_COLS = BoT_IoT_Config.TIME_COL_NAMES\n",
    "\n",
    "print(data[ATTACK_CLASS_COL_NAME].value_counts())\n",
    "\n",
    "MULTICLASS = True\n",
    "\n",
    "label_col = ATTACK_CLASS_COL_NAME\n",
    "\n",
    "\n",
    "saves_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, EXPERIMENT_NAME)\n",
    "\n",
    "checkpoint_path = os.path.join(saves_path, f\"checkpoints_{csv_file_name}.pth\")\n",
    "best_model_path = os.path.join(saves_path, f\"best_model_{csv_file_name}.pth\")\n",
    "\n",
    "os.makedirs(saves_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "449a1af1-1d3d-4179-9628-7c2ec551ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pkSeqID', 'stime', 'flgs_number', 'proto_number', 'saddr', 'sport',\n",
      "       'daddr', 'dport', 'pkts', 'bytes', 'state_number', 'ltime', 'dur',\n",
      "       'mean', 'stddev', 'sum', 'min', 'max', 'spkts', 'dpkts', 'sbytes',\n",
      "       'dbytes', 'rate', 'srate', 'drate', 'TnBPSrcIP', 'TnBPDstIP',\n",
      "       'TnP_PSrcIP', 'TnP_PDstIP', 'TnP_PerProto', 'TnP_Per_Dport',\n",
      "       'AR_P_Proto_P_SrcIP', 'AR_P_Proto_P_DstIP', 'N_IN_Conn_P_DstIP',\n",
      "       'N_IN_Conn_P_SrcIP', 'AR_P_Proto_P_Sport', 'AR_P_Proto_P_Dport',\n",
      "       'Pkts_P_State_P_Protocol_P_DestIP', 'Pkts_P_State_P_Protocol_P_SrcIP',\n",
      "       'attack', 'category'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data.drop(columns=BoT_IoT_Config.DROP_COLS,inplace=True)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a2c690c-86a4-49f7-aa9c-58f94529547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME].apply(str)\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME].apply(str)\n",
    "\n",
    "# # Combine Port and IP\n",
    "data[SOURCE_PORT_COL_NAME] = data[SOURCE_PORT_COL_NAME].apply(str)\n",
    "data[DESTINATION_PORT_COL_NAME] = data[DESTINATION_PORT_COL_NAME].apply(str)\n",
    "\n",
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME] + ':' + data[SOURCE_PORT_COL_NAME]\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME] + ':' + data[DESTINATION_PORT_COL_NAME]\n",
    "data.drop(columns=[SOURCE_PORT_COL_NAME,DESTINATION_PORT_COL_NAME],inplace=True)\n",
    "\n",
    "data = pd.get_dummies(data, columns = BoT_IoT_Config.CATEGORICAL_COLS) # One Hot Encoding for categorical data\n",
    "converted_categorical_cols = [col for col in data.columns if col.startswith(tuple(BoT_IoT_Config.CATEGORICAL_COLS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5651ef5b-0a9d-4641-aad7-5738092c46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          pkSeqID         stime                  saddr                 daddr  \\\n",
      "0        3576925  1.526344e+09       192.168.100.3:80   192.168.100.55:8080   \n",
      "1        3576926  1.526344e+09    192.168.100.46:3456      192.168.100.5:80   \n",
      "2        3576919  1.526344e+09      192.168.100.46:80      192.168.100.5:80   \n",
      "3        3576920  1.526344e+09      192.168.100.46:80      192.168.100.5:80   \n",
      "4        3576922  1.526344e+09      192.168.100.7:365     192.168.100.3:565   \n",
      "...          ...           ...                    ...                   ...   \n",
      "3668517  3668517  1.529381e+09  192.168.100.150:35062      192.168.100.3:22   \n",
      "3668518  3668518  1.529381e+09  192.168.100.150:35064      192.168.100.3:22   \n",
      "3668519  3668519  1.529381e+09  192.168.100.150:35066      192.168.100.3:22   \n",
      "3668520  3668520  1.529381e+09  192.168.100.150:35070      192.168.100.3:22   \n",
      "3668521  3668521  1.529381e+09    192.168.100.3:43001  192.168.100.150:4433   \n",
      "\n",
      "          pkts     bytes         ltime          dur      mean    stddev  ...  \\\n",
      "0        59453  60884536  1.526346e+09  1685.706909  4.947914  0.228494  ...   \n",
      "1        59452  59114121  1.526346e+09  1685.706909  4.947584  0.231615  ...   \n",
      "2        29726  28543568  1.526346e+09  1685.706909  4.947334  0.231518  ...   \n",
      "3        30157  28898696  1.526346e+09  1685.706909  4.947345  0.231582  ...   \n",
      "4        16326    979560  1.526346e+09  1685.973267  4.893885  0.231947  ...   \n",
      "...        ...       ...           ...          ...       ...       ...  ...   \n",
      "3668517      6       434  1.529381e+09     0.023737  0.023737  0.000000  ...   \n",
      "3668518      6       434  1.529381e+09     0.013165  0.013165  0.000000  ...   \n",
      "3668519      6       434  1.529381e+09     0.000574  0.000574  0.000000  ...   \n",
      "3668520     31      5472  1.529381e+09     2.874302  2.874302  0.000000  ...   \n",
      "3668521      2       134  1.529381e+09     0.000003  0.000003  0.000000  ...   \n",
      "\n",
      "         state_number_7  state_number_8  state_number_9  state_number_10  \\\n",
      "0                 False           False           False            False   \n",
      "1                 False           False           False            False   \n",
      "2                 False           False           False            False   \n",
      "3                 False           False           False            False   \n",
      "4                 False           False           False            False   \n",
      "...                 ...             ...             ...              ...   \n",
      "3668517           False           False           False            False   \n",
      "3668518           False           False           False            False   \n",
      "3668519           False           False           False            False   \n",
      "3668520           False           False           False            False   \n",
      "3668521           False           False           False            False   \n",
      "\n",
      "         state_number_11  proto_number_1  proto_number_2  proto_number_3  \\\n",
      "0                  False            True           False           False   \n",
      "1                  False           False           False            True   \n",
      "2                  False           False           False            True   \n",
      "3                  False            True           False           False   \n",
      "4                  False           False           False            True   \n",
      "...                  ...             ...             ...             ...   \n",
      "3668517            False            True           False           False   \n",
      "3668518            False            True           False           False   \n",
      "3668519            False            True           False           False   \n",
      "3668520            False            True           False           False   \n",
      "3668521            False            True           False           False   \n",
      "\n",
      "         proto_number_4  proto_number_5  \n",
      "0                 False           False  \n",
      "1                 False           False  \n",
      "2                 False           False  \n",
      "3                 False           False  \n",
      "4                 False           False  \n",
      "...                 ...             ...  \n",
      "3668517           False           False  \n",
      "3668518           False           False  \n",
      "3668519           False           False  \n",
      "3668520           False           False  \n",
      "3668521           False           False  \n",
      "\n",
      "[3668522 rows x 61 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2d96115-31f9-48cb-b3e6-7853d2d253cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          pkSeqID         stime                  saddr                 daddr  \\\n",
      "0        3576925  1.526344e+09       192.168.100.3:80   192.168.100.55:8080   \n",
      "1        3576926  1.526344e+09    192.168.100.46:3456      192.168.100.5:80   \n",
      "2        3576919  1.526344e+09      192.168.100.46:80      192.168.100.5:80   \n",
      "3        3576920  1.526344e+09      192.168.100.46:80      192.168.100.5:80   \n",
      "4        3576922  1.526344e+09      192.168.100.7:365     192.168.100.3:565   \n",
      "...          ...           ...                    ...                   ...   \n",
      "3668517  3668517  1.529381e+09  192.168.100.150:35062      192.168.100.3:22   \n",
      "3668518  3668518  1.529381e+09  192.168.100.150:35064      192.168.100.3:22   \n",
      "3668519  3668519  1.529381e+09  192.168.100.150:35066      192.168.100.3:22   \n",
      "3668520  3668520  1.529381e+09  192.168.100.150:35070      192.168.100.3:22   \n",
      "3668521  3668521  1.529381e+09    192.168.100.3:43001  192.168.100.150:4433   \n",
      "\n",
      "          pkts     bytes         ltime          dur      mean    stddev  ...  \\\n",
      "0        59453  60884536  1.526346e+09  1685.706909  4.947914  0.228494  ...   \n",
      "1        59452  59114121  1.526346e+09  1685.706909  4.947584  0.231615  ...   \n",
      "2        29726  28543568  1.526346e+09  1685.706909  4.947334  0.231518  ...   \n",
      "3        30157  28898696  1.526346e+09  1685.706909  4.947345  0.231582  ...   \n",
      "4        16326    979560  1.526346e+09  1685.973267  4.893885  0.231947  ...   \n",
      "...        ...       ...           ...          ...       ...       ...  ...   \n",
      "3668517      6       434  1.529381e+09     0.023737  0.023737  0.000000  ...   \n",
      "3668518      6       434  1.529381e+09     0.013165  0.013165  0.000000  ...   \n",
      "3668519      6       434  1.529381e+09     0.000574  0.000574  0.000000  ...   \n",
      "3668520     31      5472  1.529381e+09     2.874302  2.874302  0.000000  ...   \n",
      "3668521      2       134  1.529381e+09     0.000003  0.000003  0.000000  ...   \n",
      "\n",
      "         state_number_7  state_number_8  state_number_9  state_number_10  \\\n",
      "0                 False           False           False            False   \n",
      "1                 False           False           False            False   \n",
      "2                 False           False           False            False   \n",
      "3                 False           False           False            False   \n",
      "4                 False           False           False            False   \n",
      "...                 ...             ...             ...              ...   \n",
      "3668517           False           False           False            False   \n",
      "3668518           False           False           False            False   \n",
      "3668519           False           False           False            False   \n",
      "3668520           False           False           False            False   \n",
      "3668521           False           False           False            False   \n",
      "\n",
      "         state_number_11  proto_number_1  proto_number_2  proto_number_3  \\\n",
      "0                  False            True           False           False   \n",
      "1                  False           False           False            True   \n",
      "2                  False           False           False            True   \n",
      "3                  False            True           False           False   \n",
      "4                  False           False           False            True   \n",
      "...                  ...             ...             ...             ...   \n",
      "3668517            False            True           False           False   \n",
      "3668518            False            True           False           False   \n",
      "3668519            False            True           False           False   \n",
      "3668520            False            True           False           False   \n",
      "3668521            False            True           False           False   \n",
      "\n",
      "         proto_number_4  proto_number_5  \n",
      "0                 False           False  \n",
      "1                 False           False  \n",
      "2                 False           False  \n",
      "3                 False           False  \n",
      "4                 False           False  \n",
      "...                 ...             ...  \n",
      "3668517           False           False  \n",
      "3668518           False           False  \n",
      "3668519           False           False  \n",
      "3668520           False           False  \n",
      "3668521           False           False  \n",
      "\n",
      "[3668522 rows x 61 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "data.drop(columns=['index'],inplace=True)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1df5c4c-70a2-4566-ae5e-ee3dcc6037a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               pkts         bytes           dur          mean        stddev  \\\n",
      "count  3.668522e+06  3.668522e+06  3.668522e+06  3.668522e+06  3.668522e+06   \n",
      "mean   7.725963e+00  8.690501e+02  2.033479e+01  2.231063e+00  8.871499e-01   \n",
      "std    1.155876e+02  1.122667e+05  2.148764e+01  1.517728e+00  8.037139e-01   \n",
      "min    1.000000e+00  6.000000e+01  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    5.000000e+00  4.200000e+02  1.256256e+01  1.819670e-01  3.001900e-02   \n",
      "50%    7.000000e+00  6.000000e+02  1.550852e+01  2.690125e+00  7.938960e-01   \n",
      "75%    9.000000e+00  7.700000e+02  2.709986e+01  3.565203e+00  1.745296e+00   \n",
      "max    7.005700e+04  7.183334e+07  2.771485e+03  4.981882e+00  2.496763e+00   \n",
      "\n",
      "                sum           min           max         spkts         dpkts  \\\n",
      "count  3.668522e+06  3.668522e+06  3.668522e+06  3.668522e+06  3.668522e+06   \n",
      "mean   7.721635e+00  1.017540e+00  3.020015e+00  7.314146e+00  4.118173e-01   \n",
      "std    7.616199e+00  1.483688e+00  1.860877e+00  7.725836e+01  4.965001e+01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00   \n",
      "25%    3.445982e-01  0.000000e+00  2.806072e-01  5.000000e+00  0.000000e+00   \n",
      "50%    8.269959e+00  0.000000e+00  4.009111e+00  6.000000e+00  0.000000e+00   \n",
      "75%    1.171040e+01  2.151138e+00  4.293582e+00  8.000000e+00  0.000000e+00   \n",
      "max    1.913194e+03  4.980471e+00  4.999999e+00  3.502900e+04  3.502900e+04   \n",
      "\n",
      "       ...  TnP_PerProto  TnP_Per_Dport  AR_P_Proto_P_SrcIP  \\\n",
      "count  ...  3.668522e+06   3.668522e+06        3.668522e+06   \n",
      "mean   ...  7.535659e+02   7.369070e+02        3.327439e+02   \n",
      "std    ...  1.434385e+03   6.527134e+02        8.466031e+03   \n",
      "min    ...  1.000000e+00   1.000000e+00        0.000000e+00   \n",
      "25%    ...  5.020000e+02   5.000000e+02        2.359950e-01   \n",
      "50%    ...  7.000000e+02   7.000000e+02        3.900890e-01   \n",
      "75%    ...  9.240000e+02   9.200000e+02        5.725580e-01   \n",
      "max    ...  2.283730e+05   2.444250e+05        2.714290e+06   \n",
      "\n",
      "       AR_P_Proto_P_DstIP  N_IN_Conn_P_DstIP  N_IN_Conn_P_SrcIP  \\\n",
      "count        3.668522e+06       3.668522e+06       3.668522e+06   \n",
      "mean         2.851832e+02       9.245168e+01       8.253848e+01   \n",
      "std          4.096943e+03       1.817643e+01       2.439739e+01   \n",
      "min          0.000000e+00       1.000000e+00       1.000000e+00   \n",
      "25%          2.436680e-01       1.000000e+02       6.900000e+01   \n",
      "50%          3.986290e-01       1.000000e+02       1.000000e+02   \n",
      "75%          5.796390e-01       1.000000e+02       1.000000e+02   \n",
      "max          1.000000e+06       1.000000e+02       1.000000e+02   \n",
      "\n",
      "       AR_P_Proto_P_Sport  AR_P_Proto_P_Dport  \\\n",
      "count        3.668522e+06        3.668522e+06   \n",
      "mean         4.564945e+02        5.385196e+02   \n",
      "std          1.432917e+04        1.569824e+04   \n",
      "min          0.000000e+00        0.000000e+00   \n",
      "25%          2.314810e-01        2.457730e-01   \n",
      "50%          3.785910e-01        3.943060e-01   \n",
      "75%          5.725550e-01        5.769710e-01   \n",
      "max          3.000000e+06        2.000000e+06   \n",
      "\n",
      "       Pkts_P_State_P_Protocol_P_DestIP  Pkts_P_State_P_Protocol_P_SrcIP  \n",
      "count                      3.668522e+06                     3.668522e+06  \n",
      "mean                       6.422897e+02                     5.859984e+02  \n",
      "std                        4.533432e+02                     4.332619e+02  \n",
      "min                        1.000000e+00                     1.000000e+00  \n",
      "25%                        3.240000e+02                     2.940000e+02  \n",
      "50%                        6.000000e+02                     5.000000e+02  \n",
      "75%                        8.280000e+02                     8.000000e+02  \n",
      "max                        1.125440e+05                     1.179390e+05  \n",
      "\n",
      "[8 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_norm = BoT_IoT_Config.COLS_TO_NORM\n",
    "print(data[cols_to_norm].describe()) # Check if there's any too large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ea95177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All other columns processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_numeric_issues(df, cols_to_norm):\n",
    "    for col in cols_to_norm:\n",
    "        try:\n",
    "            # Try to coerce to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Try to clip the column\n",
    "            df[col] = df[col].clip(lower=-1e9, upper=1e9)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Column '{col}' failed with error: {e}\")\n",
    "            print(f\"  - Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "            print(f\"  - Data type: {df[col].dtype}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n✅ All other columns processed successfully.\")\n",
    "\n",
    "check_numeric_issues(data, BoT_IoT_Config.COLS_TO_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37256006-abc1-44aa-8e74-46d05dc6ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61c6e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DDoS' 'DoS' 'Normal' 'Reconnaissance' 'Theft']\n",
      "Attack label mapping: {'DDoS': 0, 'DoS': 1, 'Normal': 2, 'Reconnaissance': 3, 'Theft': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_classes = 2\n",
    "class_map = [0, 1]\n",
    "if MULTICLASS:\n",
    "    le = LabelEncoder()\n",
    "    attack_labels = le.fit_transform(data[ATTACK_CLASS_COL_NAME])\n",
    "    class_map = le.classes_\n",
    "    print(class_map)\n",
    "    print(\"Attack label mapping:\", dict(zip(class_map, range(len(class_map)))))\n",
    "    data[ATTACK_CLASS_COL_NAME] = attack_labels\n",
    "    num_classes = len(class_map)\n",
    "    class_dict = {le.inverse_transform([i])[0]: i for i in range(len(le.classes_))}\n",
    "\n",
    "BENIGN_CLASS_LABEL = le.transform([BENIGN_CLASS_NAME])[0] if MULTICLASS else 0\n",
    "ADVERSARIAL_CLASS_LABEL = len(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d35f4cdd-2716-431f-af50-b34cc3d2d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: ['pkts', 'bytes', 'dur', 'mean', 'stddev', 'sum', 'min', 'max', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate', 'TnBPSrcIP', 'TnBPDstIP', 'TnP_PSrcIP', 'TnP_PDstIP', 'TnP_PerProto', 'TnP_Per_Dport', 'AR_P_Proto_P_SrcIP', 'AR_P_Proto_P_DstIP', 'N_IN_Conn_P_DstIP', 'N_IN_Conn_P_SrcIP', 'AR_P_Proto_P_Sport', 'AR_P_Proto_P_Dport', 'Pkts_P_State_P_Protocol_P_DestIP', 'Pkts_P_State_P_Protocol_P_SrcIP', 'flgs_number_1', 'flgs_number_2', 'flgs_number_3', 'flgs_number_4', 'flgs_number_5', 'flgs_number_6', 'flgs_number_7', 'flgs_number_8', 'flgs_number_9', 'state_number_1', 'state_number_2', 'state_number_3', 'state_number_4', 'state_number_5', 'state_number_6', 'state_number_7', 'state_number_8', 'state_number_9', 'state_number_10', 'state_number_11', 'proto_number_1', 'proto_number_2', 'proto_number_3', 'proto_number_4', 'proto_number_5']\n",
      "Number of Features: 54\n"
     ]
    }
   ],
   "source": [
    "# # Maintain the order of the rows in the original dataframe\n",
    "\n",
    "feature_cols = cols_to_norm + converted_categorical_cols\n",
    "\n",
    "print('Feature Columns:', feature_cols)\n",
    "num_features = len(feature_cols)\n",
    "print('Number of Features:', num_features)\n",
    "\n",
    "data['h'] = data[ feature_cols ].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "743e7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df):\n",
    "\n",
    "    G_nx = nx.from_pandas_edgelist(df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "    G_pyg = from_networkx(G_nx)\n",
    "\n",
    "    num_nodes = G_pyg.num_nodes\n",
    "    num_edges = G_pyg.num_edges\n",
    "\n",
    "    G_pyg.x = th.ones(num_nodes, len(df['h'].iloc[0])) \n",
    "\n",
    "    edge_attr_list = []\n",
    "    edge_label_list = []\n",
    "\n",
    "    for u, v, key, data in G_nx.edges(keys=True, data=True):\n",
    "        edge_attr_list.append(data['h']) \n",
    "        edge_label_list.append(data[label_col]) \n",
    "\n",
    "    G_pyg.edge_attr = th.tensor(edge_attr_list, dtype=th.float32)\n",
    "    G_pyg.edge_label = th.tensor(edge_label_list, dtype=th.long)\n",
    "\n",
    "    return G_pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac36567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "import tqdm\n",
    "\n",
    "class Downsampler:\n",
    "    def __init__(self, downsample_classes=[BENIGN_CLASS_LABEL], downsample_ratios=[0.1]):\n",
    "        \"\"\"\n",
    "        downsample_classes: list of class names to downsample\n",
    "        downsample_ratio: keep no more than this ratio for each class\n",
    "        \"\"\"\n",
    "        assert len(downsample_classes) == len(downsample_ratios)\n",
    "        self.downsample_classes = downsample_classes\n",
    "        self.downsample_ratio = downsample_ratios\n",
    "\n",
    "    def downsample(self, label_counts_list, X, y):\n",
    "        total_counts = defaultdict(int)\n",
    "\n",
    "        class_heaps = defaultdict(list)\n",
    "        for i, lc in enumerate(label_counts_list):\n",
    "            for cls in self.downsample_classes:\n",
    "                class_label_count = lc.get(cls, 0)\n",
    "                total_counts[cls] += class_label_count\n",
    "                heapq.heappush(class_heaps[cls], (-class_label_count, i))\n",
    "\n",
    "        class_target = {\n",
    "            cls: total_counts[cls] * self.downsample_ratio[i] for i, cls in enumerate(self.downsample_classes)\n",
    "        }\n",
    "                \n",
    "        indices_to_remove = set()\n",
    "        class_counts = total_counts\n",
    "\n",
    "        # 3. For each class, remove top contributing samples until threshold reached\n",
    "        for cls in self.downsample_classes:\n",
    "            target = class_target[cls]\n",
    "            heap = class_heaps[cls]\n",
    "\n",
    "            pbar = tqdm(desc=f\"Downsampling '{cls}'\", total=len(heap))\n",
    "            while class_counts[cls] > target and heap:\n",
    "                _, idx = heapq.heappop(heap)\n",
    "                if idx in indices_to_remove:\n",
    "                    continue\n",
    "                # For each class in this sample, if it's a downsample class, decrement the count\n",
    "                for sample_cls, count in label_counts_list[idx].items():\n",
    "                    if sample_cls in self.downsample_classes:\n",
    "                        class_counts[sample_cls] -= count\n",
    "                indices_to_remove.add(idx)\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(class_label=cls, remaining=class_counts[cls], target=target)\n",
    "            pbar.close()\n",
    "\n",
    "        # 4. Apply filter\n",
    "        keep_mask = [i for i in range(len(X)) if i not in indices_to_remove]\n",
    "        X_new = [X[i] for i in keep_mask]\n",
    "        y_new = [y[i] for i in keep_mask]\n",
    "\n",
    "        return X_new, y_new\n",
    "\n",
    "downsampler = Downsampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e650028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "class RandomSplitGraphDataset:\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.total_count = len(self.y)\n",
    "\n",
    "        # Compute class weights\n",
    "        labels = []\n",
    "\n",
    "        for graph in self.X:\n",
    "            labels.append(graph.edge_label.tolist())\n",
    "\n",
    "        labels = np.concatenate(labels)\n",
    "\n",
    "        self.class_counts = Counter(labels)\n",
    "\n",
    "        # Compute the class weights\n",
    "        self.class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(labels),\n",
    "            y=labels\n",
    "        )\n",
    "\n",
    "    def graph_train_test_split(self, test_ratio: float = 0.15, random_state: int = 42):\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            np.arange(len(self.X)), \n",
    "            test_size=test_ratio, \n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        X_train = [self.X[i] for i in train_idx]\n",
    "        X_test = [self.X[i] for i in test_idx]\n",
    "\n",
    "        y_train = [self.y[i] for i in train_idx]\n",
    "        y_test = [self.y[i] for i in test_idx]\n",
    "\n",
    "        return RandomSplitGraphDataset(X_train, y_train), RandomSplitGraphDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "class StratifiedGraphDataset:\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.total_count = len(self.y)\n",
    "\n",
    "        # Compute class weights\n",
    "        labels = []\n",
    "\n",
    "        for graph in self.X:\n",
    "            labels.append(graph.edge_label.tolist())\n",
    "\n",
    "        labels = np.concatenate(labels)\n",
    "\n",
    "        self.class_counts = Counter(labels)\n",
    "\n",
    "        # Compute the class weights\n",
    "        self.class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(labels),\n",
    "            y=labels\n",
    "        )\n",
    "\n",
    "    def k_fold_split(self, k: int = 5, test_ratio: float = 0.15, random_state: int = 42):\n",
    "        cv = MultilabelStratifiedShuffleSplit(test_size=test_ratio, random_state=random_state, n_splits=k)\n",
    "\n",
    "        mlb = MultiLabelBinarizer()\n",
    "\n",
    "        y_binary = mlb.fit_transform(self.y)\n",
    "\n",
    "        return cv.split(np.zeros(len(self.X)), y_binary)\n",
    "\n",
    "    def graph_train_test_split(self, test_ratio: float = 0.15, random_state: int = 42):\n",
    "        train_idx, test_idx = next(self.k_fold_split(k = 1, test_ratio = test_ratio, random_state = random_state))\n",
    "        \n",
    "        X_train = [self.X[i] for i in train_idx]\n",
    "        X_test = [self.X[i] for i in test_idx]\n",
    "\n",
    "        y_train = [self.y[i] for i in train_idx]\n",
    "        y_test = [self.y[i] for i in test_idx]\n",
    "\n",
    "        return StratifiedGraphDataset(X_train, y_train), StratifiedGraphDataset(X_test, y_test)\n",
    "    \n",
    "    def print_class_distribution_and_weights(self):\n",
    "        # Use the label encoder to inverse transform the class labels\n",
    "        class_counts_named = {cls: count for cls, count in self.class_counts.items()}\n",
    "        class_weights_named = {cls: weight for cls, weight in enumerate(self.class_weights)}\n",
    "        print(\"Class Counts and Weights:\")\n",
    "        for cls_label in class_counts_named.keys():\n",
    "            count = class_counts_named[cls_label]\n",
    "            weight = class_weights_named[cls_label]\n",
    "            print(f\"{cls_label:<2}  {le.inverse_transform([cls_label])[0]:<15}: Count = {count:<10}, Weight = {weight:<10.4f}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_count\n",
    "\n",
    "    def __iter__(self):\n",
    "        for g in self.X:\n",
    "            yield g\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            return self.X[idx], self.y[idx]\n",
    "        elif isinstance(idx, slice):\n",
    "            return [self.X[i] for i in range(len(self.X))][idx], [self.y[i] for i in range(len(self.y))][idx]\n",
    "        else:\n",
    "            raise TypeError(\"Index must be an integer or a slice.\")\n",
    "\n",
    "def generate_graph_datasets(\n",
    "    df: pd.DataFrame, \n",
    "    window_size: int = WINDOW_SIZE, \n",
    "    # overlap_ratio: float = 0, \n",
    "    feature_cols=feature_cols,\n",
    "    ordering_cols=TIME_COLS, \n",
    "    label_col=label_col,\n",
    "    build_graph_func=create_graph,\n",
    "    ):\n",
    "\n",
    "    print(\"All Columns: \", df.columns)\n",
    "    print(\"Ordering Columns: \", ordering_cols)\n",
    "    assert all(col in df.columns for col in ordering_cols), \"All timestamp columns are required\"\n",
    "    assert label_col in df.columns, \"Edge label column 'label' is required\"\n",
    "    \n",
    "    df = df.sort_values(ordering_cols).reset_index(drop=True)\n",
    "    window_size = int(window_size)\n",
    "    \n",
    "    df.drop(columns=set(df.columns) - set(feature_cols) - set(label_col))\n",
    "\n",
    "    print(\"Final Columns: \", df.columns)\n",
    "    \n",
    "    label_counts_list = []\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    progress_bar = tqdm(range(0, len(df), window_size), desc=f\"Generating graphs\")\n",
    "    for start in progress_bar:\n",
    "        window_df = df[start: min(start + window_size, len(df))]\n",
    "        contains_label = window_df[label_col].unique()\n",
    "\n",
    "        G_pyg = build_graph_func(window_df)\n",
    "\n",
    "        label_counts = window_df[label_col].value_counts()\n",
    "\n",
    "        label_counts_list.append(label_counts)\n",
    "        X.append(G_pyg)\n",
    "        y.append(contains_label.tolist())\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "491e7421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Columns:  Index(['pkSeqID', 'stime', 'saddr', 'daddr', 'pkts', 'bytes', 'ltime', 'dur',\n",
      "       'mean', 'stddev', 'sum', 'min', 'max', 'spkts', 'dpkts', 'sbytes',\n",
      "       'dbytes', 'rate', 'srate', 'drate', 'TnBPSrcIP', 'TnBPDstIP',\n",
      "       'TnP_PSrcIP', 'TnP_PDstIP', 'TnP_PerProto', 'TnP_Per_Dport',\n",
      "       'AR_P_Proto_P_SrcIP', 'AR_P_Proto_P_DstIP', 'N_IN_Conn_P_DstIP',\n",
      "       'N_IN_Conn_P_SrcIP', 'AR_P_Proto_P_Sport', 'AR_P_Proto_P_Dport',\n",
      "       'Pkts_P_State_P_Protocol_P_DestIP', 'Pkts_P_State_P_Protocol_P_SrcIP',\n",
      "       'attack', 'category', 'flgs_number_1', 'flgs_number_2', 'flgs_number_3',\n",
      "       'flgs_number_4', 'flgs_number_5', 'flgs_number_6', 'flgs_number_7',\n",
      "       'flgs_number_8', 'flgs_number_9', 'state_number_1', 'state_number_2',\n",
      "       'state_number_3', 'state_number_4', 'state_number_5', 'state_number_6',\n",
      "       'state_number_7', 'state_number_8', 'state_number_9', 'state_number_10',\n",
      "       'state_number_11', 'proto_number_1', 'proto_number_2', 'proto_number_3',\n",
      "       'proto_number_4', 'proto_number_5', 'h'],\n",
      "      dtype='object')\n",
      "Ordering Columns:  ['stime', 'ltime']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Columns:  Index(['pkSeqID', 'stime', 'saddr', 'daddr', 'pkts', 'bytes', 'ltime', 'dur',\n",
      "       'mean', 'stddev', 'sum', 'min', 'max', 'spkts', 'dpkts', 'sbytes',\n",
      "       'dbytes', 'rate', 'srate', 'drate', 'TnBPSrcIP', 'TnBPDstIP',\n",
      "       'TnP_PSrcIP', 'TnP_PDstIP', 'TnP_PerProto', 'TnP_Per_Dport',\n",
      "       'AR_P_Proto_P_SrcIP', 'AR_P_Proto_P_DstIP', 'N_IN_Conn_P_DstIP',\n",
      "       'N_IN_Conn_P_SrcIP', 'AR_P_Proto_P_Sport', 'AR_P_Proto_P_Dport',\n",
      "       'Pkts_P_State_P_Protocol_P_DestIP', 'Pkts_P_State_P_Protocol_P_SrcIP',\n",
      "       'attack', 'category', 'flgs_number_1', 'flgs_number_2', 'flgs_number_3',\n",
      "       'flgs_number_4', 'flgs_number_5', 'flgs_number_6', 'flgs_number_7',\n",
      "       'flgs_number_8', 'flgs_number_9', 'state_number_1', 'state_number_2',\n",
      "       'state_number_3', 'state_number_4', 'state_number_5', 'state_number_6',\n",
      "       'state_number_7', 'state_number_8', 'state_number_9', 'state_number_10',\n",
      "       'state_number_11', 'proto_number_1', 'proto_number_2', 'proto_number_3',\n",
      "       'proto_number_4', 'proto_number_5', 'h'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating graphs: 100%|██████████| 1835/1835 [03:03<00:00, 10.00it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_graph_datasets(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ee16656",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stratified_graph_daataset = StratifiedGraphDataset(X, y)\n",
    "random_split_graph_dataset = RandomSplitGraphDataset(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fcea448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified Split Jensen-Shannon Divergence: 2.077331673164704e-05\n",
      "Random Split Jensen-Shannon Divergence: 0.0012107016013186479\n"
     ]
    }
   ],
   "source": [
    "num_tests = 100\n",
    "\n",
    "# Compute the Jensen-Shannon Divergence of train and test datasets\n",
    "def compute_js_divergence(counter_p, counter_q, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Compute the Jensen-Shannon Divergence and Kullback-Leibler Divergence between two class count dictionaries (Counter).\n",
    "    Returns (jsd, kld)\n",
    "    \"\"\"\n",
    "    # Get the union of all class labels\n",
    "    all_keys = sorted(set(counter_p.keys()).union(set(counter_q.keys())))\n",
    "    # Convert to arrays in the same order\n",
    "    p = np.array([counter_p.get(k, 0) for k in all_keys], dtype=np.float64)\n",
    "    q = np.array([counter_q.get(k, 0) for k in all_keys], dtype=np.float64)\n",
    "\n",
    "    p = p + epsilon\n",
    "    q = q + epsilon\n",
    "\n",
    "    # Normalize the distributions\n",
    "    p = p / np.sum(p) if np.sum(p) > 0 else p\n",
    "    q = q / np.sum(q) if np.sum(q) > 0 else q\n",
    "\n",
    "    # Compute the average distribution\n",
    "    m = 0.5 * (p + q)\n",
    "\n",
    "    # Compute the Kullback-Leibler divergence\n",
    "    kl_p_m = np.sum(np.where(p != 0, p * np.log(p / m), 0))\n",
    "    kl_q_m = np.sum(np.where(q != 0, q * np.log(q / m), 0))\n",
    "    kl_p_q = np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
    "\n",
    "    # Jensen-Shannon Divergence\n",
    "    jsd = 0.5 * (kl_p_m + kl_q_m)\n",
    "    return jsd\n",
    "\n",
    "total_stratified_js = 0\n",
    "total_random_js = 0\n",
    "\n",
    "for i in range(num_tests):\n",
    "    \n",
    "    stratified_train_graph_dataset, stratified_test_graph_dataset = stratified_graph_daataset.graph_train_test_split(test_ratio=0.15, random_state=i)\n",
    "    random_train_graph_dataset, random_test_graph_dataset = random_split_graph_dataset.graph_train_test_split(test_ratio=0.15, random_state=i)\n",
    "\n",
    "    \n",
    "\n",
    "    total_stratified_js += compute_js_divergence(\n",
    "        stratified_train_graph_dataset.class_counts,\n",
    "        stratified_test_graph_dataset.class_counts\n",
    "    )\n",
    "\n",
    "    total_random_js += compute_js_divergence(\n",
    "        random_train_graph_dataset.class_counts,\n",
    "        random_test_graph_dataset.class_counts\n",
    "    )\n",
    "    \n",
    "\n",
    "print(f\"Stratified Split Jensen-Shannon Divergence: {total_stratified_js / num_tests}\")\n",
    "print(f\"Random Split Jensen-Shannon Divergence: {total_random_js / num_tests}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
