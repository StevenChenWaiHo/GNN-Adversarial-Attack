{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb9e1b2-1c5f-49e3-82b4-3bfe52cabad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "=====Experiment=====\n",
    "Dataset: UNSW-NB15 dataset\n",
    "\n",
    "Training with whole graph\n",
    "Downsampled 90% normal traffic randomly\n",
    "Split train and test randomly\n",
    "\n",
    "Combined IP and Port features\n",
    "\n",
    "Checking the various F1s in Training Process \n",
    "'''\n",
    "\n",
    "from torch_geometric.utils import from_networkx, add_self_loops, degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "# import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Datasets.UNSW_NB15.UNSW_NB15_config import UNSW_NB15_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9ef09a-d405-43b8-971e-fe9e6a592c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3506032/3275626499.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "Normal            221876\n",
      "Generic           215481\n",
      "Exploits           44525\n",
      "Fuzzers            24246\n",
      "DoS                16353\n",
      "Reconnaissance     13987\n",
      "Analysis            2677\n",
      "Backdoors           2329\n",
      "Shellcode           1511\n",
      "Worms                174\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    321283\n",
      "0    221876\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "csv_file_name = \"all_downsampled\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n",
    "\n",
    "DATASET_NAME = \"UNSW_NB15\"\n",
    "EXPERIMENT_NAME = \"whole_graph_combined_ports_check_f1\"\n",
    "\n",
    "SOURCE_IP_COL_NAME = UNSW_NB15_Config.SOURCE_IP_COL_NAME\n",
    "DESTINATION_IP_COL_NAME = UNSW_NB15_Config.DESTINATION_IP_COL_NAME\n",
    "SOURCE_PORT_COL_NAME = UNSW_NB15_Config.SOURCE_PORT_COL_NAME\n",
    "DESTINATION_PORT_COL_NAME = UNSW_NB15_Config.DESTINATION_PORT_COL_NAME\n",
    "\n",
    "ATTACK_CLASS_COL_NAME = UNSW_NB15_Config.ATTACK_CLASS_COL_NAME\n",
    "IS_ATTACK_COL_NAME = UNSW_NB15_Config.IS_ATTACK_COL_NAME\n",
    "\n",
    "BENIGN_CLASS_NAME = UNSW_NB15_Config.BENIGN_CLASS_NAME\n",
    "\n",
    "TIME_COLS = UNSW_NB15_Config.TIME_COL_NAMES\n",
    "\n",
    "MULTICLASS = True\n",
    "label_col = ATTACK_CLASS_COL_NAME if MULTICLASS else IS_ATTACK_COL_NAME\n",
    "\n",
    "print(data[ATTACK_CLASS_COL_NAME].value_counts())\n",
    "print(data[IS_ATTACK_COL_NAME].value_counts())\n",
    "\n",
    "if MULTICLASS:\n",
    "    data.drop(columns=[IS_ATTACK_COL_NAME], inplace=True)\n",
    "else:\n",
    "    data.drop(columns=[ATTACK_CLASS_COL_NAME], inplace=True)\n",
    "\n",
    "saves_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, EXPERIMENT_NAME)\n",
    "\n",
    "checkpoint_path = os.path.join(saves_path, f\"checkpoints_{csv_file_name}.pth\")\n",
    "best_model_path = os.path.join(saves_path, f\"best_model_{csv_file_name}.pth\")\n",
    "\n",
    "os.makedirs(saves_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449a1af1-1d3d-4179-9628-7c2ec551ce0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srcip</th>\n",
       "      <th>sport</th>\n",
       "      <th>dstip</th>\n",
       "      <th>dsport</th>\n",
       "      <th>state</th>\n",
       "      <th>dur</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>source_file_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.40.85.1_0</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0.0.5_0</td>\n",
       "      <td>0</td>\n",
       "      <td>INT</td>\n",
       "      <td>50.004341</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.166.0.6_0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.4_0</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>132</td>\n",
       "      <td>164</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175.45.176.0_0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.16_0</td>\n",
       "      <td>80</td>\n",
       "      <td>FIN</td>\n",
       "      <td>2.390390</td>\n",
       "      <td>1362</td>\n",
       "      <td>268</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reconnaissance</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.166.0.3_0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.8_0</td>\n",
       "      <td>25</td>\n",
       "      <td>FIN</td>\n",
       "      <td>34.077175</td>\n",
       "      <td>37358</td>\n",
       "      <td>3380</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.40.170.2_0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.40.170.2_0</td>\n",
       "      <td>0</td>\n",
       "      <td>INT</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543154</th>\n",
       "      <td>175.45.176.1_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.11_3</td>\n",
       "      <td>6071</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.291164</td>\n",
       "      <td>732</td>\n",
       "      <td>468</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Generic</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543155</th>\n",
       "      <td>175.45.176.3_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.16_3</td>\n",
       "      <td>2140</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.011751</td>\n",
       "      <td>76</td>\n",
       "      <td>132</td>\n",
       "      <td>254</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Backdoors</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543156</th>\n",
       "      <td>59.166.0.2_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.4_3</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>146</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Normal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543157</th>\n",
       "      <td>175.45.176.1_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.11_3</td>\n",
       "      <td>5250</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.176514</td>\n",
       "      <td>10778</td>\n",
       "      <td>268</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Generic</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543158</th>\n",
       "      <td>59.166.0.2_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.2_3</td>\n",
       "      <td>8406</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.049598</td>\n",
       "      <td>2646</td>\n",
       "      <td>25564</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Normal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543159 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 srcip  sport             dstip dsport state        dur  \\\n",
       "0         10.40.85.1_0      0       224.0.0.5_0      0   INT  50.004341   \n",
       "1         59.166.0.6_0      0   149.171.126.4_0     53   CON   0.001134   \n",
       "2       175.45.176.0_0      0  149.171.126.16_0     80   FIN   2.390390   \n",
       "3         59.166.0.3_0      0   149.171.126.8_0     25   FIN  34.077175   \n",
       "4        10.40.170.2_0      0     10.40.170.2_0      0   INT   0.000000   \n",
       "...                ...    ...               ...    ...   ...        ...   \n",
       "543154  175.45.176.1_3      0  149.171.126.11_3   6071   FIN   0.291164   \n",
       "543155  175.45.176.3_3      0  149.171.126.16_3   2140   CON   0.011751   \n",
       "543156    59.166.0.2_3      0   149.171.126.4_3     53   CON   0.002410   \n",
       "543157  175.45.176.1_3      0  149.171.126.11_3   5250   FIN   0.176514   \n",
       "543158    59.166.0.2_3      0   149.171.126.2_3   8406   FIN   0.049598   \n",
       "\n",
       "        sbytes  dbytes  sttl  dttl  ...  ct_ftp_cmd  ct_srv_src  ct_srv_dst  \\\n",
       "0          384       0     1     0  ...         0.0           2           4   \n",
       "1          132     164    31    29  ...         0.0          12           7   \n",
       "2         1362     268   254   252  ...         0.0           5           2   \n",
       "3        37358    3380    31    29  ...         0.0           1           1   \n",
       "4           46       0     0     0  ...         0.0           2           2   \n",
       "...        ...     ...   ...   ...  ...         ...         ...         ...   \n",
       "543154     732     468   254   252  ...         NaN           1           1   \n",
       "543155      76     132   254    60  ...         NaN           1           1   \n",
       "543156     146     178    31    29  ...         NaN           3           5   \n",
       "543157   10778     268   254   252  ...         NaN           1           1   \n",
       "543158    2646   25564    31    29  ...         NaN           6           2   \n",
       "\n",
       "        ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  \\\n",
       "0                4           2                 2                 4   \n",
       "1                1           2                 2                 1   \n",
       "2                2           1                 1                 1   \n",
       "3               12          10                 1                 1   \n",
       "4                2           2                 2                 2   \n",
       "...            ...         ...               ...               ...   \n",
       "543154           1           1                 1                 1   \n",
       "543155           1           1                 1                 1   \n",
       "543156           3           2                 2                 2   \n",
       "543157           1           1                 1                 1   \n",
       "543158           4           7                 1                 1   \n",
       "\n",
       "        ct_dst_src_ltm      attack_cat  source_file_id  \n",
       "0                    2          Normal               0  \n",
       "1                    1          Normal               0  \n",
       "2                    1  Reconnaissance               0  \n",
       "3                    2          Normal               0  \n",
       "4                    2          Normal               0  \n",
       "...                ...             ...             ...  \n",
       "543154               2         Generic               3  \n",
       "543155               1       Backdoors               3  \n",
       "543156               4          Normal               3  \n",
       "543157               2         Generic               3  \n",
       "543158               3          Normal               3  \n",
       "\n",
       "[543159 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns=UNSW_NB15_Config.DROP_COLS,inplace=True)\n",
    "data.drop(columns=UNSW_NB15_Config.TIME_COL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2c690c-86a4-49f7-aa9c-58f94529547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME].apply(str)\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME].apply(str)\n",
    "\n",
    "# # Combine Port and IP\n",
    "data[SOURCE_PORT_COL_NAME] = data[SOURCE_PORT_COL_NAME].apply(str)\n",
    "data[DESTINATION_PORT_COL_NAME] = data[DESTINATION_PORT_COL_NAME].apply(str)\n",
    "\n",
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME] + ':' + data[SOURCE_PORT_COL_NAME]\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME] + ':' + data[DESTINATION_PORT_COL_NAME]\n",
    "data.drop(columns=[SOURCE_PORT_COL_NAME,DESTINATION_PORT_COL_NAME],inplace=True)\n",
    "\n",
    "# data[SOURCE_PORT_COL_NAME] = pd.to_numeric(data[SOURCE_PORT_COL_NAME], errors='coerce').fillna(0).astype(int)\n",
    "# data[DESTINATION_PORT_COL_NAME] = pd.to_numeric(data[DESTINATION_PORT_COL_NAME], errors='coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5651ef5b-0a9d-4641-aad7-5738092c46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                    srcip                  dstip state        dur  sbytes  \\\n",
      "0         10.40.85.1_0:0          224.0.0.5_0:0   INT  50.004341     384   \n",
      "1         59.166.0.6_0:0     149.171.126.4_0:53   CON   0.001134     132   \n",
      "2       175.45.176.0_0:0    149.171.126.16_0:80   FIN   2.390390    1362   \n",
      "3         59.166.0.3_0:0     149.171.126.8_0:25   FIN  34.077175   37358   \n",
      "4        10.40.170.2_0:0        10.40.170.2_0:0   INT   0.000000      46   \n",
      "...                  ...                    ...   ...        ...     ...   \n",
      "543154  175.45.176.1_3:0  149.171.126.11_3:6071   FIN   0.291164     732   \n",
      "543155  175.45.176.3_3:0  149.171.126.16_3:2140   CON   0.011751      76   \n",
      "543156    59.166.0.2_3:0     149.171.126.4_3:53   CON   0.002410     146   \n",
      "543157  175.45.176.1_3:0  149.171.126.11_3:5250   FIN   0.176514   10778   \n",
      "543158    59.166.0.2_3:0   149.171.126.2_3:8406   FIN   0.049598    2646   \n",
      "\n",
      "        dbytes  sttl  dttl  sloss  dloss  ...  ct_ftp_cmd  ct_srv_src  \\\n",
      "0            0     1     0      0      0  ...         0.0           2   \n",
      "1          164    31    29      0      0  ...         0.0          12   \n",
      "2          268   254   252      6      1  ...         0.0           5   \n",
      "3         3380    31    29     18      8  ...         0.0           1   \n",
      "4            0     0     0      0      0  ...         0.0           2   \n",
      "...        ...   ...   ...    ...    ...  ...         ...         ...   \n",
      "543154     468   254   252      3      2  ...         NaN           1   \n",
      "543155     132   254    60      0      0  ...         NaN           1   \n",
      "543156     178    31    29      0      0  ...         NaN           3   \n",
      "543157     268   254   252      5      1  ...         NaN           1   \n",
      "543158   25564    31    29      7     15  ...         NaN           6   \n",
      "\n",
      "        ct_srv_dst  ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  \\\n",
      "0                4           4           2                 2   \n",
      "1                7           1           2                 2   \n",
      "2                2           2           1                 1   \n",
      "3                1          12          10                 1   \n",
      "4                2           2           2                 2   \n",
      "...            ...         ...         ...               ...   \n",
      "543154           1           1           1                 1   \n",
      "543155           1           1           1                 1   \n",
      "543156           5           3           2                 2   \n",
      "543157           1           1           1                 1   \n",
      "543158           2           4           7                 1   \n",
      "\n",
      "        ct_dst_sport_ltm  ct_dst_src_ltm      attack_cat  source_file_id  \n",
      "0                      4               2          Normal               0  \n",
      "1                      1               1          Normal               0  \n",
      "2                      1               1  Reconnaissance               0  \n",
      "3                      1               2          Normal               0  \n",
      "4                      2               2          Normal               0  \n",
      "...                  ...             ...             ...             ...  \n",
      "543154                 1               2         Generic               3  \n",
      "543155                 1               1       Backdoors               3  \n",
      "543156                 2               4          Normal               3  \n",
      "543157                 1               2         Generic               3  \n",
      "543158                 1               3          Normal               3  \n",
      "\n",
      "[543159 rows x 45 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7fb458-ca34-42ca-a8af-f8e1609aff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns = UNSW_NB15_Config.CATEGORICAL_COLS) # One Hot Encoding for categorical data\n",
    "converted_categorical_cols = [col for col in data.columns if col.startswith(tuple(UNSW_NB15_Config.CATEGORICAL_COLS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d96115-31f9-48cb-b3e6-7853d2d253cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                    srcip                  dstip        dur  sbytes  dbytes  \\\n",
      "0         10.40.85.1_0:0          224.0.0.5_0:0  50.004341     384       0   \n",
      "1         59.166.0.6_0:0     149.171.126.4_0:53   0.001134     132     164   \n",
      "2       175.45.176.0_0:0    149.171.126.16_0:80   2.390390    1362     268   \n",
      "3         59.166.0.3_0:0     149.171.126.8_0:25  34.077175   37358    3380   \n",
      "4        10.40.170.2_0:0        10.40.170.2_0:0   0.000000      46       0   \n",
      "...                  ...                    ...        ...     ...     ...   \n",
      "543154  175.45.176.1_3:0  149.171.126.11_3:6071   0.291164     732     468   \n",
      "543155  175.45.176.3_3:0  149.171.126.16_3:2140   0.011751      76     132   \n",
      "543156    59.166.0.2_3:0     149.171.126.4_3:53   0.002410     146     178   \n",
      "543157  175.45.176.1_3:0  149.171.126.11_3:5250   0.176514   10778     268   \n",
      "543158    59.166.0.2_3:0   149.171.126.2_3:8406   0.049598    2646   25564   \n",
      "\n",
      "        sttl  dttl  sloss  dloss          Sload  ...  state_ECR  state_FIN  \\\n",
      "0          1     0      0      0      51.195557  ...      False      False   \n",
      "1         31    29      0      0  465608.468800  ...      False      False   \n",
      "2        254   252      6      1    4233.619141  ...      False       True   \n",
      "3         31    29     18      8    8601.652344  ...      False       True   \n",
      "4          0     0      0      0       0.000000  ...      False      False   \n",
      "...      ...   ...    ...    ...            ...  ...        ...        ...   \n",
      "543154   254   252      3      2   18436.343750  ...      False       True   \n",
      "543155   254    60      0      0   25870.138670  ...      False      False   \n",
      "543156    31    29      0      0  242323.656300  ...      False      False   \n",
      "543157   254   252      5      1  457980.656300  ...      False       True   \n",
      "543158    31    29      7     15  416629.687500  ...      False       True   \n",
      "\n",
      "        state_INT  state_MAS  state_PAR  state_REQ  state_RST  state_TST  \\\n",
      "0            True      False      False      False      False      False   \n",
      "1           False      False      False      False      False      False   \n",
      "2           False      False      False      False      False      False   \n",
      "3           False      False      False      False      False      False   \n",
      "4            True      False      False      False      False      False   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "543154      False      False      False      False      False      False   \n",
      "543155      False      False      False      False      False      False   \n",
      "543156      False      False      False      False      False      False   \n",
      "543157      False      False      False      False      False      False   \n",
      "543158      False      False      False      False      False      False   \n",
      "\n",
      "        state_TXD  state_URH  \n",
      "0           False      False  \n",
      "1           False      False  \n",
      "2           False      False  \n",
      "3           False      False  \n",
      "4           False      False  \n",
      "...           ...        ...  \n",
      "543154      False      False  \n",
      "543155      False      False  \n",
      "543156      False      False  \n",
      "543157      False      False  \n",
      "543158      False      False  \n",
      "\n",
      "[543159 rows x 58 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "data.drop(columns=['index'],inplace=True)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1df5c4c-70a2-4566-ae5e-ee3dcc6037a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 dur        sbytes        dbytes           sttl  \\\n",
      "count  543159.000000  5.431590e+05  5.431590e+05  543159.000000   \n",
      "mean        0.703562  5.129376e+03  1.912066e+04     157.223966   \n",
      "std        12.635598  1.202304e+05  1.382834e+05     108.429349   \n",
      "min         0.000000  0.000000e+00  0.000000e+00       0.000000   \n",
      "25%         0.000007  1.140000e+02  0.000000e+00      31.000000   \n",
      "50%         0.000010  2.000000e+02  0.000000e+00     254.000000   \n",
      "75%         0.070875  1.580000e+03  1.936000e+03     254.000000   \n",
      "max      8760.776367  1.435577e+07  1.465753e+07     255.000000   \n",
      "\n",
      "                dttl          sloss          dloss         Sload  \\\n",
      "count  543159.000000  543159.000000  543159.000000  5.431590e+05   \n",
      "mean       38.847354       3.789714       8.637535  6.901181e+07   \n",
      "std        77.059190      45.614073      49.869719  1.425974e+08   \n",
      "min         0.000000       0.000000       0.000000  0.000000e+00   \n",
      "25%         0.000000       0.000000       0.000000  3.760815e+05   \n",
      "50%         0.000000       0.000000       0.000000  4.560000e+07   \n",
      "75%        29.000000       3.000000       4.000000  8.888889e+07   \n",
      "max       254.000000    5319.000000    5507.000000  5.988000e+09   \n",
      "\n",
      "              Dload          Spkts  ...  ct_flw_http_mthd   is_ftp_login  \\\n",
      "count  5.431590e+05  543159.000000  ...     543159.000000  543159.000000   \n",
      "mean   1.145602e+06      20.260456  ...          0.089263       0.011459   \n",
      "std    3.125320e+06     101.785929  ...          0.568852       0.109870   \n",
      "min    0.000000e+00       0.000000  ...          0.000000       0.000000   \n",
      "25%    0.000000e+00       2.000000  ...          0.000000       0.000000   \n",
      "50%    0.000000e+00       2.000000  ...          0.000000       0.000000   \n",
      "75%    4.080209e+05      14.000000  ...          0.000000       0.000000   \n",
      "max    2.248756e+07   10646.000000  ...         36.000000       4.000000   \n",
      "\n",
      "          ct_ftp_cmd     ct_srv_src     ct_srv_dst     ct_dst_ltm  \\\n",
      "count  543159.000000  543159.000000  543159.000000  543159.000000   \n",
      "mean        0.007661      15.025361      14.853214      10.321932   \n",
      "std         0.091356      14.239878      14.314732      10.996982   \n",
      "min         0.000000       1.000000       1.000000       1.000000   \n",
      "25%         0.000000       3.000000       3.000000       2.000000   \n",
      "50%         0.000000       9.000000       8.000000       5.000000   \n",
      "75%         0.000000      26.000000      26.000000      17.000000   \n",
      "max         4.000000      67.000000      67.000000      67.000000   \n",
      "\n",
      "          ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \n",
      "count  543159.000000     543159.000000     543159.000000   543159.000000  \n",
      "mean       10.848566          9.357573          7.219855       13.786578  \n",
      "std        10.976383         11.399195          8.074346       14.983005  \n",
      "min         1.000000          1.000000          1.000000        1.000000  \n",
      "25%         2.000000          1.000000          1.000000        1.000000  \n",
      "50%         6.000000          2.000000          2.000000        5.000000  \n",
      "75%        17.000000         17.000000         15.000000       26.000000  \n",
      "max        67.000000         67.000000         60.000000       67.000000  \n",
      "\n",
      "[8 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_norm = UNSW_NB15_Config.COLS_TO_NORM\n",
    "print(data[cols_to_norm].describe()) # Check if there's any too large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea95177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All other columns processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_numeric_issues(df, cols_to_norm):\n",
    "    for col in cols_to_norm:\n",
    "        try:\n",
    "            # Try to coerce to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Try to clip the column\n",
    "            df[col] = df[col].clip(lower=-1e9, upper=1e9)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Column '{col}' failed with error: {e}\")\n",
    "            print(f\"  - Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "            print(f\"  - Data type: {df[col].dtype}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n✅ All other columns processed successfully.\")\n",
    "\n",
    "check_numeric_issues(data, UNSW_NB15_Config.COLS_TO_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37256006-abc1-44aa-8e74-46d05dc6ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c6e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "Attack label mapping: {'Analysis': 0, 'Backdoors': 1, 'DoS': 2, 'Exploits': 3, 'Fuzzers': 4, 'Generic': 5, 'Normal': 6, 'Reconnaissance': 7, 'Shellcode': 8, 'Worms': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_classes = 2\n",
    "class_map = [0, 1]\n",
    "if MULTICLASS:\n",
    "    le = LabelEncoder()\n",
    "    attack_labels = le.fit_transform(data[ATTACK_CLASS_COL_NAME])\n",
    "    class_map = le.classes_\n",
    "    print(class_map)\n",
    "    print(\"Attack label mapping:\", dict(zip(class_map, range(len(class_map)))))\n",
    "    data[ATTACK_CLASS_COL_NAME] = attack_labels\n",
    "    num_classes = len(class_map)\n",
    "    class_dict = {le.inverse_transform([i])[0]: i for i in range(len(le.classes_))}\n",
    "\n",
    "BENIGN_CLASS_LABEL = le.transform([BENIGN_CLASS_NAME])[0] if MULTICLASS else 0\n",
    "ADVERSARIAL_CLASS_LABEL = len(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d35f4cdd-2716-431f-af50-b34cc3d2d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'state_ACC', 'state_CLO', 'state_CON', 'state_ECO', 'state_ECR', 'state_FIN', 'state_INT', 'state_MAS', 'state_PAR', 'state_REQ', 'state_RST', 'state_TST', 'state_TXD', 'state_URH']\n",
      "Number of training samples: 461685\n",
      "attack_cat\n",
      "6    188595\n",
      "5    183159\n",
      "3     37846\n",
      "4     20609\n",
      "2     13900\n",
      "7     11889\n",
      "0      2275\n",
      "1      1980\n",
      "8      1284\n",
      "9       148\n",
      "Name: count, dtype: int64\n",
      "Number of test samples: 81474\n",
      "attack_cat\n",
      "6    33281\n",
      "5    32322\n",
      "3     6679\n",
      "4     3637\n",
      "2     2453\n",
      "7     2098\n",
      "0      402\n",
      "1      349\n",
      "8      227\n",
      "9       26\n",
      "Name: count, dtype: int64\n",
      "                   srcip                  dstip       dur    sbytes    dbytes  \\\n",
      "35798   175.45.176.0_0:0     149.171.126.11_0:0  0.024440 -0.037756 -0.136334   \n",
      "454500    59.166.0.2_3:0  149.171.126.1_3:39482 -0.052447 -0.010009  0.271236   \n",
      "23187   175.45.176.0_0:0     149.171.126.18_0:0 -0.017379 -0.033264 -0.136334   \n",
      "489755  175.45.176.3_3:0    149.171.126.15_3:53 -0.055680 -0.041715 -0.138272   \n",
      "134305    59.166.0.1_1:0     149.171.126.4_1:25 -0.004966  0.269172 -0.113829   \n",
      "\n",
      "            sttl      dttl     sloss     dloss     Sload     Dload     Spkts  \\\n",
      "35798   0.892527  2.766092 -0.039236 -0.153150 -0.554662 -0.365989 -0.100804   \n",
      "454500 -1.164114 -0.127790  0.070379  0.368209 -0.548462  3.128362  0.449370   \n",
      "23187   0.892527  2.766092 -0.039236 -0.153150 -0.554559 -0.365371 -0.100804   \n",
      "489755  0.892527 -0.504124 -0.083082 -0.173202 -0.018194 -0.366555 -0.179401   \n",
      "134305 -1.164114 -0.127790  0.311533 -0.012784 -0.550916 -0.353373  0.311827   \n",
      "\n",
      "           Dpkts      swin      dwin     stcpb     dtcpb   smeansz   dmeansz  \\\n",
      "35798  -0.158623  1.303104  1.304850  0.950180 -0.462170 -0.329716 -0.366711   \n",
      "454500  0.427016  1.303104  1.304850  0.274400  0.303777 -0.329716  2.437416   \n",
      "23187  -0.158623  1.303104  1.304850  1.484216  1.484299  0.017606 -0.366711   \n",
      "489755 -0.215298 -0.767401 -0.766374 -0.720896 -0.720905 -0.342580 -0.526845   \n",
      "134305  0.181426  1.303104  1.304850  1.484216 -0.715505  3.928197 -0.242162   \n",
      "\n",
      "        trans_depth  res_bdy_len      Sjit      Djit       Stime       Ltime  \\\n",
      "35798     -0.134792     -0.05343  0.153264 -0.076895  1421933745  1421933746   \n",
      "454500    -0.134792     -0.05343 -0.080902 -0.175606  1424240533  1424240533   \n",
      "23187     -0.134792     -0.05343  0.012423 -0.150894  1421931706  1421931707   \n",
      "489755    -0.134792     -0.05343 -0.080902 -0.189417  1424245264  1424245264   \n",
      "134305    -0.134792     -0.05343 -0.043454 -0.177189  1424222454  1424222454   \n",
      "\n",
      "         Sintpkt   Dintpkt    tcprtt    synack    ackdat  is_sm_ips_ports  \\\n",
      "35798  -0.008638  0.119318  3.717742  3.612803  3.416380        -0.028049   \n",
      "454500 -0.060137 -0.054933 -0.278494 -0.242636 -0.288018        -0.028049   \n",
      "23187  -0.036975  0.019956  2.899294  2.974987  2.483664        -0.028049   \n",
      "489755 -0.060450 -0.055530 -0.289971 -0.258558 -0.293097        -0.028049   \n",
      "134305 -0.053968 -0.040295 -0.277104 -0.239905 -0.288324        -0.028049   \n",
      "\n",
      "        ct_state_ttl  ct_flw_http_mthd  is_ftp_login  ct_ftp_cmd  ct_srv_src  \\\n",
      "35798      -0.098678         -0.156918     -0.104295   -0.083856   -0.633809   \n",
      "454500     -1.123769         -0.156918     -0.104295   -0.083856   -0.774260   \n",
      "23187      -0.098678         -0.156918     -0.104295   -0.083856   -0.984936   \n",
      "489755      0.926413         -0.156918     -0.104295   -0.083856    0.489797   \n",
      "134305     -1.123769         -0.156918     -0.104295   -0.083856   -0.984936   \n",
      "\n",
      "        ct_srv_dst  ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  \\\n",
      "35798    -0.618469   -0.756748   -0.806147         -0.733173   \n",
      "454500   -0.688327   -0.574879   -0.715042         -0.733173   \n",
      "23187    -0.967760   -0.483945   -0.715042         -0.733173   \n",
      "489755    0.499261    1.061935    1.015949          1.109064   \n",
      "134305   -0.967760   -0.483945   -0.441727         -0.733173   \n",
      "\n",
      "        ct_dst_sport_ltm  ct_dst_src_ltm  attack_cat  source_file_id  \\\n",
      "35798          -0.770324       -0.786664           4               0   \n",
      "454500         -0.770324       -0.786664           6               3   \n",
      "23187          -0.770324       -0.853406           3               0   \n",
      "489755          1.830509        0.548183           5               3   \n",
      "134305         -0.770324       -0.786664           6               1   \n",
      "\n",
      "        state_ACC  state_CLO  state_CON  state_ECO  state_ECR  state_FIN  \\\n",
      "35798       False      False      False      False      False       True   \n",
      "454500      False      False      False      False      False       True   \n",
      "23187       False      False      False      False      False       True   \n",
      "489755      False      False      False      False      False      False   \n",
      "134305      False      False      False      False      False       True   \n",
      "\n",
      "        state_INT  state_MAS  state_PAR  state_REQ  state_RST  state_TST  \\\n",
      "35798       False      False      False      False      False      False   \n",
      "454500      False      False      False      False      False      False   \n",
      "23187       False      False      False      False      False      False   \n",
      "489755       True      False      False      False      False      False   \n",
      "134305      False      False      False      False      False      False   \n",
      "\n",
      "        state_TXD  state_URH  \\\n",
      "35798       False      False   \n",
      "454500      False      False   \n",
      "23187       False      False   \n",
      "489755      False      False   \n",
      "134305      False      False   \n",
      "\n",
      "                                                        h  \n",
      "35798   [0.024439518249307186, -0.03775569116632746, -...  \n",
      "454500  [-0.05244682114681552, -0.01000892950063232, 0...  \n",
      "23187   [-0.01737919714689172, -0.03326430888231206, -...  \n",
      "489755  [-0.05568046603287485, -0.04171476147594104, -...  \n",
      "134305  [-0.004966318747785316, 0.269172066323925, -0....  \n"
     ]
    }
   ],
   "source": [
    "# 70% train, 15% validation, 15% test\n",
    "train_full_df, test_df = train_test_split(\n",
    "     data, test_size=0.15, random_state=42, stratify=data[label_col])\n",
    "\n",
    "\n",
    "# # Maintain the order of the rows in the original dataframe\n",
    "# train_full_df = train_full_df.sort_values(by=UNSW_NB15_Config.TIME_COL_NAMES)\n",
    "# test_df = test_df.sort_values(by=UNSW_NB15_Config.TIME_COL_NAMES)\n",
    "\n",
    "feature_cols = UNSW_NB15_Config.COLS_TO_NORM + converted_categorical_cols\n",
    "\n",
    "print('Feature Columns:', feature_cols)\n",
    "\n",
    "train_full_df['h'] = train_full_df[ feature_cols ].values.tolist()\n",
    "test_df['h'] = test_df[ feature_cols ].values.tolist()\n",
    "\n",
    "# X_train = train_full_df.drop(columns=[label_col])\n",
    "# X_val = val_df.drop(columns=[label_col])\n",
    "# X_test = test_df.drop(columns=[label_col])\n",
    "\n",
    "y_train = train_full_df[label_col]\n",
    "y_test = test_df[label_col]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_full_df))\n",
    "print(y_train.value_counts())\n",
    "print(\"Number of test samples:\", len(test_df))\n",
    "print(y_test.value_counts())\n",
    "\n",
    "print(train_full_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f83bd73-531f-42a0-9f73-e1fd98dce1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, source_ip_col, destination_ip_col, edge_attr, create_using=nx.MultiDiGraph(), **kwargs):\n",
    "    G_nx = nx.from_pandas_edgelist(df, source_ip_col, destination_ip_col, edge_attr, create_using=create_using, **kwargs)\n",
    "    G_pyg = from_networkx(G_nx)\n",
    "\n",
    "    num_nodes = G_pyg.num_nodes\n",
    "    num_edges = G_pyg.num_edges\n",
    "\n",
    "    G_pyg.x = th.ones(num_nodes, len(df['h'].iloc[0])) \n",
    "\n",
    "    edge_attr_list = []\n",
    "    edge_label_list = []\n",
    "\n",
    "    for u, v, key, data in G_nx.edges(keys=True, data=True):\n",
    "        edge_attr_list.append(data['h']) \n",
    "        edge_label_list.append(data[label_col]) \n",
    "\n",
    "    G_pyg.edge_attr = th.tensor(edge_attr_list, dtype=th.float32)\n",
    "    G_pyg.edge_label = th.tensor(edge_label_list, dtype=th.long)\n",
    "\n",
    "    print(\"Number of edges in G_pyg:\", num_edges)\n",
    "    print(\"Number of node in G_pyg:\", num_nodes)\n",
    "    print(\"Shape of node in G_pyg:\", G_pyg.x.shape)\n",
    "    print(\"Shape of edge attr in G_pyg:\", G_pyg.edge_attr.shape)\n",
    "    print(\"Shape of edge label in G_pyg:\", G_pyg.edge_label.shape)\n",
    "\n",
    "    return G_nx, G_pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8f9cdb-1316-461a-a927-8d67d90d6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 9773\n",
      "Shape of node in G_pyg: torch.Size([9773, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41795339-6036-468f-9b9d-2bb68d78ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGELayerPyG(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_dim, out_channels, activation=F.relu):\n",
    "        super().__init__(aggr='mean')  # mean aggregation\n",
    "        self.W_msg = nn.Linear(in_channels + edge_dim, out_channels)\n",
    "        self.W_apply = nn.Linear(in_channels + out_channels, out_channels)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: [num_nodes, in_channels]\n",
    "        # edge_attr: [num_edges, edge_dim]\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j: features of source nodes (neighbours)\n",
    "        msg_input = th.cat([x_j, edge_attr], dim=1)\n",
    "        return self.W_msg(msg_input)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out: [num_nodes, out_channels]\n",
    "        combined = th.cat([x, aggr_out], dim=1)\n",
    "        out = self.W_apply(combined)\n",
    "        return self.activation(out)\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.lin = nn.Linear(in_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, data, z):\n",
    "        row, col = data.edge_index\n",
    "        # Concatenate the features of source and target nodes for each edge\n",
    "        edge_feat = th.cat([z[row], z[col]], dim=1)\n",
    "        return self.lin(edge_feat)\n",
    "\n",
    "class EGraphSAGE(nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels, out_channels, dropout=0.2):\n",
    "        super(EGraphSAGE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGELayerPyG(node_in_channels, edge_in_channels, hidden_channels)\n",
    "        self.conv2 = SAGELayerPyG(hidden_channels, edge_in_channels, hidden_channels)\n",
    "        self.mlp_predictor = MLPPredictor(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return self.mlp_predictor(data, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bca25fef-29d9-40cf-8910-16b24d530693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cccdc850-b98d-4836-b82b-67aa4b9e1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89157faf-e24b-49d6-9c90-6f71dae515b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "385d37f0-713b-4abc-8d7a-3e768ae9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def grid_search(data, epochs, learning_rates, hidden_dims, drop_outs):\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_params = {}\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Precompute the train and validation graphs for all folds\n",
    "    folds = []\n",
    "    for train_idx, val_idx in skf.split(data, data[label_col]):\n",
    "        train_df = data.iloc[train_idx]\n",
    "        val_df = data.iloc[val_idx]\n",
    "\n",
    "        G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "        G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "\n",
    "        G_pyg_train = G_pyg_train.to(device)\n",
    "        G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "        G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "        G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "        G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "        G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "        folds.append((G_pyg_train, G_pyg_val))\n",
    "\n",
    "    params_results = {}\n",
    "    for lr in learning_rates:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for drop_out in drop_outs:\n",
    "                print(f\"Testing with learning rate: {lr}, hidden_dim: {hidden_dim}\")\n",
    "                fold_f1_scores = []\n",
    "\n",
    "                for fold, (G_pyg_train, G_pyg_val) in enumerate(folds):\n",
    "                    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "                    model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                                    edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                                    hidden_channels=hidden_dim,\n",
    "                                    dropout=drop_out,\n",
    "                                    out_channels=num_classes).to(device)\n",
    "\n",
    "                    model.apply(init_weights)\n",
    "\n",
    "                    labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "                    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                                    classes=np.unique(labels),\n",
    "                                                                    y=labels)\n",
    "\n",
    "                    # Normalize to stabilize training\n",
    "                    class_weights = th.FloatTensor(class_weights).to(device)\n",
    "                    print(\"Class weights:\", class_weights)\n",
    "\n",
    "                    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "                    optimizer = th.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    best_epoch_f1 = 0  # Track the best F1 score for this fold\n",
    "\n",
    "                    for epoch in range(epochs):\n",
    "                        train_loss = 0\n",
    "                        val_loss = 0\n",
    "\n",
    "                        try:\n",
    "                            model.train()\n",
    "                            out = model(G_pyg_train)\n",
    "                            \n",
    "                            optimizer.zero_grad()\n",
    "                            loss = criterion(out, G_pyg_train.edge_label)\n",
    "                            train_loss = loss.item()\n",
    "\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                            model.eval()\n",
    "                            with th.no_grad():\n",
    "                                out = model(G_pyg_val)\n",
    "                                loss = criterion(out, G_pyg_val.edge_label)\n",
    "                                val_loss = loss.item()\n",
    "\n",
    "                            val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "\n",
    "                            if val_f1 > best_epoch_f1:\n",
    "                                best_epoch_f1 = val_f1  # Update the best F1 score for this fold\n",
    "                                print(f\"Best F1 Score at epoch {epoch}: {best_epoch_f1:.4f}, Parameters: lr={lr}, hidden_dim{hidden_dim}, drop_out={drop_out}\")\n",
    "\n",
    "                            if epoch % 100 == 0:\n",
    "                                print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred at epoch {epoch}: {str(e)}\")\n",
    "                            break\n",
    "\n",
    "                    fold_f1_scores.append(best_epoch_f1)  # Append the best F1 score for this fold\n",
    "                \n",
    "                avg_f1 = sum(fold_f1_scores) / len(fold_f1_scores)\n",
    "                params_results[(drop_out, lr, hidden_dim)] = {'folds': fold_f1_scores, 'avg_f1': avg_f1}\n",
    "                print(\"Current Results: \", params_results)\n",
    "                print(f\"Average F1 Score for dropout {drop_out} learning rate {lr}, hidden_dim {hidden_dim}: {avg_f1:.4f}\")\n",
    "\n",
    "                if avg_f1 > best_f1:\n",
    "                    best_f1 = avg_f1\n",
    "                    best_params = {'learning_rate': lr, 'hidden_dim': hidden_dim, 'drop_out': drop_out}\n",
    "\n",
    "        print(f\"Best Parameters: {best_params}, Best F1 Score: {best_f1:.4f}\")\n",
    "        print(\"All results:\", params_results)\n",
    "\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "hidden_dims = [128, 256, 512]\n",
    "drop_outs = [0.2, 0.3, 0.4]\n",
    "\n",
    "# grid_search(train_full_df, epochs=100, learning_rates=learning_rates, hidden_dims=hidden_dims, drop_outs=drop_outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f52b2fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 392432\n",
      "Number of node in G_pyg: 40942\n",
      "Shape of node in G_pyg: torch.Size([40942, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([392432, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([392432])\n",
      "Number of edges in G_pyg: 69253\n",
      "Number of node in G_pyg: 8688\n",
      "Shape of node in G_pyg: torch.Size([8688, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([69253, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([69253])\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "     train_full_df, test_size=0.15, random_state=42, stratify=train_full_df[label_col])\n",
    "\n",
    "G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88bdffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([2.0291e+01, 2.3317e+01, 3.3215e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8832e+00, 3.5970e+01, 3.1145e+02], device='cuda:0')\n",
      "Resumed training from epoch 1621\n",
      "Epoch 1621, Train Loss: 0.7832, Validation Loss: 1.0482, Validation F1: 0.9051\n",
      "Epoch 1622, Train Loss: 0.7822, Validation Loss: 1.0418, Validation F1: 0.9046\n",
      "Epoch 1623, Train Loss: 0.7825, Validation Loss: 1.0435, Validation F1: 0.8987\n",
      "Epoch 1624, Train Loss: 0.7825, Validation Loss: 1.0647, Validation F1: 0.9005\n",
      "Epoch 1625, Train Loss: 0.7810, Validation Loss: 1.1033, Validation F1: 0.9015\n",
      "Epoch 1626, Train Loss: 0.7798, Validation Loss: 1.0428, Validation F1: 0.9057\n",
      "Epoch 1627, Train Loss: 0.7822, Validation Loss: 1.0381, Validation F1: 0.9057\n",
      "Epoch 1628, Train Loss: 0.7830, Validation Loss: 1.0594, Validation F1: 0.9048\n",
      "Epoch 1629, Train Loss: 0.7808, Validation Loss: 1.0634, Validation F1: 0.9008\n",
      "Epoch 1630, Train Loss: 0.7808, Validation Loss: 1.0540, Validation F1: 0.8979\n",
      "Epoch 1631, Train Loss: 0.7812, Validation Loss: 1.0790, Validation F1: 0.9013\n",
      "Epoch 1632, Train Loss: 0.7819, Validation Loss: 1.0606, Validation F1: 0.9032\n",
      "Epoch 1633, Train Loss: 0.7813, Validation Loss: 1.0470, Validation F1: 0.9062\n",
      "Epoch 1634, Train Loss: 0.7810, Validation Loss: 1.0347, Validation F1: 0.9064\n",
      "Epoch 1635, Train Loss: 0.7810, Validation Loss: 1.1082, Validation F1: 0.9030\n",
      "Epoch 1636, Train Loss: 0.7808, Validation Loss: 1.0672, Validation F1: 0.8984\n",
      "Epoch 1637, Train Loss: 0.7839, Validation Loss: 1.0562, Validation F1: 0.8975\n",
      "Epoch 1638, Train Loss: 0.7817, Validation Loss: 1.0354, Validation F1: 0.9038\n",
      "Epoch 1639, Train Loss: 0.7818, Validation Loss: 1.0539, Validation F1: 0.9067\n",
      "Epoch 1640, Train Loss: 0.7808, Validation Loss: 1.0677, Validation F1: 0.9050\n",
      "Epoch 1641, Train Loss: 0.7836, Validation Loss: 1.0527, Validation F1: 0.9039\n",
      "Epoch 1642, Train Loss: 0.7809, Validation Loss: 1.0203, Validation F1: 0.8967\n",
      "Epoch 1643, Train Loss: 0.7810, Validation Loss: 1.0488, Validation F1: 0.8944\n",
      "Epoch 1644, Train Loss: 0.7834, Validation Loss: 1.0572, Validation F1: 0.9015\n",
      "Epoch 1645, Train Loss: 0.7807, Validation Loss: 1.0615, Validation F1: 0.9060\n",
      "Epoch 1646, Train Loss: 0.7824, Validation Loss: 1.0899, Validation F1: 0.9059\n",
      "Epoch 1647, Train Loss: 0.7804, Validation Loss: 1.0990, Validation F1: 0.9030\n",
      "Epoch 1648, Train Loss: 0.7808, Validation Loss: 1.0401, Validation F1: 0.8986\n",
      "Epoch 1649, Train Loss: 0.7808, Validation Loss: 1.0779, Validation F1: 0.8988\n",
      "Epoch 1650, Train Loss: 0.7807, Validation Loss: 1.0637, Validation F1: 0.9040\n",
      "Epoch 1651, Train Loss: 0.7806, Validation Loss: 1.0543, Validation F1: 0.9053\n",
      "Epoch 1652, Train Loss: 0.7823, Validation Loss: 1.0798, Validation F1: 0.9077\n",
      "Epoch 1653, Train Loss: 0.7830, Validation Loss: 1.0626, Validation F1: 0.9023\n",
      "Epoch 1654, Train Loss: 0.7838, Validation Loss: 1.0688, Validation F1: 0.8987\n",
      "Epoch 1655, Train Loss: 0.7823, Validation Loss: 1.0551, Validation F1: 0.8998\n",
      "Epoch 1656, Train Loss: 0.7836, Validation Loss: 1.0615, Validation F1: 0.9049\n",
      "Epoch 1657, Train Loss: 0.7825, Validation Loss: 1.0758, Validation F1: 0.9063\n",
      "Epoch 1658, Train Loss: 0.7808, Validation Loss: 1.0577, Validation F1: 0.9060\n",
      "Epoch 1659, Train Loss: 0.7830, Validation Loss: 1.0662, Validation F1: 0.9061\n",
      "Epoch 1660, Train Loss: 0.7827, Validation Loss: 1.1000, Validation F1: 0.9014\n",
      "Epoch 1661, Train Loss: 0.7824, Validation Loss: 1.1111, Validation F1: 0.9004\n",
      "Epoch 1662, Train Loss: 0.7845, Validation Loss: 1.0629, Validation F1: 0.8987\n",
      "Epoch 1663 Saved best model. Best F1: 0.9085691628016049\n",
      "Epoch 1663, Train Loss: 0.7805, Validation Loss: 1.0553, Validation F1: 0.9086\n",
      "Epoch 1664, Train Loss: 0.7835, Validation Loss: 1.0690, Validation F1: 0.9063\n",
      "Epoch 1665, Train Loss: 0.7785, Validation Loss: 1.0573, Validation F1: 0.9005\n",
      "Epoch 1666, Train Loss: 0.7821, Validation Loss: 1.0489, Validation F1: 0.8969\n",
      "Epoch 1667, Train Loss: 0.7797, Validation Loss: 1.0500, Validation F1: 0.9025\n",
      "Epoch 1668, Train Loss: 0.7832, Validation Loss: 1.0755, Validation F1: 0.8998\n",
      "Epoch 1669, Train Loss: 0.7830, Validation Loss: 1.0808, Validation F1: 0.9056\n",
      "Epoch 1670, Train Loss: 0.7815, Validation Loss: 1.0562, Validation F1: 0.9054\n",
      "Epoch 1671, Train Loss: 0.7839, Validation Loss: 1.0791, Validation F1: 0.9060\n",
      "Epoch 1672, Train Loss: 0.7848, Validation Loss: 1.0848, Validation F1: 0.9051\n",
      "Epoch 1673, Train Loss: 0.7830, Validation Loss: 1.0669, Validation F1: 0.9021\n",
      "Epoch 1674, Train Loss: 0.7833, Validation Loss: 1.0735, Validation F1: 0.8990\n",
      "Epoch 1675, Train Loss: 0.7819, Validation Loss: 1.0546, Validation F1: 0.9026\n",
      "Epoch 1676, Train Loss: 0.7820, Validation Loss: 1.0923, Validation F1: 0.9015\n",
      "Epoch 1677, Train Loss: 0.7824, Validation Loss: 1.0814, Validation F1: 0.9022\n",
      "Epoch 1678, Train Loss: 0.7879, Validation Loss: 1.0977, Validation F1: 0.9039\n",
      "Epoch 1679, Train Loss: 0.7916, Validation Loss: 1.0843, Validation F1: 0.9031\n",
      "Epoch 1680, Train Loss: 0.7886, Validation Loss: 1.0804, Validation F1: 0.9045\n",
      "Epoch 1681, Train Loss: 0.7919, Validation Loss: 1.0863, Validation F1: 0.9063\n",
      "Epoch 1682, Train Loss: 0.7844, Validation Loss: 1.0898, Validation F1: 0.9054\n",
      "Epoch 1683, Train Loss: 0.7903, Validation Loss: 1.0535, Validation F1: 0.9049\n",
      "Epoch 1684, Train Loss: 0.7832, Validation Loss: 1.0662, Validation F1: 0.8991\n",
      "Epoch 1685, Train Loss: 0.7851, Validation Loss: 1.0728, Validation F1: 0.9002\n",
      "Epoch 1686, Train Loss: 0.7827, Validation Loss: 1.0843, Validation F1: 0.9036\n",
      "Epoch 1687, Train Loss: 0.7871, Validation Loss: 1.0749, Validation F1: 0.9019\n",
      "Epoch 1688, Train Loss: 0.7822, Validation Loss: 1.0606, Validation F1: 0.9012\n",
      "Epoch 1689, Train Loss: 0.7809, Validation Loss: 1.0813, Validation F1: 0.9028\n",
      "Epoch 1690, Train Loss: 0.7847, Validation Loss: 1.0708, Validation F1: 0.8971\n",
      "Epoch 1691, Train Loss: 0.7806, Validation Loss: 1.0882, Validation F1: 0.8981\n",
      "Epoch 1692, Train Loss: 0.7839, Validation Loss: 1.0597, Validation F1: 0.9026\n",
      "Epoch 1693, Train Loss: 0.7821, Validation Loss: 1.0878, Validation F1: 0.9073\n",
      "Epoch 1694, Train Loss: 0.7799, Validation Loss: 1.0626, Validation F1: 0.9066\n",
      "Epoch 1695, Train Loss: 0.7806, Validation Loss: 1.0832, Validation F1: 0.9078\n",
      "Epoch 1696, Train Loss: 0.7817, Validation Loss: 1.0677, Validation F1: 0.9015\n",
      "Epoch 1697, Train Loss: 0.7807, Validation Loss: 1.0718, Validation F1: 0.9042\n",
      "Epoch 1698, Train Loss: 0.7803, Validation Loss: 1.0823, Validation F1: 0.9031\n",
      "Epoch 1699, Train Loss: 0.7820, Validation Loss: 1.0763, Validation F1: 0.9066\n",
      "Epoch 1700, Train Loss: 0.7806, Validation Loss: 1.0911, Validation F1: 0.9031\n",
      "Epoch 1701, Train Loss: 0.7801, Validation Loss: 1.0604, Validation F1: 0.8999\n",
      "Epoch 1702, Train Loss: 0.7816, Validation Loss: 1.0895, Validation F1: 0.9008\n",
      "Epoch 1703, Train Loss: 0.7799, Validation Loss: 1.0473, Validation F1: 0.9052\n",
      "Epoch 1704, Train Loss: 0.7797, Validation Loss: 1.0551, Validation F1: 0.9049\n",
      "Epoch 1705, Train Loss: 0.7794, Validation Loss: 1.0509, Validation F1: 0.9057\n",
      "Epoch 1706, Train Loss: 0.7800, Validation Loss: 1.0993, Validation F1: 0.8981\n",
      "Epoch 1707, Train Loss: 0.7814, Validation Loss: 1.0538, Validation F1: 0.8997\n",
      "Epoch 1708, Train Loss: 0.7807, Validation Loss: 1.0506, Validation F1: 0.8991\n",
      "Epoch 1709, Train Loss: 0.7793, Validation Loss: 1.0983, Validation F1: 0.9021\n",
      "Epoch 1710, Train Loss: 0.7784, Validation Loss: 1.0733, Validation F1: 0.9057\n",
      "Epoch 1711, Train Loss: 0.7863, Validation Loss: 1.0961, Validation F1: 0.8995\n",
      "Epoch 1712, Train Loss: 0.7874, Validation Loss: 1.0765, Validation F1: 0.8995\n",
      "Epoch 1713, Train Loss: 0.7915, Validation Loss: 1.0801, Validation F1: 0.8995\n",
      "Epoch 1714, Train Loss: 0.7844, Validation Loss: 1.0608, Validation F1: 0.9026\n",
      "Epoch 1715, Train Loss: 0.7875, Validation Loss: 1.0703, Validation F1: 0.9053\n",
      "Epoch 1716, Train Loss: 0.7839, Validation Loss: 1.0696, Validation F1: 0.9043\n",
      "Epoch 1717, Train Loss: 0.7851, Validation Loss: 1.0499, Validation F1: 0.9027\n",
      "Epoch 1718, Train Loss: 0.7825, Validation Loss: 1.0795, Validation F1: 0.9042\n",
      "Epoch 1719, Train Loss: 0.7861, Validation Loss: 1.0940, Validation F1: 0.9024\n",
      "Epoch 1720, Train Loss: 0.7880, Validation Loss: 1.0588, Validation F1: 0.9008\n",
      "Epoch 1721, Train Loss: 0.7867, Validation Loss: 1.0917, Validation F1: 0.9018\n",
      "Epoch 1722, Train Loss: 0.7936, Validation Loss: 1.0623, Validation F1: 0.8976\n",
      "Epoch 1723, Train Loss: 0.7815, Validation Loss: 1.0836, Validation F1: 0.8958\n",
      "Epoch 1724, Train Loss: 0.7891, Validation Loss: 1.0583, Validation F1: 0.9014\n",
      "Epoch 1725, Train Loss: 0.7823, Validation Loss: 1.0899, Validation F1: 0.9057\n",
      "Epoch 1726, Train Loss: 0.7912, Validation Loss: 1.0871, Validation F1: 0.9021\n",
      "Epoch 1727, Train Loss: 0.7966, Validation Loss: 1.0533, Validation F1: 0.8996\n",
      "Epoch 1728, Train Loss: 0.7984, Validation Loss: 1.0699, Validation F1: 0.8989\n",
      "Epoch 1729, Train Loss: 0.7873, Validation Loss: 1.0684, Validation F1: 0.9036\n",
      "Epoch 1730, Train Loss: 0.7932, Validation Loss: 1.0556, Validation F1: 0.9042\n",
      "Epoch 1731, Train Loss: 0.7892, Validation Loss: 1.0623, Validation F1: 0.9000\n",
      "Epoch 1732, Train Loss: 0.7895, Validation Loss: 1.0242, Validation F1: 0.9049\n",
      "Epoch 1733, Train Loss: 0.7845, Validation Loss: 1.0328, Validation F1: 0.9042\n",
      "Epoch 1734, Train Loss: 0.7895, Validation Loss: 1.0614, Validation F1: 0.9069\n",
      "Epoch 1735, Train Loss: 0.7840, Validation Loss: 1.0925, Validation F1: 0.9024\n",
      "Epoch 1736, Train Loss: 0.7879, Validation Loss: 1.0773, Validation F1: 0.8951\n",
      "Epoch 1737, Train Loss: 0.7831, Validation Loss: 1.0793, Validation F1: 0.8999\n",
      "Epoch 1738, Train Loss: 0.7853, Validation Loss: 1.0359, Validation F1: 0.9039\n",
      "Epoch 1739, Train Loss: 0.7835, Validation Loss: 1.0812, Validation F1: 0.9062\n",
      "Epoch 1740, Train Loss: 0.7853, Validation Loss: 1.0440, Validation F1: 0.9012\n",
      "Epoch 1741, Train Loss: 0.7856, Validation Loss: 1.1003, Validation F1: 0.8997\n",
      "Epoch 1742, Train Loss: 0.7831, Validation Loss: 1.0498, Validation F1: 0.9021\n",
      "Epoch 1743, Train Loss: 0.7809, Validation Loss: 1.0777, Validation F1: 0.9040\n",
      "Epoch 1744, Train Loss: 0.7833, Validation Loss: 1.0382, Validation F1: 0.9028\n",
      "Epoch 1745, Train Loss: 0.7833, Validation Loss: 1.0875, Validation F1: 0.9084\n",
      "Epoch 1746, Train Loss: 0.7850, Validation Loss: 1.1105, Validation F1: 0.9051\n",
      "Epoch 1747, Train Loss: 0.7807, Validation Loss: 1.0642, Validation F1: 0.8931\n",
      "Epoch 1748, Train Loss: 0.7822, Validation Loss: 1.0434, Validation F1: 0.8969\n",
      "Epoch 1749, Train Loss: 0.7827, Validation Loss: 1.0412, Validation F1: 0.9021\n",
      "Epoch 1750, Train Loss: 0.7814, Validation Loss: 1.0802, Validation F1: 0.9061\n",
      "Epoch 1751, Train Loss: 0.7799, Validation Loss: 1.0599, Validation F1: 0.9071\n",
      "Epoch 1752, Train Loss: 0.7808, Validation Loss: 1.0599, Validation F1: 0.9068\n",
      "Epoch 1753, Train Loss: 0.7814, Validation Loss: 1.0546, Validation F1: 0.9011\n",
      "Epoch 1754, Train Loss: 0.7802, Validation Loss: 1.0414, Validation F1: 0.8944\n",
      "Epoch 1755, Train Loss: 0.7790, Validation Loss: 1.0658, Validation F1: 0.8999\n",
      "Epoch 1756, Train Loss: 0.7802, Validation Loss: 1.0557, Validation F1: 0.9035\n",
      "Epoch 1757, Train Loss: 0.7809, Validation Loss: 1.0781, Validation F1: 0.9033\n",
      "Epoch 1758, Train Loss: 0.7787, Validation Loss: 1.0665, Validation F1: 0.9029\n",
      "Epoch 1759, Train Loss: 0.7805, Validation Loss: 1.0871, Validation F1: 0.9017\n",
      "Epoch 1760, Train Loss: 0.7799, Validation Loss: 1.0765, Validation F1: 0.9040\n",
      "Epoch 1761, Train Loss: 0.7797, Validation Loss: 1.0890, Validation F1: 0.8999\n",
      "Epoch 1762, Train Loss: 0.7790, Validation Loss: 1.0983, Validation F1: 0.9002\n",
      "Epoch 1763, Train Loss: 0.7786, Validation Loss: 1.0436, Validation F1: 0.9028\n",
      "Epoch 1764, Train Loss: 0.7785, Validation Loss: 1.0741, Validation F1: 0.9029\n",
      "Epoch 1765, Train Loss: 0.7790, Validation Loss: 1.0521, Validation F1: 0.9054\n",
      "Epoch 1766, Train Loss: 0.7795, Validation Loss: 1.0555, Validation F1: 0.9008\n",
      "Epoch 1767, Train Loss: 0.7785, Validation Loss: 1.0845, Validation F1: 0.9004\n",
      "Epoch 1768, Train Loss: 0.7777, Validation Loss: 1.0868, Validation F1: 0.9034\n",
      "Epoch 1769, Train Loss: 0.7805, Validation Loss: 1.0952, Validation F1: 0.9036\n",
      "Epoch 1770, Train Loss: 0.7797, Validation Loss: 1.0582, Validation F1: 0.9015\n",
      "Epoch 1771, Train Loss: 0.7783, Validation Loss: 1.0690, Validation F1: 0.9014\n",
      "Epoch 1772, Train Loss: 0.7766, Validation Loss: 1.0762, Validation F1: 0.8989\n",
      "Epoch 1773, Train Loss: 0.7789, Validation Loss: 1.0818, Validation F1: 0.8996\n",
      "Epoch 1774, Train Loss: 0.7784, Validation Loss: 1.0681, Validation F1: 0.9048\n",
      "Epoch 1775, Train Loss: 0.7790, Validation Loss: 1.0677, Validation F1: 0.9065\n",
      "Epoch 1776, Train Loss: 0.7779, Validation Loss: 1.0750, Validation F1: 0.9045\n",
      "Epoch 1777, Train Loss: 0.7784, Validation Loss: 1.0686, Validation F1: 0.9020\n",
      "Epoch 1778, Train Loss: 0.7784, Validation Loss: 1.0710, Validation F1: 0.9025\n",
      "Epoch 1779, Train Loss: 0.7772, Validation Loss: 1.0905, Validation F1: 0.9024\n",
      "Epoch 1780, Train Loss: 0.7786, Validation Loss: 1.0948, Validation F1: 0.9067\n",
      "Epoch 1781, Train Loss: 0.7780, Validation Loss: 1.0617, Validation F1: 0.9067\n",
      "Epoch 1782, Train Loss: 0.7794, Validation Loss: 1.0710, Validation F1: 0.8982\n",
      "Epoch 1783, Train Loss: 0.7790, Validation Loss: 1.0770, Validation F1: 0.9000\n",
      "Epoch 1784, Train Loss: 0.7808, Validation Loss: 1.1130, Validation F1: 0.8995\n",
      "Epoch 1785, Train Loss: 0.7816, Validation Loss: 1.0546, Validation F1: 0.9042\n",
      "Epoch 1786, Train Loss: 0.7794, Validation Loss: 1.0845, Validation F1: 0.9042\n",
      "Epoch 1787, Train Loss: 0.7781, Validation Loss: 1.1037, Validation F1: 0.9066\n",
      "Epoch 1788, Train Loss: 0.7841, Validation Loss: 1.0629, Validation F1: 0.9023\n",
      "Epoch 1789, Train Loss: 0.7845, Validation Loss: 1.0763, Validation F1: 0.8980\n",
      "Epoch 1790, Train Loss: 0.7888, Validation Loss: 1.1005, Validation F1: 0.8995\n",
      "Epoch 1791, Train Loss: 0.7840, Validation Loss: 1.1035, Validation F1: 0.9039\n",
      "Epoch 1792, Train Loss: 0.7808, Validation Loss: 1.0896, Validation F1: 0.9077\n",
      "Epoch 1793, Train Loss: 0.7826, Validation Loss: 1.1010, Validation F1: 0.9046\n",
      "Epoch 1794, Train Loss: 0.7866, Validation Loss: 1.0776, Validation F1: 0.8974\n",
      "Epoch 1795, Train Loss: 0.7787, Validation Loss: 1.0846, Validation F1: 0.8978\n",
      "Epoch 1796, Train Loss: 0.7843, Validation Loss: 1.0828, Validation F1: 0.9022\n",
      "Epoch 1797, Train Loss: 0.7833, Validation Loss: 1.0481, Validation F1: 0.9065\n",
      "Epoch 1798, Train Loss: 0.7812, Validation Loss: 1.0884, Validation F1: 0.9068\n",
      "Epoch 1799, Train Loss: 0.7828, Validation Loss: 1.0670, Validation F1: 0.9025\n",
      "Epoch 1800, Train Loss: 0.7823, Validation Loss: 1.0624, Validation F1: 0.8974\n",
      "Epoch 1801, Train Loss: 0.7803, Validation Loss: 1.0714, Validation F1: 0.8977\n",
      "Epoch 1802, Train Loss: 0.7816, Validation Loss: 1.0948, Validation F1: 0.9009\n",
      "Epoch 1803, Train Loss: 0.7819, Validation Loss: 1.0710, Validation F1: 0.9063\n",
      "Epoch 1804, Train Loss: 0.7803, Validation Loss: 1.0955, Validation F1: 0.9072\n",
      "Epoch 1805, Train Loss: 0.7809, Validation Loss: 1.0888, Validation F1: 0.9059\n",
      "Epoch 1806, Train Loss: 0.7798, Validation Loss: 1.0598, Validation F1: 0.9011\n",
      "Epoch 1807, Train Loss: 0.7785, Validation Loss: 1.0982, Validation F1: 0.8942\n",
      "Epoch 1808, Train Loss: 0.7806, Validation Loss: 1.0938, Validation F1: 0.9012\n",
      "Epoch 1809, Train Loss: 0.7794, Validation Loss: 1.0687, Validation F1: 0.9060\n",
      "Epoch 1810, Train Loss: 0.7794, Validation Loss: 1.0636, Validation F1: 0.9015\n",
      "Epoch 1811, Train Loss: 0.7789, Validation Loss: 1.1058, Validation F1: 0.9023\n",
      "Epoch 1812, Train Loss: 0.7793, Validation Loss: 1.0853, Validation F1: 0.8997\n",
      "Epoch 1813, Train Loss: 0.7784, Validation Loss: 1.0862, Validation F1: 0.8988\n",
      "Epoch 1814, Train Loss: 0.7792, Validation Loss: 1.1106, Validation F1: 0.9032\n",
      "Epoch 1815, Train Loss: 0.7805, Validation Loss: 1.1052, Validation F1: 0.9044\n",
      "Epoch 1816, Train Loss: 0.7794, Validation Loss: 1.1003, Validation F1: 0.9038\n",
      "Epoch 1817, Train Loss: 0.7770, Validation Loss: 1.0783, Validation F1: 0.9041\n",
      "Epoch 1818, Train Loss: 0.7782, Validation Loss: 1.0942, Validation F1: 0.9051\n",
      "Epoch 1819, Train Loss: 0.7783, Validation Loss: 1.1063, Validation F1: 0.9067\n",
      "Epoch 1820, Train Loss: 0.7802, Validation Loss: 1.0889, Validation F1: 0.8988\n",
      "Epoch 1821, Train Loss: 0.7781, Validation Loss: 1.0766, Validation F1: 0.8966\n",
      "Epoch 1822, Train Loss: 0.7790, Validation Loss: 1.0692, Validation F1: 0.9052\n",
      "Epoch 1823, Train Loss: 0.7817, Validation Loss: 1.0877, Validation F1: 0.9059\n",
      "Epoch 1824, Train Loss: 0.7796, Validation Loss: 1.1044, Validation F1: 0.9056\n",
      "Epoch 1825, Train Loss: 0.7804, Validation Loss: 1.0814, Validation F1: 0.9000\n",
      "Epoch 1826, Train Loss: 0.7790, Validation Loss: 1.0937, Validation F1: 0.8988\n",
      "Epoch 1827, Train Loss: 0.7791, Validation Loss: 1.0857, Validation F1: 0.8946\n",
      "Epoch 1828, Train Loss: 0.7807, Validation Loss: 1.0582, Validation F1: 0.9051\n",
      "Epoch 1829, Train Loss: 0.7774, Validation Loss: 1.1015, Validation F1: 0.9067\n",
      "Epoch 1830, Train Loss: 0.7796, Validation Loss: 1.0662, Validation F1: 0.9049\n",
      "Epoch 1831, Train Loss: 0.7778, Validation Loss: 1.0801, Validation F1: 0.9001\n",
      "Epoch 1832, Train Loss: 0.7785, Validation Loss: 1.0492, Validation F1: 0.8986\n",
      "Epoch 1833, Train Loss: 0.7755, Validation Loss: 1.1092, Validation F1: 0.8982\n",
      "Epoch 1834, Train Loss: 0.7795, Validation Loss: 1.0543, Validation F1: 0.9012\n",
      "Epoch 1835, Train Loss: 0.7795, Validation Loss: 1.0973, Validation F1: 0.9053\n",
      "Epoch 1836, Train Loss: 0.7780, Validation Loss: 1.0797, Validation F1: 0.9074\n",
      "Epoch 1837, Train Loss: 0.7777, Validation Loss: 1.0671, Validation F1: 0.9072\n",
      "Epoch 1838, Train Loss: 0.7801, Validation Loss: 1.0993, Validation F1: 0.9046\n",
      "Epoch 1839, Train Loss: 0.7788, Validation Loss: 1.0563, Validation F1: 0.8996\n",
      "Epoch 1840, Train Loss: 0.7784, Validation Loss: 1.0638, Validation F1: 0.9014\n",
      "Epoch 1841, Train Loss: 0.7780, Validation Loss: 1.0930, Validation F1: 0.9056\n",
      "Epoch 1842, Train Loss: 0.7782, Validation Loss: 1.0777, Validation F1: 0.9074\n",
      "Epoch 1843, Train Loss: 0.7791, Validation Loss: 1.0969, Validation F1: 0.9052\n",
      "Epoch 1844, Train Loss: 0.7768, Validation Loss: 1.0969, Validation F1: 0.9026\n",
      "Epoch 1845, Train Loss: 0.7776, Validation Loss: 1.0849, Validation F1: 0.9013\n",
      "Epoch 1846, Train Loss: 0.7771, Validation Loss: 1.0991, Validation F1: 0.9040\n",
      "Epoch 1847, Train Loss: 0.7788, Validation Loss: 1.0578, Validation F1: 0.9046\n",
      "Epoch 1848, Train Loss: 0.7765, Validation Loss: 1.0690, Validation F1: 0.9041\n",
      "Epoch 1849, Train Loss: 0.7796, Validation Loss: 1.0909, Validation F1: 0.9036\n",
      "Epoch 1850, Train Loss: 0.7777, Validation Loss: 1.0785, Validation F1: 0.9063\n",
      "Epoch 1851, Train Loss: 0.7784, Validation Loss: 1.0856, Validation F1: 0.9047\n",
      "Epoch 1852, Train Loss: 0.7796, Validation Loss: 1.0976, Validation F1: 0.9020\n",
      "Epoch 1853, Train Loss: 0.7764, Validation Loss: 1.1003, Validation F1: 0.8993\n",
      "Epoch 1854, Train Loss: 0.7782, Validation Loss: 1.0763, Validation F1: 0.9010\n",
      "Epoch 1855, Train Loss: 0.7758, Validation Loss: 1.0932, Validation F1: 0.9024\n",
      "Epoch 1856, Train Loss: 0.7762, Validation Loss: 1.0997, Validation F1: 0.9042\n",
      "Epoch 1857, Train Loss: 0.7757, Validation Loss: 1.0852, Validation F1: 0.9055\n",
      "Epoch 1858, Train Loss: 0.7805, Validation Loss: 1.0793, Validation F1: 0.8996\n",
      "Epoch 1859, Train Loss: 0.7808, Validation Loss: 1.0530, Validation F1: 0.9028\n",
      "Epoch 1860, Train Loss: 0.7801, Validation Loss: 1.0834, Validation F1: 0.9048\n",
      "Epoch 1861, Train Loss: 0.7800, Validation Loss: 1.0853, Validation F1: 0.9068\n",
      "Epoch 1862, Train Loss: 0.7790, Validation Loss: 1.1067, Validation F1: 0.9048\n",
      "Epoch 1863, Train Loss: 0.7815, Validation Loss: 1.0910, Validation F1: 0.8978\n",
      "Epoch 1864, Train Loss: 0.7828, Validation Loss: 1.1071, Validation F1: 0.8924\n",
      "Epoch 1865, Train Loss: 0.7798, Validation Loss: 1.0576, Validation F1: 0.8997\n",
      "Epoch 1866, Train Loss: 0.7863, Validation Loss: 1.0961, Validation F1: 0.9073\n",
      "Epoch 1867, Train Loss: 0.7788, Validation Loss: 1.0788, Validation F1: 0.9056\n",
      "Epoch 1868, Train Loss: 0.7805, Validation Loss: 1.0956, Validation F1: 0.9019\n",
      "Epoch 1869, Train Loss: 0.7802, Validation Loss: 1.0831, Validation F1: 0.8945\n",
      "Epoch 1870, Train Loss: 0.7819, Validation Loss: 1.0661, Validation F1: 0.8942\n",
      "Epoch 1871, Train Loss: 0.7806, Validation Loss: 1.0717, Validation F1: 0.9014\n",
      "Epoch 1872, Train Loss: 0.7824, Validation Loss: 1.1012, Validation F1: 0.9066\n",
      "Epoch 1873, Train Loss: 0.7794, Validation Loss: 1.0836, Validation F1: 0.9069\n",
      "Epoch 1874, Train Loss: 0.7851, Validation Loss: 1.0943, Validation F1: 0.9035\n",
      "Epoch 1875, Train Loss: 0.7857, Validation Loss: 1.0772, Validation F1: 0.8942\n",
      "Epoch 1876, Train Loss: 0.7846, Validation Loss: 1.0821, Validation F1: 0.9033\n",
      "Epoch 1877, Train Loss: 0.7828, Validation Loss: 1.0888, Validation F1: 0.9012\n",
      "Epoch 1878, Train Loss: 0.7838, Validation Loss: 1.1168, Validation F1: 0.9051\n",
      "Epoch 1879, Train Loss: 0.7921, Validation Loss: 1.1044, Validation F1: 0.9048\n",
      "Epoch 1880, Train Loss: 0.7805, Validation Loss: 1.1079, Validation F1: 0.9033\n",
      "Epoch 1881, Train Loss: 0.7924, Validation Loss: 1.0789, Validation F1: 0.8987\n",
      "Epoch 1882, Train Loss: 0.7831, Validation Loss: 1.0929, Validation F1: 0.8943\n",
      "Epoch 1883, Train Loss: 0.7913, Validation Loss: 1.0631, Validation F1: 0.9015\n",
      "Epoch 1884, Train Loss: 0.7827, Validation Loss: 1.0861, Validation F1: 0.9058\n",
      "Epoch 1885, Train Loss: 0.7902, Validation Loss: 1.0658, Validation F1: 0.9051\n",
      "Epoch 1886, Train Loss: 0.7871, Validation Loss: 1.0826, Validation F1: 0.9027\n",
      "Epoch 1887, Train Loss: 0.7929, Validation Loss: 1.0684, Validation F1: 0.8991\n",
      "Epoch 1888, Train Loss: 0.7817, Validation Loss: 1.1151, Validation F1: 0.9003\n",
      "Epoch 1889, Train Loss: 0.8049, Validation Loss: 1.0512, Validation F1: 0.8997\n",
      "Epoch 1890, Train Loss: 0.8009, Validation Loss: 1.1094, Validation F1: 0.9002\n",
      "Epoch 1891, Train Loss: 0.8123, Validation Loss: 1.0916, Validation F1: 0.9018\n",
      "Epoch 1892, Train Loss: 0.8066, Validation Loss: 1.0809, Validation F1: 0.8971\n",
      "Epoch 1893, Train Loss: 0.8070, Validation Loss: 1.0704, Validation F1: 0.8959\n",
      "Epoch 1894, Train Loss: 0.8020, Validation Loss: 1.0485, Validation F1: 0.8992\n",
      "Epoch 1895, Train Loss: 0.8001, Validation Loss: 1.0489, Validation F1: 0.9039\n",
      "Epoch 1896, Train Loss: 0.7995, Validation Loss: 1.0556, Validation F1: 0.9038\n",
      "Epoch 1897, Train Loss: 0.7974, Validation Loss: 1.0364, Validation F1: 0.9042\n",
      "Epoch 1898, Train Loss: 0.7936, Validation Loss: 1.0693, Validation F1: 0.9036\n",
      "Epoch 1899, Train Loss: 0.7936, Validation Loss: 1.0377, Validation F1: 0.8988\n",
      "Epoch 1900, Train Loss: 0.7927, Validation Loss: 1.0722, Validation F1: 0.8964\n",
      "Epoch 1901, Train Loss: 0.7924, Validation Loss: 1.0768, Validation F1: 0.9016\n",
      "Epoch 1902, Train Loss: 0.7934, Validation Loss: 1.0555, Validation F1: 0.9008\n",
      "Epoch 1903, Train Loss: 0.7941, Validation Loss: 1.0475, Validation F1: 0.9001\n",
      "Epoch 1904, Train Loss: 0.7900, Validation Loss: 1.0505, Validation F1: 0.9011\n",
      "Epoch 1905, Train Loss: 0.7903, Validation Loss: 1.0485, Validation F1: 0.9017\n",
      "Epoch 1906, Train Loss: 0.7876, Validation Loss: 1.0832, Validation F1: 0.9045\n",
      "Epoch 1907, Train Loss: 0.7889, Validation Loss: 1.0477, Validation F1: 0.9008\n",
      "Epoch 1908, Train Loss: 0.7882, Validation Loss: 1.0645, Validation F1: 0.8975\n",
      "Epoch 1909, Train Loss: 0.7860, Validation Loss: 1.0361, Validation F1: 0.9011\n",
      "Epoch 1910, Train Loss: 0.7898, Validation Loss: 1.0632, Validation F1: 0.9033\n",
      "Epoch 1911, Train Loss: 0.7860, Validation Loss: 1.0612, Validation F1: 0.9008\n",
      "Epoch 1912, Train Loss: 0.7867, Validation Loss: 1.0545, Validation F1: 0.8999\n",
      "Epoch 1913, Train Loss: 0.7871, Validation Loss: 1.0580, Validation F1: 0.8968\n",
      "Epoch 1914, Train Loss: 0.7885, Validation Loss: 1.0555, Validation F1: 0.8993\n",
      "Epoch 1915, Train Loss: 0.7878, Validation Loss: 1.0726, Validation F1: 0.9023\n",
      "Epoch 1916, Train Loss: 0.7854, Validation Loss: 1.0512, Validation F1: 0.9044\n",
      "Epoch 1917, Train Loss: 0.7875, Validation Loss: 1.0646, Validation F1: 0.9039\n",
      "Epoch 1918, Train Loss: 0.7870, Validation Loss: 1.0515, Validation F1: 0.9041\n",
      "Epoch 1919, Train Loss: 0.7848, Validation Loss: 1.0670, Validation F1: 0.8967\n",
      "Epoch 1920, Train Loss: 0.7836, Validation Loss: 1.0722, Validation F1: 0.8984\n",
      "Epoch 1921, Train Loss: 0.7863, Validation Loss: 1.0635, Validation F1: 0.9025\n",
      "Epoch 1922, Train Loss: 0.7858, Validation Loss: 1.0712, Validation F1: 0.9000\n",
      "Epoch 1923, Train Loss: 0.7858, Validation Loss: 1.0764, Validation F1: 0.9053\n",
      "Epoch 1924, Train Loss: 0.7837, Validation Loss: 1.0508, Validation F1: 0.9049\n",
      "Epoch 1925, Train Loss: 0.7836, Validation Loss: 1.0747, Validation F1: 0.9019\n",
      "Epoch 1926, Train Loss: 0.7834, Validation Loss: 1.0434, Validation F1: 0.8969\n",
      "Epoch 1927, Train Loss: 0.7833, Validation Loss: 1.0357, Validation F1: 0.8999\n",
      "Epoch 1928, Train Loss: 0.7835, Validation Loss: 1.0858, Validation F1: 0.8973\n",
      "Epoch 1929, Train Loss: 0.7849, Validation Loss: 1.0650, Validation F1: 0.8994\n",
      "Epoch 1930, Train Loss: 0.7817, Validation Loss: 1.0358, Validation F1: 0.9043\n",
      "Epoch 1931, Train Loss: 0.7821, Validation Loss: 1.0802, Validation F1: 0.9029\n",
      "Epoch 1932, Train Loss: 0.7816, Validation Loss: 1.0813, Validation F1: 0.9031\n",
      "Epoch 1933, Train Loss: 0.7815, Validation Loss: 1.0910, Validation F1: 0.9020\n",
      "Epoch 1934, Train Loss: 0.7829, Validation Loss: 1.0470, Validation F1: 0.9007\n",
      "Epoch 1935, Train Loss: 0.7828, Validation Loss: 1.0540, Validation F1: 0.8934\n",
      "Epoch 1936, Train Loss: 0.7826, Validation Loss: 1.0683, Validation F1: 0.8972\n",
      "Epoch 1937, Train Loss: 0.7827, Validation Loss: 1.0887, Validation F1: 0.9017\n",
      "Epoch 1938, Train Loss: 0.7808, Validation Loss: 1.0479, Validation F1: 0.9069\n",
      "Epoch 1939, Train Loss: 0.7810, Validation Loss: 1.0445, Validation F1: 0.9036\n",
      "Epoch 1940, Train Loss: 0.7833, Validation Loss: 1.0290, Validation F1: 0.9004\n",
      "Epoch 1941, Train Loss: 0.7813, Validation Loss: 1.0156, Validation F1: 0.8962\n",
      "Epoch 1942, Train Loss: 0.7840, Validation Loss: 1.0427, Validation F1: 0.8941\n",
      "Epoch 1943, Train Loss: 0.7795, Validation Loss: 1.0822, Validation F1: 0.9025\n",
      "Epoch 1944, Train Loss: 0.7829, Validation Loss: 1.0743, Validation F1: 0.9057\n",
      "Epoch 1945, Train Loss: 0.7817, Validation Loss: 1.0648, Validation F1: 0.9055\n",
      "Epoch 1946, Train Loss: 0.7805, Validation Loss: 1.0651, Validation F1: 0.9047\n",
      "Epoch 1947, Train Loss: 0.7805, Validation Loss: 1.0755, Validation F1: 0.9016\n",
      "Epoch 1948, Train Loss: 0.7811, Validation Loss: 1.0675, Validation F1: 0.8944\n",
      "Epoch 1949, Train Loss: 0.7823, Validation Loss: 1.0878, Validation F1: 0.8993\n",
      "Epoch 1950, Train Loss: 0.7812, Validation Loss: 1.0978, Validation F1: 0.9053\n",
      "Epoch 1951, Train Loss: 0.7811, Validation Loss: 1.0457, Validation F1: 0.9041\n",
      "Epoch 1952, Train Loss: 0.7818, Validation Loss: 1.0527, Validation F1: 0.9043\n",
      "Epoch 1953, Train Loss: 0.7800, Validation Loss: 1.0670, Validation F1: 0.9008\n",
      "Epoch 1954, Train Loss: 0.7807, Validation Loss: 1.0469, Validation F1: 0.8982\n",
      "Epoch 1955, Train Loss: 0.7815, Validation Loss: 1.0706, Validation F1: 0.8994\n",
      "Epoch 1956, Train Loss: 0.7790, Validation Loss: 1.0875, Validation F1: 0.8998\n",
      "Epoch 1957, Train Loss: 0.7800, Validation Loss: 1.0696, Validation F1: 0.9031\n",
      "Epoch 1958, Train Loss: 0.7794, Validation Loss: 1.0845, Validation F1: 0.9047\n",
      "Epoch 1959, Train Loss: 0.7785, Validation Loss: 1.0378, Validation F1: 0.9012\n",
      "Epoch 1960, Train Loss: 0.7795, Validation Loss: 1.0803, Validation F1: 0.8980\n",
      "Epoch 1961, Train Loss: 0.7790, Validation Loss: 1.0382, Validation F1: 0.8998\n",
      "Epoch 1962, Train Loss: 0.7794, Validation Loss: 1.0428, Validation F1: 0.8989\n",
      "Epoch 1963, Train Loss: 0.7791, Validation Loss: 1.0828, Validation F1: 0.9026\n",
      "Epoch 1964, Train Loss: 0.7815, Validation Loss: 1.0600, Validation F1: 0.9009\n",
      "Epoch 1965, Train Loss: 0.7818, Validation Loss: 1.0779, Validation F1: 0.8985\n",
      "Epoch 1966, Train Loss: 0.7828, Validation Loss: 1.0853, Validation F1: 0.9044\n",
      "Epoch 1967, Train Loss: 0.7811, Validation Loss: 1.0863, Validation F1: 0.9062\n",
      "Epoch 1968, Train Loss: 0.7800, Validation Loss: 1.0890, Validation F1: 0.9031\n",
      "Epoch 1969, Train Loss: 0.7821, Validation Loss: 1.0942, Validation F1: 0.9006\n",
      "Epoch 1970, Train Loss: 0.7793, Validation Loss: 1.0902, Validation F1: 0.8983\n",
      "Epoch 1971, Train Loss: 0.7811, Validation Loss: 1.0847, Validation F1: 0.9013\n",
      "Epoch 1972, Train Loss: 0.7780, Validation Loss: 1.0640, Validation F1: 0.9039\n",
      "Epoch 1973, Train Loss: 0.7796, Validation Loss: 1.0482, Validation F1: 0.9047\n",
      "Epoch 1974, Train Loss: 0.7821, Validation Loss: 1.0243, Validation F1: 0.8943\n",
      "Epoch 1975, Train Loss: 0.7804, Validation Loss: 1.0912, Validation F1: 0.8997\n",
      "Epoch 1976, Train Loss: 0.7819, Validation Loss: 1.0679, Validation F1: 0.9008\n",
      "Epoch 1977, Train Loss: 0.7817, Validation Loss: 1.0562, Validation F1: 0.9052\n",
      "Epoch 1978, Train Loss: 0.7825, Validation Loss: 1.0821, Validation F1: 0.9045\n",
      "Epoch 1979, Train Loss: 0.7799, Validation Loss: 1.0713, Validation F1: 0.9046\n",
      "Epoch 1980, Train Loss: 0.7802, Validation Loss: 1.0704, Validation F1: 0.9014\n",
      "Epoch 1981, Train Loss: 0.7806, Validation Loss: 1.0898, Validation F1: 0.9001\n",
      "Epoch 1982, Train Loss: 0.7807, Validation Loss: 1.0685, Validation F1: 0.8989\n",
      "Epoch 1983, Train Loss: 0.7820, Validation Loss: 1.0693, Validation F1: 0.9012\n",
      "Epoch 1984, Train Loss: 0.7797, Validation Loss: 1.0677, Validation F1: 0.9052\n",
      "Epoch 1985, Train Loss: 0.7809, Validation Loss: 1.0789, Validation F1: 0.9002\n",
      "Epoch 1986, Train Loss: 0.7802, Validation Loss: 1.0661, Validation F1: 0.8978\n",
      "Epoch 1987, Train Loss: 0.7808, Validation Loss: 1.0481, Validation F1: 0.8982\n",
      "Epoch 1988, Train Loss: 0.7800, Validation Loss: 1.1125, Validation F1: 0.8996\n",
      "Epoch 1989, Train Loss: 0.7795, Validation Loss: 1.0787, Validation F1: 0.9059\n",
      "Epoch 1990, Train Loss: 0.7809, Validation Loss: 1.0649, Validation F1: 0.9042\n",
      "Epoch 1991, Train Loss: 0.7800, Validation Loss: 1.0486, Validation F1: 0.9061\n",
      "Epoch 1992, Train Loss: 0.7804, Validation Loss: 1.1143, Validation F1: 0.9026\n",
      "Epoch 1993, Train Loss: 0.7798, Validation Loss: 1.0831, Validation F1: 0.9003\n",
      "Epoch 1994, Train Loss: 0.7810, Validation Loss: 1.0438, Validation F1: 0.9022\n",
      "Epoch 1995, Train Loss: 0.7812, Validation Loss: 1.0384, Validation F1: 0.9002\n",
      "Epoch 1996, Train Loss: 0.7828, Validation Loss: 1.0628, Validation F1: 0.9025\n",
      "Epoch 1997, Train Loss: 0.7811, Validation Loss: 1.0937, Validation F1: 0.9062\n",
      "Epoch 1998, Train Loss: 0.7814, Validation Loss: 1.0770, Validation F1: 0.9035\n",
      "Epoch 1999, Train Loss: 0.7839, Validation Loss: 1.0795, Validation F1: 0.9045\n",
      "Epoch 2000, Train Loss: 0.7786, Validation Loss: 1.1003, Validation F1: 0.8927\n",
      "Epoch 2001, Train Loss: 0.7805, Validation Loss: 1.0670, Validation F1: 0.9039\n",
      "Epoch 2002, Train Loss: 0.7807, Validation Loss: 1.1071, Validation F1: 0.9068\n",
      "Epoch 2003, Train Loss: 0.7807, Validation Loss: 1.0448, Validation F1: 0.9046\n",
      "Epoch 2004, Train Loss: 0.7807, Validation Loss: 1.0806, Validation F1: 0.9018\n",
      "Epoch 2005, Train Loss: 0.7795, Validation Loss: 1.0595, Validation F1: 0.9006\n",
      "Epoch 2006, Train Loss: 0.7807, Validation Loss: 1.0746, Validation F1: 0.9024\n",
      "Epoch 2007, Train Loss: 0.7785, Validation Loss: 1.0867, Validation F1: 0.8992\n",
      "Epoch 2008, Train Loss: 0.7790, Validation Loss: 1.0453, Validation F1: 0.9069\n",
      "Epoch 2009, Train Loss: 0.7796, Validation Loss: 1.0843, Validation F1: 0.9041\n",
      "Epoch 2010, Train Loss: 0.7809, Validation Loss: 1.0789, Validation F1: 0.9021\n",
      "Epoch 2011, Train Loss: 0.7787, Validation Loss: 1.0429, Validation F1: 0.8978\n",
      "Epoch 2012, Train Loss: 0.7778, Validation Loss: 1.0526, Validation F1: 0.9002\n",
      "Epoch 2013, Train Loss: 0.7811, Validation Loss: 1.0848, Validation F1: 0.9039\n",
      "Epoch 2014, Train Loss: 0.7810, Validation Loss: 1.0705, Validation F1: 0.9051\n",
      "Epoch 2015, Train Loss: 0.7784, Validation Loss: 1.0911, Validation F1: 0.9037\n",
      "Epoch 2016, Train Loss: 0.7784, Validation Loss: 1.0815, Validation F1: 0.9028\n",
      "Epoch 2017, Train Loss: 0.7778, Validation Loss: 1.1014, Validation F1: 0.9032\n",
      "Epoch 2018, Train Loss: 0.7783, Validation Loss: 1.0985, Validation F1: 0.9019\n",
      "Epoch 2019, Train Loss: 0.7776, Validation Loss: 1.0899, Validation F1: 0.9050\n",
      "Epoch 2020, Train Loss: 0.7782, Validation Loss: 1.0834, Validation F1: 0.9038\n",
      "Epoch 2021, Train Loss: 0.7781, Validation Loss: 1.0883, Validation F1: 0.9064\n",
      "Epoch 2022, Train Loss: 0.7778, Validation Loss: 1.0952, Validation F1: 0.9025\n",
      "Epoch 2023, Train Loss: 0.7768, Validation Loss: 1.0741, Validation F1: 0.9012\n",
      "Epoch 2024, Train Loss: 0.7786, Validation Loss: 1.0797, Validation F1: 0.9046\n",
      "Epoch 2025, Train Loss: 0.7786, Validation Loss: 1.1303, Validation F1: 0.9005\n",
      "Epoch 2026, Train Loss: 0.7780, Validation Loss: 1.0748, Validation F1: 0.9034\n",
      "Epoch 2027, Train Loss: 0.7786, Validation Loss: 1.0754, Validation F1: 0.9008\n",
      "Epoch 2028, Train Loss: 0.7773, Validation Loss: 1.0367, Validation F1: 0.9023\n",
      "Epoch 2029, Train Loss: 0.7771, Validation Loss: 1.0795, Validation F1: 0.9020\n",
      "Epoch 2030, Train Loss: 0.7769, Validation Loss: 1.0674, Validation F1: 0.9046\n",
      "Epoch 2031, Train Loss: 0.7783, Validation Loss: 1.0589, Validation F1: 0.9035\n",
      "Epoch 2032, Train Loss: 0.7782, Validation Loss: 1.0805, Validation F1: 0.9014\n",
      "Epoch 2033, Train Loss: 0.7772, Validation Loss: 1.1027, Validation F1: 0.9009\n",
      "Epoch 2034, Train Loss: 0.7771, Validation Loss: 1.0989, Validation F1: 0.9040\n",
      "Epoch 2035, Train Loss: 0.7782, Validation Loss: 1.0881, Validation F1: 0.9009\n",
      "Epoch 2036, Train Loss: 0.7768, Validation Loss: 1.0919, Validation F1: 0.9003\n",
      "Epoch 2037, Train Loss: 0.7768, Validation Loss: 1.0773, Validation F1: 0.9054\n",
      "Epoch 2038, Train Loss: 0.7780, Validation Loss: 1.1058, Validation F1: 0.9055\n",
      "Epoch 2039, Train Loss: 0.7765, Validation Loss: 1.0845, Validation F1: 0.9064\n",
      "Epoch 2040, Train Loss: 0.7765, Validation Loss: 1.1115, Validation F1: 0.8999\n",
      "Epoch 2041, Train Loss: 0.7784, Validation Loss: 1.1064, Validation F1: 0.8997\n",
      "Epoch 2042, Train Loss: 0.7781, Validation Loss: 1.0732, Validation F1: 0.8988\n",
      "Epoch 2043, Train Loss: 0.7782, Validation Loss: 1.1035, Validation F1: 0.9068\n",
      "Epoch 2044, Train Loss: 0.7781, Validation Loss: 1.1113, Validation F1: 0.9054\n",
      "Epoch 2045, Train Loss: 0.7766, Validation Loss: 1.0982, Validation F1: 0.9029\n",
      "Epoch 2046, Train Loss: 0.7782, Validation Loss: 1.1015, Validation F1: 0.9048\n",
      "Epoch 2047, Train Loss: 0.7769, Validation Loss: 1.0780, Validation F1: 0.9017\n",
      "Epoch 2048, Train Loss: 0.7770, Validation Loss: 1.1072, Validation F1: 0.9015\n",
      "Epoch 2049, Train Loss: 0.7782, Validation Loss: 1.0801, Validation F1: 0.8996\n",
      "Epoch 2050, Train Loss: 0.7772, Validation Loss: 1.0697, Validation F1: 0.9025\n",
      "Epoch 2051, Train Loss: 0.7758, Validation Loss: 1.0822, Validation F1: 0.9049\n",
      "Epoch 2052, Train Loss: 0.7760, Validation Loss: 1.0970, Validation F1: 0.9025\n",
      "Epoch 2053, Train Loss: 0.7769, Validation Loss: 1.1203, Validation F1: 0.8951\n",
      "Epoch 2054, Train Loss: 0.7766, Validation Loss: 1.0498, Validation F1: 0.9010\n",
      "Epoch 2055, Train Loss: 0.7763, Validation Loss: 1.0880, Validation F1: 0.9032\n",
      "Epoch 2056, Train Loss: 0.7759, Validation Loss: 1.0917, Validation F1: 0.9053\n",
      "Epoch 2057, Train Loss: 0.7777, Validation Loss: 1.0939, Validation F1: 0.9030\n",
      "Epoch 2058, Train Loss: 0.7780, Validation Loss: 1.0626, Validation F1: 0.9019\n",
      "Epoch 2059, Train Loss: 0.7767, Validation Loss: 1.0668, Validation F1: 0.9002\n",
      "Epoch 2060, Train Loss: 0.7772, Validation Loss: 1.0499, Validation F1: 0.9021\n",
      "Epoch 2061, Train Loss: 0.7771, Validation Loss: 1.0492, Validation F1: 0.9018\n",
      "Epoch 2062, Train Loss: 0.7779, Validation Loss: 1.0583, Validation F1: 0.9049\n",
      "Epoch 2063, Train Loss: 0.7782, Validation Loss: 1.0778, Validation F1: 0.9000\n",
      "Epoch 2064, Train Loss: 0.7761, Validation Loss: 1.0670, Validation F1: 0.8973\n",
      "Epoch 2065, Train Loss: 0.7765, Validation Loss: 1.0576, Validation F1: 0.8997\n",
      "Epoch 2066, Train Loss: 0.7763, Validation Loss: 1.0854, Validation F1: 0.9021\n",
      "Epoch 2067, Train Loss: 0.7768, Validation Loss: 1.1138, Validation F1: 0.9038\n",
      "Epoch 2068, Train Loss: 0.7780, Validation Loss: 1.0707, Validation F1: 0.9066\n",
      "Epoch 2069, Train Loss: 0.7768, Validation Loss: 1.0778, Validation F1: 0.9009\n",
      "Epoch 2070, Train Loss: 0.7767, Validation Loss: 1.0890, Validation F1: 0.9000\n",
      "Epoch 2071, Train Loss: 0.7758, Validation Loss: 1.0881, Validation F1: 0.8948\n",
      "Epoch 2072, Train Loss: 0.7759, Validation Loss: 1.0803, Validation F1: 0.9045\n",
      "Epoch 2073, Train Loss: 0.7763, Validation Loss: 1.0956, Validation F1: 0.9035\n",
      "Epoch 2074, Train Loss: 0.7780, Validation Loss: 1.1103, Validation F1: 0.9041\n",
      "Epoch 2075, Train Loss: 0.7758, Validation Loss: 1.1017, Validation F1: 0.9008\n",
      "Epoch 2076, Train Loss: 0.7758, Validation Loss: 1.1017, Validation F1: 0.8957\n",
      "Epoch 2077, Train Loss: 0.7756, Validation Loss: 1.0822, Validation F1: 0.9044\n",
      "Epoch 2078, Train Loss: 0.7759, Validation Loss: 1.0871, Validation F1: 0.9036\n",
      "Epoch 2079, Train Loss: 0.7769, Validation Loss: 1.0578, Validation F1: 0.9035\n",
      "Epoch 2080, Train Loss: 0.7756, Validation Loss: 1.0831, Validation F1: 0.9021\n",
      "Epoch 2081, Train Loss: 0.7762, Validation Loss: 1.0569, Validation F1: 0.8998\n",
      "Epoch 2082, Train Loss: 0.7762, Validation Loss: 1.1092, Validation F1: 0.9020\n",
      "Epoch 2083, Train Loss: 0.7757, Validation Loss: 1.0666, Validation F1: 0.9035\n",
      "Epoch 2084, Train Loss: 0.7765, Validation Loss: 1.0614, Validation F1: 0.9028\n",
      "Epoch 2085, Train Loss: 0.7756, Validation Loss: 1.0782, Validation F1: 0.9037\n",
      "Epoch 2086, Train Loss: 0.7766, Validation Loss: 1.0779, Validation F1: 0.9015\n",
      "Epoch 2087, Train Loss: 0.7758, Validation Loss: 1.0946, Validation F1: 0.9042\n",
      "Epoch 2088, Train Loss: 0.7764, Validation Loss: 1.1056, Validation F1: 0.9060\n",
      "Epoch 2089, Train Loss: 0.7773, Validation Loss: 1.0688, Validation F1: 0.9034\n",
      "Epoch 2090, Train Loss: 0.7753, Validation Loss: 1.1072, Validation F1: 0.8993\n",
      "Epoch 2091, Train Loss: 0.7768, Validation Loss: 1.0602, Validation F1: 0.8972\n",
      "Epoch 2092, Train Loss: 0.7774, Validation Loss: 1.0931, Validation F1: 0.9034\n",
      "Epoch 2093, Train Loss: 0.7763, Validation Loss: 1.0460, Validation F1: 0.9052\n",
      "Epoch 2094, Train Loss: 0.7773, Validation Loss: 1.0560, Validation F1: 0.9043\n",
      "Epoch 2095, Train Loss: 0.7772, Validation Loss: 1.0942, Validation F1: 0.9008\n",
      "Epoch 2096, Train Loss: 0.7761, Validation Loss: 1.0664, Validation F1: 0.9036\n",
      "Epoch 2097, Train Loss: 0.7762, Validation Loss: 1.0678, Validation F1: 0.9061\n",
      "Epoch 2098, Train Loss: 0.7772, Validation Loss: 1.0978, Validation F1: 0.9072\n",
      "Epoch 2099, Train Loss: 0.7762, Validation Loss: 1.0808, Validation F1: 0.9032\n",
      "Epoch 2100, Train Loss: 0.7770, Validation Loss: 1.0724, Validation F1: 0.9011\n",
      "Epoch 2101, Train Loss: 0.7761, Validation Loss: 1.0620, Validation F1: 0.8983\n",
      "Epoch 2102, Train Loss: 0.7758, Validation Loss: 1.0597, Validation F1: 0.8933\n",
      "Epoch 2103, Train Loss: 0.7765, Validation Loss: 1.0537, Validation F1: 0.8987\n",
      "Epoch 2104, Train Loss: 0.7748, Validation Loss: 1.0884, Validation F1: 0.9029\n",
      "Epoch 2105, Train Loss: 0.7751, Validation Loss: 1.0980, Validation F1: 0.9057\n",
      "Epoch 2106, Train Loss: 0.7765, Validation Loss: 1.0802, Validation F1: 0.9055\n",
      "Epoch 2107, Train Loss: 0.7747, Validation Loss: 1.0853, Validation F1: 0.9019\n",
      "Epoch 2108, Train Loss: 0.7744, Validation Loss: 1.0711, Validation F1: 0.8989\n",
      "Epoch 2109, Train Loss: 0.7767, Validation Loss: 1.0592, Validation F1: 0.9002\n",
      "Epoch 2110, Train Loss: 0.7756, Validation Loss: 1.0788, Validation F1: 0.9028\n",
      "Epoch 2111, Train Loss: 0.7759, Validation Loss: 1.0777, Validation F1: 0.9045\n",
      "Epoch 2112, Train Loss: 0.7758, Validation Loss: 1.1183, Validation F1: 0.9029\n",
      "Epoch 2113, Train Loss: 0.7758, Validation Loss: 1.1097, Validation F1: 0.9035\n",
      "Epoch 2114, Train Loss: 0.7744, Validation Loss: 1.1048, Validation F1: 0.9002\n",
      "Epoch 2115, Train Loss: 0.7742, Validation Loss: 1.0955, Validation F1: 0.8996\n",
      "Epoch 2116, Train Loss: 0.7751, Validation Loss: 1.1009, Validation F1: 0.9033\n",
      "Epoch 2117, Train Loss: 0.7762, Validation Loss: 1.0433, Validation F1: 0.8990\n",
      "Epoch 2118, Train Loss: 0.7749, Validation Loss: 1.0977, Validation F1: 0.9044\n",
      "Epoch 2119, Train Loss: 0.7755, Validation Loss: 1.1252, Validation F1: 0.8997\n",
      "Epoch 2120, Train Loss: 0.7756, Validation Loss: 1.1120, Validation F1: 0.9023\n",
      "Epoch 2121, Train Loss: 0.7743, Validation Loss: 1.0529, Validation F1: 0.9056\n",
      "Epoch 2122, Train Loss: 0.7744, Validation Loss: 1.0859, Validation F1: 0.9033\n",
      "Epoch 2123, Train Loss: 0.7764, Validation Loss: 1.1028, Validation F1: 0.8992\n",
      "Epoch 2124, Train Loss: 0.7772, Validation Loss: 1.1150, Validation F1: 0.8996\n",
      "Epoch 2125, Train Loss: 0.7800, Validation Loss: 1.0923, Validation F1: 0.9028\n",
      "Epoch 2126, Train Loss: 0.7786, Validation Loss: 1.1099, Validation F1: 0.9037\n",
      "Epoch 2127, Train Loss: 0.7759, Validation Loss: 1.0977, Validation F1: 0.9058\n",
      "Epoch 2128, Train Loss: 0.7769, Validation Loss: 1.0669, Validation F1: 0.9064\n",
      "Epoch 2129, Train Loss: 0.7775, Validation Loss: 1.0873, Validation F1: 0.9046\n",
      "Epoch 2130, Train Loss: 0.7763, Validation Loss: 1.0902, Validation F1: 0.9031\n",
      "Epoch 2131, Train Loss: 0.7754, Validation Loss: 1.0936, Validation F1: 0.9045\n",
      "Epoch 2132, Train Loss: 0.7790, Validation Loss: 1.0810, Validation F1: 0.8949\n",
      "Epoch 2133, Train Loss: 0.7789, Validation Loss: 1.0929, Validation F1: 0.8987\n",
      "Epoch 2134, Train Loss: 0.7802, Validation Loss: 1.0977, Validation F1: 0.9049\n",
      "Epoch 2135, Train Loss: 0.7801, Validation Loss: 1.0677, Validation F1: 0.9066\n",
      "Epoch 2136, Train Loss: 0.7813, Validation Loss: 1.1372, Validation F1: 0.9010\n",
      "Epoch 2137, Train Loss: 0.7811, Validation Loss: 1.0827, Validation F1: 0.9036\n",
      "Epoch 2138, Train Loss: 0.7782, Validation Loss: 1.0933, Validation F1: 0.9041\n",
      "Epoch 2139, Train Loss: 0.7790, Validation Loss: 1.0736, Validation F1: 0.8990\n",
      "Epoch 2140, Train Loss: 0.7806, Validation Loss: 1.0996, Validation F1: 0.9052\n",
      "Epoch 2141, Train Loss: 0.7781, Validation Loss: 1.1060, Validation F1: 0.9069\n",
      "Epoch 2142, Train Loss: 0.7787, Validation Loss: 1.0947, Validation F1: 0.9021\n",
      "Epoch 2143, Train Loss: 0.7778, Validation Loss: 1.1039, Validation F1: 0.9027\n",
      "Epoch 2144, Train Loss: 0.7837, Validation Loss: 1.1088, Validation F1: 0.8982\n",
      "Epoch 2145, Train Loss: 0.7799, Validation Loss: 1.0926, Validation F1: 0.9013\n",
      "Epoch 2146, Train Loss: 0.7835, Validation Loss: 1.0921, Validation F1: 0.9034\n",
      "Epoch 2147 Saved best model. Best F1: 0.9092071939337802\n",
      "Epoch 2147, Train Loss: 0.7827, Validation Loss: 1.1017, Validation F1: 0.9092\n",
      "Epoch 2148, Train Loss: 0.7808, Validation Loss: 1.1176, Validation F1: 0.9005\n",
      "Epoch 2149, Train Loss: 0.7832, Validation Loss: 1.1051, Validation F1: 0.8984\n",
      "Epoch 2150, Train Loss: 0.7793, Validation Loss: 1.1049, Validation F1: 0.8987\n",
      "Epoch 2151, Train Loss: 0.7799, Validation Loss: 1.0686, Validation F1: 0.9019\n",
      "Epoch 2152, Train Loss: 0.7765, Validation Loss: 1.0860, Validation F1: 0.9043\n",
      "Epoch 2153, Train Loss: 0.7772, Validation Loss: 1.1005, Validation F1: 0.9058\n",
      "Epoch 2154, Train Loss: 0.7787, Validation Loss: 1.0790, Validation F1: 0.9047\n",
      "Epoch 2155, Train Loss: 0.7781, Validation Loss: 1.0609, Validation F1: 0.8976\n",
      "Epoch 2156, Train Loss: 0.7790, Validation Loss: 1.1084, Validation F1: 0.8992\n",
      "Epoch 2157, Train Loss: 0.7779, Validation Loss: 1.0979, Validation F1: 0.9002\n",
      "Epoch 2158, Train Loss: 0.7782, Validation Loss: 1.1002, Validation F1: 0.9036\n",
      "Epoch 2159, Train Loss: 0.7798, Validation Loss: 1.0956, Validation F1: 0.9045\n",
      "Epoch 2160, Train Loss: 0.7797, Validation Loss: 1.0848, Validation F1: 0.9053\n",
      "Epoch 2161, Train Loss: 0.7774, Validation Loss: 1.0920, Validation F1: 0.9052\n",
      "Epoch 2162, Train Loss: 0.7789, Validation Loss: 1.0910, Validation F1: 0.9002\n",
      "Epoch 2163, Train Loss: 0.7780, Validation Loss: 1.0758, Validation F1: 0.8940\n",
      "Epoch 2164, Train Loss: 0.7794, Validation Loss: 1.0497, Validation F1: 0.9001\n",
      "Epoch 2165, Train Loss: 0.7760, Validation Loss: 1.1041, Validation F1: 0.9061\n",
      "Epoch 2166, Train Loss: 0.7766, Validation Loss: 1.1117, Validation F1: 0.9034\n",
      "Epoch 2167, Train Loss: 0.7783, Validation Loss: 1.0781, Validation F1: 0.9040\n",
      "Epoch 2168, Train Loss: 0.7775, Validation Loss: 1.0942, Validation F1: 0.9045\n",
      "Epoch 2169, Train Loss: 0.7776, Validation Loss: 1.1172, Validation F1: 0.9027\n",
      "Epoch 2170, Train Loss: 0.7769, Validation Loss: 1.1252, Validation F1: 0.9020\n",
      "Epoch 2171, Train Loss: 0.7769, Validation Loss: 1.1156, Validation F1: 0.8980\n",
      "Epoch 2172, Train Loss: 0.7766, Validation Loss: 1.0867, Validation F1: 0.9033\n",
      "Epoch 2173, Train Loss: 0.7754, Validation Loss: 1.0959, Validation F1: 0.9030\n",
      "Epoch 2174, Train Loss: 0.7770, Validation Loss: 1.0578, Validation F1: 0.9035\n",
      "Epoch 2175, Train Loss: 0.7763, Validation Loss: 1.0946, Validation F1: 0.9052\n",
      "Epoch 2176, Train Loss: 0.7775, Validation Loss: 1.0678, Validation F1: 0.8994\n",
      "Epoch 2177, Train Loss: 0.7750, Validation Loss: 1.0985, Validation F1: 0.8986\n",
      "Epoch 2178, Train Loss: 0.7784, Validation Loss: 1.1142, Validation F1: 0.9016\n",
      "Epoch 2179, Train Loss: 0.7779, Validation Loss: 1.0848, Validation F1: 0.9071\n",
      "Epoch 2180, Train Loss: 0.7762, Validation Loss: 1.1001, Validation F1: 0.9040\n",
      "Epoch 2181, Train Loss: 0.7758, Validation Loss: 1.1368, Validation F1: 0.9009\n",
      "Epoch 2182, Train Loss: 0.7755, Validation Loss: 1.0930, Validation F1: 0.9017\n",
      "Epoch 2183, Train Loss: 0.7757, Validation Loss: 1.1157, Validation F1: 0.9011\n",
      "Epoch 2184, Train Loss: 0.7748, Validation Loss: 1.0918, Validation F1: 0.9024\n",
      "Epoch 2185, Train Loss: 0.7756, Validation Loss: 1.0949, Validation F1: 0.9037\n",
      "Epoch 2186, Train Loss: 0.7774, Validation Loss: 1.1281, Validation F1: 0.9069\n",
      "Epoch 2187, Train Loss: 0.7755, Validation Loss: 1.0897, Validation F1: 0.9054\n",
      "Epoch 2188, Train Loss: 0.7753, Validation Loss: 1.0797, Validation F1: 0.9031\n",
      "Epoch 2189, Train Loss: 0.7760, Validation Loss: 1.1129, Validation F1: 0.9022\n",
      "Epoch 2190, Train Loss: 0.7757, Validation Loss: 1.1415, Validation F1: 0.8997\n",
      "Epoch 2191, Train Loss: 0.7751, Validation Loss: 1.1173, Validation F1: 0.9034\n",
      "Epoch 2192, Train Loss: 0.7781, Validation Loss: 1.1083, Validation F1: 0.9055\n",
      "Epoch 2193, Train Loss: 0.7766, Validation Loss: 1.0835, Validation F1: 0.9078\n",
      "Epoch 2194, Train Loss: 0.7766, Validation Loss: 1.0903, Validation F1: 0.9047\n",
      "Epoch 2195, Train Loss: 0.7763, Validation Loss: 1.0904, Validation F1: 0.9014\n",
      "Epoch 2196, Train Loss: 0.7756, Validation Loss: 1.1118, Validation F1: 0.8977\n",
      "Epoch 2197, Train Loss: 0.7757, Validation Loss: 1.1202, Validation F1: 0.9042\n",
      "Epoch 2198, Train Loss: 0.7779, Validation Loss: 1.1373, Validation F1: 0.9079\n",
      "Epoch 2199, Train Loss: 0.7748, Validation Loss: 1.1288, Validation F1: 0.9082\n",
      "Epoch 2200, Train Loss: 0.7760, Validation Loss: 1.1027, Validation F1: 0.9027\n",
      "Epoch 2201, Train Loss: 0.7761, Validation Loss: 1.1066, Validation F1: 0.9004\n",
      "Epoch 2202, Train Loss: 0.7761, Validation Loss: 1.1061, Validation F1: 0.9007\n",
      "Epoch 2203, Train Loss: 0.7768, Validation Loss: 1.1051, Validation F1: 0.8998\n",
      "Epoch 2204, Train Loss: 0.7785, Validation Loss: 1.0997, Validation F1: 0.9008\n",
      "Epoch 2205, Train Loss: 0.7768, Validation Loss: 1.0875, Validation F1: 0.9041\n",
      "Epoch 2206, Train Loss: 0.7755, Validation Loss: 1.0845, Validation F1: 0.9064\n",
      "Epoch 2207, Train Loss: 0.7754, Validation Loss: 1.0927, Validation F1: 0.9052\n",
      "Epoch 2208, Train Loss: 0.7777, Validation Loss: 1.0888, Validation F1: 0.9035\n",
      "Epoch 2209, Train Loss: 0.7743, Validation Loss: 1.0889, Validation F1: 0.9036\n",
      "Epoch 2210, Train Loss: 0.7777, Validation Loss: 1.0713, Validation F1: 0.9002\n",
      "Epoch 2211, Train Loss: 0.7738, Validation Loss: 1.1107, Validation F1: 0.8999\n",
      "Epoch 2212, Train Loss: 0.7770, Validation Loss: 1.1120, Validation F1: 0.9006\n",
      "Epoch 2213, Train Loss: 0.7754, Validation Loss: 1.1276, Validation F1: 0.9028\n",
      "Epoch 2214, Train Loss: 0.7756, Validation Loss: 1.1101, Validation F1: 0.9034\n",
      "Epoch 2215, Train Loss: 0.7751, Validation Loss: 1.1135, Validation F1: 0.8991\n",
      "Epoch 2216, Train Loss: 0.7774, Validation Loss: 1.0980, Validation F1: 0.9042\n",
      "Epoch 2217, Train Loss: 0.7764, Validation Loss: 1.0897, Validation F1: 0.9010\n",
      "Epoch 2218, Train Loss: 0.7759, Validation Loss: 1.0656, Validation F1: 0.9036\n",
      "Epoch 2219, Train Loss: 0.7757, Validation Loss: 1.0636, Validation F1: 0.9043\n",
      "Epoch 2220, Train Loss: 0.7742, Validation Loss: 1.0365, Validation F1: 0.9046\n",
      "Epoch 2221, Train Loss: 0.7753, Validation Loss: 1.1038, Validation F1: 0.9023\n",
      "Epoch 2222, Train Loss: 0.7759, Validation Loss: 1.1249, Validation F1: 0.9058\n",
      "Epoch 2223, Train Loss: 0.7740, Validation Loss: 1.0751, Validation F1: 0.9049\n",
      "Epoch 2224, Train Loss: 0.7753, Validation Loss: 1.1302, Validation F1: 0.9053\n",
      "Epoch 2225, Train Loss: 0.7747, Validation Loss: 1.0967, Validation F1: 0.9053\n",
      "Epoch 2226, Train Loss: 0.7770, Validation Loss: 1.0896, Validation F1: 0.9038\n",
      "Epoch 2227, Train Loss: 0.7750, Validation Loss: 1.1277, Validation F1: 0.8998\n",
      "Epoch 2228, Train Loss: 0.7733, Validation Loss: 1.1412, Validation F1: 0.9056\n",
      "Epoch 2229, Train Loss: 0.7744, Validation Loss: 1.1364, Validation F1: 0.9066\n",
      "Epoch 2230, Train Loss: 0.7743, Validation Loss: 1.0856, Validation F1: 0.9069\n",
      "Epoch 2231, Train Loss: 0.7739, Validation Loss: 1.1215, Validation F1: 0.9040\n",
      "Epoch 2232, Train Loss: 0.7747, Validation Loss: 1.1091, Validation F1: 0.9035\n",
      "Epoch 2233, Train Loss: 0.7740, Validation Loss: 1.0938, Validation F1: 0.8999\n",
      "Epoch 2234, Train Loss: 0.7740, Validation Loss: 1.1445, Validation F1: 0.9068\n",
      "Epoch 2235, Train Loss: 0.7737, Validation Loss: 1.1276, Validation F1: 0.9063\n",
      "Epoch 2236, Train Loss: 0.7742, Validation Loss: 1.1129, Validation F1: 0.9032\n",
      "Epoch 2237, Train Loss: 0.7741, Validation Loss: 1.0816, Validation F1: 0.9028\n",
      "Epoch 2238, Train Loss: 0.7728, Validation Loss: 1.0962, Validation F1: 0.9017\n",
      "Epoch 2239, Train Loss: 0.7722, Validation Loss: 1.0913, Validation F1: 0.8999\n",
      "Epoch 2240, Train Loss: 0.7736, Validation Loss: 1.1123, Validation F1: 0.9053\n",
      "Epoch 2241, Train Loss: 0.7743, Validation Loss: 1.1173, Validation F1: 0.9037\n",
      "Epoch 2242, Train Loss: 0.7753, Validation Loss: 1.0983, Validation F1: 0.9030\n",
      "Epoch 2243, Train Loss: 0.7739, Validation Loss: 1.0938, Validation F1: 0.9025\n",
      "Epoch 2244, Train Loss: 0.7751, Validation Loss: 1.1049, Validation F1: 0.9028\n",
      "Epoch 2245, Train Loss: 0.7736, Validation Loss: 1.0933, Validation F1: 0.9035\n",
      "Epoch 2246, Train Loss: 0.7726, Validation Loss: 1.1182, Validation F1: 0.9009\n",
      "Epoch 2247, Train Loss: 0.7735, Validation Loss: 1.1091, Validation F1: 0.8990\n",
      "Epoch 2248, Train Loss: 0.7729, Validation Loss: 1.0965, Validation F1: 0.8987\n",
      "Epoch 2249, Train Loss: 0.7725, Validation Loss: 1.1474, Validation F1: 0.9056\n",
      "Epoch 2250, Train Loss: 0.7727, Validation Loss: 1.1275, Validation F1: 0.9046\n",
      "Epoch 2251, Train Loss: 0.7735, Validation Loss: 1.0862, Validation F1: 0.9018\n",
      "Epoch 2252, Train Loss: 0.7743, Validation Loss: 1.1060, Validation F1: 0.8980\n",
      "Epoch 2253, Train Loss: 0.7740, Validation Loss: 1.0965, Validation F1: 0.8978\n",
      "Epoch 2254, Train Loss: 0.7734, Validation Loss: 1.0734, Validation F1: 0.9035\n",
      "Epoch 2255, Train Loss: 0.7748, Validation Loss: 1.0989, Validation F1: 0.9074\n",
      "Epoch 2256, Train Loss: 0.7742, Validation Loss: 1.1063, Validation F1: 0.9062\n",
      "Epoch 2257, Train Loss: 0.7734, Validation Loss: 1.1166, Validation F1: 0.8953\n",
      "Epoch 2258, Train Loss: 0.7747, Validation Loss: 1.1000, Validation F1: 0.9063\n",
      "Epoch 2259, Train Loss: 0.7739, Validation Loss: 1.1025, Validation F1: 0.9059\n",
      "Epoch 2260, Train Loss: 0.7724, Validation Loss: 1.1456, Validation F1: 0.9038\n",
      "Epoch 2261, Train Loss: 0.7729, Validation Loss: 1.0724, Validation F1: 0.9046\n",
      "Epoch 2262, Train Loss: 0.7740, Validation Loss: 1.1115, Validation F1: 0.9025\n",
      "Epoch 2263, Train Loss: 0.7720, Validation Loss: 1.1168, Validation F1: 0.9047\n",
      "Epoch 2264, Train Loss: 0.7734, Validation Loss: 1.0984, Validation F1: 0.9062\n",
      "Epoch 2265, Train Loss: 0.7736, Validation Loss: 1.1065, Validation F1: 0.9049\n",
      "Epoch 2266, Train Loss: 0.7742, Validation Loss: 1.0818, Validation F1: 0.9029\n",
      "Epoch 2267, Train Loss: 0.7727, Validation Loss: 1.0839, Validation F1: 0.8969\n",
      "Epoch 2268, Train Loss: 0.7740, Validation Loss: 1.1004, Validation F1: 0.9013\n",
      "Epoch 2269, Train Loss: 0.7774, Validation Loss: 1.1248, Validation F1: 0.9059\n",
      "Epoch 2270, Train Loss: 0.7734, Validation Loss: 1.1129, Validation F1: 0.9053\n",
      "Epoch 2271, Train Loss: 0.7742, Validation Loss: 1.0963, Validation F1: 0.9016\n",
      "Epoch 2272, Train Loss: 0.7756, Validation Loss: 1.0574, Validation F1: 0.9012\n",
      "Epoch 2273, Train Loss: 0.7726, Validation Loss: 1.1276, Validation F1: 0.9043\n",
      "Epoch 2274, Train Loss: 0.7744, Validation Loss: 1.1184, Validation F1: 0.9045\n",
      "Epoch 2275, Train Loss: 0.7737, Validation Loss: 1.1141, Validation F1: 0.9018\n",
      "Epoch 2276, Train Loss: 0.7745, Validation Loss: 1.0872, Validation F1: 0.9009\n",
      "Epoch 2277, Train Loss: 0.7739, Validation Loss: 1.0981, Validation F1: 0.9030\n",
      "Epoch 2278, Train Loss: 0.7737, Validation Loss: 1.0853, Validation F1: 0.9024\n",
      "Epoch 2279, Train Loss: 0.7730, Validation Loss: 1.1204, Validation F1: 0.9064\n",
      "Epoch 2280, Train Loss: 0.7741, Validation Loss: 1.0571, Validation F1: 0.9057\n",
      "Epoch 2281, Train Loss: 0.7735, Validation Loss: 1.1009, Validation F1: 0.8986\n",
      "Epoch 2282, Train Loss: 0.7742, Validation Loss: 1.0957, Validation F1: 0.8994\n",
      "Epoch 2283, Train Loss: 0.7725, Validation Loss: 1.1281, Validation F1: 0.9010\n",
      "Epoch 2284, Train Loss: 0.7743, Validation Loss: 1.1072, Validation F1: 0.9059\n",
      "Epoch 2285, Train Loss: 0.7737, Validation Loss: 1.1024, Validation F1: 0.9043\n",
      "Epoch 2286, Train Loss: 0.7750, Validation Loss: 1.0574, Validation F1: 0.9057\n",
      "Epoch 2287, Train Loss: 0.7752, Validation Loss: 1.1377, Validation F1: 0.9016\n",
      "Epoch 2288, Train Loss: 0.7741, Validation Loss: 1.1152, Validation F1: 0.9035\n",
      "Epoch 2289, Train Loss: 0.7731, Validation Loss: 1.0878, Validation F1: 0.9049\n",
      "Epoch 2290, Train Loss: 0.7727, Validation Loss: 1.0976, Validation F1: 0.9016\n",
      "Epoch 2291, Train Loss: 0.7727, Validation Loss: 1.1087, Validation F1: 0.9059\n",
      "Epoch 2292, Train Loss: 0.7720, Validation Loss: 1.1107, Validation F1: 0.9001\n",
      "Epoch 2293, Train Loss: 0.7724, Validation Loss: 1.1135, Validation F1: 0.8994\n",
      "Epoch 2294, Train Loss: 0.7744, Validation Loss: 1.0913, Validation F1: 0.9062\n",
      "Epoch 2295, Train Loss: 0.7737, Validation Loss: 1.0843, Validation F1: 0.9030\n",
      "Epoch 2296, Train Loss: 0.7743, Validation Loss: 1.1206, Validation F1: 0.9003\n",
      "Epoch 2297, Train Loss: 0.7757, Validation Loss: 1.1276, Validation F1: 0.9032\n",
      "Epoch 2298, Train Loss: 0.7740, Validation Loss: 1.1132, Validation F1: 0.8953\n",
      "Epoch 2299, Train Loss: 0.7741, Validation Loss: 1.1167, Validation F1: 0.9049\n",
      "Epoch 2300, Train Loss: 0.7745, Validation Loss: 1.1337, Validation F1: 0.9062\n",
      "Epoch 2301, Train Loss: 0.7739, Validation Loss: 1.1146, Validation F1: 0.9058\n",
      "Epoch 2302, Train Loss: 0.7724, Validation Loss: 1.0850, Validation F1: 0.9033\n",
      "Epoch 2303, Train Loss: 0.7735, Validation Loss: 1.1160, Validation F1: 0.9062\n",
      "Epoch 2304, Train Loss: 0.7746, Validation Loss: 1.0765, Validation F1: 0.9023\n",
      "Epoch 2305, Train Loss: 0.7747, Validation Loss: 1.0892, Validation F1: 0.8996\n",
      "Epoch 2306, Train Loss: 0.7745, Validation Loss: 1.0824, Validation F1: 0.9053\n",
      "Epoch 2307, Train Loss: 0.7751, Validation Loss: 1.0966, Validation F1: 0.9057\n",
      "Epoch 2308, Train Loss: 0.7734, Validation Loss: 1.1207, Validation F1: 0.9063\n",
      "Epoch 2309, Train Loss: 0.7741, Validation Loss: 1.0809, Validation F1: 0.9057\n",
      "Epoch 2310, Train Loss: 0.7736, Validation Loss: 1.1010, Validation F1: 0.9052\n",
      "Epoch 2311, Train Loss: 0.7721, Validation Loss: 1.0899, Validation F1: 0.9017\n",
      "Epoch 2312, Train Loss: 0.7739, Validation Loss: 1.1231, Validation F1: 0.9024\n",
      "Epoch 2313, Train Loss: 0.7739, Validation Loss: 1.0932, Validation F1: 0.9044\n",
      "Epoch 2314, Train Loss: 0.7738, Validation Loss: 1.1111, Validation F1: 0.9052\n",
      "Epoch 2315, Train Loss: 0.7734, Validation Loss: 1.1163, Validation F1: 0.9054\n",
      "Epoch 2316, Train Loss: 0.7742, Validation Loss: 1.1298, Validation F1: 0.9007\n",
      "Epoch 2317, Train Loss: 0.7720, Validation Loss: 1.0942, Validation F1: 0.8990\n",
      "Epoch 2318, Train Loss: 0.7731, Validation Loss: 1.1228, Validation F1: 0.8986\n",
      "Epoch 2319, Train Loss: 0.7733, Validation Loss: 1.1094, Validation F1: 0.9026\n",
      "Epoch 2320, Train Loss: 0.7727, Validation Loss: 1.1025, Validation F1: 0.9036\n",
      "Epoch 2321, Train Loss: 0.7738, Validation Loss: 1.1168, Validation F1: 0.8967\n",
      "Epoch 2322, Train Loss: 0.7731, Validation Loss: 1.1218, Validation F1: 0.9023\n",
      "Epoch 2323, Train Loss: 0.7715, Validation Loss: 1.1011, Validation F1: 0.9059\n",
      "Epoch 2324, Train Loss: 0.7740, Validation Loss: 1.0863, Validation F1: 0.9061\n",
      "Epoch 2325, Train Loss: 0.7747, Validation Loss: 1.1181, Validation F1: 0.9046\n",
      "Epoch 2326, Train Loss: 0.7723, Validation Loss: 1.1202, Validation F1: 0.9024\n",
      "Epoch 2327, Train Loss: 0.7735, Validation Loss: 1.0919, Validation F1: 0.9030\n",
      "Epoch 2328, Train Loss: 0.7739, Validation Loss: 1.0738, Validation F1: 0.8994\n",
      "Epoch 2329, Train Loss: 0.7735, Validation Loss: 1.0810, Validation F1: 0.9028\n",
      "Epoch 2330, Train Loss: 0.7729, Validation Loss: 1.0949, Validation F1: 0.9044\n",
      "Epoch 2331, Train Loss: 0.7731, Validation Loss: 1.0775, Validation F1: 0.9023\n",
      "Epoch 2332, Train Loss: 0.7746, Validation Loss: 1.1492, Validation F1: 0.9024\n",
      "Epoch 2333, Train Loss: 0.7730, Validation Loss: 1.1298, Validation F1: 0.9020\n",
      "Epoch 2334, Train Loss: 0.7736, Validation Loss: 1.1255, Validation F1: 0.9007\n",
      "Epoch 2335, Train Loss: 0.7748, Validation Loss: 1.1241, Validation F1: 0.9014\n",
      "Epoch 2336, Train Loss: 0.7734, Validation Loss: 1.0971, Validation F1: 0.8990\n",
      "Epoch 2337, Train Loss: 0.7723, Validation Loss: 1.1305, Validation F1: 0.9016\n",
      "Epoch 2338, Train Loss: 0.7722, Validation Loss: 1.0997, Validation F1: 0.9053\n",
      "Epoch 2339, Train Loss: 0.7731, Validation Loss: 1.0984, Validation F1: 0.9050\n",
      "Epoch 2340, Train Loss: 0.7739, Validation Loss: 1.0789, Validation F1: 0.9062\n",
      "Epoch 2341, Train Loss: 0.7717, Validation Loss: 1.0970, Validation F1: 0.8994\n",
      "Epoch 2342, Train Loss: 0.7733, Validation Loss: 1.1059, Validation F1: 0.9046\n",
      "Epoch 2343, Train Loss: 0.7715, Validation Loss: 1.1045, Validation F1: 0.9039\n",
      "Epoch 2344, Train Loss: 0.7725, Validation Loss: 1.0818, Validation F1: 0.9052\n",
      "Epoch 2345, Train Loss: 0.7716, Validation Loss: 1.1439, Validation F1: 0.9049\n",
      "Epoch 2346, Train Loss: 0.7727, Validation Loss: 1.1126, Validation F1: 0.9025\n",
      "Epoch 2347, Train Loss: 0.7713, Validation Loss: 1.0981, Validation F1: 0.9009\n",
      "Epoch 2348, Train Loss: 0.7732, Validation Loss: 1.1284, Validation F1: 0.9021\n",
      "Epoch 2349, Train Loss: 0.7726, Validation Loss: 1.1114, Validation F1: 0.9019\n",
      "Epoch 2350, Train Loss: 0.7731, Validation Loss: 1.1193, Validation F1: 0.9031\n",
      "Epoch 2351, Train Loss: 0.7724, Validation Loss: 1.1063, Validation F1: 0.9001\n",
      "Epoch 2352, Train Loss: 0.7714, Validation Loss: 1.0833, Validation F1: 0.9045\n",
      "Epoch 2353, Train Loss: 0.7725, Validation Loss: 1.1331, Validation F1: 0.9007\n",
      "Epoch 2354, Train Loss: 0.7723, Validation Loss: 1.1257, Validation F1: 0.9036\n",
      "Epoch 2355, Train Loss: 0.7728, Validation Loss: 1.1447, Validation F1: 0.9062\n",
      "Epoch 2356, Train Loss: 0.7761, Validation Loss: 1.1121, Validation F1: 0.9068\n",
      "Epoch 2357, Train Loss: 0.7755, Validation Loss: 1.1082, Validation F1: 0.9035\n",
      "Epoch 2358, Train Loss: 0.7724, Validation Loss: 1.0887, Validation F1: 0.9031\n",
      "Epoch 2359, Train Loss: 0.7782, Validation Loss: 1.1460, Validation F1: 0.8984\n",
      "Epoch 2360, Train Loss: 0.7779, Validation Loss: 1.1487, Validation F1: 0.9035\n",
      "Epoch 2361, Train Loss: 0.7747, Validation Loss: 1.1097, Validation F1: 0.9062\n",
      "Epoch 2362, Train Loss: 0.7778, Validation Loss: 1.1242, Validation F1: 0.9066\n",
      "Epoch 2363, Train Loss: 0.7739, Validation Loss: 1.1113, Validation F1: 0.9054\n",
      "Epoch 2364, Train Loss: 0.7748, Validation Loss: 1.1477, Validation F1: 0.8992\n",
      "Epoch 2365, Train Loss: 0.7715, Validation Loss: 1.1108, Validation F1: 0.9019\n",
      "Epoch 2366, Train Loss: 0.7761, Validation Loss: 1.1184, Validation F1: 0.9020\n",
      "Epoch 2367, Train Loss: 0.7747, Validation Loss: 1.1194, Validation F1: 0.9061\n",
      "Epoch 2368, Train Loss: 0.7752, Validation Loss: 1.1223, Validation F1: 0.9062\n",
      "Epoch 2369, Train Loss: 0.7731, Validation Loss: 1.1117, Validation F1: 0.9023\n",
      "Epoch 2370, Train Loss: 0.7749, Validation Loss: 1.1341, Validation F1: 0.8949\n",
      "Epoch 2371, Train Loss: 0.7757, Validation Loss: 1.1156, Validation F1: 0.9016\n",
      "Epoch 2372, Train Loss: 0.7742, Validation Loss: 1.1300, Validation F1: 0.9042\n",
      "Epoch 2373, Train Loss: 0.7740, Validation Loss: 1.1256, Validation F1: 0.9053\n",
      "Epoch 2374, Train Loss: 0.7727, Validation Loss: 1.1260, Validation F1: 0.9029\n",
      "Epoch 2375, Train Loss: 0.7727, Validation Loss: 1.0806, Validation F1: 0.9013\n",
      "Epoch 2376, Train Loss: 0.7737, Validation Loss: 1.0890, Validation F1: 0.9031\n",
      "Epoch 2377, Train Loss: 0.7720, Validation Loss: 1.1228, Validation F1: 0.9002\n",
      "Epoch 2378, Train Loss: 0.7719, Validation Loss: 1.1067, Validation F1: 0.9050\n",
      "Epoch 2379, Train Loss: 0.7746, Validation Loss: 1.1069, Validation F1: 0.9038\n",
      "Epoch 2380, Train Loss: 0.7732, Validation Loss: 1.0962, Validation F1: 0.9044\n",
      "Epoch 2381, Train Loss: 0.7732, Validation Loss: 1.1042, Validation F1: 0.9022\n",
      "Epoch 2382, Train Loss: 0.7731, Validation Loss: 1.1262, Validation F1: 0.9050\n",
      "Epoch 2383, Train Loss: 0.7714, Validation Loss: 1.1177, Validation F1: 0.9033\n",
      "Epoch 2384, Train Loss: 0.7725, Validation Loss: 1.1132, Validation F1: 0.9045\n",
      "Epoch 2385, Train Loss: 0.7724, Validation Loss: 1.1349, Validation F1: 0.9037\n",
      "Epoch 2386, Train Loss: 0.7722, Validation Loss: 1.1126, Validation F1: 0.9038\n",
      "Epoch 2387, Train Loss: 0.7727, Validation Loss: 1.1171, Validation F1: 0.9003\n",
      "Epoch 2388, Train Loss: 0.7725, Validation Loss: 1.1404, Validation F1: 0.9066\n",
      "Epoch 2389, Train Loss: 0.7721, Validation Loss: 1.1307, Validation F1: 0.9071\n",
      "Epoch 2390, Train Loss: 0.7719, Validation Loss: 1.1288, Validation F1: 0.9058\n",
      "Epoch 2391, Train Loss: 0.7728, Validation Loss: 1.1253, Validation F1: 0.9019\n",
      "Epoch 2392, Train Loss: 0.7721, Validation Loss: 1.1256, Validation F1: 0.9054\n",
      "Epoch 2393, Train Loss: 0.7725, Validation Loss: 1.1341, Validation F1: 0.9058\n",
      "Epoch 2394, Train Loss: 0.7731, Validation Loss: 1.1023, Validation F1: 0.9053\n",
      "Epoch 2395, Train Loss: 0.7721, Validation Loss: 1.0964, Validation F1: 0.9022\n",
      "Epoch 2396, Train Loss: 0.7727, Validation Loss: 1.1108, Validation F1: 0.9023\n",
      "Epoch 2397, Train Loss: 0.7728, Validation Loss: 1.1299, Validation F1: 0.9000\n",
      "Epoch 2398, Train Loss: 0.7716, Validation Loss: 1.0948, Validation F1: 0.9047\n",
      "Epoch 2399, Train Loss: 0.7728, Validation Loss: 1.1243, Validation F1: 0.9027\n",
      "Epoch 2400, Train Loss: 0.7705, Validation Loss: 1.1227, Validation F1: 0.9053\n",
      "Epoch 2401, Train Loss: 0.7711, Validation Loss: 1.1233, Validation F1: 0.9043\n",
      "Epoch 2402, Train Loss: 0.7719, Validation Loss: 1.1174, Validation F1: 0.9021\n",
      "Epoch 2403, Train Loss: 0.7712, Validation Loss: 1.1272, Validation F1: 0.9043\n",
      "Epoch 2404, Train Loss: 0.7707, Validation Loss: 1.1321, Validation F1: 0.9046\n",
      "Epoch 2405, Train Loss: 0.7709, Validation Loss: 1.1041, Validation F1: 0.9047\n",
      "Epoch 2406, Train Loss: 0.7721, Validation Loss: 1.1180, Validation F1: 0.8988\n",
      "Epoch 2407, Train Loss: 0.7717, Validation Loss: 1.1236, Validation F1: 0.9028\n",
      "Epoch 2408, Train Loss: 0.7727, Validation Loss: 1.1290, Validation F1: 0.9078\n",
      "Epoch 2409, Train Loss: 0.7723, Validation Loss: 1.1216, Validation F1: 0.9076\n",
      "Epoch 2410, Train Loss: 0.7718, Validation Loss: 1.1002, Validation F1: 0.9022\n",
      "Epoch 2411, Train Loss: 0.7722, Validation Loss: 1.0909, Validation F1: 0.9027\n",
      "Epoch 2412, Train Loss: 0.7712, Validation Loss: 1.1313, Validation F1: 0.9022\n",
      "Epoch 2413, Train Loss: 0.7710, Validation Loss: 1.0766, Validation F1: 0.9044\n",
      "Epoch 2414, Train Loss: 0.7725, Validation Loss: 1.1428, Validation F1: 0.9021\n",
      "Epoch 2415, Train Loss: 0.7719, Validation Loss: 1.1092, Validation F1: 0.9013\n",
      "Epoch 2416, Train Loss: 0.7708, Validation Loss: 1.1065, Validation F1: 0.9036\n",
      "Epoch 2417, Train Loss: 0.7704, Validation Loss: 1.1024, Validation F1: 0.9064\n",
      "Epoch 2418, Train Loss: 0.7709, Validation Loss: 1.1126, Validation F1: 0.9032\n",
      "Epoch 2419, Train Loss: 0.7717, Validation Loss: 1.1250, Validation F1: 0.9032\n",
      "Epoch 2420, Train Loss: 0.7712, Validation Loss: 1.1277, Validation F1: 0.9027\n",
      "Epoch 2421, Train Loss: 0.7710, Validation Loss: 1.1193, Validation F1: 0.9032\n",
      "Epoch 2422, Train Loss: 0.7718, Validation Loss: 1.1334, Validation F1: 0.9029\n",
      "Epoch 2423, Train Loss: 0.7708, Validation Loss: 1.1628, Validation F1: 0.9050\n",
      "Epoch 2424, Train Loss: 0.7732, Validation Loss: 1.1152, Validation F1: 0.9039\n",
      "Epoch 2425, Train Loss: 0.7723, Validation Loss: 1.1091, Validation F1: 0.9050\n",
      "Epoch 2426, Train Loss: 0.7708, Validation Loss: 1.0840, Validation F1: 0.9024\n",
      "Epoch 2427, Train Loss: 0.7725, Validation Loss: 1.1599, Validation F1: 0.9012\n",
      "Epoch 2428, Train Loss: 0.7722, Validation Loss: 1.1699, Validation F1: 0.9055\n",
      "Epoch 2429, Train Loss: 0.7717, Validation Loss: 1.1096, Validation F1: 0.9008\n",
      "Epoch 2430, Train Loss: 0.7731, Validation Loss: 1.1238, Validation F1: 0.9059\n",
      "Epoch 2431, Train Loss: 0.7717, Validation Loss: 1.1486, Validation F1: 0.9049\n",
      "Epoch 2432, Train Loss: 0.7729, Validation Loss: 1.1498, Validation F1: 0.9050\n",
      "Epoch 2433, Train Loss: 0.7716, Validation Loss: 1.1631, Validation F1: 0.9037\n",
      "Epoch 2434, Train Loss: 0.7719, Validation Loss: 1.1142, Validation F1: 0.9012\n",
      "Epoch 2435, Train Loss: 0.7736, Validation Loss: 1.1018, Validation F1: 0.9023\n",
      "Epoch 2436, Train Loss: 0.7706, Validation Loss: 1.0826, Validation F1: 0.9040\n",
      "Epoch 2437, Train Loss: 0.7730, Validation Loss: 1.1371, Validation F1: 0.9041\n",
      "Epoch 2438, Train Loss: 0.7749, Validation Loss: 1.1363, Validation F1: 0.8990\n",
      "Epoch 2439, Train Loss: 0.7731, Validation Loss: 1.1616, Validation F1: 0.9048\n",
      "Epoch 2440, Train Loss: 0.7752, Validation Loss: 1.1236, Validation F1: 0.9068\n",
      "Epoch 2441, Train Loss: 0.7764, Validation Loss: 1.1713, Validation F1: 0.9006\n",
      "Epoch 2442, Train Loss: 0.7761, Validation Loss: 1.0945, Validation F1: 0.9023\n",
      "Epoch 2443, Train Loss: 0.7748, Validation Loss: 1.1281, Validation F1: 0.9015\n",
      "Epoch 2444, Train Loss: 0.7733, Validation Loss: 1.0962, Validation F1: 0.9020\n",
      "Epoch 2445, Train Loss: 0.7727, Validation Loss: 1.1126, Validation F1: 0.9054\n",
      "Epoch 2446, Train Loss: 0.7748, Validation Loss: 1.1488, Validation F1: 0.9044\n",
      "Epoch 2447, Train Loss: 0.7731, Validation Loss: 1.1692, Validation F1: 0.9038\n",
      "Epoch 2448, Train Loss: 0.7787, Validation Loss: 1.1261, Validation F1: 0.8989\n",
      "Epoch 2449, Train Loss: 0.7855, Validation Loss: 1.1097, Validation F1: 0.8979\n",
      "Epoch 2450, Train Loss: 0.7886, Validation Loss: 1.1030, Validation F1: 0.9048\n",
      "Epoch 2451, Train Loss: 0.7828, Validation Loss: 1.1127, Validation F1: 0.9073\n",
      "Epoch 2452, Train Loss: 0.7933, Validation Loss: 1.0968, Validation F1: 0.9041\n",
      "Epoch 2453, Train Loss: 0.7832, Validation Loss: 1.1149, Validation F1: 0.9043\n",
      "Epoch 2454, Train Loss: 0.7872, Validation Loss: 1.1072, Validation F1: 0.9057\n",
      "Epoch 2455, Train Loss: 0.7824, Validation Loss: 1.0748, Validation F1: 0.9039\n",
      "Epoch 2456, Train Loss: 0.7839, Validation Loss: 1.1238, Validation F1: 0.9057\n",
      "Epoch 2457, Train Loss: 0.7814, Validation Loss: 1.1224, Validation F1: 0.9053\n",
      "Epoch 2458, Train Loss: 0.7824, Validation Loss: 1.0945, Validation F1: 0.9026\n",
      "Epoch 2459, Train Loss: 0.7823, Validation Loss: 1.1367, Validation F1: 0.9043\n",
      "Epoch 2460, Train Loss: 0.7822, Validation Loss: 1.0920, Validation F1: 0.9049\n",
      "Epoch 2461, Train Loss: 0.7801, Validation Loss: 1.1241, Validation F1: 0.9056\n",
      "Epoch 2462, Train Loss: 0.7793, Validation Loss: 1.0973, Validation F1: 0.9063\n",
      "Epoch 2463, Train Loss: 0.7798, Validation Loss: 1.1107, Validation F1: 0.9067\n",
      "Epoch 2464, Train Loss: 0.7797, Validation Loss: 1.1153, Validation F1: 0.9024\n",
      "Epoch 2465, Train Loss: 0.7775, Validation Loss: 1.1126, Validation F1: 0.9024\n",
      "Epoch 2466, Train Loss: 0.7780, Validation Loss: 1.0734, Validation F1: 0.9035\n",
      "Epoch 2467, Train Loss: 0.7777, Validation Loss: 1.0925, Validation F1: 0.9030\n",
      "Epoch 2468, Train Loss: 0.7773, Validation Loss: 1.0917, Validation F1: 0.9038\n",
      "Epoch 2469, Train Loss: 0.7747, Validation Loss: 1.1145, Validation F1: 0.9015\n",
      "Epoch 2470, Train Loss: 0.7744, Validation Loss: 1.1393, Validation F1: 0.9083\n",
      "Epoch 2471, Train Loss: 0.7765, Validation Loss: 1.1300, Validation F1: 0.9038\n",
      "Epoch 2472, Train Loss: 0.7753, Validation Loss: 1.1083, Validation F1: 0.9041\n",
      "Epoch 2473, Train Loss: 0.7739, Validation Loss: 1.1211, Validation F1: 0.9056\n",
      "Epoch 2474, Train Loss: 0.7738, Validation Loss: 1.1106, Validation F1: 0.9024\n",
      "Epoch 2475, Train Loss: 0.7780, Validation Loss: 1.0823, Validation F1: 0.8996\n",
      "Epoch 2476, Train Loss: 0.7758, Validation Loss: 1.1172, Validation F1: 0.8996\n",
      "Epoch 2477, Train Loss: 0.7739, Validation Loss: 1.1303, Validation F1: 0.8990\n",
      "Epoch 2478, Train Loss: 0.7741, Validation Loss: 1.0817, Validation F1: 0.9008\n",
      "Epoch 2479, Train Loss: 0.7728, Validation Loss: 1.0958, Validation F1: 0.9014\n",
      "Epoch 2480, Train Loss: 0.7748, Validation Loss: 1.1345, Validation F1: 0.9027\n",
      "Epoch 2481, Train Loss: 0.7729, Validation Loss: 1.1005, Validation F1: 0.9031\n",
      "Epoch 2482, Train Loss: 0.7744, Validation Loss: 1.0745, Validation F1: 0.9022\n",
      "Epoch 2483, Train Loss: 0.7744, Validation Loss: 1.0950, Validation F1: 0.8991\n",
      "Epoch 2484, Train Loss: 0.7730, Validation Loss: 1.1004, Validation F1: 0.9000\n",
      "Epoch 2485, Train Loss: 0.7718, Validation Loss: 1.0940, Validation F1: 0.9051\n",
      "Epoch 2486, Train Loss: 0.7724, Validation Loss: 1.1375, Validation F1: 0.9038\n",
      "Epoch 2487, Train Loss: 0.7732, Validation Loss: 1.1173, Validation F1: 0.9071\n",
      "Epoch 2488, Train Loss: 0.7723, Validation Loss: 1.1254, Validation F1: 0.9006\n",
      "Epoch 2489, Train Loss: 0.7749, Validation Loss: 1.1329, Validation F1: 0.8986\n",
      "Epoch 2490, Train Loss: 0.7742, Validation Loss: 1.1020, Validation F1: 0.9027\n",
      "Epoch 2491, Train Loss: 0.7742, Validation Loss: 1.1205, Validation F1: 0.9045\n",
      "Epoch 2492, Train Loss: 0.7751, Validation Loss: 1.1031, Validation F1: 0.9032\n",
      "Epoch 2493, Train Loss: 0.7722, Validation Loss: 1.1413, Validation F1: 0.8991\n",
      "Epoch 2494, Train Loss: 0.7760, Validation Loss: 1.1211, Validation F1: 0.8991\n",
      "Epoch 2495, Train Loss: 0.7718, Validation Loss: 1.0875, Validation F1: 0.9069\n",
      "Epoch 2496, Train Loss: 0.7729, Validation Loss: 1.1320, Validation F1: 0.9063\n",
      "Epoch 2497, Train Loss: 0.7716, Validation Loss: 1.1222, Validation F1: 0.9028\n",
      "Epoch 2498, Train Loss: 0.7720, Validation Loss: 1.0862, Validation F1: 0.8992\n",
      "Epoch 2499, Train Loss: 0.7725, Validation Loss: 1.1057, Validation F1: 0.9012\n",
      "Epoch 2500, Train Loss: 0.7711, Validation Loss: 1.0862, Validation F1: 0.9044\n",
      "Epoch 2501, Train Loss: 0.7720, Validation Loss: 1.1250, Validation F1: 0.9058\n",
      "Epoch 2502, Train Loss: 0.7726, Validation Loss: 1.0873, Validation F1: 0.8991\n",
      "Epoch 2503, Train Loss: 0.7718, Validation Loss: 1.0967, Validation F1: 0.9013\n",
      "Epoch 2504, Train Loss: 0.7720, Validation Loss: 1.1214, Validation F1: 0.8991\n",
      "Epoch 2505, Train Loss: 0.7715, Validation Loss: 1.1209, Validation F1: 0.9030\n",
      "Epoch 2506, Train Loss: 0.7712, Validation Loss: 1.0792, Validation F1: 0.9064\n",
      "Epoch 2507, Train Loss: 0.7717, Validation Loss: 1.1518, Validation F1: 0.9058\n",
      "Epoch 2508, Train Loss: 0.7716, Validation Loss: 1.1381, Validation F1: 0.9020\n",
      "Epoch 2509, Train Loss: 0.7703, Validation Loss: 1.0972, Validation F1: 0.9029\n",
      "Epoch 2510, Train Loss: 0.7714, Validation Loss: 1.1198, Validation F1: 0.9033\n",
      "Epoch 2511, Train Loss: 0.7718, Validation Loss: 1.1156, Validation F1: 0.9023\n",
      "Epoch 2512, Train Loss: 0.7713, Validation Loss: 1.1130, Validation F1: 0.9024\n",
      "Epoch 2513, Train Loss: 0.7701, Validation Loss: 1.1304, Validation F1: 0.9045\n",
      "Epoch 2514, Train Loss: 0.7692, Validation Loss: 1.1158, Validation F1: 0.9005\n",
      "Epoch 2515, Train Loss: 0.7715, Validation Loss: 1.1451, Validation F1: 0.9019\n",
      "Epoch 2516, Train Loss: 0.7718, Validation Loss: 1.0858, Validation F1: 0.9031\n",
      "Epoch 2517, Train Loss: 0.7694, Validation Loss: 1.1040, Validation F1: 0.9040\n",
      "Epoch 2518, Train Loss: 0.7719, Validation Loss: 1.1298, Validation F1: 0.9042\n",
      "Epoch 2519, Train Loss: 0.7702, Validation Loss: 1.1109, Validation F1: 0.9034\n",
      "Epoch 2520, Train Loss: 0.7715, Validation Loss: 1.1586, Validation F1: 0.9035\n",
      "Epoch 2521, Train Loss: 0.7715, Validation Loss: 1.1364, Validation F1: 0.8952\n",
      "Epoch 2522, Train Loss: 0.7714, Validation Loss: 1.1423, Validation F1: 0.9004\n",
      "Epoch 2523, Train Loss: 0.7702, Validation Loss: 1.1058, Validation F1: 0.9009\n",
      "Epoch 2524, Train Loss: 0.7704, Validation Loss: 1.1099, Validation F1: 0.9051\n",
      "Epoch 2525, Train Loss: 0.7716, Validation Loss: 1.1447, Validation F1: 0.9021\n",
      "Epoch 2526, Train Loss: 0.7714, Validation Loss: 1.1155, Validation F1: 0.9024\n",
      "Epoch 2527, Train Loss: 0.7704, Validation Loss: 1.0913, Validation F1: 0.8994\n",
      "Epoch 2528, Train Loss: 0.7707, Validation Loss: 1.0894, Validation F1: 0.8987\n",
      "Epoch 2529, Train Loss: 0.7741, Validation Loss: 1.1078, Validation F1: 0.8988\n",
      "Epoch 2530, Train Loss: 0.7723, Validation Loss: 1.1514, Validation F1: 0.9026\n",
      "Epoch 2531, Train Loss: 0.7699, Validation Loss: 1.1127, Validation F1: 0.9042\n",
      "Epoch 2532, Train Loss: 0.7711, Validation Loss: 1.1118, Validation F1: 0.9014\n",
      "Epoch 2533, Train Loss: 0.7703, Validation Loss: 1.1102, Validation F1: 0.9021\n",
      "Epoch 2534, Train Loss: 0.7699, Validation Loss: 1.1060, Validation F1: 0.9016\n",
      "Epoch 2535, Train Loss: 0.7698, Validation Loss: 1.1566, Validation F1: 0.9018\n",
      "Epoch 2536, Train Loss: 0.7700, Validation Loss: 1.1152, Validation F1: 0.9038\n",
      "Epoch 2537, Train Loss: 0.7712, Validation Loss: 1.1330, Validation F1: 0.9069\n",
      "Epoch 2538, Train Loss: 0.7703, Validation Loss: 1.1234, Validation F1: 0.9018\n",
      "Epoch 2539, Train Loss: 0.7722, Validation Loss: 1.1221, Validation F1: 0.8969\n",
      "Epoch 2540, Train Loss: 0.7720, Validation Loss: 1.1599, Validation F1: 0.8979\n",
      "Epoch 2541, Train Loss: 0.7709, Validation Loss: 1.1133, Validation F1: 0.9055\n",
      "Epoch 2542, Train Loss: 0.7711, Validation Loss: 1.1610, Validation F1: 0.9067\n",
      "Epoch 2543, Train Loss: 0.7719, Validation Loss: 1.1142, Validation F1: 0.9034\n",
      "Epoch 2544, Train Loss: 0.7703, Validation Loss: 1.1572, Validation F1: 0.8986\n",
      "Epoch 2545, Train Loss: 0.7721, Validation Loss: 1.1430, Validation F1: 0.9003\n",
      "Epoch 2546, Train Loss: 0.7707, Validation Loss: 1.1220, Validation F1: 0.9029\n",
      "Epoch 2547, Train Loss: 0.7711, Validation Loss: 1.1315, Validation F1: 0.9066\n",
      "Epoch 2548, Train Loss: 0.7699, Validation Loss: 1.1530, Validation F1: 0.9047\n",
      "Epoch 2549, Train Loss: 0.7706, Validation Loss: 1.1184, Validation F1: 0.8997\n",
      "Epoch 2550, Train Loss: 0.7709, Validation Loss: 1.1296, Validation F1: 0.8978\n",
      "Epoch 2551, Train Loss: 0.7707, Validation Loss: 1.1175, Validation F1: 0.8973\n",
      "Epoch 2552, Train Loss: 0.7709, Validation Loss: 1.1319, Validation F1: 0.9069\n",
      "Epoch 2553, Train Loss: 0.7709, Validation Loss: 1.1155, Validation F1: 0.9074\n",
      "Epoch 2554, Train Loss: 0.7703, Validation Loss: 1.1617, Validation F1: 0.9068\n",
      "Epoch 2555, Train Loss: 0.7703, Validation Loss: 1.1106, Validation F1: 0.9042\n",
      "Epoch 2556, Train Loss: 0.7708, Validation Loss: 1.1286, Validation F1: 0.9013\n",
      "Epoch 2557, Train Loss: 0.7704, Validation Loss: 1.1268, Validation F1: 0.9011\n",
      "Epoch 2558, Train Loss: 0.7717, Validation Loss: 1.1545, Validation F1: 0.9028\n",
      "Epoch 2559, Train Loss: 0.7699, Validation Loss: 1.0991, Validation F1: 0.9064\n",
      "Epoch 2560, Train Loss: 0.7701, Validation Loss: 1.1428, Validation F1: 0.9043\n",
      "Epoch 2561, Train Loss: 0.7703, Validation Loss: 1.1431, Validation F1: 0.9019\n",
      "Epoch 2562, Train Loss: 0.7711, Validation Loss: 1.0949, Validation F1: 0.8998\n",
      "Epoch 2563, Train Loss: 0.7703, Validation Loss: 1.1452, Validation F1: 0.8995\n",
      "Epoch 2564, Train Loss: 0.7711, Validation Loss: 1.1285, Validation F1: 0.9041\n",
      "Epoch 2565, Train Loss: 0.7701, Validation Loss: 1.1110, Validation F1: 0.9074\n",
      "Epoch 2566, Train Loss: 0.7715, Validation Loss: 1.1274, Validation F1: 0.9056\n",
      "Epoch 2567, Train Loss: 0.7701, Validation Loss: 1.1198, Validation F1: 0.9016\n",
      "Epoch 2568, Train Loss: 0.7723, Validation Loss: 1.1214, Validation F1: 0.8989\n",
      "Epoch 2569, Train Loss: 0.7730, Validation Loss: 1.1658, Validation F1: 0.9030\n",
      "Epoch 2570, Train Loss: 0.7721, Validation Loss: 1.1568, Validation F1: 0.9033\n",
      "Epoch 2571, Train Loss: 0.7706, Validation Loss: 1.1012, Validation F1: 0.9070\n",
      "Epoch 2572, Train Loss: 0.7723, Validation Loss: 1.1107, Validation F1: 0.9077\n",
      "Epoch 2573, Train Loss: 0.7715, Validation Loss: 1.1222, Validation F1: 0.9001\n",
      "Epoch 2574, Train Loss: 0.7725, Validation Loss: 1.1310, Validation F1: 0.9018\n",
      "Epoch 2575, Train Loss: 0.7698, Validation Loss: 1.1222, Validation F1: 0.8983\n",
      "Epoch 2576, Train Loss: 0.7706, Validation Loss: 1.1539, Validation F1: 0.9013\n",
      "Epoch 2577, Train Loss: 0.7697, Validation Loss: 1.1436, Validation F1: 0.9058\n",
      "Epoch 2578, Train Loss: 0.7722, Validation Loss: 1.1207, Validation F1: 0.9069\n",
      "Epoch 2579, Train Loss: 0.7706, Validation Loss: 1.1079, Validation F1: 0.9013\n",
      "Epoch 2580, Train Loss: 0.7716, Validation Loss: 1.1199, Validation F1: 0.8987\n",
      "Epoch 2581, Train Loss: 0.7714, Validation Loss: 1.1240, Validation F1: 0.9034\n",
      "Epoch 2582, Train Loss: 0.7713, Validation Loss: 1.1007, Validation F1: 0.9038\n",
      "Epoch 2583, Train Loss: 0.7710, Validation Loss: 1.1359, Validation F1: 0.9040\n",
      "Epoch 2584, Train Loss: 0.7699, Validation Loss: 1.0837, Validation F1: 0.9043\n",
      "Epoch 2585, Train Loss: 0.7710, Validation Loss: 1.1296, Validation F1: 0.9009\n",
      "Epoch 2586, Train Loss: 0.7719, Validation Loss: 1.1761, Validation F1: 0.9039\n",
      "Epoch 2587, Train Loss: 0.7704, Validation Loss: 1.1310, Validation F1: 0.9049\n",
      "Epoch 2588, Train Loss: 0.7709, Validation Loss: 1.1411, Validation F1: 0.9033\n",
      "Epoch 2589, Train Loss: 0.7823, Validation Loss: 1.1463, Validation F1: 0.8982\n",
      "Epoch 2590, Train Loss: 0.7911, Validation Loss: 1.1426, Validation F1: 0.9032\n",
      "Epoch 2591, Train Loss: 0.7756, Validation Loss: 1.1019, Validation F1: 0.9033\n",
      "Epoch 2592, Train Loss: 0.7905, Validation Loss: 1.1612, Validation F1: 0.9049\n",
      "Epoch 2593, Train Loss: 0.7780, Validation Loss: 1.1689, Validation F1: 0.9043\n",
      "Epoch 2594, Train Loss: 0.7881, Validation Loss: 1.1289, Validation F1: 0.9047\n",
      "Epoch 2595, Train Loss: 0.7780, Validation Loss: 1.0657, Validation F1: 0.9055\n",
      "Epoch 2596, Train Loss: 0.7834, Validation Loss: 1.1123, Validation F1: 0.9058\n",
      "Epoch 2597, Train Loss: 0.7774, Validation Loss: 1.1176, Validation F1: 0.9042\n",
      "Epoch 2598, Train Loss: 0.7796, Validation Loss: 1.1506, Validation F1: 0.9005\n",
      "Epoch 2599, Train Loss: 0.7791, Validation Loss: 1.1380, Validation F1: 0.9002\n",
      "Epoch 2600, Train Loss: 0.7806, Validation Loss: 1.0700, Validation F1: 0.9048\n",
      "Epoch 2601, Train Loss: 0.7791, Validation Loss: 1.1496, Validation F1: 0.9050\n",
      "Epoch 2602, Train Loss: 0.7778, Validation Loss: 1.1727, Validation F1: 0.9040\n",
      "Epoch 2603, Train Loss: 0.7791, Validation Loss: 1.1318, Validation F1: 0.9051\n",
      "Epoch 2604, Train Loss: 0.7766, Validation Loss: 1.0933, Validation F1: 0.9001\n",
      "Epoch 2605, Train Loss: 0.7775, Validation Loss: 1.0920, Validation F1: 0.9019\n",
      "Epoch 2606, Train Loss: 0.7820, Validation Loss: 1.1262, Validation F1: 0.9038\n",
      "Epoch 2607, Train Loss: 0.7785, Validation Loss: 1.1131, Validation F1: 0.9053\n",
      "Epoch 2608, Train Loss: 0.7769, Validation Loss: 1.1255, Validation F1: 0.9025\n",
      "Epoch 2609, Train Loss: 0.7771, Validation Loss: 1.1579, Validation F1: 0.8995\n",
      "Epoch 2610, Train Loss: 0.7748, Validation Loss: 1.1252, Validation F1: 0.8984\n",
      "Epoch 2611, Train Loss: 0.7754, Validation Loss: 1.1472, Validation F1: 0.9012\n",
      "Epoch 2612, Train Loss: 0.7779, Validation Loss: 1.1649, Validation F1: 0.9015\n",
      "Epoch 2613, Train Loss: 0.7764, Validation Loss: 1.1510, Validation F1: 0.9016\n",
      "Epoch 2614, Train Loss: 0.7736, Validation Loss: 1.1632, Validation F1: 0.9017\n",
      "Epoch 2615, Train Loss: 0.7740, Validation Loss: 1.1470, Validation F1: 0.8988\n",
      "Epoch 2616, Train Loss: 0.7787, Validation Loss: 1.1333, Validation F1: 0.8979\n",
      "Epoch 2617, Train Loss: 0.7739, Validation Loss: 1.1254, Validation F1: 0.9029\n",
      "Epoch 2618, Train Loss: 0.7783, Validation Loss: 1.1632, Validation F1: 0.9063\n",
      "Epoch 2619, Train Loss: 0.7742, Validation Loss: 1.1342, Validation F1: 0.9068\n",
      "Epoch 2620, Train Loss: 0.7743, Validation Loss: 1.1233, Validation F1: 0.9053\n",
      "Epoch 2621, Train Loss: 0.7756, Validation Loss: 1.1313, Validation F1: 0.8943\n",
      "Epoch 2622, Train Loss: 0.7730, Validation Loss: 1.1586, Validation F1: 0.8977\n",
      "Epoch 2623, Train Loss: 0.7742, Validation Loss: 1.1299, Validation F1: 0.9020\n",
      "Epoch 2624, Train Loss: 0.7756, Validation Loss: 1.1170, Validation F1: 0.9032\n",
      "Epoch 2625, Train Loss: 0.7733, Validation Loss: 1.1067, Validation F1: 0.9055\n",
      "Epoch 2626, Train Loss: 0.7745, Validation Loss: 1.0764, Validation F1: 0.9035\n",
      "Epoch 2627, Train Loss: 0.7716, Validation Loss: 1.1578, Validation F1: 0.8967\n",
      "Epoch 2628, Train Loss: 0.7740, Validation Loss: 1.1296, Validation F1: 0.8927\n",
      "Epoch 2629, Train Loss: 0.7724, Validation Loss: 1.1610, Validation F1: 0.9003\n",
      "Epoch 2630, Train Loss: 0.7745, Validation Loss: 1.0840, Validation F1: 0.9021\n",
      "Epoch 2631, Train Loss: 0.7758, Validation Loss: 1.1383, Validation F1: 0.9006\n",
      "Epoch 2632, Train Loss: 0.7759, Validation Loss: 1.1409, Validation F1: 0.9021\n",
      "Epoch 2633, Train Loss: 0.7742, Validation Loss: 1.1730, Validation F1: 0.9035\n",
      "Epoch 2634, Train Loss: 0.7755, Validation Loss: 1.1521, Validation F1: 0.9043\n",
      "Epoch 2635, Train Loss: 0.7730, Validation Loss: 1.0806, Validation F1: 0.9057\n",
      "Epoch 2636, Train Loss: 0.7729, Validation Loss: 1.1278, Validation F1: 0.9036\n",
      "Epoch 2637, Train Loss: 0.7717, Validation Loss: 1.1442, Validation F1: 0.9039\n",
      "Epoch 2638, Train Loss: 0.7716, Validation Loss: 1.1543, Validation F1: 0.8950\n",
      "Epoch 2639, Train Loss: 0.7724, Validation Loss: 1.1922, Validation F1: 0.9022\n",
      "Epoch 2640, Train Loss: 0.7715, Validation Loss: 1.1200, Validation F1: 0.9049\n",
      "Epoch 2641, Train Loss: 0.7710, Validation Loss: 1.1298, Validation F1: 0.9029\n",
      "Epoch 2642, Train Loss: 0.7713, Validation Loss: 1.1537, Validation F1: 0.9042\n",
      "Epoch 2643, Train Loss: 0.7721, Validation Loss: 1.1137, Validation F1: 0.9014\n",
      "Epoch 2644, Train Loss: 0.7733, Validation Loss: 1.1472, Validation F1: 0.8954\n",
      "Epoch 2645, Train Loss: 0.7703, Validation Loss: 1.1166, Validation F1: 0.9038\n",
      "Epoch 2646, Train Loss: 0.7717, Validation Loss: 1.0929, Validation F1: 0.9053\n",
      "Epoch 2647, Train Loss: 0.7712, Validation Loss: 1.1559, Validation F1: 0.9044\n",
      "Epoch 2648, Train Loss: 0.7703, Validation Loss: 1.1352, Validation F1: 0.9018\n",
      "Epoch 2649, Train Loss: 0.7749, Validation Loss: 1.1113, Validation F1: 0.9028\n",
      "Epoch 2650, Train Loss: 0.7737, Validation Loss: 1.1370, Validation F1: 0.9017\n",
      "Epoch 2651, Train Loss: 0.7715, Validation Loss: 1.1210, Validation F1: 0.9003\n",
      "Epoch 2652, Train Loss: 0.7733, Validation Loss: 1.1297, Validation F1: 0.9021\n",
      "Epoch 2653, Train Loss: 0.7714, Validation Loss: 1.1409, Validation F1: 0.9023\n",
      "Epoch 2654, Train Loss: 0.7732, Validation Loss: 1.1612, Validation F1: 0.9043\n",
      "Epoch 2655, Train Loss: 0.7713, Validation Loss: 1.1543, Validation F1: 0.9010\n",
      "Epoch 2656, Train Loss: 0.7731, Validation Loss: 1.1460, Validation F1: 0.9000\n",
      "Epoch 2657, Train Loss: 0.7736, Validation Loss: 1.1320, Validation F1: 0.9017\n",
      "Epoch 2658, Train Loss: 0.7735, Validation Loss: 1.1304, Validation F1: 0.9041\n",
      "Epoch 2659, Train Loss: 0.7718, Validation Loss: 1.1368, Validation F1: 0.9044\n",
      "Epoch 2660, Train Loss: 0.7709, Validation Loss: 1.1076, Validation F1: 0.9013\n",
      "Epoch 2661, Train Loss: 0.7745, Validation Loss: 1.1395, Validation F1: 0.9012\n",
      "Epoch 2662, Train Loss: 0.7715, Validation Loss: 1.1299, Validation F1: 0.8990\n",
      "Epoch 2663, Train Loss: 0.7713, Validation Loss: 1.1706, Validation F1: 0.9007\n",
      "Epoch 2664, Train Loss: 0.7716, Validation Loss: 1.0937, Validation F1: 0.9036\n",
      "Epoch 2665, Train Loss: 0.7721, Validation Loss: 1.1484, Validation F1: 0.9070\n",
      "Epoch 2666, Train Loss: 0.7722, Validation Loss: 1.0791, Validation F1: 0.9056\n",
      "Epoch 2667, Train Loss: 0.7703, Validation Loss: 1.1406, Validation F1: 0.9008\n",
      "Epoch 2668, Train Loss: 0.7720, Validation Loss: 1.1218, Validation F1: 0.8981\n",
      "Epoch 2669, Train Loss: 0.7707, Validation Loss: 1.1363, Validation F1: 0.9005\n",
      "Epoch 2670, Train Loss: 0.7716, Validation Loss: 1.1417, Validation F1: 0.9049\n",
      "Epoch 2671, Train Loss: 0.7704, Validation Loss: 1.1129, Validation F1: 0.9055\n",
      "Epoch 2672, Train Loss: 0.7706, Validation Loss: 1.1058, Validation F1: 0.9039\n",
      "Epoch 2673, Train Loss: 0.7711, Validation Loss: 1.0924, Validation F1: 0.8991\n",
      "Epoch 2674, Train Loss: 0.7712, Validation Loss: 1.1392, Validation F1: 0.8991\n",
      "Epoch 2675, Train Loss: 0.7733, Validation Loss: 1.1274, Validation F1: 0.9017\n",
      "Epoch 2676, Train Loss: 0.7693, Validation Loss: 1.1450, Validation F1: 0.9065\n",
      "Epoch 2677, Train Loss: 0.7702, Validation Loss: 1.1228, Validation F1: 0.9069\n",
      "Epoch 2678, Train Loss: 0.7709, Validation Loss: 1.1356, Validation F1: 0.9020\n",
      "Epoch 2679, Train Loss: 0.7703, Validation Loss: 1.1229, Validation F1: 0.8953\n",
      "Epoch 2680, Train Loss: 0.7705, Validation Loss: 1.1237, Validation F1: 0.8990\n",
      "Epoch 2681, Train Loss: 0.7692, Validation Loss: 1.1164, Validation F1: 0.9047\n",
      "Epoch 2682, Train Loss: 0.7697, Validation Loss: 1.1200, Validation F1: 0.9057\n",
      "Epoch 2683, Train Loss: 0.7705, Validation Loss: 1.1305, Validation F1: 0.9064\n",
      "Epoch 2684, Train Loss: 0.7719, Validation Loss: 1.1248, Validation F1: 0.9047\n",
      "Epoch 2685, Train Loss: 0.7712, Validation Loss: 1.1423, Validation F1: 0.8943\n",
      "Epoch 2686, Train Loss: 0.7694, Validation Loss: 1.1624, Validation F1: 0.8937\n",
      "Epoch 2687, Train Loss: 0.7712, Validation Loss: 1.1605, Validation F1: 0.9048\n",
      "Epoch 2688, Train Loss: 0.7687, Validation Loss: 1.0973, Validation F1: 0.9071\n",
      "Epoch 2689, Train Loss: 0.7691, Validation Loss: 1.1647, Validation F1: 0.9056\n",
      "Epoch 2690, Train Loss: 0.7705, Validation Loss: 1.1275, Validation F1: 0.8987\n",
      "Epoch 2691, Train Loss: 0.7700, Validation Loss: 1.1242, Validation F1: 0.8975\n",
      "Epoch 2692, Train Loss: 0.7707, Validation Loss: 1.1416, Validation F1: 0.9011\n",
      "Epoch 2693, Train Loss: 0.7699, Validation Loss: 1.0896, Validation F1: 0.9057\n",
      "Epoch 2694, Train Loss: 0.7697, Validation Loss: 1.1105, Validation F1: 0.9060\n",
      "Epoch 2695, Train Loss: 0.7691, Validation Loss: 1.1645, Validation F1: 0.9052\n",
      "Epoch 2696, Train Loss: 0.7693, Validation Loss: 1.1714, Validation F1: 0.9020\n",
      "Epoch 2697, Train Loss: 0.7699, Validation Loss: 1.1538, Validation F1: 0.9026\n",
      "Epoch 2698, Train Loss: 0.7688, Validation Loss: 1.1267, Validation F1: 0.9038\n",
      "Epoch 2699, Train Loss: 0.7688, Validation Loss: 1.1115, Validation F1: 0.9063\n",
      "Epoch 2700, Train Loss: 0.7687, Validation Loss: 1.1709, Validation F1: 0.9023\n",
      "Epoch 2701, Train Loss: 0.7694, Validation Loss: 1.1033, Validation F1: 0.9018\n",
      "Epoch 2702, Train Loss: 0.7692, Validation Loss: 1.1336, Validation F1: 0.9050\n",
      "Epoch 2703, Train Loss: 0.7699, Validation Loss: 1.1339, Validation F1: 0.9032\n",
      "Epoch 2704, Train Loss: 0.7679, Validation Loss: 1.1261, Validation F1: 0.9027\n",
      "Epoch 2705, Train Loss: 0.7689, Validation Loss: 1.1407, Validation F1: 0.9057\n",
      "Epoch 2706, Train Loss: 0.7690, Validation Loss: 1.1139, Validation F1: 0.9050\n",
      "Epoch 2707, Train Loss: 0.7698, Validation Loss: 1.1257, Validation F1: 0.9040\n",
      "Epoch 2708, Train Loss: 0.7687, Validation Loss: 1.1628, Validation F1: 0.9034\n",
      "Epoch 2709, Train Loss: 0.7688, Validation Loss: 1.1184, Validation F1: 0.9015\n",
      "Epoch 2710, Train Loss: 0.7707, Validation Loss: 1.1400, Validation F1: 0.9045\n",
      "Epoch 2711, Train Loss: 0.7686, Validation Loss: 1.1457, Validation F1: 0.9054\n",
      "Epoch 2712, Train Loss: 0.7701, Validation Loss: 1.1443, Validation F1: 0.8994\n",
      "Epoch 2713, Train Loss: 0.7707, Validation Loss: 1.1281, Validation F1: 0.9016\n",
      "Epoch 2714, Train Loss: 0.7699, Validation Loss: 1.1445, Validation F1: 0.9019\n",
      "Epoch 2715, Train Loss: 0.7702, Validation Loss: 1.1163, Validation F1: 0.9036\n",
      "Epoch 2716, Train Loss: 0.7688, Validation Loss: 1.1599, Validation F1: 0.9022\n",
      "Epoch 2717, Train Loss: 0.7696, Validation Loss: 1.1172, Validation F1: 0.9026\n",
      "Epoch 2718, Train Loss: 0.7693, Validation Loss: 1.1433, Validation F1: 0.8996\n",
      "Epoch 2719, Train Loss: 0.7730, Validation Loss: 1.1364, Validation F1: 0.9041\n",
      "Epoch 2720, Train Loss: 0.7725, Validation Loss: 1.1571, Validation F1: 0.9027\n",
      "Epoch 2721, Train Loss: 0.7719, Validation Loss: 1.1749, Validation F1: 0.9032\n",
      "Epoch 2722, Train Loss: 0.7711, Validation Loss: 1.1300, Validation F1: 0.9033\n",
      "Epoch 2723, Train Loss: 0.7680, Validation Loss: 1.1077, Validation F1: 0.9051\n",
      "Epoch 2724, Train Loss: 0.7705, Validation Loss: 1.1571, Validation F1: 0.9048\n",
      "Epoch 2725, Train Loss: 0.7707, Validation Loss: 1.1576, Validation F1: 0.8991\n",
      "Epoch 2726, Train Loss: 0.7696, Validation Loss: 1.1232, Validation F1: 0.9007\n",
      "Epoch 2727, Train Loss: 0.7716, Validation Loss: 1.1257, Validation F1: 0.9010\n",
      "Epoch 2728, Train Loss: 0.7695, Validation Loss: 1.1105, Validation F1: 0.9051\n",
      "Epoch 2729, Train Loss: 0.7697, Validation Loss: 1.1450, Validation F1: 0.9050\n",
      "Epoch 2730, Train Loss: 0.7705, Validation Loss: 1.1271, Validation F1: 0.9013\n",
      "Epoch 2731, Train Loss: 0.7699, Validation Loss: 1.0998, Validation F1: 0.8990\n",
      "Epoch 2732, Train Loss: 0.7691, Validation Loss: 1.1215, Validation F1: 0.9051\n",
      "Epoch 2733, Train Loss: 0.7694, Validation Loss: 1.1161, Validation F1: 0.9038\n",
      "Epoch 2734, Train Loss: 0.7702, Validation Loss: 1.1731, Validation F1: 0.9047\n",
      "Epoch 2735, Train Loss: 0.7691, Validation Loss: 1.1338, Validation F1: 0.9063\n",
      "Epoch 2736, Train Loss: 0.7698, Validation Loss: 1.1516, Validation F1: 0.9002\n",
      "Epoch 2737, Train Loss: 0.7680, Validation Loss: 1.1546, Validation F1: 0.9009\n",
      "Epoch 2738, Train Loss: 0.7707, Validation Loss: 1.1682, Validation F1: 0.9016\n",
      "Epoch 2739, Train Loss: 0.7690, Validation Loss: 1.1373, Validation F1: 0.9053\n",
      "Epoch 2740, Train Loss: 0.7688, Validation Loss: 1.1538, Validation F1: 0.9034\n",
      "Epoch 2741, Train Loss: 0.7694, Validation Loss: 1.1344, Validation F1: 0.9036\n",
      "Epoch 2742, Train Loss: 0.7691, Validation Loss: 1.1378, Validation F1: 0.8992\n",
      "Epoch 2743, Train Loss: 0.7691, Validation Loss: 1.1351, Validation F1: 0.9063\n",
      "Epoch 2744, Train Loss: 0.7670, Validation Loss: 1.1334, Validation F1: 0.9052\n",
      "Epoch 2745, Train Loss: 0.7691, Validation Loss: 1.1464, Validation F1: 0.9054\n",
      "Epoch 2746, Train Loss: 0.7685, Validation Loss: 1.1921, Validation F1: 0.9018\n",
      "Epoch 2747, Train Loss: 0.7682, Validation Loss: 1.1828, Validation F1: 0.9004\n",
      "Epoch 2748, Train Loss: 0.7687, Validation Loss: 1.1464, Validation F1: 0.9057\n",
      "Epoch 2749, Train Loss: 0.7678, Validation Loss: 1.1827, Validation F1: 0.9006\n",
      "Epoch 2750, Train Loss: 0.7676, Validation Loss: 1.1230, Validation F1: 0.8991\n",
      "Epoch 2751, Train Loss: 0.7699, Validation Loss: 1.1342, Validation F1: 0.8997\n",
      "Epoch 2752, Train Loss: 0.7683, Validation Loss: 1.1162, Validation F1: 0.9031\n",
      "Epoch 2753, Train Loss: 0.7673, Validation Loss: 1.1258, Validation F1: 0.9057\n",
      "Epoch 2754, Train Loss: 0.7681, Validation Loss: 1.1664, Validation F1: 0.9048\n",
      "Epoch 2755, Train Loss: 0.7689, Validation Loss: 1.1845, Validation F1: 0.9049\n",
      "Epoch 2756, Train Loss: 0.7684, Validation Loss: 1.1140, Validation F1: 0.9011\n",
      "Epoch 2757, Train Loss: 0.7714, Validation Loss: 1.1555, Validation F1: 0.8998\n",
      "Epoch 2758, Train Loss: 0.7688, Validation Loss: 1.1473, Validation F1: 0.9053\n",
      "Epoch 2759, Train Loss: 0.7687, Validation Loss: 1.1699, Validation F1: 0.9068\n",
      "Epoch 2760, Train Loss: 0.7700, Validation Loss: 1.1261, Validation F1: 0.9077\n",
      "Epoch 2761, Train Loss: 0.7682, Validation Loss: 1.1609, Validation F1: 0.9026\n",
      "Epoch 2762, Train Loss: 0.7705, Validation Loss: 1.1159, Validation F1: 0.9005\n",
      "Epoch 2763, Train Loss: 0.7703, Validation Loss: 1.1432, Validation F1: 0.9023\n",
      "Epoch 2764, Train Loss: 0.7708, Validation Loss: 1.1550, Validation F1: 0.9043\n",
      "Epoch 2765, Train Loss: 0.7688, Validation Loss: 1.1396, Validation F1: 0.9065\n",
      "Epoch 2766, Train Loss: 0.7693, Validation Loss: 1.1008, Validation F1: 0.9051\n",
      "Epoch 2767, Train Loss: 0.7688, Validation Loss: 1.1447, Validation F1: 0.8985\n",
      "Epoch 2768, Train Loss: 0.7697, Validation Loss: 1.1294, Validation F1: 0.8979\n",
      "Epoch 2769, Train Loss: 0.7689, Validation Loss: 1.1378, Validation F1: 0.9008\n",
      "Epoch 2770, Train Loss: 0.7692, Validation Loss: 1.1907, Validation F1: 0.9036\n",
      "Epoch 2771, Train Loss: 0.7681, Validation Loss: 1.1848, Validation F1: 0.9048\n",
      "Epoch 2772, Train Loss: 0.7685, Validation Loss: 1.1726, Validation F1: 0.9036\n",
      "Epoch 2773, Train Loss: 0.7680, Validation Loss: 1.1761, Validation F1: 0.9012\n",
      "Epoch 2774, Train Loss: 0.7687, Validation Loss: 1.1495, Validation F1: 0.9019\n",
      "Epoch 2775, Train Loss: 0.7689, Validation Loss: 1.1182, Validation F1: 0.9016\n",
      "Epoch 2776, Train Loss: 0.7688, Validation Loss: 1.1540, Validation F1: 0.9036\n",
      "Epoch 2777, Train Loss: 0.7682, Validation Loss: 1.1364, Validation F1: 0.9021\n",
      "Epoch 2778, Train Loss: 0.7700, Validation Loss: 1.1289, Validation F1: 0.9028\n",
      "Epoch 2779, Train Loss: 0.7686, Validation Loss: 1.1432, Validation F1: 0.9026\n",
      "Epoch 2780, Train Loss: 0.7707, Validation Loss: 1.1188, Validation F1: 0.9014\n",
      "Epoch 2781, Train Loss: 0.7696, Validation Loss: 1.1150, Validation F1: 0.9038\n",
      "Epoch 2782, Train Loss: 0.7699, Validation Loss: 1.1718, Validation F1: 0.9036\n",
      "Epoch 2783, Train Loss: 0.7694, Validation Loss: 1.1728, Validation F1: 0.9022\n",
      "Epoch 2784, Train Loss: 0.7690, Validation Loss: 1.1451, Validation F1: 0.9011\n",
      "Epoch 2785, Train Loss: 0.7686, Validation Loss: 1.1624, Validation F1: 0.8983\n",
      "Epoch 2786, Train Loss: 0.7688, Validation Loss: 1.2196, Validation F1: 0.8984\n",
      "Epoch 2787, Train Loss: 0.7692, Validation Loss: 1.1602, Validation F1: 0.9033\n",
      "Epoch 2788, Train Loss: 0.7689, Validation Loss: 1.1173, Validation F1: 0.9039\n",
      "Epoch 2789, Train Loss: 0.7680, Validation Loss: 1.1432, Validation F1: 0.9051\n",
      "Epoch 2790, Train Loss: 0.7682, Validation Loss: 1.1436, Validation F1: 0.9041\n",
      "Epoch 2791, Train Loss: 0.7685, Validation Loss: 1.1347, Validation F1: 0.9030\n",
      "Epoch 2792, Train Loss: 0.7676, Validation Loss: 1.1630, Validation F1: 0.9069\n",
      "Epoch 2793, Train Loss: 0.7676, Validation Loss: 1.1788, Validation F1: 0.9036\n",
      "Epoch 2794, Train Loss: 0.7683, Validation Loss: 1.1389, Validation F1: 0.9020\n",
      "Epoch 2795, Train Loss: 0.7684, Validation Loss: 1.1389, Validation F1: 0.9007\n",
      "Epoch 2796, Train Loss: 0.7695, Validation Loss: 1.1323, Validation F1: 0.9030\n",
      "Epoch 2797, Train Loss: 0.7685, Validation Loss: 1.1251, Validation F1: 0.9028\n",
      "Epoch 2798, Train Loss: 0.7673, Validation Loss: 1.1719, Validation F1: 0.9034\n",
      "Epoch 2799, Train Loss: 0.7675, Validation Loss: 1.1330, Validation F1: 0.9013\n",
      "Epoch 2800, Train Loss: 0.7679, Validation Loss: 1.1378, Validation F1: 0.9044\n",
      "Epoch 2801, Train Loss: 0.7676, Validation Loss: 1.1252, Validation F1: 0.9041\n",
      "Epoch 2802, Train Loss: 0.7691, Validation Loss: 1.1094, Validation F1: 0.9025\n",
      "Epoch 2803, Train Loss: 0.7701, Validation Loss: 1.1079, Validation F1: 0.9026\n",
      "Epoch 2804, Train Loss: 0.7686, Validation Loss: 1.1160, Validation F1: 0.9021\n",
      "Epoch 2805, Train Loss: 0.7716, Validation Loss: 1.1220, Validation F1: 0.8996\n",
      "Epoch 2806, Train Loss: 0.7716, Validation Loss: 1.1199, Validation F1: 0.9045\n",
      "Epoch 2807, Train Loss: 0.7707, Validation Loss: 1.1526, Validation F1: 0.9055\n",
      "Epoch 2808, Train Loss: 0.7701, Validation Loss: 1.1615, Validation F1: 0.9045\n",
      "Epoch 2809, Train Loss: 0.7705, Validation Loss: 1.1388, Validation F1: 0.8986\n",
      "Epoch 2810, Train Loss: 0.7703, Validation Loss: 1.1569, Validation F1: 0.8980\n",
      "Epoch 2811, Train Loss: 0.7745, Validation Loss: 1.1286, Validation F1: 0.9010\n",
      "Epoch 2812, Train Loss: 0.7738, Validation Loss: 1.1201, Validation F1: 0.9067\n",
      "Epoch 2813, Train Loss: 0.7703, Validation Loss: 1.1444, Validation F1: 0.9063\n",
      "Epoch 2814, Train Loss: 0.7723, Validation Loss: 1.1148, Validation F1: 0.8990\n",
      "Epoch 2815, Train Loss: 0.7734, Validation Loss: 1.1523, Validation F1: 0.8939\n",
      "Epoch 2816, Train Loss: 0.7727, Validation Loss: 1.1147, Validation F1: 0.9024\n",
      "Epoch 2817, Train Loss: 0.7697, Validation Loss: 1.1549, Validation F1: 0.9018\n",
      "Epoch 2818, Train Loss: 0.7715, Validation Loss: 1.1714, Validation F1: 0.9076\n",
      "Epoch 2819, Train Loss: 0.7725, Validation Loss: 1.1365, Validation F1: 0.9069\n",
      "Epoch 2820, Train Loss: 0.7703, Validation Loss: 1.1497, Validation F1: 0.9008\n",
      "Epoch 2821, Train Loss: 0.7712, Validation Loss: 1.1473, Validation F1: 0.8942\n",
      "Epoch 2822, Train Loss: 0.7729, Validation Loss: 1.1235, Validation F1: 0.8983\n",
      "Epoch 2823, Train Loss: 0.7700, Validation Loss: 1.1447, Validation F1: 0.9050\n",
      "Epoch 2824, Train Loss: 0.7715, Validation Loss: 1.1498, Validation F1: 0.9060\n",
      "Epoch 2825, Train Loss: 0.7718, Validation Loss: 1.1546, Validation F1: 0.9066\n",
      "Epoch 2826, Train Loss: 0.7711, Validation Loss: 1.1434, Validation F1: 0.8998\n",
      "Epoch 2827, Train Loss: 0.7695, Validation Loss: 1.1412, Validation F1: 0.8953\n",
      "Epoch 2828, Train Loss: 0.7707, Validation Loss: 1.1280, Validation F1: 0.9053\n",
      "Epoch 2829, Train Loss: 0.7693, Validation Loss: 1.1535, Validation F1: 0.9037\n",
      "Epoch 2830, Train Loss: 0.7694, Validation Loss: 1.1617, Validation F1: 0.9013\n",
      "Epoch 2831, Train Loss: 0.7706, Validation Loss: 1.1739, Validation F1: 0.8983\n",
      "Epoch 2832, Train Loss: 0.7726, Validation Loss: 1.1056, Validation F1: 0.9026\n",
      "Epoch 2833, Train Loss: 0.7699, Validation Loss: 1.1330, Validation F1: 0.9044\n",
      "Epoch 2834, Train Loss: 0.7681, Validation Loss: 1.1358, Validation F1: 0.9053\n",
      "Epoch 2835, Train Loss: 0.7704, Validation Loss: 1.1195, Validation F1: 0.9053\n",
      "Epoch 2836, Train Loss: 0.7701, Validation Loss: 1.1224, Validation F1: 0.9036\n",
      "Epoch 2837, Train Loss: 0.7686, Validation Loss: 1.1671, Validation F1: 0.9005\n",
      "Epoch 2838, Train Loss: 0.7697, Validation Loss: 1.1593, Validation F1: 0.9003\n",
      "Epoch 2839, Train Loss: 0.7700, Validation Loss: 1.1537, Validation F1: 0.9016\n",
      "Epoch 2840, Train Loss: 0.7675, Validation Loss: 1.1726, Validation F1: 0.9057\n",
      "Epoch 2841, Train Loss: 0.7679, Validation Loss: 1.1515, Validation F1: 0.9037\n",
      "Epoch 2842, Train Loss: 0.7725, Validation Loss: 1.1291, Validation F1: 0.9016\n",
      "Epoch 2843, Train Loss: 0.7686, Validation Loss: 1.1026, Validation F1: 0.9006\n",
      "Epoch 2844, Train Loss: 0.7702, Validation Loss: 1.1573, Validation F1: 0.9042\n",
      "Epoch 2845, Train Loss: 0.7720, Validation Loss: 1.1437, Validation F1: 0.9066\n",
      "Epoch 2846, Train Loss: 0.7699, Validation Loss: 1.1561, Validation F1: 0.9058\n",
      "Epoch 2847, Train Loss: 0.7691, Validation Loss: 1.1376, Validation F1: 0.9048\n",
      "Epoch 2848, Train Loss: 0.7699, Validation Loss: 1.1283, Validation F1: 0.9029\n",
      "Epoch 2849, Train Loss: 0.7684, Validation Loss: 1.1784, Validation F1: 0.9021\n",
      "Epoch 2850, Train Loss: 0.7683, Validation Loss: 1.1804, Validation F1: 0.9036\n",
      "Epoch 2851, Train Loss: 0.7684, Validation Loss: 1.1674, Validation F1: 0.9039\n",
      "Epoch 2852, Train Loss: 0.7692, Validation Loss: 1.1902, Validation F1: 0.9025\n",
      "Epoch 2853, Train Loss: 0.7678, Validation Loss: 1.1677, Validation F1: 0.9008\n",
      "Epoch 2854, Train Loss: 0.7681, Validation Loss: 1.1161, Validation F1: 0.9003\n",
      "Epoch 2855, Train Loss: 0.7671, Validation Loss: 1.1576, Validation F1: 0.9013\n",
      "Epoch 2856, Train Loss: 0.7689, Validation Loss: 1.1532, Validation F1: 0.9065\n",
      "Epoch 2857, Train Loss: 0.7686, Validation Loss: 1.1132, Validation F1: 0.9066\n",
      "Epoch 2858, Train Loss: 0.7682, Validation Loss: 1.1528, Validation F1: 0.9058\n",
      "Epoch 2859, Train Loss: 0.7670, Validation Loss: 1.1509, Validation F1: 0.9036\n",
      "Epoch 2860, Train Loss: 0.7686, Validation Loss: 1.1237, Validation F1: 0.9044\n",
      "Epoch 2861, Train Loss: 0.7684, Validation Loss: 1.1378, Validation F1: 0.9050\n",
      "Epoch 2862, Train Loss: 0.7689, Validation Loss: 1.1975, Validation F1: 0.9022\n",
      "Epoch 2863, Train Loss: 0.7686, Validation Loss: 1.1922, Validation F1: 0.9060\n",
      "Epoch 2864, Train Loss: 0.7700, Validation Loss: 1.1589, Validation F1: 0.9028\n",
      "Epoch 2865, Train Loss: 0.7703, Validation Loss: 1.1567, Validation F1: 0.9031\n",
      "Epoch 2866, Train Loss: 0.7702, Validation Loss: 1.1542, Validation F1: 0.9062\n",
      "Epoch 2867, Train Loss: 0.7694, Validation Loss: 1.1335, Validation F1: 0.9052\n",
      "Epoch 2868, Train Loss: 0.7670, Validation Loss: 1.1329, Validation F1: 0.9055\n",
      "Epoch 2869, Train Loss: 0.7684, Validation Loss: 1.1483, Validation F1: 0.9020\n",
      "Epoch 2870, Train Loss: 0.7675, Validation Loss: 1.1310, Validation F1: 0.9026\n",
      "Epoch 2871, Train Loss: 0.7692, Validation Loss: 1.1238, Validation F1: 0.9007\n",
      "Epoch 2872, Train Loss: 0.7696, Validation Loss: 1.1757, Validation F1: 0.9050\n",
      "Epoch 2873, Train Loss: 0.7684, Validation Loss: 1.1384, Validation F1: 0.9041\n",
      "Epoch 2874, Train Loss: 0.7723, Validation Loss: 1.1093, Validation F1: 0.9003\n",
      "Epoch 2875, Train Loss: 0.7721, Validation Loss: 1.1193, Validation F1: 0.8975\n",
      "Epoch 2876, Train Loss: 0.7754, Validation Loss: 1.1568, Validation F1: 0.9028\n",
      "Epoch 2877, Train Loss: 0.7730, Validation Loss: 1.1510, Validation F1: 0.9068\n",
      "Epoch 2878, Train Loss: 0.7761, Validation Loss: 1.1596, Validation F1: 0.9059\n",
      "Epoch 2879, Train Loss: 0.7773, Validation Loss: 1.1452, Validation F1: 0.9041\n",
      "Epoch 2880, Train Loss: 0.7773, Validation Loss: 1.1554, Validation F1: 0.8993\n",
      "Epoch 2881, Train Loss: 0.7749, Validation Loss: 1.1487, Validation F1: 0.8991\n",
      "Epoch 2882, Train Loss: 0.7766, Validation Loss: 1.1390, Validation F1: 0.9002\n",
      "Epoch 2883, Train Loss: 0.7746, Validation Loss: 1.1543, Validation F1: 0.9059\n",
      "Epoch 2884, Train Loss: 0.7740, Validation Loss: 1.1487, Validation F1: 0.9069\n",
      "Epoch 2885, Train Loss: 0.7756, Validation Loss: 1.1381, Validation F1: 0.9025\n",
      "Epoch 2886, Train Loss: 0.7729, Validation Loss: 1.1536, Validation F1: 0.8933\n",
      "Epoch 2887, Train Loss: 0.7742, Validation Loss: 1.1749, Validation F1: 0.9002\n",
      "Epoch 2888, Train Loss: 0.7738, Validation Loss: 1.1701, Validation F1: 0.9003\n",
      "Epoch 2889, Train Loss: 0.7734, Validation Loss: 1.1501, Validation F1: 0.9060\n",
      "Epoch 2890, Train Loss: 0.7743, Validation Loss: 1.1575, Validation F1: 0.9063\n",
      "Epoch 2891, Train Loss: 0.7717, Validation Loss: 1.1582, Validation F1: 0.9049\n",
      "Epoch 2892, Train Loss: 0.7710, Validation Loss: 1.1231, Validation F1: 0.8932\n",
      "Epoch 2893, Train Loss: 0.7727, Validation Loss: 1.1883, Validation F1: 0.8962\n",
      "Epoch 2894, Train Loss: 0.7715, Validation Loss: 1.1459, Validation F1: 0.9012\n",
      "Epoch 2895, Train Loss: 0.7713, Validation Loss: 1.1386, Validation F1: 0.9049\n",
      "Epoch 2896, Train Loss: 0.7710, Validation Loss: 1.1059, Validation F1: 0.9040\n",
      "Epoch 2897, Train Loss: 0.7707, Validation Loss: 1.1018, Validation F1: 0.8990\n",
      "Epoch 2898, Train Loss: 0.7722, Validation Loss: 1.1253, Validation F1: 0.8977\n",
      "Epoch 2899, Train Loss: 0.7690, Validation Loss: 1.1700, Validation F1: 0.8985\n",
      "Epoch 2900, Train Loss: 0.7704, Validation Loss: 1.1749, Validation F1: 0.9028\n",
      "Epoch 2901, Train Loss: 0.7693, Validation Loss: 1.1415, Validation F1: 0.9052\n",
      "Epoch 2902, Train Loss: 0.7706, Validation Loss: 1.1059, Validation F1: 0.9039\n",
      "Epoch 2903, Train Loss: 0.7691, Validation Loss: 1.1754, Validation F1: 0.9026\n",
      "Epoch 2904, Train Loss: 0.7697, Validation Loss: 1.1466, Validation F1: 0.9013\n",
      "Epoch 2905, Train Loss: 0.7688, Validation Loss: 1.1540, Validation F1: 0.8985\n",
      "Epoch 2906, Train Loss: 0.7700, Validation Loss: 1.1456, Validation F1: 0.9021\n",
      "Epoch 2907, Train Loss: 0.7684, Validation Loss: 1.1104, Validation F1: 0.9066\n",
      "Epoch 2908, Train Loss: 0.7710, Validation Loss: 1.1521, Validation F1: 0.9055\n",
      "Epoch 2909, Train Loss: 0.7708, Validation Loss: 1.1536, Validation F1: 0.9036\n",
      "Epoch 2910, Train Loss: 0.7685, Validation Loss: 1.1687, Validation F1: 0.8992\n",
      "Epoch 2911, Train Loss: 0.7694, Validation Loss: 1.1252, Validation F1: 0.9020\n",
      "Epoch 2912, Train Loss: 0.7696, Validation Loss: 1.1293, Validation F1: 0.8979\n",
      "Epoch 2913, Train Loss: 0.7694, Validation Loss: 1.1579, Validation F1: 0.9064\n",
      "Epoch 2914, Train Loss: 0.7685, Validation Loss: 1.1638, Validation F1: 0.9062\n",
      "Epoch 2915, Train Loss: 0.7692, Validation Loss: 1.1124, Validation F1: 0.9036\n",
      "Epoch 2916, Train Loss: 0.7690, Validation Loss: 1.2064, Validation F1: 0.9023\n",
      "Epoch 2917, Train Loss: 0.7678, Validation Loss: 1.1185, Validation F1: 0.9028\n",
      "Epoch 2918, Train Loss: 0.7677, Validation Loss: 1.1275, Validation F1: 0.8995\n",
      "Epoch 2919, Train Loss: 0.7688, Validation Loss: 1.1684, Validation F1: 0.9037\n",
      "Epoch 2920, Train Loss: 0.7695, Validation Loss: 1.1407, Validation F1: 0.9024\n",
      "Epoch 2921, Train Loss: 0.7678, Validation Loss: 1.1182, Validation F1: 0.9020\n",
      "Epoch 2922, Train Loss: 0.7679, Validation Loss: 1.1516, Validation F1: 0.9007\n",
      "Epoch 2923, Train Loss: 0.7675, Validation Loss: 1.1697, Validation F1: 0.9022\n",
      "Epoch 2924, Train Loss: 0.7682, Validation Loss: 1.1324, Validation F1: 0.9032\n",
      "Epoch 2925, Train Loss: 0.7669, Validation Loss: 1.1926, Validation F1: 0.9021\n",
      "Epoch 2926, Train Loss: 0.7670, Validation Loss: 1.1308, Validation F1: 0.8999\n",
      "Epoch 2927, Train Loss: 0.7696, Validation Loss: 1.1350, Validation F1: 0.9012\n",
      "Epoch 2928, Train Loss: 0.7678, Validation Loss: 1.0686, Validation F1: 0.9032\n",
      "Epoch 2929, Train Loss: 0.7670, Validation Loss: 1.1784, Validation F1: 0.9021\n",
      "Epoch 2930, Train Loss: 0.7673, Validation Loss: 1.1939, Validation F1: 0.9066\n",
      "Epoch 2931, Train Loss: 0.7706, Validation Loss: 1.1381, Validation F1: 0.9000\n",
      "Epoch 2932, Train Loss: 0.7678, Validation Loss: 1.1570, Validation F1: 0.8985\n",
      "Epoch 2933, Train Loss: 0.7746, Validation Loss: 1.1403, Validation F1: 0.9034\n",
      "Epoch 2934, Train Loss: 0.7707, Validation Loss: 1.1731, Validation F1: 0.9052\n",
      "Epoch 2935, Train Loss: 0.7686, Validation Loss: 1.1397, Validation F1: 0.9054\n",
      "Epoch 2936, Train Loss: 0.7704, Validation Loss: 1.1474, Validation F1: 0.9044\n",
      "Epoch 2937, Train Loss: 0.7705, Validation Loss: 1.1155, Validation F1: 0.9010\n",
      "Epoch 2938, Train Loss: 0.7699, Validation Loss: 1.1624, Validation F1: 0.9040\n",
      "Epoch 2939, Train Loss: 0.7708, Validation Loss: 1.1534, Validation F1: 0.9049\n",
      "Epoch 2940, Train Loss: 0.7711, Validation Loss: 1.1742, Validation F1: 0.9039\n",
      "Epoch 2941, Train Loss: 0.7689, Validation Loss: 1.1733, Validation F1: 0.9024\n",
      "Epoch 2942, Train Loss: 0.7700, Validation Loss: 1.1502, Validation F1: 0.8985\n",
      "Epoch 2943, Train Loss: 0.7701, Validation Loss: 1.1737, Validation F1: 0.8996\n",
      "Epoch 2944, Train Loss: 0.7699, Validation Loss: 1.1778, Validation F1: 0.9039\n",
      "Epoch 2945, Train Loss: 0.7674, Validation Loss: 1.1580, Validation F1: 0.9059\n",
      "Epoch 2946, Train Loss: 0.7699, Validation Loss: 1.1326, Validation F1: 0.9051\n",
      "Epoch 2947, Train Loss: 0.7694, Validation Loss: 1.1693, Validation F1: 0.9020\n",
      "Epoch 2948, Train Loss: 0.7690, Validation Loss: 1.1481, Validation F1: 0.8978\n",
      "Epoch 2949, Train Loss: 0.7688, Validation Loss: 1.1328, Validation F1: 0.8977\n",
      "Epoch 2950, Train Loss: 0.7691, Validation Loss: 1.1675, Validation F1: 0.9016\n",
      "Epoch 2951, Train Loss: 0.7682, Validation Loss: 1.1411, Validation F1: 0.9055\n",
      "Epoch 2952, Train Loss: 0.7690, Validation Loss: 1.1626, Validation F1: 0.9059\n",
      "Epoch 2953, Train Loss: 0.7681, Validation Loss: 1.1569, Validation F1: 0.9014\n",
      "Epoch 2954, Train Loss: 0.7674, Validation Loss: 1.1752, Validation F1: 0.8998\n",
      "Epoch 2955, Train Loss: 0.7694, Validation Loss: 1.1644, Validation F1: 0.8995\n",
      "Epoch 2956, Train Loss: 0.7691, Validation Loss: 1.1497, Validation F1: 0.8984\n",
      "Epoch 2957, Train Loss: 0.7679, Validation Loss: 1.1663, Validation F1: 0.9055\n",
      "Epoch 2958, Train Loss: 0.7699, Validation Loss: 1.1672, Validation F1: 0.9058\n",
      "Epoch 2959, Train Loss: 0.7678, Validation Loss: 1.1721, Validation F1: 0.9059\n",
      "Epoch 2960, Train Loss: 0.7687, Validation Loss: 1.1581, Validation F1: 0.9023\n",
      "Epoch 2961, Train Loss: 0.7692, Validation Loss: 1.0980, Validation F1: 0.8990\n",
      "Epoch 2962, Train Loss: 0.7712, Validation Loss: 1.1181, Validation F1: 0.9025\n",
      "Epoch 2963, Train Loss: 0.7691, Validation Loss: 1.1824, Validation F1: 0.9032\n",
      "Epoch 2964, Train Loss: 0.7674, Validation Loss: 1.1408, Validation F1: 0.9041\n",
      "Epoch 2965, Train Loss: 0.7699, Validation Loss: 1.1666, Validation F1: 0.9044\n",
      "Epoch 2966, Train Loss: 0.7675, Validation Loss: 1.1314, Validation F1: 0.8955\n",
      "Epoch 2967, Train Loss: 0.7687, Validation Loss: 1.1372, Validation F1: 0.8956\n",
      "Epoch 2968, Train Loss: 0.7685, Validation Loss: 1.1778, Validation F1: 0.9010\n",
      "Epoch 2969, Train Loss: 0.7671, Validation Loss: 1.1461, Validation F1: 0.9043\n",
      "Epoch 2970, Train Loss: 0.7707, Validation Loss: 1.1362, Validation F1: 0.9053\n",
      "Epoch 2971, Train Loss: 0.7670, Validation Loss: 1.1877, Validation F1: 0.9034\n",
      "Epoch 2972, Train Loss: 0.7690, Validation Loss: 1.1568, Validation F1: 0.8983\n",
      "Epoch 2973, Train Loss: 0.7682, Validation Loss: 1.1582, Validation F1: 0.9016\n",
      "Epoch 2974, Train Loss: 0.7669, Validation Loss: 1.2192, Validation F1: 0.9042\n",
      "Epoch 2975, Train Loss: 0.7676, Validation Loss: 1.1740, Validation F1: 0.9045\n",
      "Epoch 2976, Train Loss: 0.7674, Validation Loss: 1.1600, Validation F1: 0.9036\n",
      "Epoch 2977, Train Loss: 0.7678, Validation Loss: 1.1681, Validation F1: 0.9036\n",
      "Epoch 2978, Train Loss: 0.7701, Validation Loss: 1.2079, Validation F1: 0.8990\n",
      "Epoch 2979, Train Loss: 0.7675, Validation Loss: 1.1746, Validation F1: 0.8989\n",
      "Epoch 2980, Train Loss: 0.7685, Validation Loss: 1.1727, Validation F1: 0.9005\n",
      "Epoch 2981, Train Loss: 0.7691, Validation Loss: 1.1231, Validation F1: 0.9041\n",
      "Epoch 2982, Train Loss: 0.7674, Validation Loss: 1.1764, Validation F1: 0.9053\n",
      "Epoch 2983, Train Loss: 0.7703, Validation Loss: 1.1253, Validation F1: 0.8981\n",
      "Epoch 2984, Train Loss: 0.7689, Validation Loss: 1.1406, Validation F1: 0.8976\n",
      "Epoch 2985, Train Loss: 0.7687, Validation Loss: 1.1753, Validation F1: 0.9003\n",
      "Epoch 2986, Train Loss: 0.7678, Validation Loss: 1.1603, Validation F1: 0.9050\n",
      "Epoch 2987, Train Loss: 0.7684, Validation Loss: 1.1263, Validation F1: 0.9035\n",
      "Epoch 2988, Train Loss: 0.7687, Validation Loss: 1.1164, Validation F1: 0.9055\n",
      "Epoch 2989, Train Loss: 0.7687, Validation Loss: 1.1866, Validation F1: 0.9010\n",
      "Epoch 2990, Train Loss: 0.7703, Validation Loss: 1.1209, Validation F1: 0.9010\n",
      "Epoch 2991, Train Loss: 0.7685, Validation Loss: 1.1622, Validation F1: 0.9032\n",
      "Epoch 2992, Train Loss: 0.7699, Validation Loss: 1.1760, Validation F1: 0.9036\n",
      "Epoch 2993, Train Loss: 0.7690, Validation Loss: 1.1413, Validation F1: 0.9042\n",
      "Epoch 2994, Train Loss: 0.7692, Validation Loss: 1.1783, Validation F1: 0.9020\n",
      "Epoch 2995, Train Loss: 0.7708, Validation Loss: 1.1583, Validation F1: 0.9042\n",
      "Epoch 2996, Train Loss: 0.7668, Validation Loss: 1.1199, Validation F1: 0.9056\n",
      "Epoch 2997, Train Loss: 0.7700, Validation Loss: 1.1547, Validation F1: 0.9053\n",
      "Epoch 2998, Train Loss: 0.7688, Validation Loss: 1.1546, Validation F1: 0.9015\n",
      "Epoch 2999, Train Loss: 0.7688, Validation Loss: 1.1642, Validation F1: 0.9017\n",
      "Epoch 3000, Train Loss: 0.7679, Validation Loss: 1.1697, Validation F1: 0.9013\n",
      "Epoch 3001, Train Loss: 0.7672, Validation Loss: 1.1933, Validation F1: 0.9050\n",
      "Epoch 3002, Train Loss: 0.7677, Validation Loss: 1.1044, Validation F1: 0.9033\n",
      "Epoch 3003, Train Loss: 0.7675, Validation Loss: 1.1872, Validation F1: 0.9030\n",
      "Epoch 3004, Train Loss: 0.7675, Validation Loss: 1.1118, Validation F1: 0.9006\n",
      "Epoch 3005, Train Loss: 0.7663, Validation Loss: 1.1729, Validation F1: 0.9015\n",
      "Epoch 3006, Train Loss: 0.7669, Validation Loss: 1.1638, Validation F1: 0.9021\n",
      "Epoch 3007, Train Loss: 0.7676, Validation Loss: 1.1830, Validation F1: 0.9027\n",
      "Epoch 3008, Train Loss: 0.7684, Validation Loss: 1.1461, Validation F1: 0.9028\n",
      "Epoch 3009, Train Loss: 0.7676, Validation Loss: 1.1377, Validation F1: 0.9041\n",
      "Epoch 3010, Train Loss: 0.7678, Validation Loss: 1.1666, Validation F1: 0.9023\n",
      "Epoch 3011, Train Loss: 0.7671, Validation Loss: 1.1184, Validation F1: 0.9011\n",
      "Epoch 3012, Train Loss: 0.7674, Validation Loss: 1.1828, Validation F1: 0.9040\n",
      "Epoch 3013, Train Loss: 0.7665, Validation Loss: 1.1509, Validation F1: 0.9022\n",
      "Epoch 3014, Train Loss: 0.7677, Validation Loss: 1.1592, Validation F1: 0.9042\n",
      "Epoch 3015, Train Loss: 0.7675, Validation Loss: 1.1890, Validation F1: 0.9022\n",
      "Epoch 3016, Train Loss: 0.7676, Validation Loss: 1.0965, Validation F1: 0.9023\n",
      "Epoch 3017, Train Loss: 0.7660, Validation Loss: 1.1985, Validation F1: 0.9031\n",
      "Epoch 3018, Train Loss: 0.7669, Validation Loss: 1.1731, Validation F1: 0.9059\n",
      "Epoch 3019, Train Loss: 0.7667, Validation Loss: 1.2041, Validation F1: 0.9040\n",
      "Epoch 3020, Train Loss: 0.7678, Validation Loss: 1.1798, Validation F1: 0.9013\n",
      "Epoch 3021, Train Loss: 0.7661, Validation Loss: 1.1490, Validation F1: 0.9027\n",
      "Epoch 3022, Train Loss: 0.7661, Validation Loss: 1.1614, Validation F1: 0.9034\n",
      "Epoch 3023, Train Loss: 0.7654, Validation Loss: 1.1729, Validation F1: 0.9040\n",
      "Epoch 3024, Train Loss: 0.7657, Validation Loss: 1.1377, Validation F1: 0.9015\n",
      "Epoch 3025, Train Loss: 0.7675, Validation Loss: 1.1694, Validation F1: 0.8997\n",
      "Epoch 3026, Train Loss: 0.7662, Validation Loss: 1.1807, Validation F1: 0.8994\n",
      "Epoch 3027, Train Loss: 0.7666, Validation Loss: 1.1058, Validation F1: 0.8947\n",
      "Epoch 3028, Train Loss: 0.7673, Validation Loss: 1.1735, Validation F1: 0.9035\n",
      "Epoch 3029, Train Loss: 0.7672, Validation Loss: 1.1368, Validation F1: 0.9035\n",
      "Epoch 3030, Train Loss: 0.7669, Validation Loss: 1.1675, Validation F1: 0.9047\n",
      "Epoch 3031, Train Loss: 0.7662, Validation Loss: 1.1897, Validation F1: 0.9039\n",
      "Epoch 3032, Train Loss: 0.7663, Validation Loss: 1.1832, Validation F1: 0.9047\n",
      "Epoch 3033, Train Loss: 0.7667, Validation Loss: 1.1276, Validation F1: 0.8953\n",
      "Epoch 3034, Train Loss: 0.7676, Validation Loss: 1.1858, Validation F1: 0.9002\n",
      "Epoch 3035, Train Loss: 0.7704, Validation Loss: 1.1303, Validation F1: 0.9020\n",
      "Epoch 3036, Train Loss: 0.7689, Validation Loss: 1.1885, Validation F1: 0.9045\n",
      "Epoch 3037, Train Loss: 0.7687, Validation Loss: 1.1990, Validation F1: 0.9047\n",
      "Epoch 3038, Train Loss: 0.7695, Validation Loss: 1.1689, Validation F1: 0.9046\n",
      "Epoch 3039, Train Loss: 0.7670, Validation Loss: 1.1664, Validation F1: 0.8977\n",
      "Epoch 3040, Train Loss: 0.7690, Validation Loss: 1.1266, Validation F1: 0.9018\n",
      "Epoch 3041, Train Loss: 0.7677, Validation Loss: 1.1771, Validation F1: 0.9054\n",
      "Epoch 3042, Train Loss: 0.7674, Validation Loss: 1.1713, Validation F1: 0.9040\n",
      "Epoch 3043, Train Loss: 0.7687, Validation Loss: 1.2049, Validation F1: 0.9020\n",
      "Epoch 3044, Train Loss: 0.7671, Validation Loss: 1.1521, Validation F1: 0.9013\n",
      "Epoch 3045, Train Loss: 0.7693, Validation Loss: 1.1676, Validation F1: 0.9003\n",
      "Epoch 3046, Train Loss: 0.7674, Validation Loss: 1.1802, Validation F1: 0.9006\n",
      "Epoch 3047, Train Loss: 0.7683, Validation Loss: 1.1947, Validation F1: 0.9026\n",
      "Epoch 3048, Train Loss: 0.7674, Validation Loss: 1.1954, Validation F1: 0.9035\n",
      "Epoch 3049, Train Loss: 0.7709, Validation Loss: 1.1854, Validation F1: 0.8986\n",
      "Epoch 3050, Train Loss: 0.7712, Validation Loss: 1.1212, Validation F1: 0.8995\n",
      "Epoch 3051, Train Loss: 0.7711, Validation Loss: 1.1585, Validation F1: 0.9055\n",
      "Epoch 3052, Train Loss: 0.7699, Validation Loss: 1.1370, Validation F1: 0.9062\n",
      "Epoch 3053, Train Loss: 0.7683, Validation Loss: 1.2019, Validation F1: 0.9046\n",
      "Epoch 3054, Train Loss: 0.7696, Validation Loss: 1.1586, Validation F1: 0.9005\n",
      "Epoch 3055, Train Loss: 0.7676, Validation Loss: 1.1433, Validation F1: 0.9030\n",
      "Epoch 3056, Train Loss: 0.7698, Validation Loss: 1.1064, Validation F1: 0.9020\n",
      "Epoch 3057, Train Loss: 0.7671, Validation Loss: 1.1388, Validation F1: 0.9039\n",
      "Epoch 3058, Train Loss: 0.7682, Validation Loss: 1.1709, Validation F1: 0.9032\n",
      "Epoch 3059, Train Loss: 0.7671, Validation Loss: 1.1692, Validation F1: 0.9014\n",
      "Epoch 3060, Train Loss: 0.7683, Validation Loss: 1.1658, Validation F1: 0.8986\n",
      "Epoch 3061, Train Loss: 0.7676, Validation Loss: 1.1762, Validation F1: 0.9012\n",
      "Epoch 3062, Train Loss: 0.7688, Validation Loss: 1.1513, Validation F1: 0.9029\n",
      "Epoch 3063, Train Loss: 0.7674, Validation Loss: 1.1287, Validation F1: 0.9025\n",
      "Epoch 3064, Train Loss: 0.7669, Validation Loss: 1.1631, Validation F1: 0.9035\n",
      "Epoch 3065, Train Loss: 0.7672, Validation Loss: 1.1502, Validation F1: 0.9037\n",
      "Epoch 3066, Train Loss: 0.7680, Validation Loss: 1.1645, Validation F1: 0.9040\n",
      "Epoch 3067, Train Loss: 0.7677, Validation Loss: 1.1882, Validation F1: 0.9007\n",
      "Epoch 3068, Train Loss: 0.7663, Validation Loss: 1.2130, Validation F1: 0.9021\n",
      "Epoch 3069, Train Loss: 0.7673, Validation Loss: 1.1858, Validation F1: 0.9022\n",
      "Epoch 3070, Train Loss: 0.7665, Validation Loss: 1.1457, Validation F1: 0.9027\n",
      "Epoch 3071, Train Loss: 0.7681, Validation Loss: 1.2087, Validation F1: 0.9066\n",
      "Epoch 3072, Train Loss: 0.7676, Validation Loss: 1.1592, Validation F1: 0.9053\n",
      "Epoch 3073, Train Loss: 0.7673, Validation Loss: 1.1626, Validation F1: 0.9039\n",
      "Epoch 3074, Train Loss: 0.7672, Validation Loss: 1.1933, Validation F1: 0.9009\n",
      "Epoch 3075, Train Loss: 0.7668, Validation Loss: 1.1631, Validation F1: 0.9018\n",
      "Epoch 3076, Train Loss: 0.7668, Validation Loss: 1.1860, Validation F1: 0.9065\n",
      "Epoch 3077, Train Loss: 0.7673, Validation Loss: 1.1728, Validation F1: 0.9043\n",
      "Epoch 3078, Train Loss: 0.7666, Validation Loss: 1.1923, Validation F1: 0.9034\n",
      "Epoch 3079, Train Loss: 0.7674, Validation Loss: 1.1858, Validation F1: 0.9010\n",
      "Epoch 3080, Train Loss: 0.7667, Validation Loss: 1.1980, Validation F1: 0.9021\n",
      "Epoch 3081, Train Loss: 0.7663, Validation Loss: 1.1633, Validation F1: 0.9030\n",
      "Epoch 3082, Train Loss: 0.7665, Validation Loss: 1.1863, Validation F1: 0.9042\n",
      "Epoch 3083, Train Loss: 0.7666, Validation Loss: 1.1366, Validation F1: 0.9011\n",
      "Epoch 3084, Train Loss: 0.7665, Validation Loss: 1.1617, Validation F1: 0.9015\n",
      "Epoch 3085, Train Loss: 0.7666, Validation Loss: 1.1600, Validation F1: 0.9008\n",
      "Epoch 3086, Train Loss: 0.7661, Validation Loss: 1.1572, Validation F1: 0.9031\n",
      "Epoch 3087, Train Loss: 0.7672, Validation Loss: 1.1407, Validation F1: 0.9027\n",
      "Epoch 3088, Train Loss: 0.7688, Validation Loss: 1.1558, Validation F1: 0.9039\n",
      "Epoch 3089, Train Loss: 0.7676, Validation Loss: 1.1386, Validation F1: 0.9009\n",
      "Epoch 3090, Train Loss: 0.7661, Validation Loss: 1.1784, Validation F1: 0.9003\n",
      "Epoch 3091, Train Loss: 0.7663, Validation Loss: 1.1894, Validation F1: 0.9019\n",
      "Epoch 3092, Train Loss: 0.7662, Validation Loss: 1.1871, Validation F1: 0.9041\n",
      "Epoch 3093, Train Loss: 0.7658, Validation Loss: 1.1828, Validation F1: 0.9021\n",
      "Epoch 3094, Train Loss: 0.7664, Validation Loss: 1.1683, Validation F1: 0.9053\n",
      "Epoch 3095, Train Loss: 0.7676, Validation Loss: 1.1714, Validation F1: 0.9058\n",
      "Epoch 3096, Train Loss: 0.7670, Validation Loss: 1.1556, Validation F1: 0.9019\n",
      "Epoch 3097, Train Loss: 0.7671, Validation Loss: 1.1531, Validation F1: 0.9016\n",
      "Epoch 3098, Train Loss: 0.7670, Validation Loss: 1.1816, Validation F1: 0.9018\n",
      "Epoch 3099, Train Loss: 0.7665, Validation Loss: 1.2036, Validation F1: 0.9021\n",
      "Epoch 3100, Train Loss: 0.7655, Validation Loss: 1.1859, Validation F1: 0.9021\n",
      "Epoch 3101, Train Loss: 0.7677, Validation Loss: 1.1989, Validation F1: 0.9073\n",
      "Epoch 3102, Train Loss: 0.7663, Validation Loss: 1.1687, Validation F1: 0.9057\n",
      "Epoch 3103, Train Loss: 0.7654, Validation Loss: 1.1906, Validation F1: 0.9051\n",
      "Epoch 3104, Train Loss: 0.7652, Validation Loss: 1.1430, Validation F1: 0.9019\n",
      "Epoch 3105, Train Loss: 0.7663, Validation Loss: 1.1665, Validation F1: 0.9011\n",
      "Epoch 3106, Train Loss: 0.7659, Validation Loss: 1.1793, Validation F1: 0.9055\n",
      "Epoch 3107, Train Loss: 0.7659, Validation Loss: 1.1144, Validation F1: 0.9055\n",
      "Epoch 3108, Train Loss: 0.7663, Validation Loss: 1.1685, Validation F1: 0.9035\n",
      "Epoch 3109, Train Loss: 0.7660, Validation Loss: 1.1472, Validation F1: 0.8998\n",
      "Epoch 3110, Train Loss: 0.7659, Validation Loss: 1.2106, Validation F1: 0.9006\n",
      "Epoch 3111, Train Loss: 0.7658, Validation Loss: 1.1795, Validation F1: 0.9012\n",
      "Epoch 3112, Train Loss: 0.7656, Validation Loss: 1.2277, Validation F1: 0.9013\n",
      "Epoch 3113, Train Loss: 0.7668, Validation Loss: 1.1738, Validation F1: 0.9037\n",
      "Epoch 3114, Train Loss: 0.7663, Validation Loss: 1.2072, Validation F1: 0.9053\n",
      "Epoch 3115, Train Loss: 0.7659, Validation Loss: 1.1557, Validation F1: 0.9055\n",
      "Epoch 3116, Train Loss: 0.7663, Validation Loss: 1.1842, Validation F1: 0.9035\n",
      "Epoch 3117, Train Loss: 0.7655, Validation Loss: 1.2020, Validation F1: 0.9034\n",
      "Epoch 3118, Train Loss: 0.7664, Validation Loss: 1.1399, Validation F1: 0.9044\n",
      "Epoch 3119, Train Loss: 0.7656, Validation Loss: 1.2068, Validation F1: 0.9032\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     98\u001b[39m val_f1_macro = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=\u001b[32m1\u001b[39m).cpu(), average=\u001b[33m'\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m val_f1 > best_f1:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# Save checkpoint\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[43mth\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_state_dict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moptimizer_state_dict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_f1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_f1\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(train_loss_history_path, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    109\u001b[39m         pickle.dump(train_loss_history, f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/torch/serialization.py:964\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    961\u001b[39m     f = os.fspath(f)\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m    970\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/torch/serialization.py:798\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__exit__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m798\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    800\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_stream.close()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Extract the best parameters from the grid search\n",
    "best_hidden_dim = 256  # Replace with the best hidden_dim found\n",
    "best_learning_rate = 0.001  # Replace with the best learning_rate found\n",
    "best_dropout = 0.3  # Replace with the best dropout found\n",
    "\n",
    "# Initialize the model with the best parameters\n",
    "model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                   edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                   hidden_channels=best_hidden_dim,\n",
    "                   dropout=best_dropout,\n",
    "                   out_channels=num_classes).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Compute class weights for the training dataset\n",
    "labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.unique(labels),\n",
    "                                                  y=labels)\n",
    "\n",
    "# Normalize class weights\n",
    "class_weights = th.FloatTensor(class_weights).to(device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# Move the graph data to the device\n",
    "G_pyg_train = G_pyg_train.to(device)\n",
    "G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "# ===== Load checkpoint if exists =====\n",
    "best_f1 = 0\n",
    "start_epoch = 0\n",
    "epochs = 5000\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = th.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "    print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_f1_history = []\n",
    "saved_model_epochs = []\n",
    "\n",
    "train_loss_history_path = os.path.join(saves_path, 'train_loss_history.pkl')\n",
    "val_loss_history_path = os.path.join(saves_path, 'val_loss_history.pkl')\n",
    "val_f1_history_path = os.path.join(saves_path, 'val_f1_history.pkl')\n",
    "saved_model_epochs_path = os.path.join(saves_path, 'saved_model_epochs.pkl')\n",
    "\n",
    "if os.path.exists(train_loss_history_path) and os.path.exists(val_loss_history_path) and os.path.exists(val_f1_history_path) and os.path.exists(saved_model_epochs_path):\n",
    "    with open(train_loss_history_path, 'rb') as f:\n",
    "        train_loss_history = pickle.load(f)\n",
    "    with open(val_loss_history_path, 'rb') as f:\n",
    "        val_loss_history = pickle.load(f)\n",
    "    with open(val_f1_history_path, 'rb') as f:\n",
    "        val_f1_history = pickle.load(f)\n",
    "    with open(saved_model_epochs_path, 'rb') as f:\n",
    "        saved_model_epochs = pickle.load(f)\n",
    "\n",
    "# ===== Start Training =====\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    out = model(G_pyg_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(out, G_pyg_train.edge_label)\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        out = model(G_pyg_val)\n",
    "        loss = criterion(out, G_pyg_val.edge_label)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "    val_f1_micro = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='micro')\n",
    "    val_f1_macro = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='macro')\n",
    "\n",
    "    if epoch % 10 == 0 or val_f1 > best_f1:\n",
    "        # Save checkpoint\n",
    "        th.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_f1': best_f1\n",
    "        }, checkpoint_path)\n",
    "        with open(train_loss_history_path, 'wb') as f:\n",
    "            pickle.dump(train_loss_history, f)\n",
    "        with open(val_loss_history_path, 'wb') as f:\n",
    "            pickle.dump(val_loss_history, f)\n",
    "        with open(val_f1_history_path, 'wb') as f:\n",
    "            pickle.dump(val_f1_history, f)\n",
    "        with open(saved_model_epochs_path, 'wb') as f:\n",
    "            pickle.dump(saved_model_epochs, f)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1  # Update the best F1 score for this fold\n",
    "        best_model_state = model.state_dict()\n",
    "        th.save(best_model_state, best_model_path)\n",
    "        print(f\"Epoch {epoch} Saved best model. Best F1:\", best_f1)\n",
    "        saved_model_epochs.append(epoch)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_f1_history.append((val_f1, val_f1_micro, val_f1_macro))\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Model training completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4b2f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_process(train_losses, val_losses, val_f1, saved_model_epochs):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "    # Plot Train Loss\n",
    "    axs[0].plot(train_losses, label='Train Loss', color='blue')\n",
    "    axs[0].plot(val_losses, label='Validation Loss', color='red')\n",
    "    axs[0].set_ylabel('Train Loss')\n",
    "    axs[0].set_title('Training Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid()\n",
    "\n",
    "    val_f1_weighted_history = []\n",
    "    val_f1_micro_history = []\n",
    "    val_f1_macro_history = []\n",
    "\n",
    "    for val_f1_weighted, val_f1_micro, val_f1_macro in val_f1:\n",
    "        val_f1_weighted_history.append(val_f1_weighted)\n",
    "        val_f1_micro_history.append(val_f1_micro)\n",
    "        val_f1_macro_history.append(val_f1_macro)\n",
    "    \n",
    "    # Plot Validation F1\n",
    "    axs[1].plot(val_f1_weighted_history, label='Validation F1 Weighted', color='green')\n",
    "    axs[1].plot(val_f1_micro_history, label='Validation F1 Micro', color='blue')\n",
    "    axs[1].plot(val_f1_macro_history, label='Validation F1 Macro', color='red')\n",
    "    average_val_f1 = np.mean(val_f1)\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Validation F1')\n",
    "    axs[1].set_title('Validation F1 Score')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid()\n",
    "\n",
    "    print(len(train_losses))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df26e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3119\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_process(train_loss_history, val_loss_history, val_f1_history, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8368fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 9773\n",
      "Shape of node in G_pyg: torch.Size([9773, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f271a-612b-4cd6-a85a-e4236dec9d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_combined_ports_final/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[   49    62   244    23     0     0     0     1     0    23]\n",
      " [    5   102   217     6     4     0     0     2     8     5]\n",
      " [   27   160  1776   208    40     5     0     9    52   176]\n",
      " [  161   377  2258  2433   170    12     3    68   127  1070]\n",
      " [   47   166   259   220  2548     6    42    25   150   174]\n",
      " [   27    29   274   252    23 31472     2    11    67   165]\n",
      " [   21     5     4    63   311     0 32824    14    18    21]\n",
      " [   30    30   278   104    25     1     0  1412    18   200]\n",
      " [    3     2     1     9     5     0     0     4   201     2]\n",
      " [    4     0     0     5     0     0     0     1     0    16]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.1310    0.1219    0.1263       402\n",
      "     Backdoors     0.1093    0.2923    0.1591       349\n",
      "           DoS     0.3344    0.7240    0.4575      2453\n",
      "      Exploits     0.7322    0.3643    0.4865      6679\n",
      "       Fuzzers     0.8151    0.7006    0.7535      3637\n",
      "       Generic     0.9992    0.9737    0.9863     32322\n",
      "        Normal     0.9986    0.9863    0.9924     33281\n",
      "Reconnaissance     0.9127    0.6730    0.7748      2098\n",
      "     Shellcode     0.3136    0.8855    0.4631       227\n",
      "         Worms     0.0086    0.6154    0.0170        26\n",
      "\n",
      "      accuracy                         0.8939     81474\n",
      "     macro avg     0.5355    0.6337    0.5217     81474\n",
      "  weighted avg     0.9363    0.8939    0.9065     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def eval(G_pyg_test, adversarial=False):\n",
    "\n",
    "    G_pyg_test = G_pyg_test.to(device)\n",
    "    G_pyg_test.edge_label = G_pyg_test.edge_label.to(device)\n",
    "    G_pyg_test.edge_attr = G_pyg_test.edge_attr.to(device)\n",
    "\n",
    "    best_model = EGraphSAGE(node_in_channels=G_pyg_test.num_node_features, \n",
    "                       edge_in_channels=G_pyg_test.num_edge_features,\n",
    "                       hidden_channels=best_hidden_dim, \n",
    "                       out_channels=num_classes).to(device)\n",
    "\n",
    "    print(\"Loading model from\", best_model_path)\n",
    "    best_model.load_state_dict(th.load(best_model_path))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    best_model.eval()\n",
    "\n",
    "    print(\"inference start\")\n",
    "    with th.no_grad():\n",
    "            \n",
    "        try:\n",
    "            out = best_model(G_pyg_test)\n",
    "            \n",
    "        except Exception as forward_error:\n",
    "            print(f\"Error during forward/backward pass at {forward_error}\")\n",
    "\n",
    "    print(\"inference done\")\n",
    "\n",
    "    # test_accuracy = compute_accuracy(out, G_pyg_test.edge_label)\n",
    "    # print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    \n",
    "    pred_labels = out.argmax(dim=1).cpu()\n",
    "    all_test_labels = G_pyg_test.edge_label.cpu()\n",
    "\n",
    "    if adversarial:\n",
    "\n",
    "        # Create a boolean mask where the label is NOT equal to the adversarial class\n",
    "        adversarial_mask = all_test_labels == ADVERSARIAL_CLASS_LABEL\n",
    "\n",
    "        # Print the class that the adversarial samples are classified as\n",
    "        cm_adversarial = confusion_matrix(all_test_labels[adversarial_mask], pred_labels[adversarial_mask], labels=range(len(class_map) + 1))\n",
    "        print(\"Adversarial confusion matrix:\", cm_adversarial)\n",
    "\n",
    "        # Apply the mask to both labels and predictions\n",
    "        all_test_labels = all_test_labels[~adversarial_mask]\n",
    "        pred_labels = pred_labels[~adversarial_mask]\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"class_map\", class_map)\n",
    "    # Generate a report\n",
    "    cm = confusion_matrix(all_test_labels, pred_labels, labels=range(len(class_map)))\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    report = classification_report(all_test_labels, pred_labels, target_names=class_map, digits=4)\n",
    "    print(report)\n",
    "\n",
    "    \n",
    "eval(G_pyg_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def confusion_matrix_plot(cm, all_test_labels, pred_labels):\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_map, yticklabels=class_map)\n",
    "#     plt.xlabel(\"Predicted\")\n",
    "#     plt.ylabel(\"True\")\n",
    "#     plt.title(\"Confusion Matrix\")\n",
    "\n",
    "#     # Compute metrics\n",
    "#     accuracy = accuracy_score(all_test_labels, pred_labels)\n",
    "#     precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_test_labels, pred_labels, average='macro', zero_division=0)\n",
    "#     precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(all_test_labels, pred_labels, average='weighted', zero_division=0)\n",
    "\n",
    "\n",
    "#     metrics_text = (\n",
    "#         f\"Accuracy: {accuracy:.4f}\\n\"\n",
    "#         f\"Macro Precision: {precision_macro:.4f}\\n\"\n",
    "#         f\"Macro Recall: {recall_macro:.4f}\\n\"\n",
    "#         f\"Macro F1: {f1_macro:.4f}\\n\"\n",
    "#         f\"Weighted Precision: {precision_weighted:.4f}\\n\"\n",
    "#         f\"Weighted Recall: {recall_weighted:.4f}\\n\"\n",
    "#         f\"Weighted F1: {f1_weighted:.4f}\"\n",
    "#     )\n",
    "\n",
    "#     # Position: bottom left corner of plot area\n",
    "#     plt.gcf().text(0.02, 0.02, metrics_text, fontsize=12, va='bottom', ha='left',\n",
    "#                 bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_traffic_to_attacker(graph, ratio=0.1, num_injected_nodes=1, is_attack=False):\n",
    "    edge_index = graph.edge_index.clone()\n",
    "    edge_attr = graph.edge_attr.clone()\n",
    "    edge_label = graph.edge_label.clone()\n",
    "    x = graph.x.clone()\n",
    "\n",
    "    num_edges = edge_index.size(1)\n",
    "    feature_dim = graph.x.size(1)\n",
    "\n",
    "    # 1. Identify attacker nodes\n",
    "    attacker_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "    attacker_nodes = th.unique(edge_index[:, attacker_edges])\n",
    "    if attacker_nodes.numel() == 0:\n",
    "        raise ValueError(\"No attacker nodes found.\")\n",
    "\n",
    "    # 2. Sample benign edge feature pool\n",
    "    if is_attack:\n",
    "        attack_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[attack_edges]\n",
    "    else:\n",
    "        benign_edges = (edge_label == BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[benign_edges]\n",
    "\n",
    "    # 3. Inject new nodes\n",
    "    original_num_nodes = x.size(0)\n",
    "\n",
    "    new_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "    x = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "    # 4. Inject edges from injected nodes to attacker nodes\n",
    "    num_to_inject = max(1, int(ratio * num_edges))\n",
    "    new_edges = []\n",
    "    new_attrs = []\n",
    "    new_labels = []\n",
    "\n",
    "    \n",
    "    for _ in range(num_to_inject):\n",
    "        src = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\n",
    "        dst = attacker_nodes[random.randint(0, len(attacker_nodes) - 1)].item()\n",
    "\n",
    "        new_edges.append([src, dst])\n",
    "        attr = inject_edge_attr_pool[random.randint(0, len(inject_edge_attr_pool) - 1)]\n",
    "        new_attrs.append(attr)\n",
    "        new_labels.append(ADVERSARIAL_CLASS_LABEL)\n",
    "\n",
    "    # Create a new empty graph to store the injected edges\n",
    "    new_graph = Data()\n",
    "\n",
    "    # 5. Merge into graph\n",
    "    if new_edges:\n",
    "        new_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "        new_attrs = th.stack(new_attrs)\n",
    "        new_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "        new_graph.edge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "        new_graph.edge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "        new_graph.edge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "        new_graph.x = x\n",
    "\n",
    "        # new_graph.first_injected_node_idx = original_num_nodes # Store injected node indices\n",
    "\n",
    "    return new_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c5bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_combined_ports_final/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0   56  771  995 1251 2314 1085 1546  129    0    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[    0    59   203     2     0     0   138     0     0     0]\n",
      " [    0    60   143     2     1     4   124    15     0     0]\n",
      " [    0    73  1399    31     9     9   905    26     1     0]\n",
      " [    0   168  2754   356    58    58  3080   202     3     0]\n",
      " [    0   179   893    44   535    19  1961     6     0     0]\n",
      " [    0     4 13606  1415   332  4993 11939    33     0     0]\n",
      " [    0     7   126    33    73     3 33039     0     0     0]\n",
      " [    0     5  1045     5     6     3  1029     5     0     0]\n",
      " [    0     5    24    23     5    21    87    62     0     0]\n",
      " [    0     0    14     0     0     0    12     0     0     0]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0000    0.0000    0.0000       402\n",
      "     Backdoors     0.1071    0.1719    0.1320       349\n",
      "           DoS     0.0692    0.5703    0.1235      2453\n",
      "      Exploits     0.1863    0.0533    0.0829      6679\n",
      "       Fuzzers     0.5250    0.1471    0.2298      3637\n",
      "       Generic     0.9771    0.1545    0.2668     32322\n",
      "        Normal     0.6316    0.9927    0.7720     33281\n",
      "Reconnaissance     0.0143    0.0024    0.0041      2098\n",
      "     Shellcode     0.0000    0.0000    0.0000       227\n",
      "         Worms     0.0000    0.0000    0.0000        26\n",
      "\n",
      "      accuracy                         0.4957     81474\n",
      "     macro avg     0.2511    0.2092    0.1611     81474\n",
      "  weighted avg     0.6872    0.4957    0.4426     81474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Inject Attack Traffic to Attacker Nodes\n",
    "G_pyg_test = G_pyg_test.cpu()\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=True)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c37b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_combined_ports_final/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  13   13  159  457  654    4 6787   19   21   20    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[    0    57    16     1   158     0   170     0     0     0]\n",
      " [    2    66     4     1   175     0   100     0     1     0]\n",
      " [   13   113    81    38  1260     0   915     4     0    29]\n",
      " [  106   236   377   434  1865     5  3472     6     1   177]\n",
      " [   16   195    66    26  1054     6  2240     0     0    34]\n",
      " [   18    18  7618    41   527   326   880 22866     1    27]\n",
      " [    0     3     7     5   121     0 33143     0     1     1]\n",
      " [   20    15    36    11   565     0  1413     1     0    37]\n",
      " [    0     0     4     4     4     0   214     0     1     0]\n",
      " [    0     0     3     0     4     0    16     0     0     3]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0000    0.0000    0.0000       402\n",
      "     Backdoors     0.0939    0.1891    0.1255       349\n",
      "           DoS     0.0099    0.0330    0.0152      2453\n",
      "      Exploits     0.7736    0.0650    0.1199      6679\n",
      "       Fuzzers     0.1838    0.2898    0.2250      3637\n",
      "       Generic     0.9674    0.0101    0.0200     32322\n",
      "        Normal     0.7787    0.9959    0.8740     33281\n",
      "Reconnaissance     0.0000    0.0005    0.0001      2098\n",
      "     Shellcode     0.2000    0.0044    0.0086       227\n",
      "         Worms     0.0097    0.1154    0.0180        26\n",
      "\n",
      "      accuracy                         0.4309     81474\n",
      "     macro avg     0.3017    0.1703    0.1406     81474\n",
      "  weighted avg     0.7747    0.4309    0.3858     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inject BENIGN Traffic to Attacker Nodes\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=False)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_random_nodes(graph, ratio=0.1, num_injected_nodes=1):\n",
    "\tedge_index = graph.edge_index.clone()\n",
    "\tedge_attr = graph.edge_attr.clone()\n",
    "\tedge_label = graph.edge_label.clone()\n",
    "\tx = graph.x.clone()\n",
    "\n",
    "\tnum_edges = edge_index.size(1)\n",
    "\tfeature_dim = graph.x.size(1)\n",
    "\n",
    "\t# 1. Inject new nodes\n",
    "\toriginal_num_nodes = x.size(0)\n",
    "\tnew_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "\tx = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "\t# 2. Inject random edges\n",
    "\tnum_to_inject = max(1, int(ratio * num_edges))\n",
    "\tnew_edges = []\n",
    "\tnew_attrs = []\n",
    "\tnew_labels = []\n",
    "\n",
    "\tfor _ in range(num_to_inject):\n",
    "\t\tsrc = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\t\tdst = random.randint(0, original_num_nodes - 1)  # to existing nodes\n",
    "\n",
    "\t\tnew_edges.append([src, dst])\n",
    "\t\tattr = edge_attr[random.randint(0, len(edge_attr) - 1)]  # Randomly sample edge attributes\n",
    "\t\tnew_attrs.append(attr)\n",
    "\t\tnew_labels.append(ADVERSARIAL_CLASS_LABEL)  # Assign benign class label to new edges\n",
    "\n",
    "\t# 3. Merge into graph\n",
    "\tif new_edges:\n",
    "\t\tnew_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "\t\tnew_attrs = th.stack(new_attrs)\n",
    "\t\tnew_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "\t\tedge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "\t\tedge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "\t\tedge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "\n",
    "\t# Create a new graph with the injected nodes and edges\n",
    "\tnew_graph = Data(\n",
    "\t\tedge_index=edge_index,\n",
    "\t\tedge_attr=edge_attr,\n",
    "\t\tedge_label=edge_label,\n",
    "\t\tx=x\n",
    "\t)\n",
    "\n",
    "\treturn new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb63898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_combined_ports_final/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   3   38  474  925  234  565 5726  112   70    0    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[    0    59   279    24    37     0     3     0     0     0]\n",
      " [    0    75   185    14    54     4    15     1     1     0]\n",
      " [    0   103  1747   182   298     9    95     9     8     2]\n",
      " [    3   258  3397  1738   580    40   555    75    30     3]\n",
      " [    0   180   763   480  1482    16   654     9    51     2]\n",
      " [    0    11 14264  1551    68  4799    59 11562     5     3]\n",
      " [    0     3   216   287   157    39 32547    21    11     0]\n",
      " [    1    17  1432    72    89     4   300   180     2     1]\n",
      " [    0     4    64    26     6    15    45    12    55     0]\n",
      " [    0     0    19     6     0     0     1     0     0     0]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0000    0.0000    0.0000       402\n",
      "     Backdoors     0.1056    0.2149    0.1416       349\n",
      "           DoS     0.0781    0.7122    0.1408      2453\n",
      "      Exploits     0.3968    0.2602    0.3143      6679\n",
      "       Fuzzers     0.5348    0.4075    0.4625      3637\n",
      "       Generic     0.9742    0.1485    0.2577     32322\n",
      "        Normal     0.9496    0.9779    0.9636     33281\n",
      "Reconnaissance     0.0152    0.0858    0.0258      2098\n",
      "     Shellcode     0.3374    0.2423    0.2821       227\n",
      "         Worms     0.0000    0.0000    0.0000        26\n",
      "\n",
      "      accuracy                         0.5231     81474\n",
      "     macro avg     0.3392    0.3049    0.2588     81474\n",
      "  weighted avg     0.8349    0.5231    0.5485     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inject Random Nodes in the graph\n",
    "injected_graph = inject_random_nodes(G_pyg_test, 0.1, num_injected_nodes=1)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
