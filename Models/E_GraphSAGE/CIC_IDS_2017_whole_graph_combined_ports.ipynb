{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb9e1b2-1c5f-49e3-82b4-3bfe52cabad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "=====Experiment=====\n",
    "Dataset: CIC_IDS_2017 dataset\n",
    "\n",
    "Training with whole graph\n",
    "Downsampled 90% normal traffic randomly\n",
    "Split train and test randomly\n",
    "\n",
    "Combined IP and Port features\n",
    "'''\n",
    "\n",
    "from torch_geometric.utils import from_networkx, add_self_loops, degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "# import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Datasets.CIC_IDS_2017.CIC_IDS_2017_config import CIC_IDS_2017_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ef09a-d405-43b8-971e-fe9e6a592c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = \"all_raw\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"CIC_IDS_2017/All/{csv_file_name}.csv\"))\n",
    "\n",
    "DATASET_NAME = \"CIC_IDS_2017\"\n",
    "\n",
    "SOURCE_IP_COL_NAME = CIC_IDS_2017_Config.SOURCE_IP_COL_NAME\n",
    "DESTINATION_IP_COL_NAME = CIC_IDS_2017_Config.DESTINATION_IP_COL_NAME\n",
    "SOURCE_PORT_COL_NAME = CIC_IDS_2017_Config.SOURCE_PORT_COL_NAME\n",
    "DESTINATION_PORT_COL_NAME = CIC_IDS_2017_Config.DESTINATION_PORT_COL_NAME\n",
    "\n",
    "ATTACK_CLASS_COL_NAME = CIC_IDS_2017_Config.ATTACK_CLASS_COL_NAME\n",
    "\n",
    "BENIGN_CLASS_NAME = CIC_IDS_2017_Config.BENIGN_CLASS_NAME\n",
    "\n",
    "TIME_COLS = CIC_IDS_2017_Config.TIME_COL_NAMES\n",
    "\n",
    "MULTICLASS = True\n",
    "label_col = ATTACK_CLASS_COL_NAME \n",
    "\n",
    "print(data[ATTACK_CLASS_COL_NAME].value_counts())\n",
    "\n",
    "checkpoint_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, f\"whole_graph_combined_ports/checkpoints_{csv_file_name}.pth\")\n",
    "best_model_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, f\"whole_graph_combined_ports/best_model_{csv_file_name}.pth\")\n",
    "\n",
    "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(best_model_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a1af1-1d3d-4179-9628-7c2ec551ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=CIC_IDS_2017_Config.DROP_COLS,inplace=True)\n",
    "data.drop(columns=CIC_IDS_2017_Config.TIME_COL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c690c-86a4-49f7-aa9c-58f94529547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME].apply(str)\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME].apply(str)\n",
    "\n",
    "# # Combine Port and IP\n",
    "data[SOURCE_PORT_COL_NAME] = data[SOURCE_PORT_COL_NAME].apply(str)\n",
    "data[DESTINATION_PORT_COL_NAME] = data[DESTINATION_PORT_COL_NAME].apply(str)\n",
    "\n",
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME] + ':' + data[SOURCE_PORT_COL_NAME]\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME] + ':' + data[DESTINATION_PORT_COL_NAME]\n",
    "data.drop(columns=[SOURCE_PORT_COL_NAME,DESTINATION_PORT_COL_NAME],inplace=True)\n",
    "\n",
    "# data[SOURCE_PORT_COL_NAME] = pd.to_numeric(data[SOURCE_PORT_COL_NAME], errors='coerce').fillna(0).astype(int)\n",
    "# data[DESTINATION_PORT_COL_NAME] = pd.to_numeric(data[DESTINATION_PORT_COL_NAME], errors='coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651ef5b-0a9d-4641-aad7-5738092c46ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7fb458-ca34-42ca-a8af-f8e1609aff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns = CIC_IDS_2017_Config.CATEGORICAL_COLS) # One Hot Encoding for categorical data\n",
    "converted_categorical_cols = [col for col in data.columns if col.startswith(tuple(CIC_IDS_2017_Config.CATEGORICAL_COLS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d96115-31f9-48cb-b3e6-7853d2d253cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "data.drop(columns=['index'],inplace=True)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df5c4c-70a2-4566-ae5e-ee3dcc6037a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_norm = CIC_IDS_2017_Config.COLS_TO_NORM\n",
    "print(data[cols_to_norm].describe()) # Check if there's any too large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea95177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numeric_issues(df, cols_to_norm):\n",
    "    for col in cols_to_norm:\n",
    "        try:\n",
    "            # Try to coerce to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Try to clip the column\n",
    "            df[col] = df[col].clip(lower=-1e9, upper=1e9)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Column '{col}' failed with error: {e}\")\n",
    "            print(f\"  - Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "            print(f\"  - Data type: {df[col].dtype}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n✅ All other columns processed successfully.\")\n",
    "\n",
    "check_numeric_issues(data, CIC_IDS_2017_Config.COLS_TO_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37256006-abc1-44aa-8e74-46d05dc6ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_classes = 2\n",
    "class_map = [0, 1]\n",
    "if MULTICLASS:\n",
    "    le = LabelEncoder()\n",
    "    attack_labels = le.fit_transform(data[ATTACK_CLASS_COL_NAME])\n",
    "    class_map = le.classes_\n",
    "    print(class_map)\n",
    "    print(\"Attack label mapping:\", dict(zip(class_map, range(len(class_map)))))\n",
    "    data[ATTACK_CLASS_COL_NAME] = attack_labels\n",
    "    num_classes = len(class_map)\n",
    "    class_dict = {le.inverse_transform([i])[0]: i for i in range(len(le.classes_))}\n",
    "\n",
    "BENIGN_CLASS_LABEL = le.transform([BENIGN_CLASS_NAME])[0] if MULTICLASS else 0\n",
    "ADVERSARIAL_CLASS_LABEL = len(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f4cdd-2716-431f-af50-b34cc3d2d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% train, 15% validation, 15% test\n",
    "train_full_df, test_df = train_test_split(\n",
    "     data, test_size=0.15, random_state=42, stratify=data[label_col])\n",
    "\n",
    "\n",
    "# # Maintain the order of the rows in the original dataframe\n",
    "# train_full_df = train_full_df.sort_values(by=CIC_IDS_2017_Config.TIME_COL_NAMES)\n",
    "# test_df = test_df.sort_values(by=CIC_IDS_2017_Config.TIME_COL_NAMES)\n",
    "\n",
    "feature_cols = CIC_IDS_2017_Config.COLS_TO_NORM + converted_categorical_cols\n",
    "\n",
    "print('Feature Columns:', feature_cols)\n",
    "\n",
    "train_full_df['h'] = train_full_df[ feature_cols ].values.tolist()\n",
    "test_df['h'] = test_df[ feature_cols ].values.tolist()\n",
    "\n",
    "# X_train = train_full_df.drop(columns=[label_col])\n",
    "# X_val = val_df.drop(columns=[label_col])\n",
    "# X_test = test_df.drop(columns=[label_col])\n",
    "\n",
    "y_train = train_full_df[label_col]\n",
    "y_test = test_df[label_col]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_full_df))\n",
    "print(y_train.value_counts())\n",
    "print(\"Number of test samples:\", len(test_df))\n",
    "print(y_test.value_counts())\n",
    "\n",
    "print(train_full_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83bd73-531f-42a0-9f73-e1fd98dce1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, source_ip_col, destination_ip_col, edge_attr, create_using=nx.MultiDiGraph(), **kwargs):\n",
    "    G_nx = nx.from_pandas_edgelist(df, source_ip_col, destination_ip_col, edge_attr, create_using=create_using, **kwargs)\n",
    "    G_pyg = from_networkx(G_nx)\n",
    "\n",
    "    num_nodes = G_pyg.num_nodes\n",
    "    num_edges = G_pyg.num_edges\n",
    "\n",
    "    G_pyg.x = th.ones(num_nodes, len(df['h'].iloc[0])) \n",
    "\n",
    "    edge_attr_list = []\n",
    "    edge_label_list = []\n",
    "\n",
    "    for u, v, key, data in G_nx.edges(keys=True, data=True):\n",
    "        edge_attr_list.append(data['h']) \n",
    "        edge_label_list.append(data[label_col]) \n",
    "\n",
    "    G_pyg.edge_attr = th.tensor(edge_attr_list, dtype=th.float32)\n",
    "    G_pyg.edge_label = th.tensor(edge_label_list, dtype=th.long)\n",
    "\n",
    "    print(\"Number of edges in G_pyg:\", num_edges)\n",
    "    print(\"Number of node in G_pyg:\", num_nodes)\n",
    "    print(\"Shape of node in G_pyg:\", G_pyg.x.shape)\n",
    "    print(\"Shape of edge attr in G_pyg:\", G_pyg.edge_attr.shape)\n",
    "    print(\"Shape of edge label in G_pyg:\", G_pyg.edge_label.shape)\n",
    "\n",
    "    return G_nx, G_pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f9cdb-1316-461a-a927-8d67d90d6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41795339-6036-468f-9b9d-2bb68d78ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGELayerPyG(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_dim, out_channels, activation=F.relu):\n",
    "        super().__init__(aggr='mean')  # mean aggregation\n",
    "        self.W_msg = nn.Linear(in_channels + edge_dim, out_channels)\n",
    "        self.W_apply = nn.Linear(in_channels + out_channels, out_channels)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: [num_nodes, in_channels]\n",
    "        # edge_attr: [num_edges, edge_dim]\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j: features of source nodes (neighbours)\n",
    "        msg_input = th.cat([x_j, edge_attr], dim=1)\n",
    "        return self.W_msg(msg_input)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out: [num_nodes, out_channels]\n",
    "        combined = th.cat([x, aggr_out], dim=1)\n",
    "        out = self.W_apply(combined)\n",
    "        return self.activation(out)\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.lin = nn.Linear(in_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, data, z):\n",
    "        row, col = data.edge_index\n",
    "        # Concatenate the features of source and target nodes for each edge\n",
    "        edge_feat = th.cat([z[row], z[col]], dim=1)\n",
    "        return self.lin(edge_feat)\n",
    "\n",
    "class EGraphSAGE(nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels, out_channels, dropout=0.2):\n",
    "        super(EGraphSAGE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGELayerPyG(node_in_channels, edge_in_channels, hidden_channels)\n",
    "        self.conv2 = SAGELayerPyG(hidden_channels, edge_in_channels, hidden_channels)\n",
    "        self.mlp_predictor = MLPPredictor(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return self.mlp_predictor(data, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca25fef-29d9-40cf-8910-16b24d530693",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccdc850-b98d-4836-b82b-67aa4b9e1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89157faf-e24b-49d6-9c90-6f71dae515b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d37f0-713b-4abc-8d7a-3e768ae9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def grid_search(data, epochs, learning_rates, hidden_dims, drop_outs):\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_params = {}\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Precompute the train and validation graphs for all folds\n",
    "    folds = []\n",
    "    for train_idx, val_idx in skf.split(data, data[label_col]):\n",
    "        train_df = data.iloc[train_idx]\n",
    "        val_df = data.iloc[val_idx]\n",
    "\n",
    "        G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "        G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "\n",
    "        G_pyg_train = G_pyg_train.to(device)\n",
    "        G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "        G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "        G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "        G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "        G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "        folds.append((G_pyg_train, G_pyg_val))\n",
    "\n",
    "    params_results = {}\n",
    "    for lr in learning_rates:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for drop_out in drop_outs:\n",
    "                print(f\"Testing with learning rate: {lr}, hidden_dim: {hidden_dim}\")\n",
    "                fold_f1_scores = []\n",
    "\n",
    "                for fold, (G_pyg_train, G_pyg_val) in enumerate(folds):\n",
    "                    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "                    model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                                    edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                                    hidden_channels=hidden_dim,\n",
    "                                    dropout=drop_out,\n",
    "                                    out_channels=num_classes).to(device)\n",
    "\n",
    "                    model.apply(init_weights)\n",
    "\n",
    "                    labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "                    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                                    classes=np.unique(labels),\n",
    "                                                                    y=labels)\n",
    "\n",
    "                    # Normalize to stabilize training\n",
    "                    class_weights = th.FloatTensor(class_weights).to(device)\n",
    "                    print(\"Class weights:\", class_weights)\n",
    "\n",
    "                    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "                    optimizer = th.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    best_epoch_f1 = 0  # Track the best F1 score for this fold\n",
    "\n",
    "                    for epoch in range(epochs):\n",
    "                        train_loss = 0\n",
    "                        val_loss = 0\n",
    "\n",
    "                        try:\n",
    "                            model.train()\n",
    "                            out = model(G_pyg_train)\n",
    "                            \n",
    "                            optimizer.zero_grad()\n",
    "                            loss = criterion(out, G_pyg_train.edge_label)\n",
    "                            train_loss = loss.item()\n",
    "\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                            model.eval()\n",
    "                            with th.no_grad():\n",
    "                                out = model(G_pyg_val)\n",
    "                                loss = criterion(out, G_pyg_val.edge_label)\n",
    "                                val_loss = loss.item()\n",
    "\n",
    "                            val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "\n",
    "                            if val_f1 > best_epoch_f1:\n",
    "                                best_epoch_f1 = val_f1  # Update the best F1 score for this fold\n",
    "                                print(f\"Best F1 Score at epoch {epoch}: {best_epoch_f1:.4f}, Parameters: lr={lr}, hidden_dim{hidden_dim}, drop_out={drop_out}\")\n",
    "\n",
    "                            if epoch % 100 == 0:\n",
    "                                print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred at epoch {epoch}: {str(e)}\")\n",
    "                            break\n",
    "\n",
    "                    fold_f1_scores.append(best_epoch_f1)  # Append the best F1 score for this fold\n",
    "                \n",
    "                avg_f1 = sum(fold_f1_scores) / len(fold_f1_scores)\n",
    "                params_results[(drop_out, lr, hidden_dim)] = {'folds': fold_f1_scores, 'avg_f1': avg_f1}\n",
    "                print(\"Current Results: \", params_results)\n",
    "                print(f\"Average F1 Score for dropout {drop_out} learning rate {lr}, hidden_dim {hidden_dim}: {avg_f1:.4f}\")\n",
    "\n",
    "                if avg_f1 > best_f1:\n",
    "                    best_f1 = avg_f1\n",
    "                    best_params = {'learning_rate': lr, 'hidden_dim': hidden_dim, 'drop_out': drop_out}\n",
    "\n",
    "        print(f\"Best Parameters: {best_params}, Best F1 Score: {best_f1:.4f}\")\n",
    "        print(\"All results:\", params_results)\n",
    "\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "hidden_dims = [128, 256, 512]\n",
    "drop_outs = [0.2, 0.3, 0.4]\n",
    "\n",
    "# grid_search(train_full_df, epochs=100, learning_rates=learning_rates, hidden_dims=hidden_dims, drop_outs=drop_outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "     train_full_df, test_size=0.15, random_state=42, stratify=train_full_df[label_col])\n",
    "\n",
    "G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab34f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_process(train_losses, val_losses, val_f1, saved_model_epochs):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "    # plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "    plt.plot(val_f1, label='Validation F1', color='green')\n",
    "    plt.scatter(saved_model_epochs, [val_f1[epoch] for epoch in saved_model_epochs], color='red', label='Saved Model', zorder=5)\n",
    "    plt.title('Train and Validation Metrics')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the best parameters from the grid search\n",
    "best_hidden_dim = 256  # Replace with the best hidden_dim found\n",
    "best_learning_rate = 0.001  # Replace with the best learning_rate found\n",
    "best_dropout = 0.3  # Replace with the best dropout found\n",
    "\n",
    "# Initialize the model with the best parameters\n",
    "model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                   edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                   hidden_channels=best_hidden_dim,\n",
    "                   dropout=best_dropout,\n",
    "                   out_channels=num_classes).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Compute class weights for the training dataset\n",
    "labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.unique(labels),\n",
    "                                                  y=labels)\n",
    "\n",
    "# Normalize class weights\n",
    "class_weights = th.FloatTensor(class_weights).to(device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# Move the graph data to the device\n",
    "G_pyg_train = G_pyg_train.to(device)\n",
    "G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "best_f1 = 0\n",
    "\n",
    "# Load checkpoint if exists\n",
    "start_epoch = 0\n",
    "epochs = 1000\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = th.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "    print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_f1_history = []\n",
    "saved_model_epochs = []\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    out = model(G_pyg_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(out, G_pyg_train.edge_label)\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        out = model(G_pyg_val)\n",
    "        loss = criterion(out, G_pyg_val.edge_label)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "\n",
    "    if epoch % 10 == 0 or val_f1 > best_f1:\n",
    "        # Save checkpoint\n",
    "        th.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_f1': best_f1\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1  # Update the best F1 score for this fold\n",
    "        best_model_state = model.state_dict()\n",
    "        th.save(best_model_state, best_model_path)\n",
    "        print(f\"Epoch {epoch} Saved best model. Best F1:\", best_f1)\n",
    "        saved_model_epochs.append(epoch)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_f1_history.append(val_f1)\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Model training completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_process(train_loss_history, val_loss_history, val_f1_history, saved_model_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8368fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f271a-612b-4cd6-a85a-e4236dec9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def eval(G_pyg_test, adversarial=False):\n",
    "\n",
    "    G_pyg_test = G_pyg_test.to(device)\n",
    "    G_pyg_test.edge_label = G_pyg_test.edge_label.to(device)\n",
    "    G_pyg_test.edge_attr = G_pyg_test.edge_attr.to(device)\n",
    "\n",
    "    best_model = EGraphSAGE(node_in_channels=G_pyg_test.num_node_features, \n",
    "                       edge_in_channels=G_pyg_test.num_edge_features,\n",
    "                       hidden_channels=best_hidden_dim, \n",
    "                       out_channels=num_classes).to(device)\n",
    "\n",
    "    print(\"Loading model from\", best_model_path)\n",
    "    best_model.load_state_dict(th.load(best_model_path))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    best_model.eval()\n",
    "\n",
    "    print(\"inference start\")\n",
    "    with th.no_grad():\n",
    "            \n",
    "        try:\n",
    "            out = best_model(G_pyg_test)\n",
    "            \n",
    "        except Exception as forward_error:\n",
    "            print(f\"Error during forward/backward pass at {forward_error}\")\n",
    "\n",
    "    print(\"inference done\")\n",
    "\n",
    "    # test_accuracy = compute_accuracy(out, G_pyg_test.edge_label)\n",
    "    # print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    \n",
    "    pred_labels = out.argmax(dim=1).cpu()\n",
    "    all_test_labels = G_pyg_test.edge_label.cpu()\n",
    "\n",
    "    if adversarial:\n",
    "\n",
    "        # Create a boolean mask where the label is NOT equal to the adversarial class\n",
    "        adversarial_mask = all_test_labels == ADVERSARIAL_CLASS_LABEL\n",
    "\n",
    "        # Print the class that the adversarial samples are classified as\n",
    "        cm_adversarial = confusion_matrix(all_test_labels[adversarial_mask], pred_labels[adversarial_mask], labels=range(len(class_map) + 1))\n",
    "        print(\"Adversarial confusion matrix:\", cm_adversarial)\n",
    "\n",
    "        # Apply the mask to both labels and predictions\n",
    "        all_test_labels = all_test_labels[~adversarial_mask]\n",
    "        pred_labels = pred_labels[~adversarial_mask]\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"class_map\", class_map)\n",
    "    # Generate a report\n",
    "    cm = confusion_matrix(all_test_labels, pred_labels, labels=range(len(class_map)))\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    report = classification_report(all_test_labels, pred_labels, target_names=class_map, digits=4)\n",
    "    print(report)\n",
    "\n",
    "    \n",
    "eval(G_pyg_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def confusion_matrix_plot(cm, all_test_labels, pred_labels):\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_map, yticklabels=class_map)\n",
    "#     plt.xlabel(\"Predicted\")\n",
    "#     plt.ylabel(\"True\")\n",
    "#     plt.title(\"Confusion Matrix\")\n",
    "\n",
    "#     # Compute metrics\n",
    "#     accuracy = accuracy_score(all_test_labels, pred_labels)\n",
    "#     precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_test_labels, pred_labels, average='macro', zero_division=0)\n",
    "#     precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(all_test_labels, pred_labels, average='weighted', zero_division=0)\n",
    "\n",
    "\n",
    "#     metrics_text = (\n",
    "#         f\"Accuracy: {accuracy:.4f}\\n\"\n",
    "#         f\"Macro Precision: {precision_macro:.4f}\\n\"\n",
    "#         f\"Macro Recall: {recall_macro:.4f}\\n\"\n",
    "#         f\"Macro F1: {f1_macro:.4f}\\n\"\n",
    "#         f\"Weighted Precision: {precision_weighted:.4f}\\n\"\n",
    "#         f\"Weighted Recall: {recall_weighted:.4f}\\n\"\n",
    "#         f\"Weighted F1: {f1_weighted:.4f}\"\n",
    "#     )\n",
    "\n",
    "#     # Position: bottom left corner of plot area\n",
    "#     plt.gcf().text(0.02, 0.02, metrics_text, fontsize=12, va='bottom', ha='left',\n",
    "#                 bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_traffic_to_attacker(graph, ratio=0.1, num_injected_nodes=1, is_attack=False):\n",
    "    edge_index = graph.edge_index.clone()\n",
    "    edge_attr = graph.edge_attr.clone()\n",
    "    edge_label = graph.edge_label.clone()\n",
    "    x = graph.x.clone()\n",
    "\n",
    "    num_edges = edge_index.size(1)\n",
    "    feature_dim = graph.x.size(1)\n",
    "\n",
    "    # 1. Identify attacker nodes\n",
    "    attacker_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "    attacker_nodes = th.unique(edge_index[:, attacker_edges])\n",
    "    if attacker_nodes.numel() == 0:\n",
    "        raise ValueError(\"No attacker nodes found.\")\n",
    "\n",
    "    # 2. Sample benign edge feature pool\n",
    "    if is_attack:\n",
    "        attack_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[attack_edges]\n",
    "    else:\n",
    "        benign_edges = (edge_label == BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[benign_edges]\n",
    "\n",
    "    # 3. Inject new nodes\n",
    "    original_num_nodes = x.size(0)\n",
    "\n",
    "    new_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "    x = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "    # 4. Inject edges from injected nodes to attacker nodes\n",
    "    num_to_inject = max(1, int(ratio * num_edges))\n",
    "    new_edges = []\n",
    "    new_attrs = []\n",
    "    new_labels = []\n",
    "\n",
    "    \n",
    "    for _ in range(num_to_inject):\n",
    "        src = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\n",
    "        dst = attacker_nodes[random.randint(0, len(attacker_nodes) - 1)].item()\n",
    "\n",
    "        new_edges.append([src, dst])\n",
    "        attr = inject_edge_attr_pool[random.randint(0, len(inject_edge_attr_pool) - 1)]\n",
    "        new_attrs.append(attr)\n",
    "        new_labels.append(ADVERSARIAL_CLASS_LABEL)\n",
    "\n",
    "    # Create a new empty graph to store the injected edges\n",
    "    new_graph = Data()\n",
    "\n",
    "    # 5. Merge into graph\n",
    "    if new_edges:\n",
    "        new_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "        new_attrs = th.stack(new_attrs)\n",
    "        new_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "        new_graph.edge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "        new_graph.edge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "        new_graph.edge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "        new_graph.x = x\n",
    "\n",
    "        # new_graph.first_injected_node_idx = original_num_nodes # Store injected node indices\n",
    "\n",
    "    return new_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject Attack Traffic to Attacker Nodes\n",
    "G_pyg_test = G_pyg_test.cpu()\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=True)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c37b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject BENIGN Traffic to Attacker Nodes\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=False)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_random_nodes(graph, ratio=0.1, num_injected_nodes=1):\n",
    "\tedge_index = graph.edge_index.clone()\n",
    "\tedge_attr = graph.edge_attr.clone()\n",
    "\tedge_label = graph.edge_label.clone()\n",
    "\tx = graph.x.clone()\n",
    "\n",
    "\tnum_edges = edge_index.size(1)\n",
    "\tfeature_dim = graph.x.size(1)\n",
    "\n",
    "\t# 1. Inject new nodes\n",
    "\toriginal_num_nodes = x.size(0)\n",
    "\tnew_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "\tx = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "\t# 2. Inject random edges\n",
    "\tnum_to_inject = max(1, int(ratio * num_edges))\n",
    "\tnew_edges = []\n",
    "\tnew_attrs = []\n",
    "\tnew_labels = []\n",
    "\n",
    "\tfor _ in range(num_to_inject):\n",
    "\t\tsrc = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\t\tdst = random.randint(0, original_num_nodes - 1)  # to existing nodes\n",
    "\n",
    "\t\tnew_edges.append([src, dst])\n",
    "\t\tattr = edge_attr[random.randint(0, len(edge_attr) - 1)]  # Randomly sample edge attributes\n",
    "\t\tnew_attrs.append(attr)\n",
    "\t\tnew_labels.append(ADVERSARIAL_CLASS_LABEL)  # Assign benign class label to new edges\n",
    "\n",
    "\t# 3. Merge into graph\n",
    "\tif new_edges:\n",
    "\t\tnew_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "\t\tnew_attrs = th.stack(new_attrs)\n",
    "\t\tnew_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "\t\tedge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "\t\tedge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "\t\tedge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "\n",
    "\t# Create a new graph with the injected nodes and edges\n",
    "\tnew_graph = Data(\n",
    "\t\tedge_index=edge_index,\n",
    "\t\tedge_attr=edge_attr,\n",
    "\t\tedge_label=edge_label,\n",
    "\t\tx=x\n",
    "\t)\n",
    "\n",
    "\treturn new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb63898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject Random Nodes in the graph\n",
    "injected_graph = inject_random_nodes(G_pyg_test, 0.1, num_injected_nodes=1)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
