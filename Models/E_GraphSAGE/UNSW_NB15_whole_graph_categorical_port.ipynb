{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb9e1b2-1c5f-49e3-82b4-3bfe52cabad8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m=====Experiment=====\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mDataset: UNSW-NB15 dataset\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03mEncode Ports in 3 Categories\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_networkx, add_self_loops, degree\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MessagePassing\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mT\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/torch_geometric/__init__.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexplain\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/torch_geometric/datasets/__init__.py:79\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbrca_tgca\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BrcaTcga\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mneurograph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NeuroGraphDataset\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mweb_qsp_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WebQSPDataset\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdbp15k\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DBP15K\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maminer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AMiner\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/torch_geometric/datasets/web_qsp_dataset.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Data, InMemoryDataset\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnlp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m     13\u001b[39m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieval_via_pcst\u001b[39m(\n\u001b[32m     15\u001b[39m     data: Data,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     cost_e: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.5\u001b[39m,\n\u001b[32m     22\u001b[39m ) -> Tuple[Data, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m     23\u001b[39m     c = \u001b[32m0.01\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/torch_geometric/nn/__init__.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdense\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     21\u001b[39m __all__ = [\n\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mReshape\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     23\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mSequential\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     31\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/torch_geometric/nn/models/__init__.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjumping_knowledge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JumpingKnowledge, HeteroJumpingKnowledge\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmeta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MetaLayer\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnode2vec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Node2Vec\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeep_graph_infomax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepGraphInfomax\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautoencoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InnerProductDecoder, GAE, VGAE, ARGA, ARGVA\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:991\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1087\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1187\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "=====Experiment=====\n",
    "Dataset: UNSW-NB15 dataset\n",
    "\n",
    "Training with whole graph\n",
    "Downsampled 90% normal traffic randomly\n",
    "Split train and test randomly\n",
    "\n",
    "IP as node\n",
    "Encode Ports in 3 Categories\n",
    "'''\n",
    "\n",
    "from torch_geometric.utils import from_networkx, add_self_loops, degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "# import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Datasets.UNSW_NB15.UNSW_NB15_config import UNSW_NB15_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ef09a-d405-43b8-971e-fe9e6a592c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1691029/3516300686.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "Normal            221876\n",
      "Generic           215481\n",
      "Exploits           44525\n",
      "Fuzzers            24246\n",
      "DoS                16353\n",
      "Reconnaissance     13987\n",
      "Analysis            2677\n",
      "Backdoors           2329\n",
      "Shellcode           1511\n",
      "Worms                174\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    321283\n",
      "0    221876\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "csv_file_name = \"all_downsampled\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n",
    "\n",
    "DATASET_NAME = \"UNSW_NB15\"\n",
    "EXPERIMENT_NAME = \"whole_graph_categorical_port\"\n",
    "\n",
    "SOURCE_IP_COL_NAME = UNSW_NB15_Config.SOURCE_IP_COL_NAME\n",
    "DESTINATION_IP_COL_NAME = UNSW_NB15_Config.DESTINATION_IP_COL_NAME\n",
    "SOURCE_PORT_COL_NAME = UNSW_NB15_Config.SOURCE_PORT_COL_NAME\n",
    "DESTINATION_PORT_COL_NAME = UNSW_NB15_Config.DESTINATION_PORT_COL_NAME\n",
    "\n",
    "ATTACK_CLASS_COL_NAME = UNSW_NB15_Config.ATTACK_CLASS_COL_NAME\n",
    "IS_ATTACK_COL_NAME = UNSW_NB15_Config.IS_ATTACK_COL_NAME\n",
    "\n",
    "BENIGN_CLASS_NAME = UNSW_NB15_Config.BENIGN_CLASS_NAME\n",
    "\n",
    "TIME_COLS = UNSW_NB15_Config.TIME_COL_NAMES\n",
    "\n",
    "CATEGORICAL_COLS = UNSW_NB15_Config.CATEGORICAL_COLS\n",
    "\n",
    "print(data[ATTACK_CLASS_COL_NAME].value_counts())\n",
    "print(data[IS_ATTACK_COL_NAME].value_counts())\n",
    "\n",
    "MULTICLASS = True\n",
    "\n",
    "if MULTICLASS:\n",
    "    label_col = ATTACK_CLASS_COL_NAME\n",
    "    data.drop(columns=[IS_ATTACK_COL_NAME], inplace=True)\n",
    "else:\n",
    "    label_col = IS_ATTACK_COL_NAME\n",
    "    data.drop(columns=[ATTACK_CLASS_COL_NAME], inplace=True)\n",
    "\n",
    "saves_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, EXPERIMENT_NAME)\n",
    "\n",
    "checkpoint_path = os.path.join(saves_path, f\"checkpoints_{csv_file_name}.pth\")\n",
    "best_model_path = os.path.join(saves_path, f\"best_model_{csv_file_name}.pth\")\n",
    "\n",
    "os.makedirs(saves_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a1af1-1d3d-4179-9628-7c2ec551ce0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srcip</th>\n",
       "      <th>sport</th>\n",
       "      <th>dstip</th>\n",
       "      <th>dsport</th>\n",
       "      <th>state</th>\n",
       "      <th>dur</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sloss</th>\n",
       "      <th>dloss</th>\n",
       "      <th>Sload</th>\n",
       "      <th>Dload</th>\n",
       "      <th>Spkts</th>\n",
       "      <th>Dpkts</th>\n",
       "      <th>swin</th>\n",
       "      <th>dwin</th>\n",
       "      <th>stcpb</th>\n",
       "      <th>dtcpb</th>\n",
       "      <th>smeansz</th>\n",
       "      <th>dmeansz</th>\n",
       "      <th>trans_depth</th>\n",
       "      <th>res_bdy_len</th>\n",
       "      <th>Sjit</th>\n",
       "      <th>Djit</th>\n",
       "      <th>Sintpkt</th>\n",
       "      <th>Dintpkt</th>\n",
       "      <th>tcprtt</th>\n",
       "      <th>synack</th>\n",
       "      <th>ackdat</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>ct_state_ttl</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>source_file_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.40.85.1_0</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0.0.5_0</td>\n",
       "      <td>0</td>\n",
       "      <td>INT</td>\n",
       "      <td>50.004341</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.195557</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10000.869000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.166.0.6_0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.4_0</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>132</td>\n",
       "      <td>164</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>465608.468800</td>\n",
       "      <td>5.784832e+05</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175.45.176.0_0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.16_0</td>\n",
       "      <td>80</td>\n",
       "      <td>FIN</td>\n",
       "      <td>2.390390</td>\n",
       "      <td>1362</td>\n",
       "      <td>268</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4233.619141</td>\n",
       "      <td>7.496685e+02</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>3897219059</td>\n",
       "      <td>2466816006</td>\n",
       "      <td>97</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18786.711400</td>\n",
       "      <td>941.724938</td>\n",
       "      <td>183.579303</td>\n",
       "      <td>474.259406</td>\n",
       "      <td>0.066088</td>\n",
       "      <td>0.017959</td>\n",
       "      <td>0.048129</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reconnaissance</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.166.0.3_0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.8_0</td>\n",
       "      <td>25</td>\n",
       "      <td>FIN</td>\n",
       "      <td>34.077175</td>\n",
       "      <td>37358</td>\n",
       "      <td>3380</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>8601.652344</td>\n",
       "      <td>7.747121e+02</td>\n",
       "      <td>52</td>\n",
       "      <td>42</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>4063826787</td>\n",
       "      <td>4060853251</td>\n",
       "      <td>718</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128935.700600</td>\n",
       "      <td>4470.006000</td>\n",
       "      <td>668.172887</td>\n",
       "      <td>831.135688</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.40.170.2_0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.40.170.2_0</td>\n",
       "      <td>0</td>\n",
       "      <td>INT</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543154</th>\n",
       "      <td>175.45.176.1_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.11_3</td>\n",
       "      <td>6071</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.291164</td>\n",
       "      <td>732</td>\n",
       "      <td>468</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>18436.343750</td>\n",
       "      <td>1.126513e+04</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>2347481213</td>\n",
       "      <td>2217355451</td>\n",
       "      <td>61</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1332.515339</td>\n",
       "      <td>58.215816</td>\n",
       "      <td>24.887364</td>\n",
       "      <td>40.483855</td>\n",
       "      <td>0.054361</td>\n",
       "      <td>0.007769</td>\n",
       "      <td>0.046592</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Generic</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543155</th>\n",
       "      <td>175.45.176.3_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.16_3</td>\n",
       "      <td>2140</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.011751</td>\n",
       "      <td>76</td>\n",
       "      <td>132</td>\n",
       "      <td>254</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25870.138670</td>\n",
       "      <td>4.493235e+04</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Backdoors</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543156</th>\n",
       "      <td>59.166.0.2_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.4_3</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>146</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>242323.656300</td>\n",
       "      <td>2.954357e+05</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Normal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543157</th>\n",
       "      <td>175.45.176.1_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.11_3</td>\n",
       "      <td>5250</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.176514</td>\n",
       "      <td>10778</td>\n",
       "      <td>268</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>457980.656300</td>\n",
       "      <td>1.015217e+04</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>3578188515</td>\n",
       "      <td>1588977055</td>\n",
       "      <td>674</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>777.573179</td>\n",
       "      <td>53.911551</td>\n",
       "      <td>10.667534</td>\n",
       "      <td>33.587000</td>\n",
       "      <td>0.065586</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.057016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Generic</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543158</th>\n",
       "      <td>59.166.0.2_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.2_3</td>\n",
       "      <td>8406</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.049598</td>\n",
       "      <td>2646</td>\n",
       "      <td>25564</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>416629.687500</td>\n",
       "      <td>4.029678e+06</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>3687522840</td>\n",
       "      <td>3703402068</td>\n",
       "      <td>63</td>\n",
       "      <td>581</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81.659946</td>\n",
       "      <td>79.554793</td>\n",
       "      <td>1.201171</td>\n",
       "      <td>1.139512</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Normal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543159 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 srcip  sport             dstip dsport state        dur  \\\n",
       "0         10.40.85.1_0      0       224.0.0.5_0      0   INT  50.004341   \n",
       "1         59.166.0.6_0      0   149.171.126.4_0     53   CON   0.001134   \n",
       "2       175.45.176.0_0      0  149.171.126.16_0     80   FIN   2.390390   \n",
       "3         59.166.0.3_0      0   149.171.126.8_0     25   FIN  34.077175   \n",
       "4        10.40.170.2_0      0     10.40.170.2_0      0   INT   0.000000   \n",
       "...                ...    ...               ...    ...   ...        ...   \n",
       "543154  175.45.176.1_3      0  149.171.126.11_3   6071   FIN   0.291164   \n",
       "543155  175.45.176.3_3      0  149.171.126.16_3   2140   CON   0.011751   \n",
       "543156    59.166.0.2_3      0   149.171.126.4_3     53   CON   0.002410   \n",
       "543157  175.45.176.1_3      0  149.171.126.11_3   5250   FIN   0.176514   \n",
       "543158    59.166.0.2_3      0   149.171.126.2_3   8406   FIN   0.049598   \n",
       "\n",
       "        sbytes  dbytes  sttl  dttl  sloss  dloss          Sload         Dload  \\\n",
       "0          384       0     1     0      0      0      51.195557  0.000000e+00   \n",
       "1          132     164    31    29      0      0  465608.468800  5.784832e+05   \n",
       "2         1362     268   254   252      6      1    4233.619141  7.496685e+02   \n",
       "3        37358    3380    31    29     18      8    8601.652344  7.747121e+02   \n",
       "4           46       0     0     0      0      0       0.000000  0.000000e+00   \n",
       "...        ...     ...   ...   ...    ...    ...            ...           ...   \n",
       "543154     732     468   254   252      3      2   18436.343750  1.126513e+04   \n",
       "543155      76     132   254    60      0      0   25870.138670  4.493235e+04   \n",
       "543156     146     178    31    29      0      0  242323.656300  2.954357e+05   \n",
       "543157   10778     268   254   252      5      1  457980.656300  1.015217e+04   \n",
       "543158    2646   25564    31    29      7     15  416629.687500  4.029678e+06   \n",
       "\n",
       "        Spkts  Dpkts  swin  dwin       stcpb       dtcpb  smeansz  dmeansz  \\\n",
       "0           6      0     0     0           0           0       64        0   \n",
       "1           2      2     0     0           0           0       66       82   \n",
       "2          14      6   255   255  3897219059  2466816006       97       45   \n",
       "3          52     42   255   255  4063826787  4060853251      718       80   \n",
       "4           1      0     0     0           0           0       46        0   \n",
       "...       ...    ...   ...   ...         ...         ...      ...      ...   \n",
       "543154     12      8   255   255  2347481213  2217355451       61       59   \n",
       "543155      2      2     0     0           0           0       38       66   \n",
       "543156      2      2     0     0           0           0       73       89   \n",
       "543157     16      6   255   255  3578188515  1588977055      674       45   \n",
       "543158     42     44   255   255  3687522840  3703402068       63      581   \n",
       "\n",
       "        trans_depth  res_bdy_len           Sjit         Djit       Sintpkt  \\\n",
       "0                 0            0       0.000000     0.000000  10000.869000   \n",
       "1                 0            0       0.000000     0.000000      0.017000   \n",
       "2                 1            0   18786.711400   941.724938    183.579303   \n",
       "3                 0            0  128935.700600  4470.006000    668.172887   \n",
       "4                 0            0       0.000000     0.000000      0.000000   \n",
       "...             ...          ...            ...          ...           ...   \n",
       "543154            0            0    1332.515339    58.215816     24.887364   \n",
       "543155            0            0       0.000000     0.000000      0.002000   \n",
       "543156            0            0       0.000000     0.000000      0.008000   \n",
       "543157            0            0     777.573179    53.911551     10.667534   \n",
       "543158            0            0      81.659946    79.554793      1.201171   \n",
       "\n",
       "           Dintpkt    tcprtt    synack    ackdat  is_sm_ips_ports  \\\n",
       "0         0.000000  0.000000  0.000000  0.000000                0   \n",
       "1         0.013000  0.000000  0.000000  0.000000                0   \n",
       "2       474.259406  0.066088  0.017959  0.048129                0   \n",
       "3       831.135688  0.000782  0.000609  0.000173                0   \n",
       "4         0.000000  0.000000  0.000000  0.000000                1   \n",
       "...            ...       ...       ...       ...              ...   \n",
       "543154   40.483855  0.054361  0.007769  0.046592                0   \n",
       "543155    0.001000  0.000000  0.000000  0.000000                0   \n",
       "543156    0.002000  0.000000  0.000000  0.000000                0   \n",
       "543157   33.587000  0.065586  0.008570  0.057016                0   \n",
       "543158    1.139512  0.000738  0.000591  0.000147                0   \n",
       "\n",
       "        ct_state_ttl  ct_flw_http_mthd  is_ftp_login  ct_ftp_cmd  ct_srv_src  \\\n",
       "0                  0               0.0           0.0         0.0           2   \n",
       "1                  0               0.0           0.0         0.0          12   \n",
       "2                  1               1.0           0.0         0.0           5   \n",
       "3                  0               0.0           0.0         0.0           1   \n",
       "4                  2               0.0           0.0         0.0           2   \n",
       "...              ...               ...           ...         ...         ...   \n",
       "543154             1               0.0           0.0         NaN           1   \n",
       "543155             0               0.0           0.0         NaN           1   \n",
       "543156             0               0.0           0.0         NaN           3   \n",
       "543157             1               0.0           0.0         NaN           1   \n",
       "543158             0               0.0           0.0         NaN           6   \n",
       "\n",
       "        ct_srv_dst  ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  \\\n",
       "0                4           4           2                 2   \n",
       "1                7           1           2                 2   \n",
       "2                2           2           1                 1   \n",
       "3                1          12          10                 1   \n",
       "4                2           2           2                 2   \n",
       "...            ...         ...         ...               ...   \n",
       "543154           1           1           1                 1   \n",
       "543155           1           1           1                 1   \n",
       "543156           5           3           2                 2   \n",
       "543157           1           1           1                 1   \n",
       "543158           2           4           7                 1   \n",
       "\n",
       "        ct_dst_sport_ltm  ct_dst_src_ltm      attack_cat  source_file_id  \n",
       "0                      4               2          Normal               0  \n",
       "1                      1               1          Normal               0  \n",
       "2                      1               1  Reconnaissance               0  \n",
       "3                      1               2          Normal               0  \n",
       "4                      2               2          Normal               0  \n",
       "...                  ...             ...             ...             ...  \n",
       "543154                 1               2         Generic               3  \n",
       "543155                 1               1       Backdoors               3  \n",
       "543156                 2               4          Normal               3  \n",
       "543157                 1               2         Generic               3  \n",
       "543158                 1               3          Normal               3  \n",
       "\n",
       "[543159 rows x 45 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns=UNSW_NB15_Config.DROP_COLS,inplace=True)\n",
    "data.drop(columns=UNSW_NB15_Config.TIME_COL_NAMES)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c690c-86a4-49f7-aa9c-58f94529547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns = CATEGORICAL_COLS) # One Hot Encoding for categorical data\n",
    "\n",
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME].apply(str)\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME].apply(str)\n",
    "\n",
    "# Port category encoding: \n",
    "def port_category(port_str):\n",
    "    try:\n",
    "        port_int = int(port_str)\n",
    "        if 1 <= port_int <= 1023:\n",
    "            return [1, 0, 0]  # Well Known\n",
    "        elif 1024 <= port_int <= 49151:\n",
    "            return [0, 1, 0]  # Registered\n",
    "        elif 49152 <= port_int <= 65535:\n",
    "            return [0, 0, 1]  # Dynamic/Private\n",
    "        else:\n",
    "            return [0, 0, 0]  # Out of range, encode as all zeros\n",
    "    except:\n",
    "        return [0, 0, 0]  # Non-integer or missing, encode as all zeros\n",
    "\n",
    "# Create new columns for binary encoding\n",
    "src_port_category = data[SOURCE_PORT_COL_NAME].apply(port_category)\n",
    "dst_port_category = data[DESTINATION_PORT_COL_NAME].apply(port_category)\n",
    "\n",
    "src_port_category_df = pd.DataFrame(src_port_category.tolist(), columns=[f\"{SOURCE_PORT_COL_NAME}_category_{i}\" for i in range(3)])\n",
    "dst_port_category_df = pd.DataFrame(dst_port_category.tolist(), columns=[f\"{DESTINATION_PORT_COL_NAME}_category_{i}\" for i in range(3)])\n",
    "\n",
    "# Concatenate the binary columns to the original dataframe\n",
    "data = pd.concat([data, src_port_category_df, dst_port_category_df], axis=1)\n",
    "\n",
    "data.drop(columns=[SOURCE_PORT_COL_NAME, DESTINATION_PORT_COL_NAME], inplace=True)\n",
    "\n",
    "converted_categorical_cols = [col for col in data.columns if col.startswith(tuple(CATEGORICAL_COLS + [SOURCE_PORT_COL_NAME, DESTINATION_PORT_COL_NAME]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651ef5b-0a9d-4641-aad7-5738092c46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                  srcip             dstip        dur  sbytes  dbytes  sttl  \\\n",
      "0         10.40.85.1_0       224.0.0.5_0  50.004341     384       0     1   \n",
      "1         59.166.0.6_0   149.171.126.4_0   0.001134     132     164    31   \n",
      "2       175.45.176.0_0  149.171.126.16_0   2.390390    1362     268   254   \n",
      "3         59.166.0.3_0   149.171.126.8_0  34.077175   37358    3380    31   \n",
      "4        10.40.170.2_0     10.40.170.2_0   0.000000      46       0     0   \n",
      "...                ...               ...        ...     ...     ...   ...   \n",
      "543154  175.45.176.1_3  149.171.126.11_3   0.291164     732     468   254   \n",
      "543155  175.45.176.3_3  149.171.126.16_3   0.011751      76     132   254   \n",
      "543156    59.166.0.2_3   149.171.126.4_3   0.002410     146     178    31   \n",
      "543157  175.45.176.1_3  149.171.126.11_3   0.176514   10778     268   254   \n",
      "543158    59.166.0.2_3   149.171.126.2_3   0.049598    2646   25564    31   \n",
      "\n",
      "        dttl  sloss  dloss          Sload         Dload  Spkts  Dpkts  swin  \\\n",
      "0          0      0      0      51.195557  0.000000e+00      6      0     0   \n",
      "1         29      0      0  465608.468800  5.784832e+05      2      2     0   \n",
      "2        252      6      1    4233.619141  7.496685e+02     14      6   255   \n",
      "3         29     18      8    8601.652344  7.747121e+02     52     42   255   \n",
      "4          0      0      0       0.000000  0.000000e+00      1      0     0   \n",
      "...      ...    ...    ...            ...           ...    ...    ...   ...   \n",
      "543154   252      3      2   18436.343750  1.126513e+04     12      8   255   \n",
      "543155    60      0      0   25870.138670  4.493235e+04      2      2     0   \n",
      "543156    29      0      0  242323.656300  2.954357e+05      2      2     0   \n",
      "543157   252      5      1  457980.656300  1.015217e+04     16      6   255   \n",
      "543158    29      7     15  416629.687500  4.029678e+06     42     44   255   \n",
      "\n",
      "        dwin       stcpb       dtcpb  smeansz  dmeansz  trans_depth  \\\n",
      "0          0           0           0       64        0            0   \n",
      "1          0           0           0       66       82            0   \n",
      "2        255  3897219059  2466816006       97       45            1   \n",
      "3        255  4063826787  4060853251      718       80            0   \n",
      "4          0           0           0       46        0            0   \n",
      "...      ...         ...         ...      ...      ...          ...   \n",
      "543154   255  2347481213  2217355451       61       59            0   \n",
      "543155     0           0           0       38       66            0   \n",
      "543156     0           0           0       73       89            0   \n",
      "543157   255  3578188515  1588977055      674       45            0   \n",
      "543158   255  3687522840  3703402068       63      581            0   \n",
      "\n",
      "        res_bdy_len           Sjit         Djit       Stime       Ltime  \\\n",
      "0                 0       0.000000     0.000000  1421927377  1421927427   \n",
      "1                 0       0.000000     0.000000  1421927414  1421927414   \n",
      "2                 0   18786.711400   941.724938  1421927414  1421927416   \n",
      "3                 0  128935.700600  4470.006000  1421927414  1421927448   \n",
      "4                 0       0.000000     0.000000  1421927415  1421927415   \n",
      "...             ...            ...          ...         ...         ...   \n",
      "543154            0    1332.515339    58.215816  1424250006  1424250006   \n",
      "543155            0       0.000000     0.000000  1424250007  1424250007   \n",
      "543156            0       0.000000     0.000000  1424250008  1424250008   \n",
      "543157            0     777.573179    53.911551  1424250008  1424250008   \n",
      "543158            0      81.659946    79.554793  1424250009  1424250009   \n",
      "\n",
      "             Sintpkt     Dintpkt    tcprtt    synack    ackdat  \\\n",
      "0       10000.869000    0.000000  0.000000  0.000000  0.000000   \n",
      "1           0.017000    0.013000  0.000000  0.000000  0.000000   \n",
      "2         183.579303  474.259406  0.066088  0.017959  0.048129   \n",
      "3         668.172887  831.135688  0.000782  0.000609  0.000173   \n",
      "4           0.000000    0.000000  0.000000  0.000000  0.000000   \n",
      "...              ...         ...       ...       ...       ...   \n",
      "543154     24.887364   40.483855  0.054361  0.007769  0.046592   \n",
      "543155      0.002000    0.001000  0.000000  0.000000  0.000000   \n",
      "543156      0.008000    0.002000  0.000000  0.000000  0.000000   \n",
      "543157     10.667534   33.587000  0.065586  0.008570  0.057016   \n",
      "543158      1.201171    1.139512  0.000738  0.000591  0.000147   \n",
      "\n",
      "        is_sm_ips_ports  ct_state_ttl  ct_flw_http_mthd  is_ftp_login  \\\n",
      "0                     0             0               0.0           0.0   \n",
      "1                     0             0               0.0           0.0   \n",
      "2                     0             1               1.0           0.0   \n",
      "3                     0             0               0.0           0.0   \n",
      "4                     1             2               0.0           0.0   \n",
      "...                 ...           ...               ...           ...   \n",
      "543154                0             1               0.0           0.0   \n",
      "543155                0             0               0.0           0.0   \n",
      "543156                0             0               0.0           0.0   \n",
      "543157                0             1               0.0           0.0   \n",
      "543158                0             0               0.0           0.0   \n",
      "\n",
      "        ct_ftp_cmd  ct_srv_src  ct_srv_dst  ct_dst_ltm  ct_src_ltm  \\\n",
      "0              0.0           2           4           4           2   \n",
      "1              0.0          12           7           1           2   \n",
      "2              0.0           5           2           2           1   \n",
      "3              0.0           1           1          12          10   \n",
      "4              0.0           2           2           2           2   \n",
      "...            ...         ...         ...         ...         ...   \n",
      "543154         NaN           1           1           1           1   \n",
      "543155         NaN           1           1           1           1   \n",
      "543156         NaN           3           5           3           2   \n",
      "543157         NaN           1           1           1           1   \n",
      "543158         NaN           6           2           4           7   \n",
      "\n",
      "        ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm      attack_cat  \\\n",
      "0                      2                 4               2          Normal   \n",
      "1                      2                 1               1          Normal   \n",
      "2                      1                 1               1  Reconnaissance   \n",
      "3                      1                 1               2          Normal   \n",
      "4                      2                 2               2          Normal   \n",
      "...                  ...               ...             ...             ...   \n",
      "543154                 1                 1               2         Generic   \n",
      "543155                 1                 1               1       Backdoors   \n",
      "543156                 2                 2               4          Normal   \n",
      "543157                 1                 1               2         Generic   \n",
      "543158                 1                 1               3          Normal   \n",
      "\n",
      "        source_file_id  state_ACC  state_CLO  state_CON  state_ECO  state_ECR  \\\n",
      "0                    0      False      False      False      False      False   \n",
      "1                    0      False      False       True      False      False   \n",
      "2                    0      False      False      False      False      False   \n",
      "3                    0      False      False      False      False      False   \n",
      "4                    0      False      False      False      False      False   \n",
      "...                ...        ...        ...        ...        ...        ...   \n",
      "543154               3      False      False      False      False      False   \n",
      "543155               3      False      False       True      False      False   \n",
      "543156               3      False      False       True      False      False   \n",
      "543157               3      False      False      False      False      False   \n",
      "543158               3      False      False      False      False      False   \n",
      "\n",
      "        state_FIN  state_INT  state_MAS  state_PAR  state_REQ  state_RST  \\\n",
      "0           False       True      False      False      False      False   \n",
      "1           False      False      False      False      False      False   \n",
      "2            True      False      False      False      False      False   \n",
      "3            True      False      False      False      False      False   \n",
      "4           False       True      False      False      False      False   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "543154       True      False      False      False      False      False   \n",
      "543155      False      False      False      False      False      False   \n",
      "543156      False      False      False      False      False      False   \n",
      "543157       True      False      False      False      False      False   \n",
      "543158       True      False      False      False      False      False   \n",
      "\n",
      "        state_TST  state_TXD  state_URH  sport_category_0  sport_category_1  \\\n",
      "0           False      False      False                 0                 0   \n",
      "1           False      False      False                 0                 0   \n",
      "2           False      False      False                 0                 0   \n",
      "3           False      False      False                 0                 0   \n",
      "4           False      False      False                 0                 0   \n",
      "...           ...        ...        ...               ...               ...   \n",
      "543154      False      False      False                 0                 0   \n",
      "543155      False      False      False                 0                 0   \n",
      "543156      False      False      False                 0                 0   \n",
      "543157      False      False      False                 0                 0   \n",
      "543158      False      False      False                 0                 0   \n",
      "\n",
      "        sport_category_2  dsport_category_0  dsport_category_1  \\\n",
      "0                      0                  0                  0   \n",
      "1                      0                  1                  0   \n",
      "2                      0                  1                  0   \n",
      "3                      0                  1                  0   \n",
      "4                      0                  0                  0   \n",
      "...                  ...                ...                ...   \n",
      "543154                 0                  0                  1   \n",
      "543155                 0                  0                  1   \n",
      "543156                 0                  1                  0   \n",
      "543157                 0                  0                  1   \n",
      "543158                 0                  0                  1   \n",
      "\n",
      "        dsport_category_2  \n",
      "0                       0  \n",
      "1                       0  \n",
      "2                       0  \n",
      "3                       0  \n",
      "4                       0  \n",
      "...                   ...  \n",
      "543154                  0  \n",
      "543155                  0  \n",
      "543156                  0  \n",
      "543157                  0  \n",
      "543158                  0  \n",
      "\n",
      "[543159 rows x 64 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d96115-31f9-48cb-b3e6-7853d2d253cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                  srcip             dstip        dur  sbytes  dbytes  sttl  \\\n",
      "0         10.40.85.1_0       224.0.0.5_0  50.004341     384       0     1   \n",
      "1         59.166.0.6_0   149.171.126.4_0   0.001134     132     164    31   \n",
      "2       175.45.176.0_0  149.171.126.16_0   2.390390    1362     268   254   \n",
      "3         59.166.0.3_0   149.171.126.8_0  34.077175   37358    3380    31   \n",
      "4        10.40.170.2_0     10.40.170.2_0   0.000000      46       0     0   \n",
      "...                ...               ...        ...     ...     ...   ...   \n",
      "543154  175.45.176.1_3  149.171.126.11_3   0.291164     732     468   254   \n",
      "543155  175.45.176.3_3  149.171.126.16_3   0.011751      76     132   254   \n",
      "543156    59.166.0.2_3   149.171.126.4_3   0.002410     146     178    31   \n",
      "543157  175.45.176.1_3  149.171.126.11_3   0.176514   10778     268   254   \n",
      "543158    59.166.0.2_3   149.171.126.2_3   0.049598    2646   25564    31   \n",
      "\n",
      "        dttl  sloss  dloss          Sload         Dload  Spkts  Dpkts  swin  \\\n",
      "0          0      0      0      51.195557  0.000000e+00      6      0     0   \n",
      "1         29      0      0  465608.468800  5.784832e+05      2      2     0   \n",
      "2        252      6      1    4233.619141  7.496685e+02     14      6   255   \n",
      "3         29     18      8    8601.652344  7.747121e+02     52     42   255   \n",
      "4          0      0      0       0.000000  0.000000e+00      1      0     0   \n",
      "...      ...    ...    ...            ...           ...    ...    ...   ...   \n",
      "543154   252      3      2   18436.343750  1.126513e+04     12      8   255   \n",
      "543155    60      0      0   25870.138670  4.493235e+04      2      2     0   \n",
      "543156    29      0      0  242323.656300  2.954357e+05      2      2     0   \n",
      "543157   252      5      1  457980.656300  1.015217e+04     16      6   255   \n",
      "543158    29      7     15  416629.687500  4.029678e+06     42     44   255   \n",
      "\n",
      "        dwin       stcpb       dtcpb  smeansz  dmeansz  trans_depth  \\\n",
      "0          0           0           0       64        0            0   \n",
      "1          0           0           0       66       82            0   \n",
      "2        255  3897219059  2466816006       97       45            1   \n",
      "3        255  4063826787  4060853251      718       80            0   \n",
      "4          0           0           0       46        0            0   \n",
      "...      ...         ...         ...      ...      ...          ...   \n",
      "543154   255  2347481213  2217355451       61       59            0   \n",
      "543155     0           0           0       38       66            0   \n",
      "543156     0           0           0       73       89            0   \n",
      "543157   255  3578188515  1588977055      674       45            0   \n",
      "543158   255  3687522840  3703402068       63      581            0   \n",
      "\n",
      "        res_bdy_len           Sjit         Djit       Stime       Ltime  \\\n",
      "0                 0       0.000000     0.000000  1421927377  1421927427   \n",
      "1                 0       0.000000     0.000000  1421927414  1421927414   \n",
      "2                 0   18786.711400   941.724938  1421927414  1421927416   \n",
      "3                 0  128935.700600  4470.006000  1421927414  1421927448   \n",
      "4                 0       0.000000     0.000000  1421927415  1421927415   \n",
      "...             ...            ...          ...         ...         ...   \n",
      "543154            0    1332.515339    58.215816  1424250006  1424250006   \n",
      "543155            0       0.000000     0.000000  1424250007  1424250007   \n",
      "543156            0       0.000000     0.000000  1424250008  1424250008   \n",
      "543157            0     777.573179    53.911551  1424250008  1424250008   \n",
      "543158            0      81.659946    79.554793  1424250009  1424250009   \n",
      "\n",
      "             Sintpkt     Dintpkt    tcprtt    synack    ackdat  \\\n",
      "0       10000.869000    0.000000  0.000000  0.000000  0.000000   \n",
      "1           0.017000    0.013000  0.000000  0.000000  0.000000   \n",
      "2         183.579303  474.259406  0.066088  0.017959  0.048129   \n",
      "3         668.172887  831.135688  0.000782  0.000609  0.000173   \n",
      "4           0.000000    0.000000  0.000000  0.000000  0.000000   \n",
      "...              ...         ...       ...       ...       ...   \n",
      "543154     24.887364   40.483855  0.054361  0.007769  0.046592   \n",
      "543155      0.002000    0.001000  0.000000  0.000000  0.000000   \n",
      "543156      0.008000    0.002000  0.000000  0.000000  0.000000   \n",
      "543157     10.667534   33.587000  0.065586  0.008570  0.057016   \n",
      "543158      1.201171    1.139512  0.000738  0.000591  0.000147   \n",
      "\n",
      "        is_sm_ips_ports  ct_state_ttl  ct_flw_http_mthd  is_ftp_login  \\\n",
      "0                     0             0               0.0           0.0   \n",
      "1                     0             0               0.0           0.0   \n",
      "2                     0             1               1.0           0.0   \n",
      "3                     0             0               0.0           0.0   \n",
      "4                     1             2               0.0           0.0   \n",
      "...                 ...           ...               ...           ...   \n",
      "543154                0             1               0.0           0.0   \n",
      "543155                0             0               0.0           0.0   \n",
      "543156                0             0               0.0           0.0   \n",
      "543157                0             1               0.0           0.0   \n",
      "543158                0             0               0.0           0.0   \n",
      "\n",
      "        ct_ftp_cmd  ct_srv_src  ct_srv_dst  ct_dst_ltm  ct_src_ltm  \\\n",
      "0              0.0           2           4           4           2   \n",
      "1              0.0          12           7           1           2   \n",
      "2              0.0           5           2           2           1   \n",
      "3              0.0           1           1          12          10   \n",
      "4              0.0           2           2           2           2   \n",
      "...            ...         ...         ...         ...         ...   \n",
      "543154         0.0           1           1           1           1   \n",
      "543155         0.0           1           1           1           1   \n",
      "543156         0.0           3           5           3           2   \n",
      "543157         0.0           1           1           1           1   \n",
      "543158         0.0           6           2           4           7   \n",
      "\n",
      "        ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm      attack_cat  \\\n",
      "0                      2                 4               2          Normal   \n",
      "1                      2                 1               1          Normal   \n",
      "2                      1                 1               1  Reconnaissance   \n",
      "3                      1                 1               2          Normal   \n",
      "4                      2                 2               2          Normal   \n",
      "...                  ...               ...             ...             ...   \n",
      "543154                 1                 1               2         Generic   \n",
      "543155                 1                 1               1       Backdoors   \n",
      "543156                 2                 2               4          Normal   \n",
      "543157                 1                 1               2         Generic   \n",
      "543158                 1                 1               3          Normal   \n",
      "\n",
      "        source_file_id  state_ACC  state_CLO  state_CON  state_ECO  state_ECR  \\\n",
      "0                    0      False      False      False      False      False   \n",
      "1                    0      False      False       True      False      False   \n",
      "2                    0      False      False      False      False      False   \n",
      "3                    0      False      False      False      False      False   \n",
      "4                    0      False      False      False      False      False   \n",
      "...                ...        ...        ...        ...        ...        ...   \n",
      "543154               3      False      False      False      False      False   \n",
      "543155               3      False      False       True      False      False   \n",
      "543156               3      False      False       True      False      False   \n",
      "543157               3      False      False      False      False      False   \n",
      "543158               3      False      False      False      False      False   \n",
      "\n",
      "        state_FIN  state_INT  state_MAS  state_PAR  state_REQ  state_RST  \\\n",
      "0           False       True      False      False      False      False   \n",
      "1           False      False      False      False      False      False   \n",
      "2            True      False      False      False      False      False   \n",
      "3            True      False      False      False      False      False   \n",
      "4           False       True      False      False      False      False   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "543154       True      False      False      False      False      False   \n",
      "543155      False      False      False      False      False      False   \n",
      "543156      False      False      False      False      False      False   \n",
      "543157       True      False      False      False      False      False   \n",
      "543158       True      False      False      False      False      False   \n",
      "\n",
      "        state_TST  state_TXD  state_URH  sport_category_0  sport_category_1  \\\n",
      "0           False      False      False                 0                 0   \n",
      "1           False      False      False                 0                 0   \n",
      "2           False      False      False                 0                 0   \n",
      "3           False      False      False                 0                 0   \n",
      "4           False      False      False                 0                 0   \n",
      "...           ...        ...        ...               ...               ...   \n",
      "543154      False      False      False                 0                 0   \n",
      "543155      False      False      False                 0                 0   \n",
      "543156      False      False      False                 0                 0   \n",
      "543157      False      False      False                 0                 0   \n",
      "543158      False      False      False                 0                 0   \n",
      "\n",
      "        sport_category_2  dsport_category_0  dsport_category_1  \\\n",
      "0                      0                  0                  0   \n",
      "1                      0                  1                  0   \n",
      "2                      0                  1                  0   \n",
      "3                      0                  1                  0   \n",
      "4                      0                  0                  0   \n",
      "...                  ...                ...                ...   \n",
      "543154                 0                  0                  1   \n",
      "543155                 0                  0                  1   \n",
      "543156                 0                  1                  0   \n",
      "543157                 0                  0                  1   \n",
      "543158                 0                  0                  1   \n",
      "\n",
      "        dsport_category_2  \n",
      "0                       0  \n",
      "1                       0  \n",
      "2                       0  \n",
      "3                       0  \n",
      "4                       0  \n",
      "...                   ...  \n",
      "543154                  0  \n",
      "543155                  0  \n",
      "543156                  0  \n",
      "543157                  0  \n",
      "543158                  0  \n",
      "\n",
      "[543159 rows x 64 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "data.drop(columns=['index'],inplace=True)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df5c4c-70a2-4566-ae5e-ee3dcc6037a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 dur        sbytes        dbytes           sttl  \\\n",
      "count  543159.000000  5.431590e+05  5.431590e+05  543159.000000   \n",
      "mean        0.703562  5.129376e+03  1.912066e+04     157.223966   \n",
      "std        12.635598  1.202304e+05  1.382834e+05     108.429349   \n",
      "min         0.000000  0.000000e+00  0.000000e+00       0.000000   \n",
      "25%         0.000007  1.140000e+02  0.000000e+00      31.000000   \n",
      "50%         0.000010  2.000000e+02  0.000000e+00     254.000000   \n",
      "75%         0.070875  1.580000e+03  1.936000e+03     254.000000   \n",
      "max      8760.776367  1.435577e+07  1.465753e+07     255.000000   \n",
      "\n",
      "                dttl          sloss          dloss         Sload  \\\n",
      "count  543159.000000  543159.000000  543159.000000  5.431590e+05   \n",
      "mean       38.847354       3.789714       8.637535  6.901181e+07   \n",
      "std        77.059190      45.614073      49.869719  1.425974e+08   \n",
      "min         0.000000       0.000000       0.000000  0.000000e+00   \n",
      "25%         0.000000       0.000000       0.000000  3.760815e+05   \n",
      "50%         0.000000       0.000000       0.000000  4.560000e+07   \n",
      "75%        29.000000       3.000000       4.000000  8.888889e+07   \n",
      "max       254.000000    5319.000000    5507.000000  5.988000e+09   \n",
      "\n",
      "              Dload          Spkts          Dpkts           swin  \\\n",
      "count  5.431590e+05  543159.000000  543159.000000  543159.000000   \n",
      "mean   1.145602e+06      20.260456      22.792969      94.511841   \n",
      "std    3.125320e+06     101.785929     105.867367     123.158491   \n",
      "min    0.000000e+00       0.000000       0.000000       0.000000   \n",
      "25%    0.000000e+00       2.000000       0.000000       0.000000   \n",
      "50%    0.000000e+00       2.000000       0.000000       0.000000   \n",
      "75%    4.080209e+05      14.000000      14.000000     255.000000   \n",
      "max    2.248756e+07   10646.000000   11018.000000     255.000000   \n",
      "\n",
      "                dwin         stcpb         dtcpb        smeansz  \\\n",
      "count  543159.000000  5.431590e+05  5.431590e+05  543159.000000   \n",
      "mean       94.352626  7.940522e+08  7.948101e+08     110.262638   \n",
      "std       123.115721  1.281916e+09  1.282407e+09     155.475375   \n",
      "min         0.000000  0.000000e+00  0.000000e+00       0.000000   \n",
      "25%         0.000000  0.000000e+00  0.000000e+00      57.000000   \n",
      "50%         0.000000  0.000000e+00  0.000000e+00      60.000000   \n",
      "75%       255.000000  1.387505e+09  1.393886e+09     100.000000   \n",
      "max       255.000000  4.294959e+09  4.294940e+09    1504.000000   \n",
      "\n",
      "             dmeansz    trans_depth   res_bdy_len          Sjit  \\\n",
      "count  543159.000000  543159.000000  5.431590e+05  5.431590e+05   \n",
      "mean      148.050946       0.069015  2.332330e+03  2.081867e+03   \n",
      "std       281.014653       0.512010  4.365179e+04  2.573320e+04   \n",
      "min         0.000000       0.000000  0.000000e+00  0.000000e+00   \n",
      "25%         0.000000       0.000000  0.000000e+00  0.000000e+00   \n",
      "50%         0.000000       0.000000  0.000000e+00  0.000000e+00   \n",
      "75%        91.000000       0.000000  0.000000e+00  8.124514e+01   \n",
      "max      1486.000000     172.000000  6.558056e+06  1.181164e+06   \n",
      "\n",
      "                Djit        Sintpkt        Dintpkt         tcprtt  \\\n",
      "count  543159.000000  543159.000000  543159.000000  543159.000000   \n",
      "mean      467.571608     119.407237      56.920381       0.015438   \n",
      "std      2468.478591    1975.197825    1025.045205       0.053239   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.007000       0.000000       0.000000   \n",
      "50%         0.000000       0.009000       0.000000       0.000000   \n",
      "75%        31.116955       1.819236       1.276314       0.000675   \n",
      "max    781221.118300   84371.496000   52488.332000       4.564215   \n",
      "\n",
      "              synack         ackdat  is_sm_ips_ports   ct_state_ttl  \\\n",
      "count  543159.000000  543159.000000    543159.000000  543159.000000   \n",
      "mean        0.007763       0.007675         0.000786       1.096263   \n",
      "std         0.030023       0.026186         0.028027       0.975524   \n",
      "min         0.000000       0.000000         0.000000       0.000000   \n",
      "25%         0.000000       0.000000         0.000000       0.000000   \n",
      "50%         0.000000       0.000000         0.000000       1.000000   \n",
      "75%         0.000536       0.000134         0.000000       2.000000   \n",
      "max         3.387552       1.888074         1.000000       6.000000   \n",
      "\n",
      "       ct_flw_http_mthd   is_ftp_login     ct_ftp_cmd     ct_srv_src  \\\n",
      "count     543159.000000  543159.000000  543159.000000  543159.000000   \n",
      "mean           0.089263       0.011459       0.007661      15.025361   \n",
      "std            0.568852       0.109870       0.091356      14.239878   \n",
      "min            0.000000       0.000000       0.000000       1.000000   \n",
      "25%            0.000000       0.000000       0.000000       3.000000   \n",
      "50%            0.000000       0.000000       0.000000       9.000000   \n",
      "75%            0.000000       0.000000       0.000000      26.000000   \n",
      "max           36.000000       4.000000       4.000000      67.000000   \n",
      "\n",
      "          ct_srv_dst     ct_dst_ltm     ct_src_ltm  ct_src_dport_ltm  \\\n",
      "count  543159.000000  543159.000000  543159.000000     543159.000000   \n",
      "mean       14.853214      10.321932      10.848566          9.357573   \n",
      "std        14.314732      10.996982      10.976383         11.399195   \n",
      "min         1.000000       1.000000       1.000000          1.000000   \n",
      "25%         3.000000       2.000000       2.000000          1.000000   \n",
      "50%         8.000000       5.000000       6.000000          2.000000   \n",
      "75%        26.000000      17.000000      17.000000         17.000000   \n",
      "max        67.000000      67.000000      67.000000         67.000000   \n",
      "\n",
      "       ct_dst_sport_ltm  ct_dst_src_ltm  \n",
      "count     543159.000000   543159.000000  \n",
      "mean           7.219855       13.786578  \n",
      "std            8.074346       14.983005  \n",
      "min            1.000000        1.000000  \n",
      "25%            1.000000        1.000000  \n",
      "50%            2.000000        5.000000  \n",
      "75%           15.000000       26.000000  \n",
      "max           60.000000       67.000000  \n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_norm = UNSW_NB15_Config.COLS_TO_NORM\n",
    "print(data[cols_to_norm].describe()) # Check if there's any too large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea95177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All other columns processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_numeric_issues(df, cols_to_norm):\n",
    "    for col in cols_to_norm:\n",
    "        try:\n",
    "            # Try to coerce to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Try to clip the column\n",
    "            df[col] = df[col].clip(lower=-1e9, upper=1e9)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Column '{col}' failed with error: {e}\")\n",
    "            print(f\"  - Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "            print(f\"  - Data type: {df[col].dtype}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\nâœ… All other columns processed successfully.\")\n",
    "\n",
    "check_numeric_issues(data, UNSW_NB15_Config.COLS_TO_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37256006-abc1-44aa-8e74-46d05dc6ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "Attack label mapping: {'Analysis': 0, 'Backdoors': 1, 'DoS': 2, 'Exploits': 3, 'Fuzzers': 4, 'Generic': 5, 'Normal': 6, 'Reconnaissance': 7, 'Shellcode': 8, 'Worms': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_classes = 2\n",
    "class_map = [0, 1]\n",
    "if MULTICLASS:\n",
    "    le = LabelEncoder()\n",
    "    attack_labels = le.fit_transform(data[ATTACK_CLASS_COL_NAME])\n",
    "    class_map = le.classes_\n",
    "    print(class_map)\n",
    "    print(\"Attack label mapping:\", dict(zip(class_map, range(len(class_map)))))\n",
    "    data[ATTACK_CLASS_COL_NAME] = attack_labels\n",
    "    num_classes = len(class_map)\n",
    "    class_dict = {le.inverse_transform([i])[0]: i for i in range(len(le.classes_))}\n",
    "\n",
    "BENIGN_CLASS_LABEL = le.transform([BENIGN_CLASS_NAME])[0] if MULTICLASS else 0\n",
    "ADVERSARIAL_CLASS_LABEL = len(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f4cdd-2716-431f-af50-b34cc3d2d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'state_ACC', 'state_CLO', 'state_CON', 'state_ECO', 'state_ECR', 'state_FIN', 'state_INT', 'state_MAS', 'state_PAR', 'state_REQ', 'state_RST', 'state_TST', 'state_TXD', 'state_URH', 'sport_category_0', 'sport_category_1', 'sport_category_2', 'dsport_category_0', 'dsport_category_1', 'dsport_category_2']\n",
      "Number of training samples: 461685\n",
      "attack_cat\n",
      "6    188595\n",
      "5    183159\n",
      "3     37846\n",
      "4     20609\n",
      "2     13900\n",
      "7     11889\n",
      "0      2275\n",
      "1      1980\n",
      "8      1284\n",
      "9       148\n",
      "Name: count, dtype: int64\n",
      "Number of test samples: 81474\n",
      "attack_cat\n",
      "6    33281\n",
      "5    32322\n",
      "3     6679\n",
      "4     3637\n",
      "2     2453\n",
      "7     2098\n",
      "0      402\n",
      "1      349\n",
      "8      227\n",
      "9       26\n",
      "Name: count, dtype: int64\n",
      "                 srcip             dstip       dur    sbytes    dbytes  \\\n",
      "35798   175.45.176.0_0  149.171.126.11_0  0.024440 -0.037756 -0.136334   \n",
      "454500    59.166.0.2_3   149.171.126.1_3 -0.052447 -0.010009  0.271236   \n",
      "23187   175.45.176.0_0  149.171.126.18_0 -0.017379 -0.033264 -0.136334   \n",
      "489755  175.45.176.3_3  149.171.126.15_3 -0.055680 -0.041715 -0.138272   \n",
      "134305    59.166.0.1_1   149.171.126.4_1 -0.004966  0.269172 -0.113829   \n",
      "\n",
      "            sttl      dttl     sloss     dloss     Sload     Dload     Spkts  \\\n",
      "35798   0.892527  2.766092 -0.039236 -0.153150 -0.554662 -0.365989 -0.100804   \n",
      "454500 -1.164114 -0.127790  0.070379  0.368209 -0.548462  3.128362  0.449370   \n",
      "23187   0.892527  2.766092 -0.039236 -0.153150 -0.554559 -0.365371 -0.100804   \n",
      "489755  0.892527 -0.504124 -0.083082 -0.173202 -0.018194 -0.366555 -0.179401   \n",
      "134305 -1.164114 -0.127790  0.311533 -0.012784 -0.550916 -0.353373  0.311827   \n",
      "\n",
      "           Dpkts      swin      dwin     stcpb     dtcpb   smeansz   dmeansz  \\\n",
      "35798  -0.158623  1.303104  1.304850  0.950180 -0.462170 -0.329716 -0.366711   \n",
      "454500  0.427016  1.303104  1.304850  0.274400  0.303777 -0.329716  2.437416   \n",
      "23187  -0.158623  1.303104  1.304850  1.484216  1.484299  0.017606 -0.366711   \n",
      "489755 -0.215298 -0.767401 -0.766374 -0.720896 -0.720905 -0.342580 -0.526845   \n",
      "134305  0.181426  1.303104  1.304850  1.484216 -0.715505  3.928197 -0.242162   \n",
      "\n",
      "        trans_depth  res_bdy_len      Sjit      Djit       Stime       Ltime  \\\n",
      "35798     -0.134792     -0.05343  0.153264 -0.076895  1421933745  1421933746   \n",
      "454500    -0.134792     -0.05343 -0.080902 -0.175606  1424240533  1424240533   \n",
      "23187     -0.134792     -0.05343  0.012423 -0.150894  1421931706  1421931707   \n",
      "489755    -0.134792     -0.05343 -0.080902 -0.189417  1424245264  1424245264   \n",
      "134305    -0.134792     -0.05343 -0.043454 -0.177189  1424222454  1424222454   \n",
      "\n",
      "         Sintpkt   Dintpkt    tcprtt    synack    ackdat  is_sm_ips_ports  \\\n",
      "35798  -0.008638  0.119318  3.717742  3.612803  3.416380        -0.028049   \n",
      "454500 -0.060137 -0.054933 -0.278494 -0.242636 -0.288018        -0.028049   \n",
      "23187  -0.036975  0.019956  2.899294  2.974987  2.483664        -0.028049   \n",
      "489755 -0.060450 -0.055530 -0.289971 -0.258558 -0.293097        -0.028049   \n",
      "134305 -0.053968 -0.040295 -0.277104 -0.239905 -0.288324        -0.028049   \n",
      "\n",
      "        ct_state_ttl  ct_flw_http_mthd  is_ftp_login  ct_ftp_cmd  ct_srv_src  \\\n",
      "35798      -0.098678         -0.156918     -0.104295   -0.083856   -0.633809   \n",
      "454500     -1.123769         -0.156918     -0.104295   -0.083856   -0.774260   \n",
      "23187      -0.098678         -0.156918     -0.104295   -0.083856   -0.984936   \n",
      "489755      0.926413         -0.156918     -0.104295   -0.083856    0.489797   \n",
      "134305     -1.123769         -0.156918     -0.104295   -0.083856   -0.984936   \n",
      "\n",
      "        ct_srv_dst  ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  \\\n",
      "35798    -0.618469   -0.756748   -0.806147         -0.733173   \n",
      "454500   -0.688327   -0.574879   -0.715042         -0.733173   \n",
      "23187    -0.967760   -0.483945   -0.715042         -0.733173   \n",
      "489755    0.499261    1.061935    1.015949          1.109064   \n",
      "134305   -0.967760   -0.483945   -0.441727         -0.733173   \n",
      "\n",
      "        ct_dst_sport_ltm  ct_dst_src_ltm  attack_cat  source_file_id  \\\n",
      "35798          -0.770324       -0.786664           4               0   \n",
      "454500         -0.770324       -0.786664           6               3   \n",
      "23187          -0.770324       -0.853406           3               0   \n",
      "489755          1.830509        0.548183           5               3   \n",
      "134305         -0.770324       -0.786664           6               1   \n",
      "\n",
      "        state_ACC  state_CLO  state_CON  state_ECO  state_ECR  state_FIN  \\\n",
      "35798       False      False      False      False      False       True   \n",
      "454500      False      False      False      False      False       True   \n",
      "23187       False      False      False      False      False       True   \n",
      "489755      False      False      False      False      False      False   \n",
      "134305      False      False      False      False      False       True   \n",
      "\n",
      "        state_INT  state_MAS  state_PAR  state_REQ  state_RST  state_TST  \\\n",
      "35798       False      False      False      False      False      False   \n",
      "454500      False      False      False      False      False      False   \n",
      "23187       False      False      False      False      False      False   \n",
      "489755       True      False      False      False      False      False   \n",
      "134305      False      False      False      False      False      False   \n",
      "\n",
      "        state_TXD  state_URH  sport_category_0  sport_category_1  \\\n",
      "35798       False      False                 0                 0   \n",
      "454500      False      False                 0                 0   \n",
      "23187       False      False                 0                 0   \n",
      "489755      False      False                 0                 0   \n",
      "134305      False      False                 0                 0   \n",
      "\n",
      "        sport_category_2  dsport_category_0  dsport_category_1  \\\n",
      "35798                  0                  0                  0   \n",
      "454500                 0                  0                  1   \n",
      "23187                  0                  0                  0   \n",
      "489755                 0                  1                  0   \n",
      "134305                 0                  1                  0   \n",
      "\n",
      "        dsport_category_2                                                  h  \n",
      "35798                   0  [0.024439518249307186, -0.03775569116632746, -...  \n",
      "454500                  0  [-0.05244682114681552, -0.01000892950063232, 0...  \n",
      "23187                   0  [-0.01737919714689172, -0.03326430888231206, -...  \n",
      "489755                  0  [-0.05568046603287485, -0.04171476147594104, -...  \n",
      "134305                  0  [-0.004966318747785316, 0.269172066323925, -0....  \n"
     ]
    }
   ],
   "source": [
    "# 70% train, 15% validation, 15% test\n",
    "train_full_df, test_df = train_test_split(\n",
    "     data, test_size=0.15, random_state=42, stratify=data[label_col])\n",
    "\n",
    "\n",
    "feature_cols = UNSW_NB15_Config.COLS_TO_NORM + converted_categorical_cols\n",
    "\n",
    "print('Feature Columns:', feature_cols)\n",
    "\n",
    "train_full_df['h'] = train_full_df[ feature_cols ].values.tolist()\n",
    "test_df['h'] = test_df[ feature_cols ].values.tolist()\n",
    "\n",
    "y_train = train_full_df[label_col]\n",
    "y_test = test_df[label_col]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_full_df))\n",
    "print(y_train.value_counts())\n",
    "print(\"Number of test samples:\", len(test_df))\n",
    "print(y_test.value_counts())\n",
    "\n",
    "print(train_full_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83bd73-531f-42a0-9f73-e1fd98dce1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, source_ip_col, destination_ip_col, edge_attr, create_using=nx.MultiDiGraph(), **kwargs):\n",
    "    G_nx = nx.from_pandas_edgelist(df, source_ip_col, destination_ip_col, edge_attr, create_using=create_using, **kwargs)\n",
    "    G_pyg = from_networkx(G_nx)\n",
    "\n",
    "    num_nodes = G_pyg.num_nodes\n",
    "    num_edges = G_pyg.num_edges\n",
    "\n",
    "    G_pyg.x = th.ones(num_nodes, len(df['h'].iloc[0])) \n",
    "\n",
    "    edge_attr_list = []\n",
    "    edge_label_list = []\n",
    "\n",
    "    for u, v, key, data in G_nx.edges(keys=True, data=True):\n",
    "        edge_attr_list.append(data['h']) \n",
    "        edge_label_list.append(data[label_col]) \n",
    "\n",
    "    G_pyg.edge_attr = th.tensor(edge_attr_list, dtype=th.float32)\n",
    "    G_pyg.edge_label = th.tensor(edge_label_list, dtype=th.long)\n",
    "\n",
    "    print(\"Number of edges in G_pyg:\", num_edges)\n",
    "    print(\"Number of node in G_pyg:\", num_nodes)\n",
    "    print(\"Shape of node in G_pyg:\", G_pyg.x.shape)\n",
    "    print(\"Shape of edge attr in G_pyg:\", G_pyg.edge_attr.shape)\n",
    "    print(\"Shape of edge label in G_pyg:\", G_pyg.edge_label.shape)\n",
    "\n",
    "    return G_nx, G_pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f9cdb-1316-461a-a927-8d67d90d6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 175\n",
      "Shape of node in G_pyg: torch.Size([175, 58])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 58])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41795339-6036-468f-9b9d-2bb68d78ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGELayerPyG(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_dim, out_channels, activation=F.relu):\n",
    "        super().__init__(aggr='mean')  # mean aggregation\n",
    "        self.W_msg = nn.Linear(in_channels + edge_dim, out_channels)\n",
    "        self.W_apply = nn.Linear(in_channels + out_channels, out_channels)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: [num_nodes, in_channels]\n",
    "        # edge_attr: [num_edges, edge_dim]\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j: features of source nodes (neighbours)\n",
    "        msg_input = th.cat([x_j, edge_attr], dim=1)\n",
    "        return self.W_msg(msg_input)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out: [num_nodes, out_channels]\n",
    "        combined = th.cat([x, aggr_out], dim=1)\n",
    "        out = self.W_apply(combined)\n",
    "        return self.activation(out)\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.lin = nn.Linear(in_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, data, z):\n",
    "        row, col = data.edge_index\n",
    "        # Concatenate the features of source and target nodes for each edge\n",
    "        edge_feat = th.cat([z[row], z[col]], dim=1)\n",
    "        return self.lin(edge_feat)\n",
    "\n",
    "class EGraphSAGE(nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels, out_channels, dropout=0.2):\n",
    "        super(EGraphSAGE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGELayerPyG(node_in_channels, edge_in_channels, hidden_channels)\n",
    "        self.conv2 = SAGELayerPyG(hidden_channels, edge_in_channels, hidden_channels)\n",
    "        self.mlp_predictor = MLPPredictor(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return self.mlp_predictor(data, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca25fef-29d9-40cf-8910-16b24d530693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccdc850-b98d-4836-b82b-67aa4b9e1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89157faf-e24b-49d6-9c90-6f71dae515b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d37f0-713b-4abc-8d7a-3e768ae9a2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 307790\n",
      "Number of node in G_pyg: 178\n",
      "Shape of node in G_pyg: torch.Size([178, 58])\n",
      "Shape of edge attr in G_pyg: torch.Size([307790, 58])\n",
      "Shape of edge label in G_pyg: torch.Size([307790])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    114\u001b[39m hidden_dims = [\u001b[32m128\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m512\u001b[39m]\n\u001b[32m    115\u001b[39m drop_outs = [\u001b[32m0.2\u001b[39m, \u001b[32m0.3\u001b[39m, \u001b[32m0.4\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_full_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_outs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_outs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mgrid_search\u001b[39m\u001b[34m(data, epochs, learning_rates, hidden_dims, drop_outs)\u001b[39m\n\u001b[32m     16\u001b[39m val_df = data.iloc[val_idx]\n\u001b[32m     18\u001b[39m G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, [\u001b[33m'\u001b[39m\u001b[33mh\u001b[39m\u001b[33m'\u001b[39m, label_col], create_using=nx.MultiDiGraph())\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m G_nx_val, G_pyg_val = \u001b[43mcreate_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOURCE_IP_COL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDESTINATION_IP_COL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mh\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_using\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMultiDiGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m G_pyg_train = G_pyg_train.to(device)\n\u001b[32m     22\u001b[39m G_pyg_val = G_pyg_val.to(device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mcreate_graph\u001b[39m\u001b[34m(df, source_ip_col, destination_ip_col, edge_attr, create_using, **kwargs)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_graph\u001b[39m(df, source_ip_col, destination_ip_col, edge_attr, create_using=nx.MultiDiGraph(), **kwargs):\n\u001b[32m      2\u001b[39m     G_nx = nx.from_pandas_edgelist(df, source_ip_col, destination_ip_col, edge_attr, create_using=create_using, **kwargs)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     G_pyg = \u001b[43mfrom_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG_nx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     num_nodes = G_pyg.num_nodes\n\u001b[32m      6\u001b[39m     num_edges = G_pyg.num_edges\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/torch_geometric/utils/convert.py:278\u001b[39m, in \u001b[36mfrom_networkx\u001b[39m\u001b[34m(G, group_node_attrs, group_edge_attrs)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m         data_dict[key] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    280\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def grid_search(data, epochs, learning_rates, hidden_dims, drop_outs):\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_params = {}\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Precompute the train and validation graphs for all folds\n",
    "    folds = []\n",
    "    for train_idx, val_idx in skf.split(data, data[label_col]):\n",
    "        train_df = data.iloc[train_idx]\n",
    "        val_df = data.iloc[val_idx]\n",
    "\n",
    "        G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "        G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "\n",
    "        G_pyg_train = G_pyg_train.to(device)\n",
    "        G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "        G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "        G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "        G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "        G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "        folds.append((G_pyg_train, G_pyg_val))\n",
    "\n",
    "    params_results = {}\n",
    "    for lr in learning_rates:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for drop_out in drop_outs:\n",
    "                print(f\"Testing with learning rate: {lr}, hidden_dim: {hidden_dim}, drop_out: {drop_out}\")\n",
    "                fold_f1_scores = []\n",
    "\n",
    "                for fold, (G_pyg_train, G_pyg_val) in enumerate(folds):\n",
    "                    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "                    model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                                    edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                                    hidden_channels=hidden_dim,\n",
    "                                    dropout=drop_out,\n",
    "                                    out_channels=num_classes).to(device)\n",
    "\n",
    "                    model.apply(init_weights)\n",
    "\n",
    "                    labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "                    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                                    classes=np.unique(labels),\n",
    "                                                                    y=labels)\n",
    "\n",
    "                    # Normalize to stabilize training\n",
    "                    class_weights = th.FloatTensor(class_weights).to(device)\n",
    "                    print(\"Class weights:\", class_weights)\n",
    "\n",
    "                    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "                    optimizer = th.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    best_epoch_f1 = 0  # Track the best F1 score for this fold\n",
    "\n",
    "                    for epoch in range(epochs):\n",
    "                        train_loss = 0\n",
    "                        val_loss = 0\n",
    "\n",
    "                        try:\n",
    "                            model.train()\n",
    "                            out = model(G_pyg_train)\n",
    "                            \n",
    "                            optimizer.zero_grad()\n",
    "                            loss = criterion(out, G_pyg_train.edge_label)\n",
    "                            train_loss = loss.item()\n",
    "\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                            model.eval()\n",
    "                            with th.no_grad():\n",
    "                                out = model(G_pyg_val)\n",
    "                                loss = criterion(out, G_pyg_val.edge_label)\n",
    "                                val_loss = loss.item()\n",
    "\n",
    "                            val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "\n",
    "                            if val_f1 > best_epoch_f1:\n",
    "                                best_epoch_f1 = val_f1  # Update the best F1 score for this fold\n",
    "                                print(f\"Best F1 Score at epoch {epoch}: {best_epoch_f1:.4f}, Parameters: lr={lr}, hidden_dim{hidden_dim}, drop_out={drop_out}\")\n",
    "\n",
    "                            if epoch % 100 == 0:\n",
    "                                print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred at epoch {epoch}: {str(e)}\")\n",
    "                            break\n",
    "\n",
    "                    fold_f1_scores.append(best_epoch_f1)  # Append the best F1 score for this fold\n",
    "                \n",
    "                avg_f1 = sum(fold_f1_scores) / len(fold_f1_scores)\n",
    "                params_results[(drop_out, lr, hidden_dim)] = {'folds': fold_f1_scores, 'avg_f1': avg_f1}\n",
    "                print(\"Current Results: \", params_results)\n",
    "                print(f\"Average F1 Score for dropout {drop_out}, learning rate {lr}, hidden_dim {hidden_dim}: {avg_f1:.4f}\")\n",
    "\n",
    "                if avg_f1 > best_f1:\n",
    "                    best_f1 = avg_f1\n",
    "                    best_params = {'learning_rate': lr, 'hidden_dim': hidden_dim, 'drop_out': drop_out}\n",
    "\n",
    "        print(f\"Best Parameters: {best_params}, Best F1 Score: {best_f1:.4f}\")\n",
    "        print(\"All results:\", params_results)\n",
    "\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "hidden_dims = [128, 256, 512]\n",
    "drop_outs = [0.2, 0.3, 0.4]\n",
    "\n",
    "grid_search(train_full_df, epochs=100, learning_rates=learning_rates, hidden_dims=hidden_dims, drop_outs=drop_outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b2fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 392432\n",
      "Number of node in G_pyg: 179\n",
      "Shape of node in G_pyg: torch.Size([179, 58])\n",
      "Shape of edge attr in G_pyg: torch.Size([392432, 58])\n",
      "Shape of edge label in G_pyg: torch.Size([392432])\n",
      "Number of edges in G_pyg: 69253\n",
      "Number of node in G_pyg: 173\n",
      "Shape of node in G_pyg: torch.Size([173, 58])\n",
      "Shape of edge attr in G_pyg: torch.Size([69253, 58])\n",
      "Shape of edge label in G_pyg: torch.Size([69253])\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "     train_full_df, test_size=0.15, random_state=42, stratify=train_full_df[label_col])\n",
    "\n",
    "G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([2.0291e+01, 2.3317e+01, 3.3215e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8832e+00, 3.5970e+01, 3.1145e+02], device='cuda:0')\n",
      "Resumed training from epoch 661\n",
      "Epoch 661, Train Loss: 1.5641, Validation Loss: 1.7131, Validation F1: 0.8186\n",
      "Epoch 662, Train Loss: 1.5647, Validation Loss: 1.7061, Validation F1: 0.8162\n",
      "Epoch 663, Train Loss: 1.5640, Validation Loss: 1.6988, Validation F1: 0.8186\n",
      "Epoch 664, Train Loss: 1.5632, Validation Loss: 1.6988, Validation F1: 0.8176\n",
      "Epoch 665, Train Loss: 1.5638, Validation Loss: 1.6824, Validation F1: 0.8240\n",
      "Epoch 666, Train Loss: 1.5642, Validation Loss: 1.6916, Validation F1: 0.8219\n",
      "Epoch 667, Train Loss: 1.5633, Validation Loss: 1.6924, Validation F1: 0.8229\n",
      "Epoch 668, Train Loss: 1.5639, Validation Loss: 1.7048, Validation F1: 0.8237\n",
      "Epoch 669, Train Loss: 1.5635, Validation Loss: 1.7136, Validation F1: 0.8190\n",
      "Epoch 670, Train Loss: 1.5631, Validation Loss: 1.7006, Validation F1: 0.8195\n",
      "Epoch 671, Train Loss: 1.5628, Validation Loss: 1.6948, Validation F1: 0.8150\n",
      "Epoch 672, Train Loss: 1.5627, Validation Loss: 1.6953, Validation F1: 0.8165\n",
      "Epoch 673, Train Loss: 1.5656, Validation Loss: 1.6799, Validation F1: 0.8160\n",
      "Epoch 674, Train Loss: 1.5636, Validation Loss: 1.7058, Validation F1: 0.8195\n",
      "Epoch 675, Train Loss: 1.5628, Validation Loss: 1.7077, Validation F1: 0.8188\n",
      "Epoch 676, Train Loss: 1.5638, Validation Loss: 1.7174, Validation F1: 0.8208\n",
      "Epoch 677, Train Loss: 1.5648, Validation Loss: 1.7151, Validation F1: 0.8153\n",
      "Epoch 678, Train Loss: 1.5683, Validation Loss: 1.7038, Validation F1: 0.8151\n",
      "Epoch 679, Train Loss: 1.5627, Validation Loss: 1.6928, Validation F1: 0.8159\n",
      "Epoch 680, Train Loss: 1.5657, Validation Loss: 1.6773, Validation F1: 0.8147\n",
      "Epoch 681, Train Loss: 1.5638, Validation Loss: 1.7113, Validation F1: 0.8175\n",
      "Epoch 682, Train Loss: 1.5631, Validation Loss: 1.7273, Validation F1: 0.8209\n",
      "Epoch 683, Train Loss: 1.5675, Validation Loss: 1.7086, Validation F1: 0.8196\n",
      "Epoch 684, Train Loss: 1.5645, Validation Loss: 1.7040, Validation F1: 0.8185\n",
      "Epoch 685, Train Loss: 1.5642, Validation Loss: 1.6971, Validation F1: 0.8148\n",
      "Epoch 686, Train Loss: 1.5651, Validation Loss: 1.6933, Validation F1: 0.8136\n",
      "Epoch 687, Train Loss: 1.5638, Validation Loss: 1.7150, Validation F1: 0.8169\n",
      "Epoch 688, Train Loss: 1.5643, Validation Loss: 1.7165, Validation F1: 0.8170\n",
      "Epoch 689, Train Loss: 1.5656, Validation Loss: 1.7059, Validation F1: 0.8194\n",
      "Epoch 690, Train Loss: 1.5644, Validation Loss: 1.6953, Validation F1: 0.8221\n",
      "Epoch 691, Train Loss: 1.5644, Validation Loss: 1.6953, Validation F1: 0.8194\n",
      "Epoch 692, Train Loss: 1.5629, Validation Loss: 1.6866, Validation F1: 0.8147\n",
      "Epoch 693, Train Loss: 1.5635, Validation Loss: 1.6837, Validation F1: 0.8147\n",
      "Epoch 694, Train Loss: 1.5678, Validation Loss: 1.7014, Validation F1: 0.8148\n",
      "Epoch 695, Train Loss: 1.5631, Validation Loss: 1.6917, Validation F1: 0.8168\n",
      "Epoch 696, Train Loss: 1.5640, Validation Loss: 1.7017, Validation F1: 0.8188\n",
      "Epoch 697, Train Loss: 1.5647, Validation Loss: 1.6801, Validation F1: 0.8240\n",
      "Epoch 698, Train Loss: 1.5659, Validation Loss: 1.6917, Validation F1: 0.8196\n",
      "Epoch 699, Train Loss: 1.5640, Validation Loss: 1.7067, Validation F1: 0.8179\n",
      "Epoch 700, Train Loss: 1.5626, Validation Loss: 1.7076, Validation F1: 0.8185\n",
      "Epoch 701, Train Loss: 1.5636, Validation Loss: 1.7232, Validation F1: 0.8160\n",
      "Epoch 702, Train Loss: 1.5634, Validation Loss: 1.6957, Validation F1: 0.8166\n",
      "Epoch 703, Train Loss: 1.5635, Validation Loss: 1.6884, Validation F1: 0.8218\n",
      "Epoch 704, Train Loss: 1.5632, Validation Loss: 1.7046, Validation F1: 0.8213\n",
      "Epoch 705, Train Loss: 1.5635, Validation Loss: 1.6904, Validation F1: 0.8172\n",
      "Epoch 706, Train Loss: 1.5636, Validation Loss: 1.6963, Validation F1: 0.8185\n",
      "Epoch 707, Train Loss: 1.5627, Validation Loss: 1.7162, Validation F1: 0.8185\n",
      "Epoch 708, Train Loss: 1.5637, Validation Loss: 1.6955, Validation F1: 0.8155\n",
      "Epoch 709, Train Loss: 1.5629, Validation Loss: 1.7006, Validation F1: 0.8151\n",
      "Epoch 710, Train Loss: 1.5623, Validation Loss: 1.6990, Validation F1: 0.8174\n",
      "Epoch 711, Train Loss: 1.5626, Validation Loss: 1.6959, Validation F1: 0.8185\n",
      "Epoch 712, Train Loss: 1.5634, Validation Loss: 1.7107, Validation F1: 0.8194\n",
      "Epoch 713, Train Loss: 1.5625, Validation Loss: 1.6905, Validation F1: 0.8178\n",
      "Epoch 714, Train Loss: 1.5632, Validation Loss: 1.6975, Validation F1: 0.8175\n",
      "Epoch 715, Train Loss: 1.5629, Validation Loss: 1.7001, Validation F1: 0.8172\n",
      "Epoch 716, Train Loss: 1.5628, Validation Loss: 1.6813, Validation F1: 0.8151\n",
      "Epoch 717, Train Loss: 1.5635, Validation Loss: 1.7079, Validation F1: 0.8178\n",
      "Epoch 718, Train Loss: 1.5631, Validation Loss: 1.7362, Validation F1: 0.8164\n",
      "Epoch 719, Train Loss: 1.5648, Validation Loss: 1.7257, Validation F1: 0.8166\n",
      "Epoch 720, Train Loss: 1.5637, Validation Loss: 1.7042, Validation F1: 0.8222\n",
      "Epoch 721, Train Loss: 1.5630, Validation Loss: 1.6989, Validation F1: 0.8195\n",
      "Epoch 722, Train Loss: 1.5646, Validation Loss: 1.7164, Validation F1: 0.8168\n",
      "Epoch 723, Train Loss: 1.5622, Validation Loss: 1.7099, Validation F1: 0.8208\n",
      "Epoch 724, Train Loss: 1.5645, Validation Loss: 1.7070, Validation F1: 0.8184\n",
      "Epoch 725, Train Loss: 1.5651, Validation Loss: 1.6945, Validation F1: 0.8123\n",
      "Epoch 726, Train Loss: 1.5638, Validation Loss: 1.7057, Validation F1: 0.8173\n",
      "Epoch 727, Train Loss: 1.5627, Validation Loss: 1.7040, Validation F1: 0.8219\n",
      "Epoch 728, Train Loss: 1.5638, Validation Loss: 1.7062, Validation F1: 0.8206\n",
      "Epoch 729, Train Loss: 1.5621, Validation Loss: 1.6953, Validation F1: 0.8221\n",
      "Epoch 730, Train Loss: 1.5628, Validation Loss: 1.6816, Validation F1: 0.8240\n",
      "Epoch 731, Train Loss: 1.5639, Validation Loss: 1.6962, Validation F1: 0.8183\n",
      "Epoch 732, Train Loss: 1.5634, Validation Loss: 1.6907, Validation F1: 0.8173\n",
      "Epoch 733, Train Loss: 1.5634, Validation Loss: 1.7007, Validation F1: 0.8139\n",
      "Epoch 734, Train Loss: 1.5631, Validation Loss: 1.6994, Validation F1: 0.8149\n",
      "Epoch 735, Train Loss: 1.5633, Validation Loss: 1.6816, Validation F1: 0.8202\n",
      "Epoch 736, Train Loss: 1.5644, Validation Loss: 1.6974, Validation F1: 0.8173\n",
      "Epoch 737, Train Loss: 1.5629, Validation Loss: 1.7097, Validation F1: 0.8189\n",
      "Epoch 738, Train Loss: 1.5634, Validation Loss: 1.6976, Validation F1: 0.8203\n",
      "Epoch 739, Train Loss: 1.5626, Validation Loss: 1.6950, Validation F1: 0.8119\n",
      "Epoch 740, Train Loss: 1.5630, Validation Loss: 1.6952, Validation F1: 0.8153\n",
      "Epoch 741, Train Loss: 1.5625, Validation Loss: 1.7084, Validation F1: 0.8172\n",
      "Epoch 742, Train Loss: 1.5614, Validation Loss: 1.7148, Validation F1: 0.8198\n",
      "Epoch 743, Train Loss: 1.5627, Validation Loss: 1.6948, Validation F1: 0.8173\n",
      "Epoch 744, Train Loss: 1.5626, Validation Loss: 1.6963, Validation F1: 0.8197\n",
      "Epoch 745, Train Loss: 1.5615, Validation Loss: 1.6998, Validation F1: 0.8201\n",
      "Epoch 746, Train Loss: 1.5632, Validation Loss: 1.7008, Validation F1: 0.8129\n",
      "Epoch 747, Train Loss: 1.5623, Validation Loss: 1.7220, Validation F1: 0.8179\n",
      "Epoch 748, Train Loss: 1.5633, Validation Loss: 1.7193, Validation F1: 0.8163\n",
      "Epoch 749, Train Loss: 1.5638, Validation Loss: 1.7225, Validation F1: 0.8173\n",
      "Epoch 750, Train Loss: 1.5629, Validation Loss: 1.7065, Validation F1: 0.8197\n",
      "Epoch 751, Train Loss: 1.5633, Validation Loss: 1.6843, Validation F1: 0.8196\n",
      "Epoch 752, Train Loss: 1.5631, Validation Loss: 1.6838, Validation F1: 0.8152\n",
      "Epoch 753, Train Loss: 1.5647, Validation Loss: 1.7109, Validation F1: 0.8158\n",
      "Epoch 754, Train Loss: 1.5616, Validation Loss: 1.7217, Validation F1: 0.8182\n",
      "Epoch 755, Train Loss: 1.5633, Validation Loss: 1.7224, Validation F1: 0.8173\n",
      "Epoch 756, Train Loss: 1.5627, Validation Loss: 1.7259, Validation F1: 0.8190\n",
      "Epoch 757, Train Loss: 1.5647, Validation Loss: 1.7029, Validation F1: 0.8204\n",
      "Epoch 758, Train Loss: 1.5646, Validation Loss: 1.6987, Validation F1: 0.8145\n",
      "Epoch 759, Train Loss: 1.5644, Validation Loss: 1.6903, Validation F1: 0.8157\n",
      "Epoch 760, Train Loss: 1.5646, Validation Loss: 1.7085, Validation F1: 0.8174\n",
      "Epoch 761, Train Loss: 1.5628, Validation Loss: 1.7061, Validation F1: 0.8187\n",
      "Epoch 762, Train Loss: 1.5630, Validation Loss: 1.7020, Validation F1: 0.8162\n",
      "Epoch 763, Train Loss: 1.5657, Validation Loss: 1.6929, Validation F1: 0.8191\n",
      "Epoch 764, Train Loss: 1.5637, Validation Loss: 1.6808, Validation F1: 0.8187\n",
      "Epoch 765, Train Loss: 1.5628, Validation Loss: 1.6869, Validation F1: 0.8192\n",
      "Epoch 766, Train Loss: 1.5652, Validation Loss: 1.7028, Validation F1: 0.8203\n",
      "Epoch 767, Train Loss: 1.5626, Validation Loss: 1.7192, Validation F1: 0.8184\n",
      "Epoch 768, Train Loss: 1.5623, Validation Loss: 1.7038, Validation F1: 0.8162\n",
      "Epoch 769, Train Loss: 1.5626, Validation Loss: 1.7146, Validation F1: 0.8178\n",
      "Epoch 770, Train Loss: 1.5625, Validation Loss: 1.7058, Validation F1: 0.8134\n",
      "Epoch 771, Train Loss: 1.5622, Validation Loss: 1.7066, Validation F1: 0.8155\n",
      "Epoch 772, Train Loss: 1.5622, Validation Loss: 1.6901, Validation F1: 0.8170\n",
      "Epoch 773, Train Loss: 1.5627, Validation Loss: 1.6991, Validation F1: 0.8185\n",
      "Epoch 774, Train Loss: 1.5624, Validation Loss: 1.7052, Validation F1: 0.8192\n",
      "Epoch 775, Train Loss: 1.5619, Validation Loss: 1.7091, Validation F1: 0.8167\n",
      "Epoch 776, Train Loss: 1.5619, Validation Loss: 1.7036, Validation F1: 0.8185\n",
      "Epoch 777, Train Loss: 1.5637, Validation Loss: 1.7082, Validation F1: 0.8182\n",
      "Epoch 778, Train Loss: 1.5616, Validation Loss: 1.6931, Validation F1: 0.8186\n",
      "Epoch 779, Train Loss: 1.5632, Validation Loss: 1.7053, Validation F1: 0.8181\n",
      "Epoch 780, Train Loss: 1.5620, Validation Loss: 1.7107, Validation F1: 0.8190\n",
      "Epoch 781, Train Loss: 1.5613, Validation Loss: 1.7265, Validation F1: 0.8185\n",
      "Epoch 782, Train Loss: 1.5612, Validation Loss: 1.7253, Validation F1: 0.8205\n",
      "Epoch 783, Train Loss: 1.5630, Validation Loss: 1.7250, Validation F1: 0.8177\n",
      "Epoch 784, Train Loss: 1.5639, Validation Loss: 1.7172, Validation F1: 0.8168\n",
      "Epoch 785, Train Loss: 1.5619, Validation Loss: 1.7021, Validation F1: 0.8160\n",
      "Epoch 786, Train Loss: 1.5629, Validation Loss: 1.7023, Validation F1: 0.8154\n",
      "Epoch 787, Train Loss: 1.5624, Validation Loss: 1.7132, Validation F1: 0.8156\n",
      "Epoch 788, Train Loss: 1.5616, Validation Loss: 1.7101, Validation F1: 0.8169\n",
      "Epoch 789, Train Loss: 1.5606, Validation Loss: 1.7027, Validation F1: 0.8197\n",
      "Epoch 790, Train Loss: 1.5612, Validation Loss: 1.7093, Validation F1: 0.8202\n",
      "Epoch 791, Train Loss: 1.5615, Validation Loss: 1.6960, Validation F1: 0.8165\n",
      "Epoch 792, Train Loss: 1.5627, Validation Loss: 1.6834, Validation F1: 0.8176\n",
      "Epoch 793, Train Loss: 1.5625, Validation Loss: 1.6874, Validation F1: 0.8179\n",
      "Epoch 794, Train Loss: 1.5631, Validation Loss: 1.6983, Validation F1: 0.8173\n",
      "Epoch 795, Train Loss: 1.5615, Validation Loss: 1.7011, Validation F1: 0.8183\n",
      "Epoch 796, Train Loss: 1.5627, Validation Loss: 1.7008, Validation F1: 0.8199\n",
      "Epoch 797, Train Loss: 1.5632, Validation Loss: 1.6901, Validation F1: 0.8168\n",
      "Epoch 798, Train Loss: 1.5620, Validation Loss: 1.7059, Validation F1: 0.8189\n",
      "Epoch 799, Train Loss: 1.5614, Validation Loss: 1.6994, Validation F1: 0.8172\n",
      "Epoch 800, Train Loss: 1.5621, Validation Loss: 1.7052, Validation F1: 0.8176\n",
      "Epoch 801, Train Loss: 1.5621, Validation Loss: 1.7039, Validation F1: 0.8180\n",
      "Epoch 802, Train Loss: 1.5619, Validation Loss: 1.6886, Validation F1: 0.8143\n",
      "Epoch 803, Train Loss: 1.5623, Validation Loss: 1.6866, Validation F1: 0.8217\n",
      "Epoch 804, Train Loss: 1.5606, Validation Loss: 1.6760, Validation F1: 0.8214\n",
      "Epoch 805, Train Loss: 1.5636, Validation Loss: 1.6777, Validation F1: 0.8187\n",
      "Epoch 806, Train Loss: 1.5616, Validation Loss: 1.6981, Validation F1: 0.8186\n",
      "Epoch 807, Train Loss: 1.5629, Validation Loss: 1.7108, Validation F1: 0.8156\n",
      "Epoch 808, Train Loss: 1.5617, Validation Loss: 1.7213, Validation F1: 0.8115\n",
      "Epoch 809, Train Loss: 1.5616, Validation Loss: 1.7167, Validation F1: 0.8169\n",
      "Epoch 810, Train Loss: 1.5614, Validation Loss: 1.6988, Validation F1: 0.8151\n",
      "Epoch 811, Train Loss: 1.5614, Validation Loss: 1.7005, Validation F1: 0.8160\n",
      "Epoch 812, Train Loss: 1.5625, Validation Loss: 1.6945, Validation F1: 0.8219\n",
      "Epoch 813, Train Loss: 1.5616, Validation Loss: 1.7167, Validation F1: 0.8178\n",
      "Epoch 814, Train Loss: 1.5609, Validation Loss: 1.6932, Validation F1: 0.8178\n",
      "Epoch 815, Train Loss: 1.5625, Validation Loss: 1.7015, Validation F1: 0.8149\n",
      "Epoch 816, Train Loss: 1.5619, Validation Loss: 1.6886, Validation F1: 0.8169\n",
      "Epoch 817, Train Loss: 1.5621, Validation Loss: 1.6987, Validation F1: 0.8188\n",
      "Epoch 818, Train Loss: 1.5614, Validation Loss: 1.6927, Validation F1: 0.8194\n",
      "Epoch 819, Train Loss: 1.5623, Validation Loss: 1.7157, Validation F1: 0.8207\n",
      "Epoch 820, Train Loss: 1.5607, Validation Loss: 1.7068, Validation F1: 0.8223\n",
      "Epoch 821, Train Loss: 1.5613, Validation Loss: 1.7027, Validation F1: 0.8186\n",
      "Epoch 822, Train Loss: 1.5637, Validation Loss: 1.6850, Validation F1: 0.8237\n",
      "Epoch 823, Train Loss: 1.5610, Validation Loss: 1.6872, Validation F1: 0.8164\n",
      "Epoch 824, Train Loss: 1.5615, Validation Loss: 1.6844, Validation F1: 0.8197\n",
      "Epoch 825, Train Loss: 1.5617, Validation Loss: 1.6879, Validation F1: 0.8186\n",
      "Epoch 826, Train Loss: 1.5616, Validation Loss: 1.7093, Validation F1: 0.8199\n",
      "Epoch 827, Train Loss: 1.5634, Validation Loss: 1.6926, Validation F1: 0.8203\n",
      "Epoch 828, Train Loss: 1.5613, Validation Loss: 1.6835, Validation F1: 0.8197\n",
      "Epoch 829, Train Loss: 1.5621, Validation Loss: 1.6999, Validation F1: 0.8184\n",
      "Epoch 830, Train Loss: 1.5613, Validation Loss: 1.6990, Validation F1: 0.8171\n",
      "Epoch 831, Train Loss: 1.5621, Validation Loss: 1.7039, Validation F1: 0.8154\n",
      "Epoch 832, Train Loss: 1.5614, Validation Loss: 1.7142, Validation F1: 0.8175\n",
      "Epoch 833, Train Loss: 1.5631, Validation Loss: 1.6941, Validation F1: 0.8196\n",
      "Epoch 834, Train Loss: 1.5608, Validation Loss: 1.6850, Validation F1: 0.8164\n",
      "Epoch 835, Train Loss: 1.5619, Validation Loss: 1.7032, Validation F1: 0.8177\n",
      "Epoch 836, Train Loss: 1.5616, Validation Loss: 1.7100, Validation F1: 0.8143\n",
      "Epoch 837, Train Loss: 1.5610, Validation Loss: 1.7225, Validation F1: 0.8143\n",
      "Epoch 838, Train Loss: 1.5618, Validation Loss: 1.7098, Validation F1: 0.8164\n",
      "Epoch 839, Train Loss: 1.5622, Validation Loss: 1.6947, Validation F1: 0.8171\n",
      "Epoch 840, Train Loss: 1.5616, Validation Loss: 1.6942, Validation F1: 0.8182\n",
      "Epoch 841, Train Loss: 1.5624, Validation Loss: 1.6984, Validation F1: 0.8191\n",
      "Epoch 842, Train Loss: 1.5619, Validation Loss: 1.6972, Validation F1: 0.8199\n",
      "Epoch 843, Train Loss: 1.5621, Validation Loss: 1.7144, Validation F1: 0.8214\n",
      "Epoch 844, Train Loss: 1.5617, Validation Loss: 1.7210, Validation F1: 0.8180\n",
      "Epoch 845, Train Loss: 1.5606, Validation Loss: 1.7078, Validation F1: 0.8159\n",
      "Epoch 846, Train Loss: 1.5613, Validation Loss: 1.7125, Validation F1: 0.8167\n",
      "Epoch 847, Train Loss: 1.5617, Validation Loss: 1.6981, Validation F1: 0.8153\n",
      "Epoch 848, Train Loss: 1.5622, Validation Loss: 1.7122, Validation F1: 0.8203\n",
      "Epoch 849, Train Loss: 1.5613, Validation Loss: 1.6965, Validation F1: 0.8186\n",
      "Epoch 850, Train Loss: 1.5605, Validation Loss: 1.6903, Validation F1: 0.8210\n",
      "Epoch 851, Train Loss: 1.5610, Validation Loss: 1.6909, Validation F1: 0.8202\n",
      "Epoch 852, Train Loss: 1.5607, Validation Loss: 1.6892, Validation F1: 0.8238\n",
      "Epoch 853, Train Loss: 1.5609, Validation Loss: 1.7042, Validation F1: 0.8198\n",
      "Epoch 854, Train Loss: 1.5606, Validation Loss: 1.7033, Validation F1: 0.8199\n",
      "Epoch 855, Train Loss: 1.5609, Validation Loss: 1.6970, Validation F1: 0.8211\n",
      "Epoch 856, Train Loss: 1.5619, Validation Loss: 1.7036, Validation F1: 0.8179\n",
      "Epoch 857, Train Loss: 1.5623, Validation Loss: 1.7070, Validation F1: 0.8210\n",
      "Epoch 858, Train Loss: 1.5605, Validation Loss: 1.7179, Validation F1: 0.8200\n",
      "Epoch 859, Train Loss: 1.5632, Validation Loss: 1.7216, Validation F1: 0.8204\n",
      "Epoch 860, Train Loss: 1.5602, Validation Loss: 1.7060, Validation F1: 0.8188\n",
      "Epoch 861, Train Loss: 1.5610, Validation Loss: 1.7156, Validation F1: 0.8157\n",
      "Epoch 862, Train Loss: 1.5605, Validation Loss: 1.7106, Validation F1: 0.8173\n",
      "Epoch 863, Train Loss: 1.5604, Validation Loss: 1.7181, Validation F1: 0.8172\n",
      "Epoch 864, Train Loss: 1.5607, Validation Loss: 1.7287, Validation F1: 0.8153\n",
      "Epoch 865, Train Loss: 1.5617, Validation Loss: 1.7042, Validation F1: 0.8138\n",
      "Epoch 866, Train Loss: 1.5606, Validation Loss: 1.7031, Validation F1: 0.8158\n",
      "Epoch 867, Train Loss: 1.5614, Validation Loss: 1.6958, Validation F1: 0.8168\n",
      "Epoch 868, Train Loss: 1.5612, Validation Loss: 1.6934, Validation F1: 0.8153\n",
      "Epoch 869, Train Loss: 1.5622, Validation Loss: 1.6821, Validation F1: 0.8184\n",
      "Epoch 870, Train Loss: 1.5619, Validation Loss: 1.6878, Validation F1: 0.8194\n",
      "Epoch 871, Train Loss: 1.5625, Validation Loss: 1.6973, Validation F1: 0.8179\n",
      "Epoch 872, Train Loss: 1.5623, Validation Loss: 1.6913, Validation F1: 0.8208\n",
      "Epoch 873, Train Loss: 1.5608, Validation Loss: 1.7078, Validation F1: 0.8189\n",
      "Epoch 874, Train Loss: 1.5613, Validation Loss: 1.6931, Validation F1: 0.8195\n",
      "Epoch 875, Train Loss: 1.5612, Validation Loss: 1.6889, Validation F1: 0.8160\n",
      "Epoch 876, Train Loss: 1.5627, Validation Loss: 1.6885, Validation F1: 0.8189\n",
      "Epoch 877, Train Loss: 1.5632, Validation Loss: 1.6907, Validation F1: 0.8189\n",
      "Epoch 878, Train Loss: 1.5610, Validation Loss: 1.7056, Validation F1: 0.8180\n",
      "Epoch 879, Train Loss: 1.5611, Validation Loss: 1.7139, Validation F1: 0.8196\n",
      "Epoch 880, Train Loss: 1.5612, Validation Loss: 1.7000, Validation F1: 0.8200\n",
      "Epoch 881, Train Loss: 1.5604, Validation Loss: 1.6927, Validation F1: 0.8199\n",
      "Epoch 882, Train Loss: 1.5607, Validation Loss: 1.6906, Validation F1: 0.8173\n",
      "Epoch 883, Train Loss: 1.5625, Validation Loss: 1.6969, Validation F1: 0.8200\n",
      "Epoch 884, Train Loss: 1.5613, Validation Loss: 1.7050, Validation F1: 0.8173\n",
      "Epoch 885, Train Loss: 1.5603, Validation Loss: 1.7092, Validation F1: 0.8174\n",
      "Epoch 886, Train Loss: 1.5634, Validation Loss: 1.7004, Validation F1: 0.8189\n",
      "Epoch 887, Train Loss: 1.5616, Validation Loss: 1.7061, Validation F1: 0.8175\n",
      "Epoch 888, Train Loss: 1.5620, Validation Loss: 1.6925, Validation F1: 0.8183\n",
      "Epoch 889, Train Loss: 1.5627, Validation Loss: 1.6995, Validation F1: 0.8193\n",
      "Epoch 890, Train Loss: 1.5614, Validation Loss: 1.7179, Validation F1: 0.8176\n",
      "Epoch 891, Train Loss: 1.5608, Validation Loss: 1.7108, Validation F1: 0.8174\n",
      "Epoch 892, Train Loss: 1.5624, Validation Loss: 1.7188, Validation F1: 0.8173\n",
      "Epoch 893, Train Loss: 1.5611, Validation Loss: 1.6886, Validation F1: 0.8177\n",
      "Epoch 894, Train Loss: 1.5608, Validation Loss: 1.6905, Validation F1: 0.8156\n",
      "Epoch 895, Train Loss: 1.5633, Validation Loss: 1.6983, Validation F1: 0.8206\n",
      "Epoch 896, Train Loss: 1.5629, Validation Loss: 1.7135, Validation F1: 0.8207\n",
      "Epoch 897, Train Loss: 1.5613, Validation Loss: 1.6976, Validation F1: 0.8209\n",
      "Epoch 898, Train Loss: 1.5632, Validation Loss: 1.7013, Validation F1: 0.8182\n",
      "Epoch 899, Train Loss: 1.5612, Validation Loss: 1.6910, Validation F1: 0.8186\n",
      "Epoch 900, Train Loss: 1.5610, Validation Loss: 1.6835, Validation F1: 0.8186\n",
      "Epoch 901, Train Loss: 1.5616, Validation Loss: 1.6932, Validation F1: 0.8150\n",
      "Epoch 902, Train Loss: 1.5614, Validation Loss: 1.6960, Validation F1: 0.8163\n",
      "Epoch 903, Train Loss: 1.5617, Validation Loss: 1.7170, Validation F1: 0.8177\n",
      "Epoch 904, Train Loss: 1.5619, Validation Loss: 1.7149, Validation F1: 0.8199\n",
      "Epoch 905, Train Loss: 1.5618, Validation Loss: 1.7165, Validation F1: 0.8193\n",
      "Epoch 906, Train Loss: 1.5617, Validation Loss: 1.7086, Validation F1: 0.8183\n",
      "Epoch 907, Train Loss: 1.5618, Validation Loss: 1.6955, Validation F1: 0.8146\n",
      "Epoch 908, Train Loss: 1.5610, Validation Loss: 1.7046, Validation F1: 0.8171\n",
      "Epoch 909, Train Loss: 1.5606, Validation Loss: 1.7087, Validation F1: 0.8186\n",
      "Epoch 910, Train Loss: 1.5611, Validation Loss: 1.7059, Validation F1: 0.8195\n",
      "Epoch 911, Train Loss: 1.5609, Validation Loss: 1.7131, Validation F1: 0.8187\n",
      "Epoch 912, Train Loss: 1.5617, Validation Loss: 1.7079, Validation F1: 0.8211\n",
      "Epoch 913, Train Loss: 1.5613, Validation Loss: 1.6992, Validation F1: 0.8197\n",
      "Epoch 914, Train Loss: 1.5607, Validation Loss: 1.6806, Validation F1: 0.8201\n",
      "Epoch 915, Train Loss: 1.5628, Validation Loss: 1.6945, Validation F1: 0.8181\n",
      "Epoch 916, Train Loss: 1.5633, Validation Loss: 1.6947, Validation F1: 0.8190\n",
      "Epoch 917, Train Loss: 1.5602, Validation Loss: 1.7275, Validation F1: 0.8176\n",
      "Epoch 918, Train Loss: 1.5625, Validation Loss: 1.7192, Validation F1: 0.8185\n",
      "Epoch 919, Train Loss: 1.5622, Validation Loss: 1.7076, Validation F1: 0.8209\n",
      "Epoch 920, Train Loss: 1.5611, Validation Loss: 1.6904, Validation F1: 0.8196\n",
      "Epoch 921, Train Loss: 1.5631, Validation Loss: 1.6942, Validation F1: 0.8182\n",
      "Epoch 922, Train Loss: 1.5618, Validation Loss: 1.7117, Validation F1: 0.8187\n",
      "Epoch 923, Train Loss: 1.5606, Validation Loss: 1.7059, Validation F1: 0.8159\n",
      "Epoch 924, Train Loss: 1.5622, Validation Loss: 1.7092, Validation F1: 0.8170\n",
      "Epoch 925, Train Loss: 1.5610, Validation Loss: 1.6961, Validation F1: 0.8163\n",
      "Epoch 926, Train Loss: 1.5617, Validation Loss: 1.6892, Validation F1: 0.8193\n",
      "Epoch 927, Train Loss: 1.5617, Validation Loss: 1.7017, Validation F1: 0.8186\n",
      "Epoch 928, Train Loss: 1.5612, Validation Loss: 1.7037, Validation F1: 0.8150\n",
      "Epoch 929, Train Loss: 1.5604, Validation Loss: 1.7041, Validation F1: 0.8171\n",
      "Epoch 930, Train Loss: 1.5603, Validation Loss: 1.6986, Validation F1: 0.8169\n",
      "Epoch 931, Train Loss: 1.5613, Validation Loss: 1.7044, Validation F1: 0.8176\n",
      "Epoch 932, Train Loss: 1.5610, Validation Loss: 1.6906, Validation F1: 0.8174\n",
      "Epoch 933, Train Loss: 1.5615, Validation Loss: 1.6949, Validation F1: 0.8180\n",
      "Epoch 934, Train Loss: 1.5619, Validation Loss: 1.6989, Validation F1: 0.8188\n",
      "Epoch 935, Train Loss: 1.5626, Validation Loss: 1.7148, Validation F1: 0.8181\n",
      "Epoch 936, Train Loss: 1.5624, Validation Loss: 1.7190, Validation F1: 0.8154\n",
      "Epoch 937, Train Loss: 1.5629, Validation Loss: 1.7029, Validation F1: 0.8190\n",
      "Epoch 938, Train Loss: 1.5610, Validation Loss: 1.6988, Validation F1: 0.8168\n",
      "Epoch 939, Train Loss: 1.5626, Validation Loss: 1.7008, Validation F1: 0.8174\n",
      "Epoch 940, Train Loss: 1.5612, Validation Loss: 1.7109, Validation F1: 0.8175\n",
      "Epoch 941, Train Loss: 1.5611, Validation Loss: 1.7135, Validation F1: 0.8133\n",
      "Epoch 942, Train Loss: 1.5610, Validation Loss: 1.7205, Validation F1: 0.8140\n",
      "Epoch 943, Train Loss: 1.5618, Validation Loss: 1.7123, Validation F1: 0.8158\n",
      "Epoch 944, Train Loss: 1.5621, Validation Loss: 1.6986, Validation F1: 0.8193\n",
      "Epoch 945, Train Loss: 1.5619, Validation Loss: 1.6865, Validation F1: 0.8217\n",
      "Epoch 946, Train Loss: 1.5622, Validation Loss: 1.6828, Validation F1: 0.8175\n",
      "Epoch 947, Train Loss: 1.5631, Validation Loss: 1.7004, Validation F1: 0.8190\n",
      "Epoch 948, Train Loss: 1.5606, Validation Loss: 1.7190, Validation F1: 0.8158\n",
      "Epoch 949, Train Loss: 1.5619, Validation Loss: 1.7237, Validation F1: 0.8155\n",
      "Epoch 950, Train Loss: 1.5625, Validation Loss: 1.7090, Validation F1: 0.8140\n",
      "Epoch 951, Train Loss: 1.5607, Validation Loss: 1.6913, Validation F1: 0.8183\n",
      "Epoch 952, Train Loss: 1.5613, Validation Loss: 1.6870, Validation F1: 0.8151\n",
      "Epoch 953, Train Loss: 1.5613, Validation Loss: 1.6833, Validation F1: 0.8180\n",
      "Epoch 954, Train Loss: 1.5621, Validation Loss: 1.6905, Validation F1: 0.8159\n",
      "Epoch 955, Train Loss: 1.5614, Validation Loss: 1.7047, Validation F1: 0.8174\n",
      "Epoch 956, Train Loss: 1.5608, Validation Loss: 1.6978, Validation F1: 0.8162\n",
      "Epoch 957, Train Loss: 1.5603, Validation Loss: 1.6992, Validation F1: 0.8174\n",
      "Epoch 958, Train Loss: 1.5613, Validation Loss: 1.7007, Validation F1: 0.8182\n",
      "Epoch 959, Train Loss: 1.5613, Validation Loss: 1.6882, Validation F1: 0.8199\n",
      "Epoch 960, Train Loss: 1.5611, Validation Loss: 1.6892, Validation F1: 0.8213\n",
      "Epoch 961, Train Loss: 1.5617, Validation Loss: 1.7005, Validation F1: 0.8179\n",
      "Epoch 962, Train Loss: 1.5602, Validation Loss: 1.7102, Validation F1: 0.8187\n",
      "Epoch 963, Train Loss: 1.5607, Validation Loss: 1.7185, Validation F1: 0.8185\n",
      "Epoch 964, Train Loss: 1.5616, Validation Loss: 1.7006, Validation F1: 0.8142\n",
      "Epoch 965, Train Loss: 1.5614, Validation Loss: 1.6938, Validation F1: 0.8180\n",
      "Epoch 966, Train Loss: 1.5605, Validation Loss: 1.6989, Validation F1: 0.8191\n",
      "Epoch 967, Train Loss: 1.5619, Validation Loss: 1.6950, Validation F1: 0.8184\n",
      "Epoch 968, Train Loss: 1.5604, Validation Loss: 1.6974, Validation F1: 0.8176\n",
      "Epoch 969, Train Loss: 1.5602, Validation Loss: 1.7040, Validation F1: 0.8168\n",
      "Epoch 970, Train Loss: 1.5605, Validation Loss: 1.7098, Validation F1: 0.8182\n",
      "Epoch 971, Train Loss: 1.5605, Validation Loss: 1.7018, Validation F1: 0.8180\n",
      "Epoch 972, Train Loss: 1.5610, Validation Loss: 1.7025, Validation F1: 0.8199\n",
      "Epoch 973, Train Loss: 1.5601, Validation Loss: 1.6838, Validation F1: 0.8173\n",
      "Epoch 974, Train Loss: 1.5602, Validation Loss: 1.6896, Validation F1: 0.8178\n",
      "Epoch 975, Train Loss: 1.5621, Validation Loss: 1.7015, Validation F1: 0.8166\n",
      "Epoch 976, Train Loss: 1.5608, Validation Loss: 1.7225, Validation F1: 0.8156\n",
      "Epoch 977, Train Loss: 1.5615, Validation Loss: 1.7064, Validation F1: 0.8151\n",
      "Epoch 978, Train Loss: 1.5609, Validation Loss: 1.7090, Validation F1: 0.8204\n",
      "Epoch 979, Train Loss: 1.5597, Validation Loss: 1.6878, Validation F1: 0.8207\n",
      "Epoch 980, Train Loss: 1.5617, Validation Loss: 1.7014, Validation F1: 0.8195\n",
      "Epoch 981, Train Loss: 1.5620, Validation Loss: 1.7246, Validation F1: 0.8175\n",
      "Epoch 982, Train Loss: 1.5614, Validation Loss: 1.7265, Validation F1: 0.8154\n",
      "Epoch 983, Train Loss: 1.5630, Validation Loss: 1.7186, Validation F1: 0.8168\n",
      "Epoch 984, Train Loss: 1.5606, Validation Loss: 1.6916, Validation F1: 0.8173\n",
      "Epoch 985, Train Loss: 1.5616, Validation Loss: 1.6863, Validation F1: 0.8175\n",
      "Epoch 986, Train Loss: 1.5620, Validation Loss: 1.7032, Validation F1: 0.8188\n",
      "Epoch 987, Train Loss: 1.5605, Validation Loss: 1.6935, Validation F1: 0.8210\n",
      "Epoch 988, Train Loss: 1.5613, Validation Loss: 1.7035, Validation F1: 0.8176\n",
      "Epoch 989, Train Loss: 1.5605, Validation Loss: 1.6996, Validation F1: 0.8175\n",
      "Epoch 990, Train Loss: 1.5611, Validation Loss: 1.7016, Validation F1: 0.8161\n",
      "Epoch 991, Train Loss: 1.5616, Validation Loss: 1.7020, Validation F1: 0.8177\n",
      "Epoch 992, Train Loss: 1.5628, Validation Loss: 1.6939, Validation F1: 0.8188\n",
      "Epoch 993, Train Loss: 1.5600, Validation Loss: 1.6865, Validation F1: 0.8185\n",
      "Epoch 994, Train Loss: 1.5616, Validation Loss: 1.6995, Validation F1: 0.8176\n",
      "Epoch 995, Train Loss: 1.5605, Validation Loss: 1.6911, Validation F1: 0.8190\n",
      "Epoch 996, Train Loss: 1.5603, Validation Loss: 1.7103, Validation F1: 0.8181\n",
      "Epoch 997, Train Loss: 1.5607, Validation Loss: 1.7141, Validation F1: 0.8186\n",
      "Epoch 998, Train Loss: 1.5609, Validation Loss: 1.6993, Validation F1: 0.8195\n",
      "Epoch 999, Train Loss: 1.5622, Validation Loss: 1.6911, Validation F1: 0.8172\n",
      "Model training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Extract the best parameters from the grid search\n",
    "best_hidden_dim = 256  # Replace with the best hidden_dim found\n",
    "best_learning_rate = 0.001  # Replace with the best learning_rate found\n",
    "best_dropout = 0.3  # Replace with the best dropout found\n",
    "\n",
    "# Initialize the model with the best parameters\n",
    "model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                   edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                   hidden_channels=best_hidden_dim,\n",
    "                   dropout=best_dropout,\n",
    "                   out_channels=num_classes).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Compute class weights for the training dataset\n",
    "labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.unique(labels),\n",
    "                                                  y=labels)\n",
    "\n",
    "# Normalize class weights\n",
    "class_weights = th.FloatTensor(class_weights).to(device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# Move the graph data to the device\n",
    "G_pyg_train = G_pyg_train.to(device)\n",
    "G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "# ===== Load checkpoint if exists =====\n",
    "best_f1 = 0\n",
    "start_epoch = 0\n",
    "epochs = 5000\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = th.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "    print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_f1_history = []\n",
    "saved_model_epochs = []\n",
    "\n",
    "train_loss_history_path = os.path.join(saves_path, 'train_loss_history.pkl')\n",
    "val_loss_history_path = os.path.join(saves_path, 'val_loss_history.pkl')\n",
    "val_f1_history_path = os.path.join(saves_path, 'val_f1_history.pkl')\n",
    "saved_model_epochs_path = os.path.join(saves_path, 'saved_model_epochs.pkl')\n",
    "\n",
    "if os.path.exists(train_loss_history_path) and os.path.exists(val_loss_history_path) and os.path.exists(val_f1_history_path) and os.path.exists(saved_model_epochs_path):\n",
    "    with open(train_loss_history_path, 'rb') as f:\n",
    "        train_loss_history = pickle.load(f)\n",
    "    with open(val_loss_history_path, 'rb') as f:\n",
    "        val_loss_history = pickle.load(f)\n",
    "    with open(val_f1_history_path, 'rb') as f:\n",
    "        val_f1_history = pickle.load(f)\n",
    "    with open(saved_model_epochs_path, 'rb') as f:\n",
    "        saved_model_epochs = pickle.load(f)\n",
    "\n",
    "# ===== Start Training =====\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    out = model(G_pyg_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(out, G_pyg_train.edge_label)\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        out = model(G_pyg_val)\n",
    "        loss = criterion(out, G_pyg_val.edge_label)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "    val_f1_micro = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='micro')\n",
    "    val_f1_macro = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='macro')\n",
    "\n",
    "    if epoch % 10 == 0 or val_f1 > best_f1:\n",
    "        # Save checkpoint\n",
    "        th.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_f1': best_f1\n",
    "        }, checkpoint_path)\n",
    "        with open(train_loss_history_path, 'wb') as f:\n",
    "            pickle.dump(train_loss_history, f)\n",
    "        with open(val_loss_history_path, 'wb') as f:\n",
    "            pickle.dump(val_loss_history, f)\n",
    "        with open(val_f1_history_path, 'wb') as f:\n",
    "            pickle.dump(val_f1_history, f)\n",
    "        with open(saved_model_epochs_path, 'wb') as f:\n",
    "            pickle.dump(saved_model_epochs, f)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1  # Update the best F1 score for this fold\n",
    "        best_model_state = model.state_dict()\n",
    "        th.save(best_model_state, best_model_path)\n",
    "        print(f\"Epoch {epoch} Saved best model. Best F1:\", best_f1)\n",
    "        saved_model_epochs.append(epoch)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_f1_history.append((val_f1, val_f1_micro, val_f1_macro))\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Model training completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2376aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_process(train_losses, val_losses, val_f1, saved_model_epochs):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "    # Plot Train Loss\n",
    "    axs[0].plot(train_losses, label='Train Loss', color='blue')\n",
    "    axs[0].plot(val_losses, label='Validation Loss', color='red')\n",
    "    axs[0].set_ylabel('Train Loss')\n",
    "    axs[0].set_title('Training Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid()\n",
    "\n",
    "    val_f1_weighted_history = []\n",
    "    val_f1_micro_history = []\n",
    "    val_f1_macro_history = []\n",
    "\n",
    "    for val_f1_weighted, val_f1_micro, val_f1_macro in val_f1:\n",
    "        val_f1_weighted_history.append(val_f1_weighted)\n",
    "        val_f1_micro_history.append(val_f1_micro)\n",
    "        val_f1_macro_history.append(val_f1_macro)\n",
    "    \n",
    "    # Plot Validation F1\n",
    "    axs[1].plot(val_f1_weighted_history, label='Validation F1 Weighted', color='green')\n",
    "    axs[1].plot(val_f1_micro_history, label='Validation F1 Micro', color='blue')\n",
    "    axs[1].plot(val_f1_macro_history, label='Validation F1 Macro', color='red')\n",
    "    average_val_f1 = np.mean(val_f1)\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Validation F1')\n",
    "    axs[1].set_title('Validation F1 Score')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid()\n",
    "\n",
    "    print(len(train_losses))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_process(train_loss_history, val_loss_history, val_f1_history, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8368fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 175\n",
      "Shape of node in G_pyg: torch.Size([175, 58])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 58])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f271a-612b-4cd6-a85a-e4236dec9d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_binary_port/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[  137     0   197     0    16     1     0     1    28    22]\n",
      " [   82     0   198     8    13     5     0     3    22    18]\n",
      " [  430     0  1434    56   121    79     0    19   163   151]\n",
      " [ 1469     0  1773   327   788   372     0   135   949   866]\n",
      " [  758     0   386   231   773   245     0   125   617   502]\n",
      " [  144     0   224    48    98 31521     0    24   141   122]\n",
      " [   86     0    37    35   138    47 32686    17   128   107]\n",
      " [  370     0   325   134   294   184     0    50   389   352]\n",
      " [   46     0    14    20    27    20     0     9    45    46]\n",
      " [    6     0     1     0     7     1     0     0     6     5]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0388    0.3408    0.0697       402\n",
      "     Backdoors     0.0000    0.0000    0.0000       349\n",
      "           DoS     0.3125    0.5846    0.4073      2453\n",
      "      Exploits     0.3807    0.0490    0.0868      6679\n",
      "       Fuzzers     0.3398    0.2125    0.2615      3637\n",
      "       Generic     0.9706    0.9752    0.9729     32322\n",
      "        Normal     1.0000    0.9821    0.9910     33281\n",
      "Reconnaissance     0.1305    0.0238    0.0403      2098\n",
      "     Shellcode     0.0181    0.1982    0.0331       227\n",
      "         Worms     0.0023    0.1923    0.0045        26\n",
      "\n",
      "      accuracy                         0.8221     81474\n",
      "     macro avg     0.3193    0.3559    0.2867     81474\n",
      "  weighted avg     0.8529    0.8221    0.8233     81474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def eval(G_pyg_test, adversarial=False):\n",
    "\n",
    "    G_pyg_test = G_pyg_test.to(device)\n",
    "    G_pyg_test.edge_label = G_pyg_test.edge_label.to(device)\n",
    "    G_pyg_test.edge_attr = G_pyg_test.edge_attr.to(device)\n",
    "\n",
    "    best_model = EGraphSAGE(node_in_channels=G_pyg_test.num_node_features, \n",
    "                       edge_in_channels=G_pyg_test.num_edge_features,\n",
    "                       hidden_channels=best_hidden_dim, \n",
    "                       out_channels=num_classes).to(device)\n",
    "\n",
    "    print(\"Loading model from\", best_model_path)\n",
    "    best_model.load_state_dict(th.load(best_model_path))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    best_model.eval()\n",
    "\n",
    "    print(\"inference start\")\n",
    "    with th.no_grad():\n",
    "            \n",
    "        try:\n",
    "            out = best_model(G_pyg_test)\n",
    "            \n",
    "        except Exception as forward_error:\n",
    "            print(f\"Error during forward/backward pass at {forward_error}\")\n",
    "\n",
    "    print(\"inference done\")\n",
    "\n",
    "    pred_labels = out.argmax(dim=1).cpu()\n",
    "    all_test_labels = G_pyg_test.edge_label.cpu()\n",
    "\n",
    "    if adversarial:\n",
    "\n",
    "        # Create a boolean mask where the label is NOT equal to the adversarial class\n",
    "        adversarial_mask = all_test_labels == ADVERSARIAL_CLASS_LABEL\n",
    "\n",
    "        # Print the class that the adversarial samples are classified as\n",
    "        cm_adversarial = confusion_matrix(all_test_labels[adversarial_mask], pred_labels[adversarial_mask], labels=range(len(class_map) + 1))\n",
    "        print(\"Adversarial confusion matrix:\", cm_adversarial)\n",
    "\n",
    "        # Apply the mask to both labels and predictions\n",
    "        all_test_labels = all_test_labels[~adversarial_mask]\n",
    "        pred_labels = pred_labels[~adversarial_mask]\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"class_map\", class_map)\n",
    "    # Generate a report\n",
    "    cm = confusion_matrix(all_test_labels, pred_labels, labels=range(len(class_map)))\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    report = classification_report(all_test_labels, pred_labels, target_names=class_map, digits=4)\n",
    "    print(report)\n",
    "\n",
    "    \n",
    "eval(G_pyg_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_traffic_to_attacker(graph, ratio=0.1, num_injected_nodes=1, is_attack=False):\n",
    "    edge_index = graph.edge_index.clone()\n",
    "    edge_attr = graph.edge_attr.clone()\n",
    "    edge_label = graph.edge_label.clone()\n",
    "    x = graph.x.clone()\n",
    "\n",
    "    num_edges = edge_index.size(1)\n",
    "    feature_dim = graph.x.size(1)\n",
    "\n",
    "    # 1. Identify attacker nodes\n",
    "    attacker_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "    attacker_nodes = th.unique(edge_index[:, attacker_edges])\n",
    "    if attacker_nodes.numel() == 0:\n",
    "        raise ValueError(\"No attacker nodes found.\")\n",
    "\n",
    "    # 2. Sample benign edge feature pool\n",
    "    if is_attack:\n",
    "        attack_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[attack_edges]\n",
    "    else:\n",
    "        benign_edges = (edge_label == BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[benign_edges]\n",
    "\n",
    "    # 3. Inject new nodes\n",
    "    original_num_nodes = x.size(0)\n",
    "\n",
    "    new_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "    x = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "    # 4. Inject edges from injected nodes to attacker nodes\n",
    "    num_to_inject = max(1, int(ratio * num_edges))\n",
    "    new_edges = []\n",
    "    new_attrs = []\n",
    "    new_labels = []\n",
    "\n",
    "    \n",
    "    for _ in range(num_to_inject):\n",
    "        src = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\n",
    "        dst = attacker_nodes[random.randint(0, len(attacker_nodes) - 1)].item()\n",
    "\n",
    "        new_edges.append([src, dst])\n",
    "        attr = inject_edge_attr_pool[random.randint(0, len(inject_edge_attr_pool) - 1)]\n",
    "        new_attrs.append(attr)\n",
    "        new_labels.append(ADVERSARIAL_CLASS_LABEL)\n",
    "\n",
    "    # Create a new empty graph to store the injected edges\n",
    "    new_graph = Data()\n",
    "\n",
    "    # 5. Merge into graph\n",
    "    if new_edges:\n",
    "        new_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "        new_attrs = th.stack(new_attrs)\n",
    "        new_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "        new_graph.edge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "        new_graph.edge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "        new_graph.edge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "        new_graph.x = x\n",
    "\n",
    "        # new_graph.first_injected_node_idx = original_num_nodes # Store injected node indices\n",
    "\n",
    "    return new_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c5bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_binary_port/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 436    0  302  143 2730    0 2264    0    0 2272    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[    0   104   194     0     4    13     0     0    49    38]\n",
      " [    6   101   166     0    27    14     0     0    29     6]\n",
      " [   22   402  1412     0   191   165     0     0   215    46]\n",
      " [  177   721  2122     0  1245   819     0     0  1292   303]\n",
      " [   49   401   445     0  1180   499     0     0   877   186]\n",
      " [   25    89   223     0   171 31590     0     0   184    40]\n",
      " [   15    27    49     0   162    84 32684     0   210    50]\n",
      " [   60   155   383     0   491   359     0     0   499   151]\n",
      " [   12    20    15     0    53    42     0     0    66    19]\n",
      " [    0     1     5     0     5     3     0     0     9     3]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0000    0.0000    0.0000       402\n",
      "     Backdoors     0.0500    0.2894    0.0852       349\n",
      "           DoS     0.2816    0.5756    0.3782      2453\n",
      "      Exploits     0.0000    0.0000    0.0000      6679\n",
      "       Fuzzers     0.3344    0.3244    0.3293      3637\n",
      "       Generic     0.9405    0.9774    0.9586     32322\n",
      "        Normal     1.0000    0.9821    0.9909     33281\n",
      "Reconnaissance     0.0000    0.0000    0.0000      2098\n",
      "     Shellcode     0.0192    0.2907    0.0361       227\n",
      "         Worms     0.0036    0.1154    0.0069        26\n",
      "\n",
      "      accuracy                         0.8228     81474\n",
      "     macro avg     0.2629    0.3555    0.2785     81474\n",
      "  weighted avg     0.8053    0.8228    0.8116     81474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Inject Attack Traffic to Attacker Nodes\n",
    "G_pyg_test = G_pyg_test.cpu()\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=True)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c37b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_binary_port/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 131    0  159    0    0    0 4446  593  140 2678    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[    0   229    81     0     0     1     0    13    50    28]\n",
      " [    0   243    34     0     0     7     1    31    31     2]\n",
      " [    0  1333   554     0     4    90    21   176   266     9]\n",
      " [    0  2317   932     0    22   431   145  1144  1611    77]\n",
      " [    0   851   215     0    18   264   118   752  1366    53]\n",
      " [    0   293    82     0     1 30495    12   169   221  1049]\n",
      " [    0    81    28     0     2    56 32700   145   255    14]\n",
      " [    0   518   189     0     5   203    60   450   643    30]\n",
      " [    0    41    11     0     0    26     5    60    78     6]\n",
      " [    0     5     3     0     0     1     0     4    12     1]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0000    0.0000    0.0000       402\n",
      "     Backdoors     0.0411    0.6963    0.0776       349\n",
      "           DoS     0.2602    0.2258    0.2418      2453\n",
      "      Exploits     0.0000    0.0000    0.0000      6679\n",
      "       Fuzzers     0.3462    0.0049    0.0098      3637\n",
      "       Generic     0.9658    0.9435    0.9545     32322\n",
      "        Normal     0.9891    0.9825    0.9858     33281\n",
      "Reconnaissance     0.1529    0.2145    0.1785      2098\n",
      "     Shellcode     0.0172    0.3436    0.0328       227\n",
      "         Worms     0.0008    0.0385    0.0015        26\n",
      "\n",
      "      accuracy                         0.7921     81474\n",
      "     macro avg     0.2773    0.3450    0.2482     81474\n",
      "  weighted avg     0.8146    0.7921    0.7941     81474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Inject BENIGN Traffic to Attacker Nodes\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=False)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_random_nodes(graph, ratio=0.1, num_injected_nodes=1):\n",
    "\tedge_index = graph.edge_index.clone()\n",
    "\tedge_attr = graph.edge_attr.clone()\n",
    "\tedge_label = graph.edge_label.clone()\n",
    "\tx = graph.x.clone()\n",
    "\n",
    "\tnum_edges = edge_index.size(1)\n",
    "\tfeature_dim = graph.x.size(1)\n",
    "\n",
    "\t# 1. Inject new nodes\n",
    "\toriginal_num_nodes = x.size(0)\n",
    "\tnew_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "\tx = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "\t# 2. Inject random edges\n",
    "\tnum_to_inject = max(1, int(ratio * num_edges))\n",
    "\tnew_edges = []\n",
    "\tnew_attrs = []\n",
    "\tnew_labels = []\n",
    "\n",
    "\tfor _ in range(num_to_inject):\n",
    "\t\tsrc = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\t\tdst = random.randint(0, original_num_nodes - 1)  # to existing nodes\n",
    "\n",
    "\t\tnew_edges.append([src, dst])\n",
    "\t\tattr = edge_attr[random.randint(0, len(edge_attr) - 1)]  # Randomly sample edge attributes\n",
    "\t\tnew_attrs.append(attr)\n",
    "\t\tnew_labels.append(ADVERSARIAL_CLASS_LABEL)  # Assign benign class label to new edges\n",
    "\n",
    "\t# 3. Merge into graph\n",
    "\tif new_edges:\n",
    "\t\tnew_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "\t\tnew_attrs = th.stack(new_attrs)\n",
    "\t\tnew_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "\t\tedge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "\t\tedge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "\t\tedge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "\n",
    "\t# Create a new graph with the injected nodes and edges\n",
    "\tnew_graph = Data(\n",
    "\t\tedge_index=edge_index,\n",
    "\t\tedge_attr=edge_attr,\n",
    "\t\tedge_label=edge_label,\n",
    "\t\tx=x\n",
    "\t)\n",
    "\n",
    "\treturn new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb63898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_binary_port/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  83    0   32    0  591    0 6306    0  191  944    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[   41   146   111     0     1    13     0     1    50    39]\n",
      " [    5   142   120     2    16    14     0     8    34     8]\n",
      " [  257   717   843    17   120   165     0    43   223    68]\n",
      " [  646  1028  1203    90   712   819     0   223  1499   459]\n",
      " [  167   476   246    74   659   499     0   100  1130   286]\n",
      " [   54   129   134    14   101 31590     0    36   204    60]\n",
      " [   16    33    27    13    93    84 32686    16   243    70]\n",
      " [  111   228   213    46   282   359     0    95   573   191]\n",
      " [   11    20     9     5    30    42     0     7    75    28]\n",
      " [    2     3     1     0     2     3     0     1    10     4]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0313    0.1020    0.0479       402\n",
      "     Backdoors     0.0486    0.4069    0.0868       349\n",
      "           DoS     0.2900    0.3437    0.3146      2453\n",
      "      Exploits     0.3448    0.0135    0.0259      6679\n",
      "       Fuzzers     0.3269    0.1812    0.2332      3637\n",
      "       Generic     0.9405    0.9774    0.9586     32322\n",
      "        Normal     1.0000    0.9821    0.9910     33281\n",
      "Reconnaissance     0.1792    0.0453    0.0723      2098\n",
      "     Shellcode     0.0186    0.3304    0.0351       227\n",
      "         Worms     0.0033    0.1538    0.0065        26\n",
      "\n",
      "      accuracy                         0.8128     81474\n",
      "     macro avg     0.3183    0.3536    0.2772     81474\n",
      "  weighted avg     0.8382    0.8128    0.8097     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inject Random Nodes in the graph\n",
    "injected_graph = inject_random_nodes(G_pyg_test, 0.1, num_injected_nodes=1)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
