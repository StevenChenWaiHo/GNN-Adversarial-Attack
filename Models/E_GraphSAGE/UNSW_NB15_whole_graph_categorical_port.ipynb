{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb9e1b2-1c5f-49e3-82b4-3bfe52cabad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "=====Experiment=====\n",
    "Dataset: UNSW-NB15 dataset\n",
    "\n",
    "Training with whole graph\n",
    "Downsampled 90% normal traffic randomly\n",
    "Split train and test randomly\n",
    "\n",
    "IP as node\n",
    "Encode Ports in 3 Categories\n",
    "'''\n",
    "\n",
    "from torch_geometric.utils import from_networkx, add_self_loops, degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "# import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Datasets.UNSW_NB15.UNSW_NB15_config import UNSW_NB15_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9ef09a-d405-43b8-971e-fe9e6a592c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "Normal            221876\n",
      "Generic           215481\n",
      "Exploits           44525\n",
      "Fuzzers            24246\n",
      "DoS                16353\n",
      "Reconnaissance     13987\n",
      "Analysis            2677\n",
      "Backdoors           2329\n",
      "Shellcode           1511\n",
      "Worms                174\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    321283\n",
      "0    221876\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "csv_file_name = \"all_downsampled\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n",
    "\n",
    "DATASET_NAME = \"UNSW_NB15\"\n",
    "EXPERIMENT_NAME = \"whole_graph_categorical_port\"\n",
    "\n",
    "SOURCE_IP_COL_NAME = UNSW_NB15_Config.SOURCE_IP_COL_NAME\n",
    "DESTINATION_IP_COL_NAME = UNSW_NB15_Config.DESTINATION_IP_COL_NAME\n",
    "SOURCE_PORT_COL_NAME = UNSW_NB15_Config.SOURCE_PORT_COL_NAME\n",
    "DESTINATION_PORT_COL_NAME = UNSW_NB15_Config.DESTINATION_PORT_COL_NAME\n",
    "\n",
    "ATTACK_CLASS_COL_NAME = UNSW_NB15_Config.ATTACK_CLASS_COL_NAME\n",
    "IS_ATTACK_COL_NAME = UNSW_NB15_Config.IS_ATTACK_COL_NAME\n",
    "\n",
    "BENIGN_CLASS_NAME = UNSW_NB15_Config.BENIGN_CLASS_NAME\n",
    "\n",
    "TIME_COLS = UNSW_NB15_Config.TIME_COL_NAMES\n",
    "\n",
    "CATEGORICAL_COLS = UNSW_NB15_Config.CATEGORICAL_COLS\n",
    "\n",
    "print(data[ATTACK_CLASS_COL_NAME].value_counts())\n",
    "print(data[IS_ATTACK_COL_NAME].value_counts())\n",
    "\n",
    "MULTICLASS = True\n",
    "\n",
    "if MULTICLASS:\n",
    "    label_col = ATTACK_CLASS_COL_NAME\n",
    "    data.drop(columns=[IS_ATTACK_COL_NAME], inplace=True)\n",
    "else:\n",
    "    label_col = IS_ATTACK_COL_NAME\n",
    "    data.drop(columns=[ATTACK_CLASS_COL_NAME], inplace=True)\n",
    "\n",
    "saves_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, EXPERIMENT_NAME)\n",
    "\n",
    "checkpoint_path = os.path.join(saves_path, f\"checkpoints_{csv_file_name}.pth\")\n",
    "best_model_path = os.path.join(saves_path, f\"best_model_{csv_file_name}.pth\")\n",
    "\n",
    "os.makedirs(saves_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449a1af1-1d3d-4179-9628-7c2ec551ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['srcip', 'sport', 'dstip', 'dsport', 'state', 'dur', 'sbytes', 'dbytes',\n",
      "       'sttl', 'dttl', 'sloss', 'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts',\n",
      "       'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth',\n",
      "       'res_bdy_len', 'Sjit', 'Djit', 'Stime', 'Ltime', 'Sintpkt', 'Dintpkt',\n",
      "       'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl',\n",
      "       'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src',\n",
      "       'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm',\n",
      "       'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat', 'source_file_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data.drop(columns=UNSW_NB15_Config.DROP_COLS,inplace=True)\n",
    "data.drop(columns=UNSW_NB15_Config.TIME_COL_NAMES)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2c690c-86a4-49f7-aa9c-58f94529547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns = CATEGORICAL_COLS) # One Hot Encoding for categorical data\n",
    "\n",
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME].apply(str)\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME].apply(str)\n",
    "\n",
    "# Port category encoding: \n",
    "def port_category(port_str):\n",
    "    try:\n",
    "        port_int = int(port_str)\n",
    "        if 1 <= port_int <= 1023:\n",
    "            return [1, 0, 0]  # Well Known\n",
    "        elif 1024 <= port_int <= 49151:\n",
    "            return [0, 1, 0]  # Registered\n",
    "        elif 49152 <= port_int <= 65535:\n",
    "            return [0, 0, 1]  # Dynamic/Private\n",
    "        else:\n",
    "            return [0, 0, 0]  # Out of range, encode as all zeros\n",
    "    except:\n",
    "        return [0, 0, 0]  # Non-integer or missing, encode as all zeros\n",
    "\n",
    "# Create new columns for binary encoding\n",
    "src_port_category = data[SOURCE_PORT_COL_NAME].apply(port_category)\n",
    "dst_port_category = data[DESTINATION_PORT_COL_NAME].apply(port_category)\n",
    "\n",
    "src_port_category_df = pd.DataFrame(src_port_category.tolist(), columns=[f\"{SOURCE_PORT_COL_NAME}_category_{i}\" for i in range(3)])\n",
    "dst_port_category_df = pd.DataFrame(dst_port_category.tolist(), columns=[f\"{DESTINATION_PORT_COL_NAME}_category_{i}\" for i in range(3)])\n",
    "\n",
    "# Concatenate the binary columns to the original dataframe\n",
    "data = pd.concat([data, src_port_category_df, dst_port_category_df], axis=1)\n",
    "\n",
    "data.drop(columns=[SOURCE_PORT_COL_NAME, DESTINATION_PORT_COL_NAME], inplace=True)\n",
    "\n",
    "converted_categorical_cols = [col for col in data.columns if col.startswith(tuple(CATEGORICAL_COLS + [SOURCE_PORT_COL_NAME, DESTINATION_PORT_COL_NAME]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5651ef5b-0a9d-4641-aad7-5738092c46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                  srcip             dstip        dur  sbytes  dbytes  sttl  \\\n",
      "0         10.40.85.1_0       224.0.0.5_0  50.004341     384       0     1   \n",
      "1         59.166.0.6_0   149.171.126.4_0   0.001134     132     164    31   \n",
      "2       175.45.176.0_0  149.171.126.16_0   2.390390    1362     268   254   \n",
      "3         59.166.0.3_0   149.171.126.8_0  34.077175   37358    3380    31   \n",
      "4        10.40.170.2_0     10.40.170.2_0   0.000000      46       0     0   \n",
      "...                ...               ...        ...     ...     ...   ...   \n",
      "543154  175.45.176.1_3  149.171.126.11_3   0.291164     732     468   254   \n",
      "543155  175.45.176.3_3  149.171.126.16_3   0.011751      76     132   254   \n",
      "543156    59.166.0.2_3   149.171.126.4_3   0.002410     146     178    31   \n",
      "543157  175.45.176.1_3  149.171.126.11_3   0.176514   10778     268   254   \n",
      "543158    59.166.0.2_3   149.171.126.2_3   0.049598    2646   25564    31   \n",
      "\n",
      "        dttl  sloss  dloss          Sload  ...  state_RST  state_TST  \\\n",
      "0          0      0      0      51.195557  ...      False      False   \n",
      "1         29      0      0  465608.468800  ...      False      False   \n",
      "2        252      6      1    4233.619141  ...      False      False   \n",
      "3         29     18      8    8601.652344  ...      False      False   \n",
      "4          0      0      0       0.000000  ...      False      False   \n",
      "...      ...    ...    ...            ...  ...        ...        ...   \n",
      "543154   252      3      2   18436.343750  ...      False      False   \n",
      "543155    60      0      0   25870.138670  ...      False      False   \n",
      "543156    29      0      0  242323.656300  ...      False      False   \n",
      "543157   252      5      1  457980.656300  ...      False      False   \n",
      "543158    29      7     15  416629.687500  ...      False      False   \n",
      "\n",
      "        state_TXD  state_URH  sport_category_0  sport_category_1  \\\n",
      "0           False      False                 0                 0   \n",
      "1           False      False                 0                 1   \n",
      "2           False      False                 0                 1   \n",
      "3           False      False                 0                 1   \n",
      "4           False      False                 0                 0   \n",
      "...           ...        ...               ...               ...   \n",
      "543154      False      False                 0                 1   \n",
      "543155      False      False                 0                 1   \n",
      "543156      False      False                 0                 1   \n",
      "543157      False      False                 0                 1   \n",
      "543158      False      False                 0                 1   \n",
      "\n",
      "        sport_category_2  dsport_category_0  dsport_category_1  \\\n",
      "0                      0                  0                  0   \n",
      "1                      0                  1                  0   \n",
      "2                      0                  1                  0   \n",
      "3                      0                  1                  0   \n",
      "4                      0                  0                  0   \n",
      "...                  ...                ...                ...   \n",
      "543154                 0                  0                  1   \n",
      "543155                 0                  0                  1   \n",
      "543156                 0                  1                  0   \n",
      "543157                 0                  0                  1   \n",
      "543158                 0                  0                  1   \n",
      "\n",
      "        dsport_category_2  \n",
      "0                       0  \n",
      "1                       0  \n",
      "2                       0  \n",
      "3                       0  \n",
      "4                       0  \n",
      "...                   ...  \n",
      "543154                  0  \n",
      "543155                  0  \n",
      "543156                  0  \n",
      "543157                  0  \n",
      "543158                  0  \n",
      "\n",
      "[543159 rows x 64 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2d96115-31f9-48cb-b3e6-7853d2d253cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                  srcip             dstip        dur  sbytes  dbytes  sttl  \\\n",
      "0         10.40.85.1_0       224.0.0.5_0  50.004341     384       0     1   \n",
      "1         59.166.0.6_0   149.171.126.4_0   0.001134     132     164    31   \n",
      "2       175.45.176.0_0  149.171.126.16_0   2.390390    1362     268   254   \n",
      "3         59.166.0.3_0   149.171.126.8_0  34.077175   37358    3380    31   \n",
      "4        10.40.170.2_0     10.40.170.2_0   0.000000      46       0     0   \n",
      "...                ...               ...        ...     ...     ...   ...   \n",
      "543154  175.45.176.1_3  149.171.126.11_3   0.291164     732     468   254   \n",
      "543155  175.45.176.3_3  149.171.126.16_3   0.011751      76     132   254   \n",
      "543156    59.166.0.2_3   149.171.126.4_3   0.002410     146     178    31   \n",
      "543157  175.45.176.1_3  149.171.126.11_3   0.176514   10778     268   254   \n",
      "543158    59.166.0.2_3   149.171.126.2_3   0.049598    2646   25564    31   \n",
      "\n",
      "        dttl  sloss  dloss          Sload  ...  state_RST  state_TST  \\\n",
      "0          0      0      0      51.195557  ...      False      False   \n",
      "1         29      0      0  465608.468800  ...      False      False   \n",
      "2        252      6      1    4233.619141  ...      False      False   \n",
      "3         29     18      8    8601.652344  ...      False      False   \n",
      "4          0      0      0       0.000000  ...      False      False   \n",
      "...      ...    ...    ...            ...  ...        ...        ...   \n",
      "543154   252      3      2   18436.343750  ...      False      False   \n",
      "543155    60      0      0   25870.138670  ...      False      False   \n",
      "543156    29      0      0  242323.656300  ...      False      False   \n",
      "543157   252      5      1  457980.656300  ...      False      False   \n",
      "543158    29      7     15  416629.687500  ...      False      False   \n",
      "\n",
      "        state_TXD  state_URH  sport_category_0  sport_category_1  \\\n",
      "0           False      False                 0                 0   \n",
      "1           False      False                 0                 1   \n",
      "2           False      False                 0                 1   \n",
      "3           False      False                 0                 1   \n",
      "4           False      False                 0                 0   \n",
      "...           ...        ...               ...               ...   \n",
      "543154      False      False                 0                 1   \n",
      "543155      False      False                 0                 1   \n",
      "543156      False      False                 0                 1   \n",
      "543157      False      False                 0                 1   \n",
      "543158      False      False                 0                 1   \n",
      "\n",
      "        sport_category_2  dsport_category_0  dsport_category_1  \\\n",
      "0                      0                  0                  0   \n",
      "1                      0                  1                  0   \n",
      "2                      0                  1                  0   \n",
      "3                      0                  1                  0   \n",
      "4                      0                  0                  0   \n",
      "...                  ...                ...                ...   \n",
      "543154                 0                  0                  1   \n",
      "543155                 0                  0                  1   \n",
      "543156                 0                  1                  0   \n",
      "543157                 0                  0                  1   \n",
      "543158                 0                  0                  1   \n",
      "\n",
      "        dsport_category_2  \n",
      "0                       0  \n",
      "1                       0  \n",
      "2                       0  \n",
      "3                       0  \n",
      "4                       0  \n",
      "...                   ...  \n",
      "543154                  0  \n",
      "543155                  0  \n",
      "543156                  0  \n",
      "543157                  0  \n",
      "543158                  0  \n",
      "\n",
      "[543159 rows x 64 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "data.drop(columns=['index'],inplace=True)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1df5c4c-70a2-4566-ae5e-ee3dcc6037a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 dur        sbytes        dbytes           sttl  \\\n",
      "count  543159.000000  5.431590e+05  5.431590e+05  543159.000000   \n",
      "mean        0.703562  5.129376e+03  1.912066e+04     157.223966   \n",
      "std        12.635598  1.202304e+05  1.382834e+05     108.429349   \n",
      "min         0.000000  0.000000e+00  0.000000e+00       0.000000   \n",
      "25%         0.000007  1.140000e+02  0.000000e+00      31.000000   \n",
      "50%         0.000010  2.000000e+02  0.000000e+00     254.000000   \n",
      "75%         0.070875  1.580000e+03  1.936000e+03     254.000000   \n",
      "max      8760.776367  1.435577e+07  1.465753e+07     255.000000   \n",
      "\n",
      "                dttl          sloss          dloss         Sload  \\\n",
      "count  543159.000000  543159.000000  543159.000000  5.431590e+05   \n",
      "mean       38.847354       3.789714       8.637535  6.901181e+07   \n",
      "std        77.059190      45.614073      49.869719  1.425974e+08   \n",
      "min         0.000000       0.000000       0.000000  0.000000e+00   \n",
      "25%         0.000000       0.000000       0.000000  3.760815e+05   \n",
      "50%         0.000000       0.000000       0.000000  4.560000e+07   \n",
      "75%        29.000000       3.000000       4.000000  8.888889e+07   \n",
      "max       254.000000    5319.000000    5507.000000  5.988000e+09   \n",
      "\n",
      "              Dload          Spkts  ...  ct_flw_http_mthd   is_ftp_login  \\\n",
      "count  5.431590e+05  543159.000000  ...     543159.000000  543159.000000   \n",
      "mean   1.145602e+06      20.260456  ...          0.089263       0.011459   \n",
      "std    3.125320e+06     101.785929  ...          0.568852       0.109870   \n",
      "min    0.000000e+00       0.000000  ...          0.000000       0.000000   \n",
      "25%    0.000000e+00       2.000000  ...          0.000000       0.000000   \n",
      "50%    0.000000e+00       2.000000  ...          0.000000       0.000000   \n",
      "75%    4.080209e+05      14.000000  ...          0.000000       0.000000   \n",
      "max    2.248756e+07   10646.000000  ...         36.000000       4.000000   \n",
      "\n",
      "          ct_ftp_cmd     ct_srv_src     ct_srv_dst     ct_dst_ltm  \\\n",
      "count  543159.000000  543159.000000  543159.000000  543159.000000   \n",
      "mean        0.007661      15.025361      14.853214      10.321932   \n",
      "std         0.091356      14.239878      14.314732      10.996982   \n",
      "min         0.000000       1.000000       1.000000       1.000000   \n",
      "25%         0.000000       3.000000       3.000000       2.000000   \n",
      "50%         0.000000       9.000000       8.000000       5.000000   \n",
      "75%         0.000000      26.000000      26.000000      17.000000   \n",
      "max         4.000000      67.000000      67.000000      67.000000   \n",
      "\n",
      "          ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \n",
      "count  543159.000000     543159.000000     543159.000000   543159.000000  \n",
      "mean       10.848566          9.357573          7.219855       13.786578  \n",
      "std        10.976383         11.399195          8.074346       14.983005  \n",
      "min         1.000000          1.000000          1.000000        1.000000  \n",
      "25%         2.000000          1.000000          1.000000        1.000000  \n",
      "50%         6.000000          2.000000          2.000000        5.000000  \n",
      "75%        17.000000         17.000000         15.000000       26.000000  \n",
      "max        67.000000         67.000000         60.000000       67.000000  \n",
      "\n",
      "[8 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_norm = UNSW_NB15_Config.COLS_TO_NORM\n",
    "print(data[cols_to_norm].describe()) # Check if there's any too large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea95177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All other columns processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_numeric_issues(df, cols_to_norm):\n",
    "    for col in cols_to_norm:\n",
    "        try:\n",
    "            # Try to coerce to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Try to clip the column\n",
    "            df[col] = df[col].clip(lower=-1e9, upper=1e9)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Column '{col}' failed with error: {e}\")\n",
    "            print(f\"  - Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "            print(f\"  - Data type: {df[col].dtype}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n✅ All other columns processed successfully.\")\n",
    "\n",
    "check_numeric_issues(data, UNSW_NB15_Config.COLS_TO_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37256006-abc1-44aa-8e74-46d05dc6ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c6e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "Attack label mapping: {'Analysis': 0, 'Backdoors': 1, 'DoS': 2, 'Exploits': 3, 'Fuzzers': 4, 'Generic': 5, 'Normal': 6, 'Reconnaissance': 7, 'Shellcode': 8, 'Worms': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_classes = 2\n",
    "class_map = [0, 1]\n",
    "if MULTICLASS:\n",
    "    le = LabelEncoder()\n",
    "    attack_labels = le.fit_transform(data[ATTACK_CLASS_COL_NAME])\n",
    "    class_map = le.classes_\n",
    "    print(class_map)\n",
    "    print(\"Attack label mapping:\", dict(zip(class_map, range(len(class_map)))))\n",
    "    data[ATTACK_CLASS_COL_NAME] = attack_labels\n",
    "    num_classes = len(class_map)\n",
    "    class_dict = {le.inverse_transform([i])[0]: i for i in range(len(le.classes_))}\n",
    "\n",
    "BENIGN_CLASS_LABEL = le.transform([BENIGN_CLASS_NAME])[0] if MULTICLASS else 0\n",
    "ADVERSARIAL_CLASS_LABEL = len(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d35f4cdd-2716-431f-af50-b34cc3d2d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'state_ACC', 'state_CLO', 'state_CON', 'state_ECO', 'state_ECR', 'state_FIN', 'state_INT', 'state_MAS', 'state_PAR', 'state_REQ', 'state_RST', 'state_TST', 'state_TXD', 'state_URH', 'sport_category_0', 'sport_category_1', 'sport_category_2', 'dsport_category_0', 'dsport_category_1', 'dsport_category_2']\n",
      "Number of training samples: 461685\n",
      "attack_cat\n",
      "6    188595\n",
      "5    183159\n",
      "3     37846\n",
      "4     20609\n",
      "2     13900\n",
      "7     11889\n",
      "0      2275\n",
      "1      1980\n",
      "8      1284\n",
      "9       148\n",
      "Name: count, dtype: int64\n",
      "Number of test samples: 81474\n",
      "attack_cat\n",
      "6    33281\n",
      "5    32322\n",
      "3     6679\n",
      "4     3637\n",
      "2     2453\n",
      "7     2098\n",
      "0      402\n",
      "1      349\n",
      "8      227\n",
      "9       26\n",
      "Name: count, dtype: int64\n",
      "                 srcip             dstip       dur    sbytes    dbytes  \\\n",
      "35798   175.45.176.0_0  149.171.126.11_0  0.024440 -0.037756 -0.136334   \n",
      "454500    59.166.0.2_3   149.171.126.1_3 -0.052447 -0.010009  0.271236   \n",
      "23187   175.45.176.0_0  149.171.126.18_0 -0.017379 -0.033264 -0.136334   \n",
      "489755  175.45.176.3_3  149.171.126.15_3 -0.055680 -0.041715 -0.138272   \n",
      "134305    59.166.0.1_1   149.171.126.4_1 -0.004966  0.269172 -0.113829   \n",
      "\n",
      "            sttl      dttl     sloss     dloss     Sload     Dload     Spkts  \\\n",
      "35798   0.892527  2.766092 -0.039236 -0.153150 -0.554662 -0.365989 -0.100804   \n",
      "454500 -1.164114 -0.127790  0.070379  0.368209 -0.548462  3.128362  0.449370   \n",
      "23187   0.892527  2.766092 -0.039236 -0.153150 -0.554559 -0.365371 -0.100804   \n",
      "489755  0.892527 -0.504124 -0.083082 -0.173202 -0.018194 -0.366555 -0.179401   \n",
      "134305 -1.164114 -0.127790  0.311533 -0.012784 -0.550916 -0.353373  0.311827   \n",
      "\n",
      "           Dpkts      swin      dwin     stcpb     dtcpb   smeansz   dmeansz  \\\n",
      "35798  -0.158623  1.303104  1.304850  0.950180 -0.462170 -0.329716 -0.366711   \n",
      "454500  0.427016  1.303104  1.304850  0.274400  0.303777 -0.329716  2.437416   \n",
      "23187  -0.158623  1.303104  1.304850  1.484216  1.484299  0.017606 -0.366711   \n",
      "489755 -0.215298 -0.767401 -0.766374 -0.720896 -0.720905 -0.342580 -0.526845   \n",
      "134305  0.181426  1.303104  1.304850  1.484216 -0.715505  3.928197 -0.242162   \n",
      "\n",
      "        trans_depth  res_bdy_len      Sjit      Djit       Stime       Ltime  \\\n",
      "35798     -0.134792     -0.05343  0.153264 -0.076895  1421933745  1421933746   \n",
      "454500    -0.134792     -0.05343 -0.080902 -0.175606  1424240533  1424240533   \n",
      "23187     -0.134792     -0.05343  0.012423 -0.150894  1421931706  1421931707   \n",
      "489755    -0.134792     -0.05343 -0.080902 -0.189417  1424245264  1424245264   \n",
      "134305    -0.134792     -0.05343 -0.043454 -0.177189  1424222454  1424222454   \n",
      "\n",
      "         Sintpkt   Dintpkt    tcprtt    synack    ackdat  is_sm_ips_ports  \\\n",
      "35798  -0.008638  0.119318  3.717742  3.612803  3.416380        -0.028049   \n",
      "454500 -0.060137 -0.054933 -0.278494 -0.242636 -0.288018        -0.028049   \n",
      "23187  -0.036975  0.019956  2.899294  2.974987  2.483664        -0.028049   \n",
      "489755 -0.060450 -0.055530 -0.289971 -0.258558 -0.293097        -0.028049   \n",
      "134305 -0.053968 -0.040295 -0.277104 -0.239905 -0.288324        -0.028049   \n",
      "\n",
      "        ct_state_ttl  ct_flw_http_mthd  is_ftp_login  ct_ftp_cmd  ct_srv_src  \\\n",
      "35798      -0.098678         -0.156918     -0.104295   -0.083856   -0.633809   \n",
      "454500     -1.123769         -0.156918     -0.104295   -0.083856   -0.774260   \n",
      "23187      -0.098678         -0.156918     -0.104295   -0.083856   -0.984936   \n",
      "489755      0.926413         -0.156918     -0.104295   -0.083856    0.489797   \n",
      "134305     -1.123769         -0.156918     -0.104295   -0.083856   -0.984936   \n",
      "\n",
      "        ct_srv_dst  ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  \\\n",
      "35798    -0.618469   -0.756748   -0.806147         -0.733173   \n",
      "454500   -0.688327   -0.574879   -0.715042         -0.733173   \n",
      "23187    -0.967760   -0.483945   -0.715042         -0.733173   \n",
      "489755    0.499261    1.061935    1.015949          1.109064   \n",
      "134305   -0.967760   -0.483945   -0.441727         -0.733173   \n",
      "\n",
      "        ct_dst_sport_ltm  ct_dst_src_ltm  attack_cat  source_file_id  \\\n",
      "35798          -0.770324       -0.786664           4               0   \n",
      "454500         -0.770324       -0.786664           6               3   \n",
      "23187          -0.770324       -0.853406           3               0   \n",
      "489755          1.830509        0.548183           5               3   \n",
      "134305         -0.770324       -0.786664           6               1   \n",
      "\n",
      "        state_ACC  state_CLO  state_CON  state_ECO  state_ECR  state_FIN  \\\n",
      "35798       False      False      False      False      False       True   \n",
      "454500      False      False      False      False      False       True   \n",
      "23187       False      False      False      False      False       True   \n",
      "489755      False      False      False      False      False      False   \n",
      "134305      False      False      False      False      False       True   \n",
      "\n",
      "        state_INT  state_MAS  state_PAR  state_REQ  state_RST  state_TST  \\\n",
      "35798       False      False      False      False      False      False   \n",
      "454500      False      False      False      False      False      False   \n",
      "23187       False      False      False      False      False      False   \n",
      "489755       True      False      False      False      False      False   \n",
      "134305      False      False      False      False      False      False   \n",
      "\n",
      "        state_TXD  state_URH  sport_category_0  sport_category_1  \\\n",
      "35798       False      False                 0                 1   \n",
      "454500      False      False                 0                 1   \n",
      "23187       False      False                 0                 1   \n",
      "489755      False      False                 0                 1   \n",
      "134305      False      False                 0                 1   \n",
      "\n",
      "        sport_category_2  dsport_category_0  dsport_category_1  \\\n",
      "35798                  0                  1                  0   \n",
      "454500                 0                  0                  1   \n",
      "23187                  0                  1                  0   \n",
      "489755                 0                  1                  0   \n",
      "134305                 0                  1                  0   \n",
      "\n",
      "        dsport_category_2                                                  h  \n",
      "35798                   0  [0.024439518249307186, -0.03775569116632746, -...  \n",
      "454500                  0  [-0.05244682114681552, -0.01000892950063232, 0...  \n",
      "23187                   0  [-0.01737919714689172, -0.03326430888231206, -...  \n",
      "489755                  0  [-0.05568046603287485, -0.04171476147594104, -...  \n",
      "134305                  0  [-0.004966318747785316, 0.269172066323925, -0....  \n"
     ]
    }
   ],
   "source": [
    "# 70% train, 15% validation, 15% test\n",
    "train_full_df, test_df = train_test_split(\n",
    "     data, test_size=0.15, random_state=42, stratify=data[label_col])\n",
    "\n",
    "\n",
    "feature_cols = UNSW_NB15_Config.COLS_TO_NORM + converted_categorical_cols\n",
    "\n",
    "print('Feature Columns:', feature_cols)\n",
    "\n",
    "train_full_df['h'] = train_full_df[ feature_cols ].values.tolist()\n",
    "test_df['h'] = test_df[ feature_cols ].values.tolist()\n",
    "\n",
    "y_train = train_full_df[label_col]\n",
    "y_test = test_df[label_col]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_full_df))\n",
    "print(y_train.value_counts())\n",
    "print(\"Number of test samples:\", len(test_df))\n",
    "print(y_test.value_counts())\n",
    "\n",
    "print(train_full_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f83bd73-531f-42a0-9f73-e1fd98dce1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, source_ip_col, destination_ip_col, edge_attr, create_using=nx.MultiDiGraph(), **kwargs):\n",
    "    G_nx = nx.from_pandas_edgelist(df, source_ip_col, destination_ip_col, edge_attr, create_using=create_using, **kwargs)\n",
    "    G_pyg = from_networkx(G_nx)\n",
    "\n",
    "    num_nodes = G_pyg.num_nodes\n",
    "    num_edges = G_pyg.num_edges\n",
    "\n",
    "    G_pyg.x = th.ones(num_nodes, len(df['h'].iloc[0])) \n",
    "\n",
    "    edge_attr_list = []\n",
    "    edge_label_list = []\n",
    "\n",
    "    for u, v, key, data in G_nx.edges(keys=True, data=True):\n",
    "        edge_attr_list.append(data['h']) \n",
    "        edge_label_list.append(data[label_col]) \n",
    "\n",
    "    G_pyg.edge_attr = th.tensor(edge_attr_list, dtype=th.float32)\n",
    "    G_pyg.edge_label = th.tensor(edge_label_list, dtype=th.long)\n",
    "\n",
    "    print(\"Number of edges in G_pyg:\", num_edges)\n",
    "    print(\"Number of node in G_pyg:\", num_nodes)\n",
    "    print(\"Shape of node in G_pyg:\", G_pyg.x.shape)\n",
    "    print(\"Shape of edge attr in G_pyg:\", G_pyg.edge_attr.shape)\n",
    "    print(\"Shape of edge label in G_pyg:\", G_pyg.edge_label.shape)\n",
    "\n",
    "    return G_nx, G_pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c8f9cdb-1316-461a-a927-8d67d90d6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 175\n",
      "Shape of node in G_pyg: torch.Size([175, 58])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 58])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41795339-6036-468f-9b9d-2bb68d78ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGELayerPyG(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_dim, out_channels, activation=F.relu):\n",
    "        super().__init__(aggr='mean')  # mean aggregation\n",
    "        self.W_msg = nn.Linear(in_channels + edge_dim, out_channels)\n",
    "        self.W_apply = nn.Linear(in_channels + out_channels, out_channels)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: [num_nodes, in_channels]\n",
    "        # edge_attr: [num_edges, edge_dim]\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j: features of source nodes (neighbours)\n",
    "        msg_input = th.cat([x_j, edge_attr], dim=1)\n",
    "        return self.W_msg(msg_input)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out: [num_nodes, out_channels]\n",
    "        combined = th.cat([x, aggr_out], dim=1)\n",
    "        out = self.W_apply(combined)\n",
    "        return self.activation(out)\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.lin = nn.Linear(in_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, data, z):\n",
    "        row, col = data.edge_index\n",
    "        # Concatenate the features of source and target nodes for each edge\n",
    "        edge_feat = th.cat([z[row], z[col]], dim=1)\n",
    "        return self.lin(edge_feat)\n",
    "\n",
    "class EGraphSAGE(nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels, out_channels, dropout=0.2):\n",
    "        super(EGraphSAGE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGELayerPyG(node_in_channels, edge_in_channels, hidden_channels)\n",
    "        self.conv2 = SAGELayerPyG(hidden_channels, edge_in_channels, hidden_channels)\n",
    "        self.mlp_predictor = MLPPredictor(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return self.mlp_predictor(data, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bca25fef-29d9-40cf-8910-16b24d530693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cccdc850-b98d-4836-b82b-67aa4b9e1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89157faf-e24b-49d6-9c90-6f71dae515b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "385d37f0-713b-4abc-8d7a-3e768ae9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def grid_search(data, epochs, learning_rates, hidden_dims, drop_outs):\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_params = {}\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Precompute the train and validation graphs for all folds\n",
    "    folds = []\n",
    "    for train_idx, val_idx in skf.split(data, data[label_col]):\n",
    "        train_df = data.iloc[train_idx]\n",
    "        val_df = data.iloc[val_idx]\n",
    "\n",
    "        G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "        G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "\n",
    "        G_pyg_train = G_pyg_train.to(device)\n",
    "        G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "        G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "        G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "        G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "        G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "        folds.append((G_pyg_train, G_pyg_val))\n",
    "\n",
    "    params_results = {}\n",
    "    for lr in learning_rates:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for drop_out in drop_outs:\n",
    "                print(f\"Testing with learning rate: {lr}, hidden_dim: {hidden_dim}, drop_out: {drop_out}\")\n",
    "                fold_f1_scores = []\n",
    "\n",
    "                for fold, (G_pyg_train, G_pyg_val) in enumerate(folds):\n",
    "                    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "                    model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                                    edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                                    hidden_channels=hidden_dim,\n",
    "                                    dropout=drop_out,\n",
    "                                    out_channels=num_classes).to(device)\n",
    "\n",
    "                    model.apply(init_weights)\n",
    "\n",
    "                    labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "                    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                                    classes=np.unique(labels),\n",
    "                                                                    y=labels)\n",
    "\n",
    "                    # Normalize to stabilize training\n",
    "                    class_weights = th.FloatTensor(class_weights).to(device)\n",
    "                    print(\"Class weights:\", class_weights)\n",
    "\n",
    "                    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "                    optimizer = th.optim.Adam(model.parameters(), lr=lr)\n",
    "                    scheduler = th.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "                    best_epoch_f1 = 0  # Track the best F1 score for this fold\n",
    "\n",
    "                    for epoch in range(epochs):\n",
    "                        train_loss = 0\n",
    "                        val_loss = 0\n",
    "\n",
    "                        try:\n",
    "                            model.train()\n",
    "                            out = model(G_pyg_train)\n",
    "                            \n",
    "                            optimizer.zero_grad()\n",
    "                            loss = criterion(out, G_pyg_train.edge_label)\n",
    "                            train_loss = loss.item()\n",
    "\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            scheduler.step()\n",
    "\n",
    "                            model.eval()\n",
    "                            with th.no_grad():\n",
    "                                out = model(G_pyg_val)\n",
    "                                loss = criterion(out, G_pyg_val.edge_label)\n",
    "                                val_loss = loss.item()\n",
    "\n",
    "                            val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "\n",
    "                            if val_f1 > best_epoch_f1:\n",
    "                                best_epoch_f1 = val_f1  # Update the best F1 score for this fold\n",
    "                                print(f\"Best F1 Score at epoch {epoch}: {best_epoch_f1:.4f}, Parameters: lr={lr}, hidden_dim{hidden_dim}, drop_out={drop_out}\")\n",
    "\n",
    "                            if epoch % 100 == 0:\n",
    "                                print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred at epoch {epoch}: {str(e)}\")\n",
    "                            break\n",
    "\n",
    "                    fold_f1_scores.append(best_epoch_f1)  # Append the best F1 score for this fold\n",
    "                \n",
    "                avg_f1 = sum(fold_f1_scores) / len(fold_f1_scores)\n",
    "                params_results[(drop_out, lr, hidden_dim)] = {'folds': fold_f1_scores, 'avg_f1': avg_f1}\n",
    "                print(\"Current Results: \", params_results)\n",
    "                print(f\"Average F1 Score for dropout {drop_out}, learning rate {lr}, hidden_dim {hidden_dim}: {avg_f1:.4f}\")\n",
    "\n",
    "                if avg_f1 > best_f1:\n",
    "                    best_f1 = avg_f1\n",
    "                    best_params = {'learning_rate': lr, 'hidden_dim': hidden_dim, 'drop_out': drop_out}\n",
    "\n",
    "        print(f\"Best Parameters: {best_params}, Best F1 Score: {best_f1:.4f}\")\n",
    "        print(\"All results:\", params_results)\n",
    "\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "hidden_dims = [128, 256, 512]\n",
    "drop_outs = [0.2, 0.3, 0.4]\n",
    "\n",
    "# grid_search(train_full_df, epochs=100, learning_rates=learning_rates, hidden_dims=hidden_dims, drop_outs=drop_outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f52b2fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 392432\n",
      "Number of node in G_pyg: 179\n",
      "Shape of node in G_pyg: torch.Size([179, 58])\n",
      "Shape of edge attr in G_pyg: torch.Size([392432, 58])\n",
      "Shape of edge label in G_pyg: torch.Size([392432])\n",
      "Number of edges in G_pyg: 69253\n",
      "Number of node in G_pyg: 173\n",
      "Shape of node in G_pyg: torch.Size([173, 58])\n",
      "Shape of edge attr in G_pyg: torch.Size([69253, 58])\n",
      "Shape of edge label in G_pyg: torch.Size([69253])\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "     train_full_df, test_size=0.15, random_state=42, stratify=train_full_df[label_col])\n",
    "\n",
    "G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88bdffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([2.0291e+01, 2.3317e+01, 3.3215e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8832e+00, 3.5970e+01, 3.1145e+02], device='cuda:0')\n",
      "Epoch 0 Saved best model. Best F1: 0.3825972703716421\n",
      "Epoch 0, Train Loss: 2.3512, Validation Loss: 4.3043, Validation F1: 0.3826\n",
      "Epoch 1 Saved best model. Best F1: 0.7061084189098815\n",
      "Epoch 1, Train Loss: 4.4695, Validation Loss: 3.3078, Validation F1: 0.7061\n",
      "Epoch 2, Train Loss: 3.3759, Validation Loss: 2.5032, Validation F1: 0.3942\n",
      "Epoch 3, Train Loss: 2.5542, Validation Loss: 2.0596, Validation F1: 0.4079\n",
      "Epoch 4 Saved best model. Best F1: 0.760822361308185\n",
      "Epoch 4, Train Loss: 2.0676, Validation Loss: 1.9146, Validation F1: 0.7608\n",
      "Epoch 5, Train Loss: 1.9069, Validation Loss: 1.8758, Validation F1: 0.7551\n",
      "Epoch 6, Train Loss: 1.8505, Validation Loss: 1.8285, Validation F1: 0.7353\n",
      "Epoch 7, Train Loss: 1.8001, Validation Loss: 1.7945, Validation F1: 0.7578\n",
      "Epoch 8, Train Loss: 1.7602, Validation Loss: 1.7861, Validation F1: 0.7419\n",
      "Epoch 9, Train Loss: 1.7430, Validation Loss: 1.7734, Validation F1: 0.7570\n",
      "Epoch 10 Saved best model. Best F1: 0.7674557420386365\n",
      "Epoch 10, Train Loss: 1.7259, Validation Loss: 1.7631, Validation F1: 0.7675\n",
      "Epoch 11 Saved best model. Best F1: 0.7932725887669491\n",
      "Epoch 11, Train Loss: 1.7270, Validation Loss: 1.7606, Validation F1: 0.7933\n",
      "Epoch 12, Train Loss: 1.7012, Validation Loss: 1.7543, Validation F1: 0.7921\n",
      "Epoch 13 Saved best model. Best F1: 0.8128941775915203\n",
      "Epoch 13, Train Loss: 1.7120, Validation Loss: 1.7488, Validation F1: 0.8129\n",
      "Epoch 14, Train Loss: 1.6980, Validation Loss: 1.7496, Validation F1: 0.8013\n",
      "Epoch 15, Train Loss: 1.6836, Validation Loss: 1.7357, Validation F1: 0.7970\n",
      "Epoch 16, Train Loss: 1.6823, Validation Loss: 1.7297, Validation F1: 0.8038\n",
      "Epoch 17 Saved best model. Best F1: 0.8217394886553212\n",
      "Epoch 17, Train Loss: 1.6785, Validation Loss: 1.7225, Validation F1: 0.8217\n",
      "Epoch 18, Train Loss: 1.6663, Validation Loss: 1.7180, Validation F1: 0.8152\n",
      "Epoch 19, Train Loss: 1.6649, Validation Loss: 1.7088, Validation F1: 0.8010\n",
      "Epoch 20, Train Loss: 1.6611, Validation Loss: 1.7184, Validation F1: 0.7943\n",
      "Epoch 21, Train Loss: 1.6563, Validation Loss: 1.7132, Validation F1: 0.7990\n",
      "Epoch 22, Train Loss: 1.6562, Validation Loss: 1.7158, Validation F1: 0.7969\n",
      "Epoch 23, Train Loss: 1.6462, Validation Loss: 1.7098, Validation F1: 0.8120\n",
      "Epoch 24 Saved best model. Best F1: 0.8316468734327627\n",
      "Epoch 24, Train Loss: 1.6484, Validation Loss: 1.7119, Validation F1: 0.8316\n",
      "Epoch 25, Train Loss: 1.6476, Validation Loss: 1.6953, Validation F1: 0.8106\n",
      "Epoch 26, Train Loss: 1.6406, Validation Loss: 1.6996, Validation F1: 0.8042\n",
      "Epoch 27, Train Loss: 1.6374, Validation Loss: 1.7022, Validation F1: 0.7980\n",
      "Epoch 28, Train Loss: 1.6385, Validation Loss: 1.7018, Validation F1: 0.8207\n",
      "Epoch 29, Train Loss: 1.6305, Validation Loss: 1.7039, Validation F1: 0.8227\n",
      "Epoch 30, Train Loss: 1.6339, Validation Loss: 1.6999, Validation F1: 0.8158\n",
      "Epoch 31, Train Loss: 1.6319, Validation Loss: 1.6952, Validation F1: 0.8037\n",
      "Epoch 32, Train Loss: 1.6314, Validation Loss: 1.6963, Validation F1: 0.8054\n",
      "Epoch 33, Train Loss: 1.6272, Validation Loss: 1.7008, Validation F1: 0.8134\n",
      "Epoch 34, Train Loss: 1.6255, Validation Loss: 1.7043, Validation F1: 0.8179\n",
      "Epoch 35, Train Loss: 1.6242, Validation Loss: 1.6944, Validation F1: 0.8078\n",
      "Epoch 36, Train Loss: 1.6250, Validation Loss: 1.6976, Validation F1: 0.8145\n",
      "Epoch 37, Train Loss: 1.6202, Validation Loss: 1.7007, Validation F1: 0.8115\n",
      "Epoch 38, Train Loss: 1.6230, Validation Loss: 1.6929, Validation F1: 0.8155\n",
      "Epoch 39, Train Loss: 1.6185, Validation Loss: 1.6982, Validation F1: 0.8092\n",
      "Epoch 40, Train Loss: 1.6169, Validation Loss: 1.6929, Validation F1: 0.8091\n",
      "Epoch 41, Train Loss: 1.6168, Validation Loss: 1.6838, Validation F1: 0.8103\n",
      "Epoch 42, Train Loss: 1.6145, Validation Loss: 1.6815, Validation F1: 0.8068\n",
      "Epoch 43, Train Loss: 1.6156, Validation Loss: 1.6831, Validation F1: 0.8179\n",
      "Epoch 44, Train Loss: 1.6162, Validation Loss: 1.6848, Validation F1: 0.8091\n",
      "Epoch 45, Train Loss: 1.6120, Validation Loss: 1.6829, Validation F1: 0.8030\n",
      "Epoch 46, Train Loss: 1.6143, Validation Loss: 1.6746, Validation F1: 0.7956\n",
      "Epoch 47, Train Loss: 1.6129, Validation Loss: 1.6794, Validation F1: 0.8094\n",
      "Epoch 48, Train Loss: 1.6126, Validation Loss: 1.6898, Validation F1: 0.8235\n",
      "Epoch 49, Train Loss: 1.6155, Validation Loss: 1.6841, Validation F1: 0.8208\n",
      "Epoch 50, Train Loss: 1.6111, Validation Loss: 1.6779, Validation F1: 0.8164\n",
      "Epoch 51, Train Loss: 1.6109, Validation Loss: 1.6755, Validation F1: 0.8060\n",
      "Epoch 52, Train Loss: 1.6087, Validation Loss: 1.6744, Validation F1: 0.8166\n",
      "Epoch 53, Train Loss: 1.6057, Validation Loss: 1.6739, Validation F1: 0.8146\n",
      "Epoch 54, Train Loss: 1.6079, Validation Loss: 1.6755, Validation F1: 0.8202\n",
      "Epoch 55, Train Loss: 1.6065, Validation Loss: 1.6780, Validation F1: 0.8137\n",
      "Epoch 56, Train Loss: 1.6075, Validation Loss: 1.6931, Validation F1: 0.8122\n",
      "Epoch 57, Train Loss: 1.6108, Validation Loss: 1.6913, Validation F1: 0.8210\n",
      "Epoch 58, Train Loss: 1.6091, Validation Loss: 1.6820, Validation F1: 0.8166\n",
      "Epoch 59, Train Loss: 1.6058, Validation Loss: 1.6880, Validation F1: 0.8122\n",
      "Epoch 60, Train Loss: 1.6034, Validation Loss: 1.6727, Validation F1: 0.8111\n",
      "Epoch 61, Train Loss: 1.6031, Validation Loss: 1.6769, Validation F1: 0.8099\n",
      "Epoch 62, Train Loss: 1.6037, Validation Loss: 1.6771, Validation F1: 0.8114\n",
      "Epoch 63, Train Loss: 1.6049, Validation Loss: 1.6753, Validation F1: 0.8152\n",
      "Epoch 64, Train Loss: 1.6025, Validation Loss: 1.6799, Validation F1: 0.8161\n",
      "Epoch 65, Train Loss: 1.6036, Validation Loss: 1.6732, Validation F1: 0.8145\n",
      "Epoch 66, Train Loss: 1.5995, Validation Loss: 1.6805, Validation F1: 0.8112\n",
      "Epoch 67, Train Loss: 1.5992, Validation Loss: 1.6680, Validation F1: 0.8072\n",
      "Epoch 68, Train Loss: 1.5974, Validation Loss: 1.6699, Validation F1: 0.8152\n",
      "Epoch 69, Train Loss: 1.5984, Validation Loss: 1.6632, Validation F1: 0.8193\n",
      "Epoch 70, Train Loss: 1.5988, Validation Loss: 1.6842, Validation F1: 0.8098\n",
      "Epoch 71, Train Loss: 1.6001, Validation Loss: 1.6710, Validation F1: 0.8124\n",
      "Epoch 72, Train Loss: 1.5997, Validation Loss: 1.6634, Validation F1: 0.8082\n",
      "Epoch 73, Train Loss: 1.5957, Validation Loss: 1.6724, Validation F1: 0.8178\n",
      "Epoch 74, Train Loss: 1.5978, Validation Loss: 1.6800, Validation F1: 0.8216\n",
      "Epoch 75, Train Loss: 1.6004, Validation Loss: 1.6835, Validation F1: 0.8205\n",
      "Epoch 76, Train Loss: 1.6018, Validation Loss: 1.6762, Validation F1: 0.8143\n",
      "Epoch 77, Train Loss: 1.5968, Validation Loss: 1.6659, Validation F1: 0.8038\n",
      "Epoch 78, Train Loss: 1.5959, Validation Loss: 1.6654, Validation F1: 0.8067\n",
      "Epoch 79, Train Loss: 1.5951, Validation Loss: 1.6733, Validation F1: 0.8195\n",
      "Epoch 80, Train Loss: 1.6001, Validation Loss: 1.6677, Validation F1: 0.8025\n",
      "Epoch 81, Train Loss: 1.5972, Validation Loss: 1.6718, Validation F1: 0.8090\n",
      "Epoch 82, Train Loss: 1.6023, Validation Loss: 1.6764, Validation F1: 0.8117\n",
      "Epoch 83, Train Loss: 1.6016, Validation Loss: 1.6683, Validation F1: 0.8188\n",
      "Epoch 84, Train Loss: 1.5940, Validation Loss: 1.6609, Validation F1: 0.8139\n",
      "Epoch 85, Train Loss: 1.5938, Validation Loss: 1.6606, Validation F1: 0.8190\n",
      "Epoch 86, Train Loss: 1.5941, Validation Loss: 1.6754, Validation F1: 0.8078\n",
      "Epoch 87, Train Loss: 1.5916, Validation Loss: 1.6666, Validation F1: 0.8100\n",
      "Epoch 88, Train Loss: 1.5934, Validation Loss: 1.6695, Validation F1: 0.8179\n",
      "Epoch 89, Train Loss: 1.5943, Validation Loss: 1.6669, Validation F1: 0.8261\n",
      "Epoch 90, Train Loss: 1.5962, Validation Loss: 1.6753, Validation F1: 0.8224\n",
      "Epoch 91, Train Loss: 1.5957, Validation Loss: 1.6707, Validation F1: 0.8090\n",
      "Epoch 92, Train Loss: 1.5935, Validation Loss: 1.6614, Validation F1: 0.8140\n",
      "Epoch 93, Train Loss: 1.5952, Validation Loss: 1.6647, Validation F1: 0.8117\n",
      "Epoch 94, Train Loss: 1.5957, Validation Loss: 1.6778, Validation F1: 0.8199\n",
      "Epoch 95, Train Loss: 1.5914, Validation Loss: 1.6972, Validation F1: 0.8080\n",
      "Epoch 96, Train Loss: 1.5929, Validation Loss: 1.6745, Validation F1: 0.8164\n",
      "Epoch 97, Train Loss: 1.5893, Validation Loss: 1.6679, Validation F1: 0.8183\n",
      "Epoch 98, Train Loss: 1.5939, Validation Loss: 1.6708, Validation F1: 0.8130\n",
      "Epoch 99, Train Loss: 1.5894, Validation Loss: 1.6888, Validation F1: 0.8146\n",
      "Epoch 100, Train Loss: 1.5916, Validation Loss: 1.6805, Validation F1: 0.8136\n",
      "Epoch 101, Train Loss: 1.5919, Validation Loss: 1.6705, Validation F1: 0.8179\n",
      "Epoch 102, Train Loss: 1.5869, Validation Loss: 1.6709, Validation F1: 0.8147\n",
      "Epoch 103, Train Loss: 1.5880, Validation Loss: 1.6642, Validation F1: 0.8175\n",
      "Epoch 104, Train Loss: 1.5904, Validation Loss: 1.6721, Validation F1: 0.8108\n",
      "Epoch 105, Train Loss: 1.5867, Validation Loss: 1.6760, Validation F1: 0.8026\n",
      "Epoch 106, Train Loss: 1.5900, Validation Loss: 1.6711, Validation F1: 0.8067\n",
      "Epoch 107, Train Loss: 1.5879, Validation Loss: 1.6674, Validation F1: 0.8118\n",
      "Epoch 108, Train Loss: 1.5875, Validation Loss: 1.6720, Validation F1: 0.8159\n",
      "Epoch 109, Train Loss: 1.5872, Validation Loss: 1.6678, Validation F1: 0.8108\n",
      "Epoch 110, Train Loss: 1.5855, Validation Loss: 1.6681, Validation F1: 0.8077\n",
      "Epoch 111, Train Loss: 1.5845, Validation Loss: 1.6568, Validation F1: 0.8158\n",
      "Epoch 112, Train Loss: 1.5853, Validation Loss: 1.6778, Validation F1: 0.8160\n",
      "Epoch 113, Train Loss: 1.5834, Validation Loss: 1.6677, Validation F1: 0.8181\n",
      "Epoch 114, Train Loss: 1.5842, Validation Loss: 1.6707, Validation F1: 0.8184\n",
      "Epoch 115, Train Loss: 1.5854, Validation Loss: 1.6653, Validation F1: 0.8100\n",
      "Epoch 116, Train Loss: 1.5838, Validation Loss: 1.6719, Validation F1: 0.8093\n",
      "Epoch 117, Train Loss: 1.5862, Validation Loss: 1.6575, Validation F1: 0.8114\n",
      "Epoch 118, Train Loss: 1.5822, Validation Loss: 1.6639, Validation F1: 0.8088\n",
      "Epoch 119, Train Loss: 1.5859, Validation Loss: 1.6760, Validation F1: 0.8090\n",
      "Epoch 120, Train Loss: 1.5817, Validation Loss: 1.6743, Validation F1: 0.8109\n",
      "Epoch 121, Train Loss: 1.5856, Validation Loss: 1.6650, Validation F1: 0.8050\n",
      "Epoch 122, Train Loss: 1.5816, Validation Loss: 1.6548, Validation F1: 0.8110\n",
      "Epoch 123, Train Loss: 1.5814, Validation Loss: 1.6662, Validation F1: 0.8107\n",
      "Epoch 124, Train Loss: 1.5829, Validation Loss: 1.6760, Validation F1: 0.8038\n",
      "Epoch 125, Train Loss: 1.5849, Validation Loss: 1.6557, Validation F1: 0.8172\n",
      "Epoch 126, Train Loss: 1.5806, Validation Loss: 1.6576, Validation F1: 0.8161\n",
      "Epoch 127, Train Loss: 1.5877, Validation Loss: 1.6677, Validation F1: 0.8171\n",
      "Epoch 128, Train Loss: 1.5869, Validation Loss: 1.6930, Validation F1: 0.8088\n",
      "Epoch 129, Train Loss: 1.5851, Validation Loss: 1.6624, Validation F1: 0.8203\n",
      "Epoch 130, Train Loss: 1.5849, Validation Loss: 1.6566, Validation F1: 0.8142\n",
      "Epoch 131, Train Loss: 1.5819, Validation Loss: 1.6611, Validation F1: 0.8136\n",
      "Epoch 132, Train Loss: 1.5806, Validation Loss: 1.6685, Validation F1: 0.8126\n",
      "Epoch 133, Train Loss: 1.5837, Validation Loss: 1.6575, Validation F1: 0.8183\n",
      "Epoch 134, Train Loss: 1.5839, Validation Loss: 1.6544, Validation F1: 0.8189\n",
      "Epoch 135, Train Loss: 1.5866, Validation Loss: 1.6684, Validation F1: 0.8040\n",
      "Epoch 136, Train Loss: 1.5833, Validation Loss: 1.6832, Validation F1: 0.8009\n",
      "Epoch 137, Train Loss: 1.5827, Validation Loss: 1.6615, Validation F1: 0.8109\n",
      "Epoch 138, Train Loss: 1.5829, Validation Loss: 1.6699, Validation F1: 0.8148\n",
      "Epoch 139, Train Loss: 1.5775, Validation Loss: 1.6821, Validation F1: 0.8177\n",
      "Epoch 140, Train Loss: 1.5823, Validation Loss: 1.6729, Validation F1: 0.8155\n",
      "Epoch 141, Train Loss: 1.5832, Validation Loss: 1.6818, Validation F1: 0.8082\n",
      "Epoch 142, Train Loss: 1.5780, Validation Loss: 1.6697, Validation F1: 0.8066\n",
      "Epoch 143, Train Loss: 1.5797, Validation Loss: 1.6688, Validation F1: 0.8174\n",
      "Epoch 144, Train Loss: 1.5764, Validation Loss: 1.6709, Validation F1: 0.8091\n",
      "Epoch 145, Train Loss: 1.5776, Validation Loss: 1.6864, Validation F1: 0.8135\n",
      "Epoch 146, Train Loss: 1.5772, Validation Loss: 1.6743, Validation F1: 0.8141\n",
      "Epoch 147, Train Loss: 1.5787, Validation Loss: 1.6811, Validation F1: 0.8085\n",
      "Epoch 148, Train Loss: 1.5798, Validation Loss: 1.6753, Validation F1: 0.8150\n",
      "Epoch 149, Train Loss: 1.5756, Validation Loss: 1.6684, Validation F1: 0.8182\n",
      "Epoch 150, Train Loss: 1.5798, Validation Loss: 1.6827, Validation F1: 0.8122\n",
      "Epoch 151, Train Loss: 1.5768, Validation Loss: 1.6706, Validation F1: 0.8090\n",
      "Epoch 152, Train Loss: 1.5774, Validation Loss: 1.6801, Validation F1: 0.8132\n",
      "Epoch 153, Train Loss: 1.5758, Validation Loss: 1.6824, Validation F1: 0.8184\n",
      "Epoch 154, Train Loss: 1.5827, Validation Loss: 1.6666, Validation F1: 0.8175\n",
      "Epoch 155, Train Loss: 1.5741, Validation Loss: 1.6735, Validation F1: 0.8113\n",
      "Epoch 156, Train Loss: 1.5769, Validation Loss: 1.6599, Validation F1: 0.8148\n",
      "Epoch 157, Train Loss: 1.5765, Validation Loss: 1.6540, Validation F1: 0.8143\n",
      "Epoch 158, Train Loss: 1.5768, Validation Loss: 1.6696, Validation F1: 0.8158\n",
      "Epoch 159, Train Loss: 1.5743, Validation Loss: 1.6611, Validation F1: 0.8164\n",
      "Epoch 160, Train Loss: 1.5766, Validation Loss: 1.6626, Validation F1: 0.8151\n",
      "Epoch 161, Train Loss: 1.5770, Validation Loss: 1.6570, Validation F1: 0.8152\n",
      "Epoch 162, Train Loss: 1.5746, Validation Loss: 1.6822, Validation F1: 0.8091\n",
      "Epoch 163, Train Loss: 1.5842, Validation Loss: 1.6712, Validation F1: 0.8148\n",
      "Epoch 164, Train Loss: 1.5775, Validation Loss: 1.6591, Validation F1: 0.8166\n",
      "Epoch 165, Train Loss: 1.5763, Validation Loss: 1.6688, Validation F1: 0.8121\n",
      "Epoch 166, Train Loss: 1.5772, Validation Loss: 1.6751, Validation F1: 0.8138\n",
      "Epoch 167, Train Loss: 1.5759, Validation Loss: 1.6843, Validation F1: 0.8116\n",
      "Epoch 168, Train Loss: 1.5742, Validation Loss: 1.6689, Validation F1: 0.8118\n",
      "Epoch 169, Train Loss: 1.5765, Validation Loss: 1.6711, Validation F1: 0.8148\n",
      "Epoch 170, Train Loss: 1.5803, Validation Loss: 1.6668, Validation F1: 0.8150\n",
      "Epoch 171, Train Loss: 1.5764, Validation Loss: 1.6919, Validation F1: 0.8069\n",
      "Epoch 172, Train Loss: 1.5755, Validation Loss: 1.6745, Validation F1: 0.8072\n",
      "Epoch 173, Train Loss: 1.5809, Validation Loss: 1.6697, Validation F1: 0.8174\n",
      "Epoch 174, Train Loss: 1.5756, Validation Loss: 1.6785, Validation F1: 0.8193\n",
      "Epoch 175, Train Loss: 1.5783, Validation Loss: 1.6885, Validation F1: 0.8088\n",
      "Epoch 176, Train Loss: 1.5759, Validation Loss: 1.6765, Validation F1: 0.8123\n",
      "Epoch 177, Train Loss: 1.5732, Validation Loss: 1.6684, Validation F1: 0.8136\n",
      "Epoch 178, Train Loss: 1.5769, Validation Loss: 1.6791, Validation F1: 0.8145\n",
      "Epoch 179, Train Loss: 1.5716, Validation Loss: 1.6856, Validation F1: 0.8180\n",
      "Epoch 180, Train Loss: 1.5813, Validation Loss: 1.6883, Validation F1: 0.8124\n",
      "Epoch 181, Train Loss: 1.5734, Validation Loss: 1.6725, Validation F1: 0.8106\n",
      "Epoch 182, Train Loss: 1.5788, Validation Loss: 1.6567, Validation F1: 0.8113\n",
      "Epoch 183, Train Loss: 1.5724, Validation Loss: 1.6667, Validation F1: 0.8175\n",
      "Epoch 184, Train Loss: 1.5741, Validation Loss: 1.6592, Validation F1: 0.8197\n",
      "Epoch 185, Train Loss: 1.5773, Validation Loss: 1.6736, Validation F1: 0.8146\n",
      "Epoch 186, Train Loss: 1.5798, Validation Loss: 1.6741, Validation F1: 0.8059\n",
      "Epoch 187, Train Loss: 1.5758, Validation Loss: 1.6700, Validation F1: 0.8100\n",
      "Epoch 188, Train Loss: 1.5798, Validation Loss: 1.6684, Validation F1: 0.8167\n",
      "Epoch 189, Train Loss: 1.5774, Validation Loss: 1.6729, Validation F1: 0.8169\n",
      "Epoch 190, Train Loss: 1.5788, Validation Loss: 1.6984, Validation F1: 0.8141\n",
      "Epoch 191, Train Loss: 1.5814, Validation Loss: 1.6625, Validation F1: 0.8126\n",
      "Epoch 192, Train Loss: 1.5735, Validation Loss: 1.6588, Validation F1: 0.8150\n",
      "Epoch 193, Train Loss: 1.5789, Validation Loss: 1.6689, Validation F1: 0.8099\n",
      "Epoch 194, Train Loss: 1.5735, Validation Loss: 1.6909, Validation F1: 0.8015\n",
      "Epoch 195, Train Loss: 1.5762, Validation Loss: 1.6697, Validation F1: 0.8182\n",
      "Epoch 196, Train Loss: 1.5770, Validation Loss: 1.6728, Validation F1: 0.8229\n",
      "Epoch 197, Train Loss: 1.5751, Validation Loss: 1.6697, Validation F1: 0.8165\n",
      "Epoch 198, Train Loss: 1.5736, Validation Loss: 1.6785, Validation F1: 0.8098\n",
      "Epoch 199, Train Loss: 1.5734, Validation Loss: 1.6799, Validation F1: 0.8133\n",
      "Epoch 200, Train Loss: 1.5713, Validation Loss: 1.6741, Validation F1: 0.8130\n",
      "Epoch 201, Train Loss: 1.5751, Validation Loss: 1.6845, Validation F1: 0.8126\n",
      "Epoch 202, Train Loss: 1.5748, Validation Loss: 1.6786, Validation F1: 0.8175\n",
      "Epoch 203, Train Loss: 1.5713, Validation Loss: 1.6595, Validation F1: 0.8135\n",
      "Epoch 204, Train Loss: 1.5767, Validation Loss: 1.6683, Validation F1: 0.8135\n",
      "Epoch 205, Train Loss: 1.5766, Validation Loss: 1.6803, Validation F1: 0.8057\n",
      "Epoch 206, Train Loss: 1.5765, Validation Loss: 1.6819, Validation F1: 0.8093\n",
      "Epoch 207, Train Loss: 1.5732, Validation Loss: 1.6744, Validation F1: 0.8165\n",
      "Epoch 208, Train Loss: 1.5754, Validation Loss: 1.6630, Validation F1: 0.8170\n",
      "Epoch 209, Train Loss: 1.5723, Validation Loss: 1.6720, Validation F1: 0.8104\n",
      "Epoch 210, Train Loss: 1.5709, Validation Loss: 1.6797, Validation F1: 0.8091\n",
      "Epoch 211, Train Loss: 1.5731, Validation Loss: 1.6648, Validation F1: 0.8165\n",
      "Epoch 212, Train Loss: 1.5746, Validation Loss: 1.6753, Validation F1: 0.8177\n",
      "Epoch 213, Train Loss: 1.5722, Validation Loss: 1.6816, Validation F1: 0.8149\n",
      "Epoch 214, Train Loss: 1.5772, Validation Loss: 1.6671, Validation F1: 0.8151\n",
      "Epoch 215, Train Loss: 1.5744, Validation Loss: 1.6695, Validation F1: 0.8126\n",
      "Epoch 216, Train Loss: 1.5729, Validation Loss: 1.6778, Validation F1: 0.8102\n",
      "Epoch 217, Train Loss: 1.5715, Validation Loss: 1.6930, Validation F1: 0.8158\n",
      "Epoch 218, Train Loss: 1.5717, Validation Loss: 1.6837, Validation F1: 0.8189\n",
      "Epoch 219, Train Loss: 1.5706, Validation Loss: 1.6719, Validation F1: 0.8157\n",
      "Epoch 220, Train Loss: 1.5707, Validation Loss: 1.6755, Validation F1: 0.8103\n",
      "Epoch 221, Train Loss: 1.5709, Validation Loss: 1.6898, Validation F1: 0.8089\n",
      "Epoch 222, Train Loss: 1.5733, Validation Loss: 1.6678, Validation F1: 0.8138\n",
      "Epoch 223, Train Loss: 1.5717, Validation Loss: 1.6702, Validation F1: 0.8158\n",
      "Epoch 224, Train Loss: 1.5750, Validation Loss: 1.6809, Validation F1: 0.8157\n",
      "Epoch 225, Train Loss: 1.5718, Validation Loss: 1.6899, Validation F1: 0.8094\n",
      "Epoch 226, Train Loss: 1.5751, Validation Loss: 1.6769, Validation F1: 0.8126\n",
      "Epoch 227, Train Loss: 1.5707, Validation Loss: 1.6701, Validation F1: 0.8135\n",
      "Epoch 228, Train Loss: 1.5712, Validation Loss: 1.6852, Validation F1: 0.8122\n",
      "Epoch 229, Train Loss: 1.5726, Validation Loss: 1.6802, Validation F1: 0.8110\n",
      "Epoch 230, Train Loss: 1.5735, Validation Loss: 1.6849, Validation F1: 0.8216\n",
      "Epoch 231, Train Loss: 1.5719, Validation Loss: 1.6707, Validation F1: 0.8178\n",
      "Epoch 232, Train Loss: 1.5713, Validation Loss: 1.6725, Validation F1: 0.8125\n",
      "Epoch 233, Train Loss: 1.5769, Validation Loss: 1.6719, Validation F1: 0.8143\n",
      "Epoch 234, Train Loss: 1.5724, Validation Loss: 1.6813, Validation F1: 0.8142\n",
      "Epoch 235, Train Loss: 1.5724, Validation Loss: 1.6748, Validation F1: 0.8156\n",
      "Epoch 236, Train Loss: 1.5704, Validation Loss: 1.6749, Validation F1: 0.8137\n",
      "Epoch 237, Train Loss: 1.5719, Validation Loss: 1.6733, Validation F1: 0.8105\n",
      "Epoch 238, Train Loss: 1.5706, Validation Loss: 1.6740, Validation F1: 0.8120\n",
      "Epoch 239, Train Loss: 1.5716, Validation Loss: 1.6727, Validation F1: 0.8134\n",
      "Epoch 240, Train Loss: 1.5722, Validation Loss: 1.6761, Validation F1: 0.8142\n",
      "Epoch 241, Train Loss: 1.5747, Validation Loss: 1.6781, Validation F1: 0.8180\n",
      "Epoch 242, Train Loss: 1.5733, Validation Loss: 1.6656, Validation F1: 0.8144\n",
      "Epoch 243, Train Loss: 1.5702, Validation Loss: 1.6692, Validation F1: 0.8112\n",
      "Epoch 244, Train Loss: 1.5734, Validation Loss: 1.6846, Validation F1: 0.8101\n",
      "Epoch 245, Train Loss: 1.5729, Validation Loss: 1.6932, Validation F1: 0.8090\n",
      "Epoch 246, Train Loss: 1.5767, Validation Loss: 1.6877, Validation F1: 0.8164\n",
      "Epoch 247, Train Loss: 1.5757, Validation Loss: 1.6829, Validation F1: 0.8209\n",
      "Epoch 248, Train Loss: 1.5726, Validation Loss: 1.6787, Validation F1: 0.8031\n",
      "Epoch 249, Train Loss: 1.5724, Validation Loss: 1.6840, Validation F1: 0.8116\n",
      "Epoch 250, Train Loss: 1.5714, Validation Loss: 1.6787, Validation F1: 0.8151\n",
      "Epoch 251, Train Loss: 1.5762, Validation Loss: 1.6798, Validation F1: 0.8194\n",
      "Epoch 252, Train Loss: 1.5716, Validation Loss: 1.7214, Validation F1: 0.8120\n",
      "Epoch 253, Train Loss: 1.5776, Validation Loss: 1.6818, Validation F1: 0.8135\n",
      "Epoch 254, Train Loss: 1.5702, Validation Loss: 1.6769, Validation F1: 0.8121\n",
      "Epoch 255, Train Loss: 1.5764, Validation Loss: 1.6750, Validation F1: 0.8148\n",
      "Epoch 256, Train Loss: 1.5740, Validation Loss: 1.7056, Validation F1: 0.8098\n",
      "Epoch 257, Train Loss: 1.5731, Validation Loss: 1.6925, Validation F1: 0.8141\n",
      "Epoch 258, Train Loss: 1.5770, Validation Loss: 1.6766, Validation F1: 0.8174\n",
      "Epoch 259, Train Loss: 1.5707, Validation Loss: 1.6756, Validation F1: 0.8114\n",
      "Epoch 260, Train Loss: 1.5718, Validation Loss: 1.6677, Validation F1: 0.8143\n",
      "Epoch 261, Train Loss: 1.5720, Validation Loss: 1.6745, Validation F1: 0.8114\n",
      "Epoch 262, Train Loss: 1.5701, Validation Loss: 1.6687, Validation F1: 0.8157\n",
      "Epoch 263, Train Loss: 1.5690, Validation Loss: 1.6854, Validation F1: 0.8182\n",
      "Epoch 264, Train Loss: 1.5732, Validation Loss: 1.6799, Validation F1: 0.8136\n",
      "Epoch 265, Train Loss: 1.5721, Validation Loss: 1.6661, Validation F1: 0.8136\n",
      "Epoch 266, Train Loss: 1.5707, Validation Loss: 1.6790, Validation F1: 0.8130\n",
      "Epoch 267, Train Loss: 1.5731, Validation Loss: 1.6794, Validation F1: 0.8113\n",
      "Epoch 268, Train Loss: 1.5698, Validation Loss: 1.6898, Validation F1: 0.8139\n",
      "Epoch 269, Train Loss: 1.5695, Validation Loss: 1.6788, Validation F1: 0.8135\n",
      "Epoch 270, Train Loss: 1.5725, Validation Loss: 1.6768, Validation F1: 0.8140\n",
      "Epoch 271, Train Loss: 1.5730, Validation Loss: 1.6824, Validation F1: 0.8064\n",
      "Epoch 272, Train Loss: 1.5709, Validation Loss: 1.6835, Validation F1: 0.8064\n",
      "Epoch 273, Train Loss: 1.5770, Validation Loss: 1.6590, Validation F1: 0.8154\n",
      "Epoch 274, Train Loss: 1.5740, Validation Loss: 1.6674, Validation F1: 0.8196\n",
      "Epoch 275, Train Loss: 1.5719, Validation Loss: 1.6810, Validation F1: 0.8111\n",
      "Epoch 276, Train Loss: 1.5734, Validation Loss: 1.6875, Validation F1: 0.8098\n",
      "Epoch 277, Train Loss: 1.5714, Validation Loss: 1.6674, Validation F1: 0.8104\n",
      "Epoch 278, Train Loss: 1.5690, Validation Loss: 1.6769, Validation F1: 0.8126\n",
      "Epoch 279, Train Loss: 1.5688, Validation Loss: 1.6755, Validation F1: 0.8156\n",
      "Epoch 280, Train Loss: 1.5683, Validation Loss: 1.6808, Validation F1: 0.8162\n",
      "Epoch 281, Train Loss: 1.5703, Validation Loss: 1.6899, Validation F1: 0.8141\n",
      "Epoch 282, Train Loss: 1.5702, Validation Loss: 1.6746, Validation F1: 0.8120\n",
      "Epoch 283, Train Loss: 1.5673, Validation Loss: 1.6706, Validation F1: 0.8127\n",
      "Epoch 284, Train Loss: 1.5708, Validation Loss: 1.6755, Validation F1: 0.8149\n",
      "Epoch 285, Train Loss: 1.5714, Validation Loss: 1.6720, Validation F1: 0.8124\n",
      "Epoch 286, Train Loss: 1.5688, Validation Loss: 1.6820, Validation F1: 0.8123\n",
      "Epoch 287, Train Loss: 1.5685, Validation Loss: 1.6757, Validation F1: 0.8130\n",
      "Epoch 288, Train Loss: 1.5707, Validation Loss: 1.6803, Validation F1: 0.8149\n",
      "Epoch 289, Train Loss: 1.5682, Validation Loss: 1.6777, Validation F1: 0.8115\n",
      "Epoch 290, Train Loss: 1.5688, Validation Loss: 1.6868, Validation F1: 0.8135\n",
      "Epoch 291, Train Loss: 1.5674, Validation Loss: 1.6863, Validation F1: 0.8133\n",
      "Epoch 292, Train Loss: 1.5687, Validation Loss: 1.6944, Validation F1: 0.8163\n",
      "Epoch 293, Train Loss: 1.5683, Validation Loss: 1.6797, Validation F1: 0.8132\n",
      "Epoch 294, Train Loss: 1.5676, Validation Loss: 1.6818, Validation F1: 0.8165\n",
      "Epoch 295, Train Loss: 1.5687, Validation Loss: 1.6765, Validation F1: 0.8129\n",
      "Epoch 296, Train Loss: 1.5712, Validation Loss: 1.6778, Validation F1: 0.8148\n",
      "Epoch 297, Train Loss: 1.5712, Validation Loss: 1.6809, Validation F1: 0.8178\n",
      "Epoch 298, Train Loss: 1.5683, Validation Loss: 1.6822, Validation F1: 0.8210\n",
      "Epoch 299, Train Loss: 1.5676, Validation Loss: 1.6705, Validation F1: 0.8114\n",
      "Epoch 300, Train Loss: 1.5679, Validation Loss: 1.6736, Validation F1: 0.8101\n",
      "Epoch 301, Train Loss: 1.5681, Validation Loss: 1.6795, Validation F1: 0.8158\n",
      "Epoch 302, Train Loss: 1.5672, Validation Loss: 1.6745, Validation F1: 0.8158\n",
      "Epoch 303, Train Loss: 1.5710, Validation Loss: 1.6714, Validation F1: 0.8144\n",
      "Epoch 304, Train Loss: 1.5672, Validation Loss: 1.6885, Validation F1: 0.8133\n",
      "Epoch 305, Train Loss: 1.5722, Validation Loss: 1.6810, Validation F1: 0.8105\n",
      "Epoch 306, Train Loss: 1.5666, Validation Loss: 1.6627, Validation F1: 0.8119\n",
      "Epoch 307, Train Loss: 1.5711, Validation Loss: 1.6652, Validation F1: 0.8139\n",
      "Epoch 308, Train Loss: 1.5666, Validation Loss: 1.6938, Validation F1: 0.8123\n",
      "Epoch 309, Train Loss: 1.5694, Validation Loss: 1.6868, Validation F1: 0.8145\n",
      "Epoch 310, Train Loss: 1.5676, Validation Loss: 1.6763, Validation F1: 0.8145\n",
      "Epoch 311, Train Loss: 1.5720, Validation Loss: 1.6752, Validation F1: 0.8119\n",
      "Epoch 312, Train Loss: 1.5686, Validation Loss: 1.6761, Validation F1: 0.8108\n",
      "Epoch 313, Train Loss: 1.5665, Validation Loss: 1.6819, Validation F1: 0.8139\n",
      "Epoch 314, Train Loss: 1.5654, Validation Loss: 1.6729, Validation F1: 0.8168\n",
      "Epoch 315, Train Loss: 1.5676, Validation Loss: 1.6773, Validation F1: 0.8154\n",
      "Epoch 316, Train Loss: 1.5674, Validation Loss: 1.6822, Validation F1: 0.8056\n",
      "Epoch 317, Train Loss: 1.5657, Validation Loss: 1.6755, Validation F1: 0.8083\n",
      "Epoch 318, Train Loss: 1.5665, Validation Loss: 1.6666, Validation F1: 0.8118\n",
      "Epoch 319, Train Loss: 1.5671, Validation Loss: 1.6739, Validation F1: 0.8191\n",
      "Epoch 320, Train Loss: 1.5686, Validation Loss: 1.6686, Validation F1: 0.8158\n",
      "Epoch 321, Train Loss: 1.5684, Validation Loss: 1.6811, Validation F1: 0.8113\n",
      "Epoch 322, Train Loss: 1.5677, Validation Loss: 1.6640, Validation F1: 0.8144\n",
      "Epoch 323, Train Loss: 1.5659, Validation Loss: 1.6723, Validation F1: 0.8178\n",
      "Epoch 324, Train Loss: 1.5660, Validation Loss: 1.6785, Validation F1: 0.8135\n",
      "Epoch 325, Train Loss: 1.5677, Validation Loss: 1.6815, Validation F1: 0.8107\n",
      "Epoch 326, Train Loss: 1.5648, Validation Loss: 1.6738, Validation F1: 0.8136\n",
      "Epoch 327, Train Loss: 1.5680, Validation Loss: 1.6709, Validation F1: 0.8146\n",
      "Epoch 328, Train Loss: 1.5657, Validation Loss: 1.6821, Validation F1: 0.8140\n",
      "Epoch 329, Train Loss: 1.5683, Validation Loss: 1.6886, Validation F1: 0.8120\n",
      "Epoch 330, Train Loss: 1.5683, Validation Loss: 1.6772, Validation F1: 0.8092\n",
      "Epoch 331, Train Loss: 1.5676, Validation Loss: 1.6810, Validation F1: 0.8127\n",
      "Epoch 332, Train Loss: 1.5680, Validation Loss: 1.6749, Validation F1: 0.8160\n",
      "Epoch 333, Train Loss: 1.5660, Validation Loss: 1.6674, Validation F1: 0.8140\n",
      "Epoch 334, Train Loss: 1.5671, Validation Loss: 1.6961, Validation F1: 0.8092\n",
      "Epoch 335, Train Loss: 1.5671, Validation Loss: 1.6862, Validation F1: 0.8093\n",
      "Epoch 336, Train Loss: 1.5659, Validation Loss: 1.6816, Validation F1: 0.8191\n",
      "Epoch 337, Train Loss: 1.5671, Validation Loss: 1.6703, Validation F1: 0.8145\n",
      "Epoch 338, Train Loss: 1.5685, Validation Loss: 1.6840, Validation F1: 0.8107\n",
      "Epoch 339, Train Loss: 1.5684, Validation Loss: 1.6705, Validation F1: 0.8104\n",
      "Epoch 340, Train Loss: 1.5670, Validation Loss: 1.6721, Validation F1: 0.8143\n",
      "Epoch 341, Train Loss: 1.5655, Validation Loss: 1.6737, Validation F1: 0.8121\n",
      "Epoch 342, Train Loss: 1.5681, Validation Loss: 1.6793, Validation F1: 0.8101\n",
      "Epoch 343, Train Loss: 1.5661, Validation Loss: 1.6806, Validation F1: 0.8098\n",
      "Epoch 344, Train Loss: 1.5677, Validation Loss: 1.6798, Validation F1: 0.8135\n",
      "Epoch 345, Train Loss: 1.5646, Validation Loss: 1.6749, Validation F1: 0.8123\n",
      "Epoch 346, Train Loss: 1.5667, Validation Loss: 1.6840, Validation F1: 0.8119\n",
      "Epoch 347, Train Loss: 1.5661, Validation Loss: 1.6741, Validation F1: 0.8123\n",
      "Epoch 348, Train Loss: 1.5652, Validation Loss: 1.6733, Validation F1: 0.8149\n",
      "Epoch 349, Train Loss: 1.5655, Validation Loss: 1.6751, Validation F1: 0.8116\n",
      "Epoch 350, Train Loss: 1.5653, Validation Loss: 1.6855, Validation F1: 0.8117\n",
      "Epoch 351, Train Loss: 1.5665, Validation Loss: 1.6713, Validation F1: 0.8151\n",
      "Epoch 352, Train Loss: 1.5649, Validation Loss: 1.6779, Validation F1: 0.8127\n",
      "Epoch 353, Train Loss: 1.5651, Validation Loss: 1.6794, Validation F1: 0.8087\n",
      "Epoch 354, Train Loss: 1.5669, Validation Loss: 1.6820, Validation F1: 0.8085\n",
      "Epoch 355, Train Loss: 1.5672, Validation Loss: 1.6774, Validation F1: 0.8121\n",
      "Epoch 356, Train Loss: 1.5655, Validation Loss: 1.6752, Validation F1: 0.8155\n",
      "Epoch 357, Train Loss: 1.5662, Validation Loss: 1.6822, Validation F1: 0.8106\n",
      "Epoch 358, Train Loss: 1.5657, Validation Loss: 1.6864, Validation F1: 0.8085\n",
      "Epoch 359, Train Loss: 1.5670, Validation Loss: 1.6842, Validation F1: 0.8125\n",
      "Epoch 360, Train Loss: 1.5665, Validation Loss: 1.6654, Validation F1: 0.8157\n",
      "Epoch 361, Train Loss: 1.5708, Validation Loss: 1.6875, Validation F1: 0.8065\n",
      "Epoch 362, Train Loss: 1.5675, Validation Loss: 1.6851, Validation F1: 0.8104\n",
      "Epoch 363, Train Loss: 1.5685, Validation Loss: 1.6769, Validation F1: 0.8126\n",
      "Epoch 364, Train Loss: 1.5668, Validation Loss: 1.6739, Validation F1: 0.8138\n",
      "Epoch 365, Train Loss: 1.5654, Validation Loss: 1.6842, Validation F1: 0.8137\n",
      "Epoch 366, Train Loss: 1.5644, Validation Loss: 1.6836, Validation F1: 0.8087\n",
      "Epoch 367, Train Loss: 1.5682, Validation Loss: 1.6727, Validation F1: 0.8144\n",
      "Epoch 368, Train Loss: 1.5639, Validation Loss: 1.6805, Validation F1: 0.8176\n",
      "Epoch 369, Train Loss: 1.5659, Validation Loss: 1.6781, Validation F1: 0.8099\n",
      "Epoch 370, Train Loss: 1.5638, Validation Loss: 1.6865, Validation F1: 0.8071\n",
      "Epoch 371, Train Loss: 1.5681, Validation Loss: 1.6746, Validation F1: 0.8138\n",
      "Epoch 372, Train Loss: 1.5675, Validation Loss: 1.6710, Validation F1: 0.8166\n",
      "Epoch 373, Train Loss: 1.5659, Validation Loss: 1.6725, Validation F1: 0.8138\n",
      "Epoch 374, Train Loss: 1.5653, Validation Loss: 1.6769, Validation F1: 0.8103\n",
      "Epoch 375, Train Loss: 1.5650, Validation Loss: 1.6800, Validation F1: 0.8108\n",
      "Epoch 376, Train Loss: 1.5685, Validation Loss: 1.6799, Validation F1: 0.8161\n",
      "Epoch 377, Train Loss: 1.5657, Validation Loss: 1.6839, Validation F1: 0.8117\n",
      "Epoch 378, Train Loss: 1.5674, Validation Loss: 1.6926, Validation F1: 0.8123\n",
      "Epoch 379, Train Loss: 1.5653, Validation Loss: 1.6762, Validation F1: 0.8127\n",
      "Epoch 380, Train Loss: 1.5647, Validation Loss: 1.6696, Validation F1: 0.8146\n",
      "Epoch 381, Train Loss: 1.5659, Validation Loss: 1.6777, Validation F1: 0.8094\n",
      "Epoch 382, Train Loss: 1.5646, Validation Loss: 1.6806, Validation F1: 0.8140\n",
      "Epoch 383, Train Loss: 1.5659, Validation Loss: 1.6850, Validation F1: 0.8123\n",
      "Epoch 384, Train Loss: 1.5641, Validation Loss: 1.6749, Validation F1: 0.8133\n",
      "Epoch 385, Train Loss: 1.5677, Validation Loss: 1.6731, Validation F1: 0.8124\n",
      "Epoch 386, Train Loss: 1.5667, Validation Loss: 1.6822, Validation F1: 0.8101\n",
      "Epoch 387, Train Loss: 1.5674, Validation Loss: 1.6840, Validation F1: 0.8147\n",
      "Epoch 388, Train Loss: 1.5652, Validation Loss: 1.6756, Validation F1: 0.8149\n",
      "Epoch 389, Train Loss: 1.5643, Validation Loss: 1.6778, Validation F1: 0.8096\n",
      "Epoch 390, Train Loss: 1.5671, Validation Loss: 1.6824, Validation F1: 0.8065\n",
      "Epoch 391, Train Loss: 1.5662, Validation Loss: 1.6884, Validation F1: 0.8081\n",
      "Epoch 392, Train Loss: 1.5661, Validation Loss: 1.6753, Validation F1: 0.8143\n",
      "Epoch 393, Train Loss: 1.5659, Validation Loss: 1.6819, Validation F1: 0.8166\n",
      "Epoch 394, Train Loss: 1.5647, Validation Loss: 1.6846, Validation F1: 0.8119\n",
      "Epoch 395, Train Loss: 1.5667, Validation Loss: 1.6888, Validation F1: 0.8079\n",
      "Epoch 396, Train Loss: 1.5660, Validation Loss: 1.6736, Validation F1: 0.8129\n",
      "Epoch 397, Train Loss: 1.5646, Validation Loss: 1.6696, Validation F1: 0.8136\n",
      "Epoch 398, Train Loss: 1.5655, Validation Loss: 1.6688, Validation F1: 0.8173\n",
      "Epoch 399, Train Loss: 1.5639, Validation Loss: 1.6785, Validation F1: 0.8091\n",
      "Epoch 400, Train Loss: 1.5661, Validation Loss: 1.6786, Validation F1: 0.8138\n",
      "Epoch 401, Train Loss: 1.5629, Validation Loss: 1.6706, Validation F1: 0.8141\n",
      "Epoch 402, Train Loss: 1.5644, Validation Loss: 1.6716, Validation F1: 0.8126\n",
      "Epoch 403, Train Loss: 1.5658, Validation Loss: 1.6737, Validation F1: 0.8092\n",
      "Epoch 404, Train Loss: 1.5642, Validation Loss: 1.6804, Validation F1: 0.8140\n",
      "Epoch 405, Train Loss: 1.5667, Validation Loss: 1.6867, Validation F1: 0.8155\n",
      "Epoch 406, Train Loss: 1.5672, Validation Loss: 1.6813, Validation F1: 0.8091\n",
      "Epoch 407, Train Loss: 1.5639, Validation Loss: 1.6725, Validation F1: 0.8117\n",
      "Epoch 408, Train Loss: 1.5648, Validation Loss: 1.6788, Validation F1: 0.8141\n",
      "Epoch 409, Train Loss: 1.5633, Validation Loss: 1.6842, Validation F1: 0.8118\n",
      "Epoch 410, Train Loss: 1.5672, Validation Loss: 1.6853, Validation F1: 0.8138\n",
      "Epoch 411, Train Loss: 1.5649, Validation Loss: 1.6815, Validation F1: 0.8149\n",
      "Epoch 412, Train Loss: 1.5636, Validation Loss: 1.6776, Validation F1: 0.8142\n",
      "Epoch 413, Train Loss: 1.5646, Validation Loss: 1.6822, Validation F1: 0.8141\n",
      "Epoch 414, Train Loss: 1.5638, Validation Loss: 1.6886, Validation F1: 0.8105\n",
      "Epoch 415, Train Loss: 1.5637, Validation Loss: 1.6790, Validation F1: 0.8131\n",
      "Epoch 416, Train Loss: 1.5657, Validation Loss: 1.6805, Validation F1: 0.8162\n",
      "Epoch 417, Train Loss: 1.5654, Validation Loss: 1.6831, Validation F1: 0.8109\n",
      "Epoch 418, Train Loss: 1.5642, Validation Loss: 1.6738, Validation F1: 0.8105\n",
      "Epoch 419, Train Loss: 1.5641, Validation Loss: 1.6860, Validation F1: 0.8091\n",
      "Epoch 420, Train Loss: 1.5646, Validation Loss: 1.6750, Validation F1: 0.8126\n",
      "Epoch 421, Train Loss: 1.5661, Validation Loss: 1.6812, Validation F1: 0.8106\n",
      "Epoch 422, Train Loss: 1.5658, Validation Loss: 1.6819, Validation F1: 0.8115\n",
      "Epoch 423, Train Loss: 1.5648, Validation Loss: 1.6814, Validation F1: 0.8147\n",
      "Epoch 424, Train Loss: 1.5644, Validation Loss: 1.6740, Validation F1: 0.8151\n",
      "Epoch 425, Train Loss: 1.5648, Validation Loss: 1.6713, Validation F1: 0.8141\n",
      "Epoch 426, Train Loss: 1.5658, Validation Loss: 1.6742, Validation F1: 0.8093\n",
      "Epoch 427, Train Loss: 1.5659, Validation Loss: 1.6787, Validation F1: 0.8138\n",
      "Epoch 428, Train Loss: 1.5651, Validation Loss: 1.6791, Validation F1: 0.8146\n",
      "Epoch 429, Train Loss: 1.5644, Validation Loss: 1.6747, Validation F1: 0.8124\n",
      "Epoch 430, Train Loss: 1.5632, Validation Loss: 1.6726, Validation F1: 0.8124\n",
      "Epoch 431, Train Loss: 1.5644, Validation Loss: 1.6750, Validation F1: 0.8113\n",
      "Epoch 432, Train Loss: 1.5634, Validation Loss: 1.6793, Validation F1: 0.8123\n",
      "Epoch 433, Train Loss: 1.5645, Validation Loss: 1.6828, Validation F1: 0.8158\n",
      "Epoch 434, Train Loss: 1.5633, Validation Loss: 1.6891, Validation F1: 0.8119\n",
      "Epoch 435, Train Loss: 1.5639, Validation Loss: 1.6814, Validation F1: 0.8087\n",
      "Epoch 436, Train Loss: 1.5662, Validation Loss: 1.6850, Validation F1: 0.8112\n",
      "Epoch 437, Train Loss: 1.5629, Validation Loss: 1.6784, Validation F1: 0.8120\n",
      "Epoch 438, Train Loss: 1.5635, Validation Loss: 1.6748, Validation F1: 0.8151\n",
      "Epoch 439, Train Loss: 1.5624, Validation Loss: 1.6789, Validation F1: 0.8134\n",
      "Epoch 440, Train Loss: 1.5645, Validation Loss: 1.6728, Validation F1: 0.8129\n",
      "Epoch 441, Train Loss: 1.5633, Validation Loss: 1.6774, Validation F1: 0.8089\n",
      "Epoch 442, Train Loss: 1.5643, Validation Loss: 1.6725, Validation F1: 0.8094\n",
      "Epoch 443, Train Loss: 1.5654, Validation Loss: 1.6826, Validation F1: 0.8115\n",
      "Epoch 444, Train Loss: 1.5630, Validation Loss: 1.6750, Validation F1: 0.8153\n",
      "Epoch 445, Train Loss: 1.5624, Validation Loss: 1.6749, Validation F1: 0.8170\n",
      "Epoch 446, Train Loss: 1.5628, Validation Loss: 1.6765, Validation F1: 0.8147\n",
      "Epoch 447, Train Loss: 1.5666, Validation Loss: 1.6859, Validation F1: 0.8104\n",
      "Epoch 448, Train Loss: 1.5631, Validation Loss: 1.6860, Validation F1: 0.8123\n",
      "Epoch 449, Train Loss: 1.5650, Validation Loss: 1.6666, Validation F1: 0.8134\n",
      "Epoch 450, Train Loss: 1.5635, Validation Loss: 1.6761, Validation F1: 0.8146\n",
      "Epoch 451, Train Loss: 1.5637, Validation Loss: 1.6879, Validation F1: 0.8157\n",
      "Epoch 452, Train Loss: 1.5639, Validation Loss: 1.6776, Validation F1: 0.8135\n",
      "Epoch 453, Train Loss: 1.5653, Validation Loss: 1.6841, Validation F1: 0.8123\n",
      "Epoch 454, Train Loss: 1.5646, Validation Loss: 1.6881, Validation F1: 0.8149\n",
      "Epoch 455, Train Loss: 1.5622, Validation Loss: 1.6764, Validation F1: 0.8103\n",
      "Epoch 456, Train Loss: 1.5629, Validation Loss: 1.6778, Validation F1: 0.8117\n",
      "Epoch 457, Train Loss: 1.5627, Validation Loss: 1.6684, Validation F1: 0.8151\n",
      "Epoch 458, Train Loss: 1.5629, Validation Loss: 1.6773, Validation F1: 0.8145\n",
      "Epoch 459, Train Loss: 1.5642, Validation Loss: 1.6768, Validation F1: 0.8148\n",
      "Epoch 460, Train Loss: 1.5633, Validation Loss: 1.6836, Validation F1: 0.8142\n",
      "Epoch 461, Train Loss: 1.5624, Validation Loss: 1.6793, Validation F1: 0.8133\n",
      "Epoch 462, Train Loss: 1.5650, Validation Loss: 1.6764, Validation F1: 0.8126\n",
      "Epoch 463, Train Loss: 1.5624, Validation Loss: 1.6794, Validation F1: 0.8113\n",
      "Epoch 464, Train Loss: 1.5620, Validation Loss: 1.6805, Validation F1: 0.8161\n",
      "Epoch 465, Train Loss: 1.5626, Validation Loss: 1.6692, Validation F1: 0.8131\n",
      "Epoch 466, Train Loss: 1.5641, Validation Loss: 1.6756, Validation F1: 0.8112\n",
      "Epoch 467, Train Loss: 1.5634, Validation Loss: 1.6886, Validation F1: 0.8078\n",
      "Epoch 468, Train Loss: 1.5622, Validation Loss: 1.6791, Validation F1: 0.8113\n",
      "Epoch 469, Train Loss: 1.5639, Validation Loss: 1.6780, Validation F1: 0.8116\n",
      "Epoch 470, Train Loss: 1.5617, Validation Loss: 1.6773, Validation F1: 0.8132\n",
      "Epoch 471, Train Loss: 1.5620, Validation Loss: 1.6760, Validation F1: 0.8156\n",
      "Epoch 472, Train Loss: 1.5629, Validation Loss: 1.6742, Validation F1: 0.8113\n",
      "Epoch 473, Train Loss: 1.5634, Validation Loss: 1.6764, Validation F1: 0.8132\n",
      "Epoch 474, Train Loss: 1.5625, Validation Loss: 1.6817, Validation F1: 0.8127\n",
      "Epoch 475, Train Loss: 1.5624, Validation Loss: 1.6794, Validation F1: 0.8145\n",
      "Epoch 476, Train Loss: 1.5612, Validation Loss: 1.6870, Validation F1: 0.8130\n",
      "Epoch 477, Train Loss: 1.5613, Validation Loss: 1.6831, Validation F1: 0.8131\n",
      "Epoch 478, Train Loss: 1.5633, Validation Loss: 1.6757, Validation F1: 0.8121\n",
      "Epoch 479, Train Loss: 1.5611, Validation Loss: 1.6796, Validation F1: 0.8135\n",
      "Epoch 480, Train Loss: 1.5629, Validation Loss: 1.6760, Validation F1: 0.8134\n",
      "Epoch 481, Train Loss: 1.5639, Validation Loss: 1.6777, Validation F1: 0.8123\n",
      "Epoch 482, Train Loss: 1.5640, Validation Loss: 1.6890, Validation F1: 0.8078\n",
      "Epoch 483, Train Loss: 1.5625, Validation Loss: 1.6806, Validation F1: 0.8124\n",
      "Epoch 484, Train Loss: 1.5644, Validation Loss: 1.6704, Validation F1: 0.8137\n",
      "Epoch 485, Train Loss: 1.5627, Validation Loss: 1.6748, Validation F1: 0.8135\n",
      "Epoch 486, Train Loss: 1.5640, Validation Loss: 1.6857, Validation F1: 0.8105\n",
      "Epoch 487, Train Loss: 1.5654, Validation Loss: 1.6802, Validation F1: 0.8080\n",
      "Epoch 488, Train Loss: 1.5640, Validation Loss: 1.6912, Validation F1: 0.8131\n",
      "Epoch 489, Train Loss: 1.5640, Validation Loss: 1.6796, Validation F1: 0.8142\n",
      "Epoch 490, Train Loss: 1.5625, Validation Loss: 1.6804, Validation F1: 0.8110\n",
      "Epoch 491, Train Loss: 1.5639, Validation Loss: 1.6890, Validation F1: 0.8112\n",
      "Epoch 492, Train Loss: 1.5636, Validation Loss: 1.6786, Validation F1: 0.8108\n",
      "Epoch 493, Train Loss: 1.5611, Validation Loss: 1.6821, Validation F1: 0.8121\n",
      "Epoch 494, Train Loss: 1.5639, Validation Loss: 1.6810, Validation F1: 0.8111\n",
      "Epoch 495, Train Loss: 1.5618, Validation Loss: 1.6744, Validation F1: 0.8109\n",
      "Epoch 496, Train Loss: 1.5612, Validation Loss: 1.6720, Validation F1: 0.8136\n",
      "Epoch 497, Train Loss: 1.5610, Validation Loss: 1.6753, Validation F1: 0.8101\n",
      "Epoch 498, Train Loss: 1.5610, Validation Loss: 1.6790, Validation F1: 0.8131\n",
      "Epoch 499, Train Loss: 1.5643, Validation Loss: 1.6802, Validation F1: 0.8118\n",
      "Epoch 500, Train Loss: 1.5614, Validation Loss: 1.6818, Validation F1: 0.8135\n",
      "Epoch 501, Train Loss: 1.5619, Validation Loss: 1.6887, Validation F1: 0.8133\n",
      "Epoch 502, Train Loss: 1.5632, Validation Loss: 1.6777, Validation F1: 0.8107\n",
      "Epoch 503, Train Loss: 1.5649, Validation Loss: 1.6825, Validation F1: 0.8114\n",
      "Epoch 504, Train Loss: 1.5627, Validation Loss: 1.6799, Validation F1: 0.8121\n",
      "Epoch 505, Train Loss: 1.5617, Validation Loss: 1.6767, Validation F1: 0.8138\n",
      "Epoch 506, Train Loss: 1.5625, Validation Loss: 1.6726, Validation F1: 0.8126\n",
      "Epoch 507, Train Loss: 1.5628, Validation Loss: 1.6750, Validation F1: 0.8117\n",
      "Epoch 508, Train Loss: 1.5614, Validation Loss: 1.6840, Validation F1: 0.8113\n",
      "Epoch 509, Train Loss: 1.5612, Validation Loss: 1.6783, Validation F1: 0.8118\n",
      "Epoch 510, Train Loss: 1.5608, Validation Loss: 1.6758, Validation F1: 0.8132\n",
      "Epoch 511, Train Loss: 1.5611, Validation Loss: 1.6785, Validation F1: 0.8127\n",
      "Epoch 512, Train Loss: 1.5627, Validation Loss: 1.6790, Validation F1: 0.8113\n",
      "Epoch 513, Train Loss: 1.5637, Validation Loss: 1.6934, Validation F1: 0.8100\n",
      "Epoch 514, Train Loss: 1.5615, Validation Loss: 1.6818, Validation F1: 0.8107\n",
      "Epoch 515, Train Loss: 1.5629, Validation Loss: 1.6820, Validation F1: 0.8102\n",
      "Epoch 516, Train Loss: 1.5616, Validation Loss: 1.6805, Validation F1: 0.8093\n",
      "Epoch 517, Train Loss: 1.5636, Validation Loss: 1.6872, Validation F1: 0.8099\n",
      "Epoch 518, Train Loss: 1.5637, Validation Loss: 1.6889, Validation F1: 0.8101\n",
      "Epoch 519, Train Loss: 1.5614, Validation Loss: 1.6907, Validation F1: 0.8117\n",
      "Epoch 520, Train Loss: 1.5641, Validation Loss: 1.6881, Validation F1: 0.8123\n",
      "Epoch 521, Train Loss: 1.5641, Validation Loss: 1.6886, Validation F1: 0.8087\n",
      "Epoch 522, Train Loss: 1.5636, Validation Loss: 1.6810, Validation F1: 0.8081\n",
      "Epoch 523, Train Loss: 1.5633, Validation Loss: 1.6804, Validation F1: 0.8106\n",
      "Epoch 524, Train Loss: 1.5635, Validation Loss: 1.6809, Validation F1: 0.8119\n",
      "Epoch 525, Train Loss: 1.5622, Validation Loss: 1.6783, Validation F1: 0.8128\n",
      "Epoch 526, Train Loss: 1.5609, Validation Loss: 1.6806, Validation F1: 0.8110\n",
      "Epoch 527, Train Loss: 1.5626, Validation Loss: 1.6785, Validation F1: 0.8112\n",
      "Epoch 528, Train Loss: 1.5630, Validation Loss: 1.6792, Validation F1: 0.8119\n",
      "Epoch 529, Train Loss: 1.5606, Validation Loss: 1.6810, Validation F1: 0.8117\n",
      "Epoch 530, Train Loss: 1.5645, Validation Loss: 1.6756, Validation F1: 0.8126\n",
      "Epoch 531, Train Loss: 1.5620, Validation Loss: 1.6828, Validation F1: 0.8105\n",
      "Epoch 532, Train Loss: 1.5610, Validation Loss: 1.6777, Validation F1: 0.8092\n",
      "Epoch 533, Train Loss: 1.5610, Validation Loss: 1.6752, Validation F1: 0.8111\n",
      "Epoch 534, Train Loss: 1.5623, Validation Loss: 1.6757, Validation F1: 0.8120\n",
      "Epoch 535, Train Loss: 1.5623, Validation Loss: 1.6880, Validation F1: 0.8105\n",
      "Epoch 536, Train Loss: 1.5618, Validation Loss: 1.6809, Validation F1: 0.8136\n",
      "Epoch 537, Train Loss: 1.5617, Validation Loss: 1.6806, Validation F1: 0.8161\n",
      "Epoch 538, Train Loss: 1.5627, Validation Loss: 1.6887, Validation F1: 0.8110\n",
      "Epoch 539, Train Loss: 1.5607, Validation Loss: 1.6725, Validation F1: 0.8115\n",
      "Epoch 540, Train Loss: 1.5626, Validation Loss: 1.6771, Validation F1: 0.8112\n",
      "Epoch 541, Train Loss: 1.5622, Validation Loss: 1.6786, Validation F1: 0.8113\n",
      "Epoch 542, Train Loss: 1.5608, Validation Loss: 1.6823, Validation F1: 0.8122\n",
      "Epoch 543, Train Loss: 1.5611, Validation Loss: 1.6797, Validation F1: 0.8125\n",
      "Epoch 544, Train Loss: 1.5611, Validation Loss: 1.6754, Validation F1: 0.8111\n",
      "Epoch 545, Train Loss: 1.5604, Validation Loss: 1.6750, Validation F1: 0.8110\n",
      "Epoch 546, Train Loss: 1.5610, Validation Loss: 1.6769, Validation F1: 0.8117\n",
      "Epoch 547, Train Loss: 1.5611, Validation Loss: 1.6773, Validation F1: 0.8149\n",
      "Epoch 548, Train Loss: 1.5628, Validation Loss: 1.6737, Validation F1: 0.8117\n",
      "Epoch 549, Train Loss: 1.5640, Validation Loss: 1.6819, Validation F1: 0.8119\n",
      "Epoch 550, Train Loss: 1.5612, Validation Loss: 1.6749, Validation F1: 0.8129\n",
      "Epoch 551, Train Loss: 1.5615, Validation Loss: 1.6785, Validation F1: 0.8108\n",
      "Epoch 552, Train Loss: 1.5610, Validation Loss: 1.6705, Validation F1: 0.8129\n",
      "Epoch 553, Train Loss: 1.5620, Validation Loss: 1.6778, Validation F1: 0.8112\n",
      "Epoch 554, Train Loss: 1.5607, Validation Loss: 1.6794, Validation F1: 0.8120\n",
      "Epoch 555, Train Loss: 1.5627, Validation Loss: 1.6773, Validation F1: 0.8113\n",
      "Epoch 556, Train Loss: 1.5633, Validation Loss: 1.6738, Validation F1: 0.8112\n",
      "Epoch 557, Train Loss: 1.5630, Validation Loss: 1.6818, Validation F1: 0.8119\n",
      "Epoch 558, Train Loss: 1.5624, Validation Loss: 1.6732, Validation F1: 0.8130\n",
      "Epoch 559, Train Loss: 1.5631, Validation Loss: 1.6806, Validation F1: 0.8117\n",
      "Epoch 560, Train Loss: 1.5603, Validation Loss: 1.6795, Validation F1: 0.8104\n",
      "Epoch 561, Train Loss: 1.5625, Validation Loss: 1.6808, Validation F1: 0.8117\n",
      "Epoch 562, Train Loss: 1.5607, Validation Loss: 1.6812, Validation F1: 0.8130\n",
      "Epoch 563, Train Loss: 1.5605, Validation Loss: 1.6853, Validation F1: 0.8135\n",
      "Epoch 564, Train Loss: 1.5612, Validation Loss: 1.6822, Validation F1: 0.8092\n",
      "Epoch 565, Train Loss: 1.5603, Validation Loss: 1.6870, Validation F1: 0.8084\n",
      "Epoch 566, Train Loss: 1.5638, Validation Loss: 1.6833, Validation F1: 0.8097\n",
      "Epoch 567, Train Loss: 1.5625, Validation Loss: 1.6857, Validation F1: 0.8118\n",
      "Epoch 568, Train Loss: 1.5617, Validation Loss: 1.6783, Validation F1: 0.8115\n",
      "Epoch 569, Train Loss: 1.5626, Validation Loss: 1.6800, Validation F1: 0.8120\n",
      "Epoch 570, Train Loss: 1.5607, Validation Loss: 1.6751, Validation F1: 0.8131\n",
      "Epoch 571, Train Loss: 1.5620, Validation Loss: 1.6711, Validation F1: 0.8131\n",
      "Epoch 572, Train Loss: 1.5610, Validation Loss: 1.6728, Validation F1: 0.8125\n",
      "Epoch 573, Train Loss: 1.5603, Validation Loss: 1.6759, Validation F1: 0.8128\n",
      "Epoch 574, Train Loss: 1.5614, Validation Loss: 1.6753, Validation F1: 0.8118\n",
      "Epoch 575, Train Loss: 1.5635, Validation Loss: 1.6824, Validation F1: 0.8126\n",
      "Epoch 576, Train Loss: 1.5631, Validation Loss: 1.6831, Validation F1: 0.8106\n",
      "Epoch 577, Train Loss: 1.5607, Validation Loss: 1.6774, Validation F1: 0.8114\n",
      "Epoch 578, Train Loss: 1.5606, Validation Loss: 1.6780, Validation F1: 0.8127\n",
      "Epoch 579, Train Loss: 1.5643, Validation Loss: 1.6744, Validation F1: 0.8128\n",
      "Epoch 580, Train Loss: 1.5613, Validation Loss: 1.6800, Validation F1: 0.8105\n",
      "Epoch 581, Train Loss: 1.5606, Validation Loss: 1.6779, Validation F1: 0.8128\n",
      "Epoch 582, Train Loss: 1.5602, Validation Loss: 1.6780, Validation F1: 0.8108\n",
      "Epoch 583, Train Loss: 1.5604, Validation Loss: 1.6725, Validation F1: 0.8126\n",
      "Epoch 584, Train Loss: 1.5603, Validation Loss: 1.6767, Validation F1: 0.8108\n",
      "Epoch 585, Train Loss: 1.5616, Validation Loss: 1.6845, Validation F1: 0.8132\n",
      "Epoch 586, Train Loss: 1.5599, Validation Loss: 1.6770, Validation F1: 0.8104\n",
      "Epoch 587, Train Loss: 1.5630, Validation Loss: 1.6805, Validation F1: 0.8135\n",
      "Epoch 588, Train Loss: 1.5610, Validation Loss: 1.6755, Validation F1: 0.8126\n",
      "Epoch 589, Train Loss: 1.5602, Validation Loss: 1.6883, Validation F1: 0.8123\n",
      "Epoch 590, Train Loss: 1.5603, Validation Loss: 1.6771, Validation F1: 0.8109\n",
      "Epoch 591, Train Loss: 1.5624, Validation Loss: 1.6822, Validation F1: 0.8095\n",
      "Epoch 592, Train Loss: 1.5613, Validation Loss: 1.6764, Validation F1: 0.8103\n",
      "Epoch 593, Train Loss: 1.5620, Validation Loss: 1.6767, Validation F1: 0.8134\n",
      "Epoch 594, Train Loss: 1.5602, Validation Loss: 1.6743, Validation F1: 0.8146\n",
      "Epoch 595, Train Loss: 1.5639, Validation Loss: 1.6799, Validation F1: 0.8117\n",
      "Epoch 596, Train Loss: 1.5603, Validation Loss: 1.6846, Validation F1: 0.8104\n",
      "Epoch 597, Train Loss: 1.5612, Validation Loss: 1.6816, Validation F1: 0.8127\n",
      "Epoch 598, Train Loss: 1.5609, Validation Loss: 1.6779, Validation F1: 0.8104\n",
      "Epoch 599, Train Loss: 1.5607, Validation Loss: 1.6808, Validation F1: 0.8130\n",
      "Epoch 600, Train Loss: 1.5624, Validation Loss: 1.6796, Validation F1: 0.8131\n",
      "Epoch 601, Train Loss: 1.5621, Validation Loss: 1.6769, Validation F1: 0.8108\n",
      "Epoch 602, Train Loss: 1.5602, Validation Loss: 1.6793, Validation F1: 0.8112\n",
      "Epoch 603, Train Loss: 1.5596, Validation Loss: 1.6812, Validation F1: 0.8124\n",
      "Epoch 604, Train Loss: 1.5606, Validation Loss: 1.6800, Validation F1: 0.8128\n",
      "Epoch 605, Train Loss: 1.5622, Validation Loss: 1.6742, Validation F1: 0.8120\n",
      "Epoch 606, Train Loss: 1.5619, Validation Loss: 1.6836, Validation F1: 0.8091\n",
      "Epoch 607, Train Loss: 1.5622, Validation Loss: 1.6865, Validation F1: 0.8096\n",
      "Epoch 608, Train Loss: 1.5616, Validation Loss: 1.6823, Validation F1: 0.8124\n",
      "Epoch 609, Train Loss: 1.5632, Validation Loss: 1.6834, Validation F1: 0.8101\n",
      "Epoch 610, Train Loss: 1.5604, Validation Loss: 1.6807, Validation F1: 0.8105\n",
      "Epoch 611, Train Loss: 1.5606, Validation Loss: 1.6827, Validation F1: 0.8122\n",
      "Epoch 612, Train Loss: 1.5598, Validation Loss: 1.6814, Validation F1: 0.8114\n",
      "Epoch 613, Train Loss: 1.5609, Validation Loss: 1.6742, Validation F1: 0.8114\n",
      "Epoch 614, Train Loss: 1.5623, Validation Loss: 1.6781, Validation F1: 0.8115\n",
      "Epoch 615, Train Loss: 1.5626, Validation Loss: 1.6824, Validation F1: 0.8099\n",
      "Epoch 616, Train Loss: 1.5618, Validation Loss: 1.6840, Validation F1: 0.8128\n",
      "Epoch 617, Train Loss: 1.5603, Validation Loss: 1.6852, Validation F1: 0.8113\n",
      "Epoch 618, Train Loss: 1.5615, Validation Loss: 1.6757, Validation F1: 0.8120\n",
      "Epoch 619, Train Loss: 1.5613, Validation Loss: 1.6870, Validation F1: 0.8105\n",
      "Epoch 620, Train Loss: 1.5602, Validation Loss: 1.6856, Validation F1: 0.8089\n",
      "Epoch 621, Train Loss: 1.5618, Validation Loss: 1.6761, Validation F1: 0.8105\n",
      "Epoch 622, Train Loss: 1.5606, Validation Loss: 1.6756, Validation F1: 0.8124\n",
      "Epoch 623, Train Loss: 1.5617, Validation Loss: 1.6767, Validation F1: 0.8099\n",
      "Epoch 624, Train Loss: 1.5623, Validation Loss: 1.6811, Validation F1: 0.8102\n",
      "Epoch 625, Train Loss: 1.5620, Validation Loss: 1.6887, Validation F1: 0.8100\n",
      "Epoch 626, Train Loss: 1.5611, Validation Loss: 1.6765, Validation F1: 0.8106\n",
      "Epoch 627, Train Loss: 1.5596, Validation Loss: 1.6756, Validation F1: 0.8109\n",
      "Epoch 628, Train Loss: 1.5628, Validation Loss: 1.6836, Validation F1: 0.8128\n",
      "Epoch 629, Train Loss: 1.5601, Validation Loss: 1.6791, Validation F1: 0.8096\n",
      "Epoch 630, Train Loss: 1.5625, Validation Loss: 1.6861, Validation F1: 0.8104\n",
      "Epoch 631, Train Loss: 1.5594, Validation Loss: 1.6781, Validation F1: 0.8131\n",
      "Epoch 632, Train Loss: 1.5598, Validation Loss: 1.6786, Validation F1: 0.8133\n",
      "Epoch 633, Train Loss: 1.5611, Validation Loss: 1.6837, Validation F1: 0.8108\n",
      "Epoch 634, Train Loss: 1.5604, Validation Loss: 1.6860, Validation F1: 0.8112\n",
      "Epoch 635, Train Loss: 1.5603, Validation Loss: 1.6839, Validation F1: 0.8102\n",
      "Epoch 636, Train Loss: 1.5628, Validation Loss: 1.6789, Validation F1: 0.8106\n",
      "Epoch 637, Train Loss: 1.5599, Validation Loss: 1.6763, Validation F1: 0.8120\n",
      "Epoch 638, Train Loss: 1.5610, Validation Loss: 1.6776, Validation F1: 0.8132\n",
      "Epoch 639, Train Loss: 1.5594, Validation Loss: 1.6729, Validation F1: 0.8119\n",
      "Epoch 640, Train Loss: 1.5597, Validation Loss: 1.6829, Validation F1: 0.8129\n",
      "Epoch 641, Train Loss: 1.5604, Validation Loss: 1.6805, Validation F1: 0.8102\n",
      "Epoch 642, Train Loss: 1.5616, Validation Loss: 1.6813, Validation F1: 0.8119\n",
      "Epoch 643, Train Loss: 1.5629, Validation Loss: 1.6827, Validation F1: 0.8125\n",
      "Epoch 644, Train Loss: 1.5619, Validation Loss: 1.6820, Validation F1: 0.8142\n",
      "Epoch 645, Train Loss: 1.5609, Validation Loss: 1.6817, Validation F1: 0.8109\n",
      "Epoch 646, Train Loss: 1.5599, Validation Loss: 1.6875, Validation F1: 0.8097\n",
      "Epoch 647, Train Loss: 1.5594, Validation Loss: 1.6877, Validation F1: 0.8108\n",
      "Epoch 648, Train Loss: 1.5603, Validation Loss: 1.6801, Validation F1: 0.8123\n",
      "Epoch 649, Train Loss: 1.5614, Validation Loss: 1.6843, Validation F1: 0.8120\n",
      "Epoch 650, Train Loss: 1.5602, Validation Loss: 1.6764, Validation F1: 0.8121\n",
      "Epoch 651, Train Loss: 1.5595, Validation Loss: 1.6893, Validation F1: 0.8116\n",
      "Epoch 652, Train Loss: 1.5617, Validation Loss: 1.6826, Validation F1: 0.8136\n",
      "Epoch 653, Train Loss: 1.5605, Validation Loss: 1.6800, Validation F1: 0.8104\n",
      "Epoch 654, Train Loss: 1.5626, Validation Loss: 1.6830, Validation F1: 0.8118\n",
      "Epoch 655, Train Loss: 1.5621, Validation Loss: 1.6795, Validation F1: 0.8112\n",
      "Epoch 656, Train Loss: 1.5599, Validation Loss: 1.6800, Validation F1: 0.8110\n",
      "Epoch 657, Train Loss: 1.5614, Validation Loss: 1.6837, Validation F1: 0.8112\n",
      "Epoch 658, Train Loss: 1.5597, Validation Loss: 1.6779, Validation F1: 0.8124\n",
      "Epoch 659, Train Loss: 1.5616, Validation Loss: 1.6754, Validation F1: 0.8108\n",
      "Epoch 660, Train Loss: 1.5623, Validation Loss: 1.6808, Validation F1: 0.8106\n",
      "Epoch 661, Train Loss: 1.5601, Validation Loss: 1.6820, Validation F1: 0.8112\n",
      "Epoch 662, Train Loss: 1.5598, Validation Loss: 1.6818, Validation F1: 0.8096\n",
      "Epoch 663, Train Loss: 1.5613, Validation Loss: 1.6843, Validation F1: 0.8110\n",
      "Epoch 664, Train Loss: 1.5625, Validation Loss: 1.6829, Validation F1: 0.8109\n",
      "Epoch 665, Train Loss: 1.5608, Validation Loss: 1.6743, Validation F1: 0.8106\n",
      "Epoch 666, Train Loss: 1.5603, Validation Loss: 1.6787, Validation F1: 0.8137\n",
      "Epoch 667, Train Loss: 1.5606, Validation Loss: 1.6835, Validation F1: 0.8119\n",
      "Epoch 668, Train Loss: 1.5601, Validation Loss: 1.6773, Validation F1: 0.8105\n",
      "Epoch 669, Train Loss: 1.5614, Validation Loss: 1.6777, Validation F1: 0.8113\n",
      "Epoch 670, Train Loss: 1.5595, Validation Loss: 1.6802, Validation F1: 0.8130\n",
      "Epoch 671, Train Loss: 1.5621, Validation Loss: 1.6837, Validation F1: 0.8109\n",
      "Epoch 672, Train Loss: 1.5609, Validation Loss: 1.6818, Validation F1: 0.8102\n",
      "Epoch 673, Train Loss: 1.5593, Validation Loss: 1.6771, Validation F1: 0.8115\n",
      "Epoch 674, Train Loss: 1.5594, Validation Loss: 1.6799, Validation F1: 0.8131\n",
      "Epoch 675, Train Loss: 1.5612, Validation Loss: 1.6751, Validation F1: 0.8138\n",
      "Epoch 676, Train Loss: 1.5608, Validation Loss: 1.6882, Validation F1: 0.8110\n",
      "Epoch 677, Train Loss: 1.5609, Validation Loss: 1.6799, Validation F1: 0.8103\n",
      "Epoch 678, Train Loss: 1.5608, Validation Loss: 1.6778, Validation F1: 0.8104\n",
      "Epoch 679, Train Loss: 1.5593, Validation Loss: 1.6925, Validation F1: 0.8116\n",
      "Epoch 680, Train Loss: 1.5594, Validation Loss: 1.6833, Validation F1: 0.8109\n",
      "Epoch 681, Train Loss: 1.5605, Validation Loss: 1.6848, Validation F1: 0.8130\n",
      "Epoch 682, Train Loss: 1.5613, Validation Loss: 1.6778, Validation F1: 0.8118\n",
      "Epoch 683, Train Loss: 1.5592, Validation Loss: 1.6842, Validation F1: 0.8105\n",
      "Epoch 684, Train Loss: 1.5600, Validation Loss: 1.6796, Validation F1: 0.8121\n",
      "Epoch 685, Train Loss: 1.5591, Validation Loss: 1.6840, Validation F1: 0.8120\n",
      "Epoch 686, Train Loss: 1.5593, Validation Loss: 1.6745, Validation F1: 0.8119\n",
      "Epoch 687, Train Loss: 1.5589, Validation Loss: 1.6836, Validation F1: 0.8117\n",
      "Epoch 688, Train Loss: 1.5625, Validation Loss: 1.6782, Validation F1: 0.8135\n",
      "Epoch 689, Train Loss: 1.5610, Validation Loss: 1.6815, Validation F1: 0.8097\n",
      "Epoch 690, Train Loss: 1.5594, Validation Loss: 1.6834, Validation F1: 0.8107\n",
      "Epoch 691, Train Loss: 1.5614, Validation Loss: 1.6866, Validation F1: 0.8107\n",
      "Epoch 692, Train Loss: 1.5591, Validation Loss: 1.6853, Validation F1: 0.8110\n",
      "Epoch 693, Train Loss: 1.5600, Validation Loss: 1.6856, Validation F1: 0.8108\n",
      "Epoch 694, Train Loss: 1.5611, Validation Loss: 1.6836, Validation F1: 0.8115\n",
      "Epoch 695, Train Loss: 1.5589, Validation Loss: 1.6871, Validation F1: 0.8113\n",
      "Epoch 696, Train Loss: 1.5615, Validation Loss: 1.6839, Validation F1: 0.8105\n",
      "Epoch 697, Train Loss: 1.5615, Validation Loss: 1.6856, Validation F1: 0.8100\n",
      "Epoch 698, Train Loss: 1.5607, Validation Loss: 1.6790, Validation F1: 0.8109\n",
      "Epoch 699, Train Loss: 1.5619, Validation Loss: 1.6818, Validation F1: 0.8110\n",
      "Epoch 700, Train Loss: 1.5595, Validation Loss: 1.6769, Validation F1: 0.8103\n",
      "Epoch 701, Train Loss: 1.5591, Validation Loss: 1.6796, Validation F1: 0.8121\n",
      "Epoch 702, Train Loss: 1.5606, Validation Loss: 1.6847, Validation F1: 0.8100\n",
      "Epoch 703, Train Loss: 1.5593, Validation Loss: 1.6850, Validation F1: 0.8127\n",
      "Epoch 704, Train Loss: 1.5612, Validation Loss: 1.6745, Validation F1: 0.8111\n",
      "Epoch 705, Train Loss: 1.5599, Validation Loss: 1.6804, Validation F1: 0.8127\n",
      "Epoch 706, Train Loss: 1.5590, Validation Loss: 1.6824, Validation F1: 0.8121\n",
      "Epoch 707, Train Loss: 1.5608, Validation Loss: 1.6863, Validation F1: 0.8099\n",
      "Epoch 708, Train Loss: 1.5595, Validation Loss: 1.6858, Validation F1: 0.8116\n",
      "Epoch 709, Train Loss: 1.5606, Validation Loss: 1.6833, Validation F1: 0.8107\n",
      "Epoch 710, Train Loss: 1.5597, Validation Loss: 1.6808, Validation F1: 0.8117\n",
      "Epoch 711, Train Loss: 1.5590, Validation Loss: 1.6786, Validation F1: 0.8155\n",
      "Epoch 712, Train Loss: 1.5614, Validation Loss: 1.6807, Validation F1: 0.8103\n",
      "Epoch 713, Train Loss: 1.5593, Validation Loss: 1.6798, Validation F1: 0.8093\n",
      "Epoch 714, Train Loss: 1.5592, Validation Loss: 1.6870, Validation F1: 0.8097\n",
      "Epoch 715, Train Loss: 1.5611, Validation Loss: 1.6768, Validation F1: 0.8101\n",
      "Epoch 716, Train Loss: 1.5607, Validation Loss: 1.6819, Validation F1: 0.8101\n",
      "Epoch 717, Train Loss: 1.5610, Validation Loss: 1.6730, Validation F1: 0.8144\n",
      "Epoch 718, Train Loss: 1.5624, Validation Loss: 1.6851, Validation F1: 0.8119\n",
      "Epoch 719, Train Loss: 1.5592, Validation Loss: 1.6793, Validation F1: 0.8128\n",
      "Epoch 720, Train Loss: 1.5595, Validation Loss: 1.6885, Validation F1: 0.8114\n",
      "Epoch 721, Train Loss: 1.5594, Validation Loss: 1.6855, Validation F1: 0.8114\n",
      "Epoch 722, Train Loss: 1.5610, Validation Loss: 1.6773, Validation F1: 0.8109\n",
      "Epoch 723, Train Loss: 1.5613, Validation Loss: 1.6878, Validation F1: 0.8102\n",
      "Epoch 724, Train Loss: 1.5606, Validation Loss: 1.6808, Validation F1: 0.8120\n",
      "Epoch 725, Train Loss: 1.5606, Validation Loss: 1.6799, Validation F1: 0.8103\n",
      "Epoch 726, Train Loss: 1.5609, Validation Loss: 1.6873, Validation F1: 0.8098\n",
      "Epoch 727, Train Loss: 1.5603, Validation Loss: 1.6749, Validation F1: 0.8120\n",
      "Epoch 728, Train Loss: 1.5609, Validation Loss: 1.6851, Validation F1: 0.8113\n",
      "Epoch 729, Train Loss: 1.5590, Validation Loss: 1.6811, Validation F1: 0.8108\n",
      "Epoch 730, Train Loss: 1.5604, Validation Loss: 1.6816, Validation F1: 0.8116\n",
      "Epoch 731, Train Loss: 1.5606, Validation Loss: 1.6801, Validation F1: 0.8127\n",
      "Epoch 732, Train Loss: 1.5595, Validation Loss: 1.6735, Validation F1: 0.8107\n",
      "Epoch 733, Train Loss: 1.5592, Validation Loss: 1.6843, Validation F1: 0.8111\n",
      "Epoch 734, Train Loss: 1.5614, Validation Loss: 1.6878, Validation F1: 0.8113\n",
      "Epoch 735, Train Loss: 1.5607, Validation Loss: 1.6800, Validation F1: 0.8120\n",
      "Epoch 736, Train Loss: 1.5593, Validation Loss: 1.6808, Validation F1: 0.8122\n",
      "Epoch 737, Train Loss: 1.5603, Validation Loss: 1.6890, Validation F1: 0.8122\n",
      "Epoch 738, Train Loss: 1.5595, Validation Loss: 1.6812, Validation F1: 0.8134\n",
      "Epoch 739, Train Loss: 1.5599, Validation Loss: 1.6809, Validation F1: 0.8109\n",
      "Epoch 740, Train Loss: 1.5593, Validation Loss: 1.6818, Validation F1: 0.8128\n",
      "Epoch 741, Train Loss: 1.5590, Validation Loss: 1.6775, Validation F1: 0.8115\n",
      "Epoch 742, Train Loss: 1.5606, Validation Loss: 1.6832, Validation F1: 0.8128\n",
      "Epoch 743, Train Loss: 1.5624, Validation Loss: 1.6804, Validation F1: 0.8115\n",
      "Epoch 744, Train Loss: 1.5589, Validation Loss: 1.6827, Validation F1: 0.8130\n",
      "Epoch 745, Train Loss: 1.5605, Validation Loss: 1.6819, Validation F1: 0.8111\n",
      "Epoch 746, Train Loss: 1.5610, Validation Loss: 1.6927, Validation F1: 0.8125\n",
      "Epoch 747, Train Loss: 1.5595, Validation Loss: 1.6808, Validation F1: 0.8105\n",
      "Epoch 748, Train Loss: 1.5590, Validation Loss: 1.6841, Validation F1: 0.8115\n",
      "Epoch 749, Train Loss: 1.5598, Validation Loss: 1.6858, Validation F1: 0.8113\n",
      "Epoch 750, Train Loss: 1.5610, Validation Loss: 1.6943, Validation F1: 0.8111\n",
      "Epoch 751, Train Loss: 1.5595, Validation Loss: 1.6821, Validation F1: 0.8097\n",
      "Epoch 752, Train Loss: 1.5599, Validation Loss: 1.6927, Validation F1: 0.8108\n",
      "Epoch 753, Train Loss: 1.5588, Validation Loss: 1.6788, Validation F1: 0.8108\n",
      "Epoch 754, Train Loss: 1.5594, Validation Loss: 1.6772, Validation F1: 0.8127\n",
      "Epoch 755, Train Loss: 1.5609, Validation Loss: 1.6810, Validation F1: 0.8108\n",
      "Epoch 756, Train Loss: 1.5590, Validation Loss: 1.6829, Validation F1: 0.8108\n",
      "Epoch 757, Train Loss: 1.5614, Validation Loss: 1.6797, Validation F1: 0.8103\n",
      "Epoch 758, Train Loss: 1.5607, Validation Loss: 1.6869, Validation F1: 0.8111\n",
      "Epoch 759, Train Loss: 1.5589, Validation Loss: 1.6797, Validation F1: 0.8116\n",
      "Epoch 760, Train Loss: 1.5606, Validation Loss: 1.6841, Validation F1: 0.8137\n",
      "Epoch 761, Train Loss: 1.5630, Validation Loss: 1.6766, Validation F1: 0.8111\n",
      "Epoch 762, Train Loss: 1.5619, Validation Loss: 1.6899, Validation F1: 0.8124\n",
      "Epoch 763, Train Loss: 1.5623, Validation Loss: 1.6806, Validation F1: 0.8109\n",
      "Epoch 764, Train Loss: 1.5620, Validation Loss: 1.6801, Validation F1: 0.8112\n",
      "Epoch 765, Train Loss: 1.5613, Validation Loss: 1.6838, Validation F1: 0.8122\n",
      "Epoch 766, Train Loss: 1.5620, Validation Loss: 1.6804, Validation F1: 0.8115\n",
      "Epoch 767, Train Loss: 1.5592, Validation Loss: 1.6875, Validation F1: 0.8107\n",
      "Epoch 768, Train Loss: 1.5601, Validation Loss: 1.6815, Validation F1: 0.8107\n",
      "Epoch 769, Train Loss: 1.5606, Validation Loss: 1.6884, Validation F1: 0.8123\n",
      "Epoch 770, Train Loss: 1.5590, Validation Loss: 1.6841, Validation F1: 0.8127\n",
      "Epoch 771, Train Loss: 1.5600, Validation Loss: 1.6815, Validation F1: 0.8117\n",
      "Epoch 772, Train Loss: 1.5601, Validation Loss: 1.6792, Validation F1: 0.8107\n",
      "Epoch 773, Train Loss: 1.5606, Validation Loss: 1.6810, Validation F1: 0.8118\n",
      "Epoch 774, Train Loss: 1.5604, Validation Loss: 1.6905, Validation F1: 0.8116\n",
      "Epoch 775, Train Loss: 1.5601, Validation Loss: 1.6857, Validation F1: 0.8119\n",
      "Epoch 776, Train Loss: 1.5590, Validation Loss: 1.6864, Validation F1: 0.8110\n",
      "Epoch 777, Train Loss: 1.5600, Validation Loss: 1.6828, Validation F1: 0.8110\n",
      "Epoch 778, Train Loss: 1.5592, Validation Loss: 1.6878, Validation F1: 0.8103\n",
      "Epoch 779, Train Loss: 1.5600, Validation Loss: 1.6844, Validation F1: 0.8122\n",
      "Epoch 780, Train Loss: 1.5606, Validation Loss: 1.6823, Validation F1: 0.8093\n",
      "Epoch 781, Train Loss: 1.5599, Validation Loss: 1.6801, Validation F1: 0.8106\n",
      "Epoch 782, Train Loss: 1.5602, Validation Loss: 1.6808, Validation F1: 0.8097\n",
      "Epoch 783, Train Loss: 1.5589, Validation Loss: 1.6843, Validation F1: 0.8121\n",
      "Epoch 784, Train Loss: 1.5609, Validation Loss: 1.6798, Validation F1: 0.8116\n",
      "Epoch 785, Train Loss: 1.5591, Validation Loss: 1.6791, Validation F1: 0.8091\n",
      "Epoch 786, Train Loss: 1.5598, Validation Loss: 1.6755, Validation F1: 0.8118\n",
      "Epoch 787, Train Loss: 1.5590, Validation Loss: 1.6812, Validation F1: 0.8127\n",
      "Epoch 788, Train Loss: 1.5591, Validation Loss: 1.6838, Validation F1: 0.8120\n",
      "Epoch 789, Train Loss: 1.5588, Validation Loss: 1.6875, Validation F1: 0.8103\n",
      "Epoch 790, Train Loss: 1.5621, Validation Loss: 1.6833, Validation F1: 0.8100\n",
      "Epoch 791, Train Loss: 1.5602, Validation Loss: 1.6868, Validation F1: 0.8127\n",
      "Epoch 792, Train Loss: 1.5600, Validation Loss: 1.6825, Validation F1: 0.8111\n",
      "Epoch 793, Train Loss: 1.5593, Validation Loss: 1.6846, Validation F1: 0.8105\n",
      "Epoch 794, Train Loss: 1.5599, Validation Loss: 1.6857, Validation F1: 0.8112\n",
      "Epoch 795, Train Loss: 1.5605, Validation Loss: 1.6912, Validation F1: 0.8123\n",
      "Epoch 796, Train Loss: 1.5601, Validation Loss: 1.6809, Validation F1: 0.8123\n",
      "Epoch 797, Train Loss: 1.5608, Validation Loss: 1.6864, Validation F1: 0.8105\n",
      "Epoch 798, Train Loss: 1.5588, Validation Loss: 1.6909, Validation F1: 0.8115\n",
      "Epoch 799, Train Loss: 1.5596, Validation Loss: 1.6838, Validation F1: 0.8107\n",
      "Epoch 800, Train Loss: 1.5617, Validation Loss: 1.6835, Validation F1: 0.8107\n",
      "Epoch 801, Train Loss: 1.5599, Validation Loss: 1.6858, Validation F1: 0.8105\n",
      "Epoch 802, Train Loss: 1.5588, Validation Loss: 1.6854, Validation F1: 0.8109\n",
      "Epoch 803, Train Loss: 1.5607, Validation Loss: 1.6835, Validation F1: 0.8140\n",
      "Epoch 804, Train Loss: 1.5600, Validation Loss: 1.6841, Validation F1: 0.8117\n",
      "Epoch 805, Train Loss: 1.5596, Validation Loss: 1.6852, Validation F1: 0.8108\n",
      "Epoch 806, Train Loss: 1.5603, Validation Loss: 1.6889, Validation F1: 0.8103\n",
      "Epoch 807, Train Loss: 1.5593, Validation Loss: 1.6806, Validation F1: 0.8117\n",
      "Epoch 808, Train Loss: 1.5611, Validation Loss: 1.6859, Validation F1: 0.8110\n",
      "Epoch 809, Train Loss: 1.5594, Validation Loss: 1.6760, Validation F1: 0.8108\n",
      "Epoch 810, Train Loss: 1.5618, Validation Loss: 1.6875, Validation F1: 0.8107\n",
      "Epoch 811, Train Loss: 1.5612, Validation Loss: 1.6886, Validation F1: 0.8110\n",
      "Epoch 812, Train Loss: 1.5590, Validation Loss: 1.6867, Validation F1: 0.8125\n",
      "Epoch 813, Train Loss: 1.5592, Validation Loss: 1.6911, Validation F1: 0.8122\n",
      "Epoch 814, Train Loss: 1.5601, Validation Loss: 1.6853, Validation F1: 0.8119\n",
      "Epoch 815, Train Loss: 1.5590, Validation Loss: 1.6874, Validation F1: 0.8127\n",
      "Epoch 816, Train Loss: 1.5590, Validation Loss: 1.6796, Validation F1: 0.8106\n",
      "Epoch 817, Train Loss: 1.5602, Validation Loss: 1.6838, Validation F1: 0.8136\n",
      "Epoch 818, Train Loss: 1.5590, Validation Loss: 1.6847, Validation F1: 0.8119\n",
      "Epoch 819, Train Loss: 1.5604, Validation Loss: 1.6821, Validation F1: 0.8093\n",
      "Epoch 820, Train Loss: 1.5622, Validation Loss: 1.6838, Validation F1: 0.8097\n",
      "Epoch 821, Train Loss: 1.5590, Validation Loss: 1.6867, Validation F1: 0.8115\n",
      "Epoch 822, Train Loss: 1.5600, Validation Loss: 1.6846, Validation F1: 0.8108\n",
      "Epoch 823, Train Loss: 1.5618, Validation Loss: 1.6887, Validation F1: 0.8124\n",
      "Epoch 824, Train Loss: 1.5613, Validation Loss: 1.6938, Validation F1: 0.8118\n",
      "Epoch 825, Train Loss: 1.5603, Validation Loss: 1.6838, Validation F1: 0.8125\n",
      "Epoch 826, Train Loss: 1.5599, Validation Loss: 1.6814, Validation F1: 0.8120\n",
      "Epoch 827, Train Loss: 1.5590, Validation Loss: 1.6871, Validation F1: 0.8119\n",
      "Epoch 828, Train Loss: 1.5627, Validation Loss: 1.6893, Validation F1: 0.8093\n",
      "Epoch 829, Train Loss: 1.5611, Validation Loss: 1.6864, Validation F1: 0.8089\n",
      "Epoch 830, Train Loss: 1.5598, Validation Loss: 1.6849, Validation F1: 0.8094\n",
      "Epoch 831, Train Loss: 1.5593, Validation Loss: 1.6873, Validation F1: 0.8095\n",
      "Epoch 832, Train Loss: 1.5606, Validation Loss: 1.6938, Validation F1: 0.8113\n",
      "Epoch 833, Train Loss: 1.5591, Validation Loss: 1.6856, Validation F1: 0.8109\n",
      "Epoch 834, Train Loss: 1.5598, Validation Loss: 1.6841, Validation F1: 0.8105\n",
      "Epoch 835, Train Loss: 1.5608, Validation Loss: 1.6861, Validation F1: 0.8112\n",
      "Epoch 836, Train Loss: 1.5587, Validation Loss: 1.6817, Validation F1: 0.8115\n",
      "Epoch 837, Train Loss: 1.5590, Validation Loss: 1.6890, Validation F1: 0.8121\n",
      "Epoch 838, Train Loss: 1.5593, Validation Loss: 1.6823, Validation F1: 0.8115\n",
      "Epoch 839, Train Loss: 1.5602, Validation Loss: 1.6860, Validation F1: 0.8136\n",
      "Epoch 840, Train Loss: 1.5616, Validation Loss: 1.6818, Validation F1: 0.8126\n",
      "Epoch 841, Train Loss: 1.5605, Validation Loss: 1.6882, Validation F1: 0.8101\n",
      "Epoch 842, Train Loss: 1.5587, Validation Loss: 1.6818, Validation F1: 0.8114\n",
      "Epoch 843, Train Loss: 1.5611, Validation Loss: 1.6831, Validation F1: 0.8110\n",
      "Epoch 844, Train Loss: 1.5587, Validation Loss: 1.6799, Validation F1: 0.8124\n",
      "Epoch 845, Train Loss: 1.5617, Validation Loss: 1.6821, Validation F1: 0.8116\n",
      "Epoch 846, Train Loss: 1.5589, Validation Loss: 1.6796, Validation F1: 0.8122\n",
      "Epoch 847, Train Loss: 1.5585, Validation Loss: 1.6795, Validation F1: 0.8127\n",
      "Epoch 848, Train Loss: 1.5587, Validation Loss: 1.6770, Validation F1: 0.8107\n",
      "Epoch 849, Train Loss: 1.5585, Validation Loss: 1.6852, Validation F1: 0.8116\n",
      "Epoch 850, Train Loss: 1.5589, Validation Loss: 1.6923, Validation F1: 0.8129\n",
      "Epoch 851, Train Loss: 1.5599, Validation Loss: 1.6934, Validation F1: 0.8112\n",
      "Epoch 852, Train Loss: 1.5601, Validation Loss: 1.6800, Validation F1: 0.8097\n",
      "Epoch 853, Train Loss: 1.5585, Validation Loss: 1.6871, Validation F1: 0.8113\n",
      "Epoch 854, Train Loss: 1.5591, Validation Loss: 1.6885, Validation F1: 0.8120\n",
      "Epoch 855, Train Loss: 1.5621, Validation Loss: 1.6833, Validation F1: 0.8115\n",
      "Epoch 856, Train Loss: 1.5587, Validation Loss: 1.6927, Validation F1: 0.8109\n",
      "Epoch 857, Train Loss: 1.5588, Validation Loss: 1.6915, Validation F1: 0.8115\n",
      "Epoch 858, Train Loss: 1.5586, Validation Loss: 1.6806, Validation F1: 0.8119\n",
      "Epoch 859, Train Loss: 1.5602, Validation Loss: 1.6870, Validation F1: 0.8106\n",
      "Epoch 860, Train Loss: 1.5586, Validation Loss: 1.6804, Validation F1: 0.8132\n",
      "Epoch 861, Train Loss: 1.5599, Validation Loss: 1.6818, Validation F1: 0.8100\n",
      "Epoch 862, Train Loss: 1.5598, Validation Loss: 1.6796, Validation F1: 0.8106\n",
      "Epoch 863, Train Loss: 1.5597, Validation Loss: 1.6802, Validation F1: 0.8115\n",
      "Epoch 864, Train Loss: 1.5591, Validation Loss: 1.6844, Validation F1: 0.8119\n",
      "Epoch 865, Train Loss: 1.5599, Validation Loss: 1.6830, Validation F1: 0.8130\n",
      "Epoch 866, Train Loss: 1.5601, Validation Loss: 1.6909, Validation F1: 0.8129\n",
      "Epoch 867, Train Loss: 1.5586, Validation Loss: 1.6906, Validation F1: 0.8134\n",
      "Epoch 868, Train Loss: 1.5604, Validation Loss: 1.6907, Validation F1: 0.8113\n",
      "Epoch 869, Train Loss: 1.5586, Validation Loss: 1.6857, Validation F1: 0.8107\n",
      "Epoch 870, Train Loss: 1.5603, Validation Loss: 1.6858, Validation F1: 0.8122\n",
      "Epoch 871, Train Loss: 1.5600, Validation Loss: 1.6887, Validation F1: 0.8114\n",
      "Epoch 872, Train Loss: 1.5598, Validation Loss: 1.6877, Validation F1: 0.8106\n",
      "Epoch 873, Train Loss: 1.5589, Validation Loss: 1.6775, Validation F1: 0.8131\n",
      "Epoch 874, Train Loss: 1.5587, Validation Loss: 1.6843, Validation F1: 0.8121\n",
      "Epoch 875, Train Loss: 1.5591, Validation Loss: 1.6909, Validation F1: 0.8122\n",
      "Epoch 876, Train Loss: 1.5589, Validation Loss: 1.6798, Validation F1: 0.8114\n",
      "Epoch 877, Train Loss: 1.5593, Validation Loss: 1.6884, Validation F1: 0.8123\n",
      "Epoch 878, Train Loss: 1.5607, Validation Loss: 1.6873, Validation F1: 0.8100\n",
      "Epoch 879, Train Loss: 1.5621, Validation Loss: 1.6842, Validation F1: 0.8112\n",
      "Epoch 880, Train Loss: 1.5616, Validation Loss: 1.6881, Validation F1: 0.8105\n",
      "Epoch 881, Train Loss: 1.5604, Validation Loss: 1.6895, Validation F1: 0.8113\n",
      "Epoch 882, Train Loss: 1.5602, Validation Loss: 1.6854, Validation F1: 0.8124\n",
      "Epoch 883, Train Loss: 1.5591, Validation Loss: 1.6889, Validation F1: 0.8125\n",
      "Epoch 884, Train Loss: 1.5603, Validation Loss: 1.6811, Validation F1: 0.8097\n",
      "Epoch 885, Train Loss: 1.5609, Validation Loss: 1.6892, Validation F1: 0.8108\n",
      "Epoch 886, Train Loss: 1.5598, Validation Loss: 1.6832, Validation F1: 0.8104\n",
      "Epoch 887, Train Loss: 1.5596, Validation Loss: 1.6876, Validation F1: 0.8109\n",
      "Epoch 888, Train Loss: 1.5598, Validation Loss: 1.6808, Validation F1: 0.8099\n",
      "Epoch 889, Train Loss: 1.5590, Validation Loss: 1.6870, Validation F1: 0.8113\n",
      "Epoch 890, Train Loss: 1.5605, Validation Loss: 1.6806, Validation F1: 0.8123\n",
      "Epoch 891, Train Loss: 1.5600, Validation Loss: 1.6941, Validation F1: 0.8123\n",
      "Epoch 892, Train Loss: 1.5605, Validation Loss: 1.6859, Validation F1: 0.8111\n",
      "Epoch 893, Train Loss: 1.5596, Validation Loss: 1.6853, Validation F1: 0.8114\n",
      "Epoch 894, Train Loss: 1.5602, Validation Loss: 1.6850, Validation F1: 0.8111\n",
      "Epoch 895, Train Loss: 1.5591, Validation Loss: 1.6897, Validation F1: 0.8097\n",
      "Epoch 896, Train Loss: 1.5617, Validation Loss: 1.6804, Validation F1: 0.8115\n",
      "Epoch 897, Train Loss: 1.5587, Validation Loss: 1.6884, Validation F1: 0.8114\n",
      "Epoch 898, Train Loss: 1.5612, Validation Loss: 1.6846, Validation F1: 0.8098\n",
      "Epoch 899, Train Loss: 1.5620, Validation Loss: 1.6835, Validation F1: 0.8103\n",
      "Epoch 900, Train Loss: 1.5591, Validation Loss: 1.6781, Validation F1: 0.8126\n",
      "Epoch 901, Train Loss: 1.5615, Validation Loss: 1.6909, Validation F1: 0.8112\n",
      "Epoch 902, Train Loss: 1.5603, Validation Loss: 1.6867, Validation F1: 0.8109\n",
      "Epoch 903, Train Loss: 1.5604, Validation Loss: 1.6838, Validation F1: 0.8129\n",
      "Epoch 904, Train Loss: 1.5593, Validation Loss: 1.6802, Validation F1: 0.8118\n",
      "Epoch 905, Train Loss: 1.5599, Validation Loss: 1.6823, Validation F1: 0.8113\n",
      "Epoch 906, Train Loss: 1.5590, Validation Loss: 1.6822, Validation F1: 0.8117\n",
      "Epoch 907, Train Loss: 1.5591, Validation Loss: 1.6848, Validation F1: 0.8110\n",
      "Epoch 908, Train Loss: 1.5588, Validation Loss: 1.6888, Validation F1: 0.8112\n",
      "Epoch 909, Train Loss: 1.5604, Validation Loss: 1.6838, Validation F1: 0.8110\n",
      "Epoch 910, Train Loss: 1.5597, Validation Loss: 1.6852, Validation F1: 0.8115\n",
      "Epoch 911, Train Loss: 1.5588, Validation Loss: 1.6868, Validation F1: 0.8120\n",
      "Epoch 912, Train Loss: 1.5591, Validation Loss: 1.6940, Validation F1: 0.8116\n",
      "Epoch 913, Train Loss: 1.5590, Validation Loss: 1.6888, Validation F1: 0.8115\n",
      "Epoch 914, Train Loss: 1.5594, Validation Loss: 1.6792, Validation F1: 0.8109\n",
      "Epoch 915, Train Loss: 1.5609, Validation Loss: 1.6852, Validation F1: 0.8125\n",
      "Epoch 916, Train Loss: 1.5622, Validation Loss: 1.6859, Validation F1: 0.8114\n",
      "Epoch 917, Train Loss: 1.5591, Validation Loss: 1.6907, Validation F1: 0.8124\n",
      "Epoch 918, Train Loss: 1.5587, Validation Loss: 1.6889, Validation F1: 0.8108\n",
      "Epoch 919, Train Loss: 1.5599, Validation Loss: 1.6887, Validation F1: 0.8102\n",
      "Epoch 920, Train Loss: 1.5601, Validation Loss: 1.6845, Validation F1: 0.8110\n",
      "Epoch 921, Train Loss: 1.5586, Validation Loss: 1.6850, Validation F1: 0.8108\n",
      "Epoch 922, Train Loss: 1.5602, Validation Loss: 1.6863, Validation F1: 0.8105\n",
      "Epoch 923, Train Loss: 1.5614, Validation Loss: 1.6893, Validation F1: 0.8115\n",
      "Epoch 924, Train Loss: 1.5602, Validation Loss: 1.6890, Validation F1: 0.8127\n",
      "Epoch 925, Train Loss: 1.5598, Validation Loss: 1.6922, Validation F1: 0.8109\n",
      "Epoch 926, Train Loss: 1.5601, Validation Loss: 1.6783, Validation F1: 0.8108\n",
      "Epoch 927, Train Loss: 1.5591, Validation Loss: 1.6884, Validation F1: 0.8107\n",
      "Epoch 928, Train Loss: 1.5608, Validation Loss: 1.6883, Validation F1: 0.8130\n",
      "Epoch 929, Train Loss: 1.5599, Validation Loss: 1.6856, Validation F1: 0.8100\n",
      "Epoch 930, Train Loss: 1.5593, Validation Loss: 1.6888, Validation F1: 0.8111\n",
      "Epoch 931, Train Loss: 1.5617, Validation Loss: 1.6855, Validation F1: 0.8123\n",
      "Epoch 932, Train Loss: 1.5587, Validation Loss: 1.6797, Validation F1: 0.8113\n",
      "Epoch 933, Train Loss: 1.5587, Validation Loss: 1.6857, Validation F1: 0.8125\n",
      "Epoch 934, Train Loss: 1.5594, Validation Loss: 1.6934, Validation F1: 0.8108\n",
      "Epoch 935, Train Loss: 1.5589, Validation Loss: 1.6901, Validation F1: 0.8103\n",
      "Epoch 936, Train Loss: 1.5591, Validation Loss: 1.6817, Validation F1: 0.8116\n",
      "Epoch 937, Train Loss: 1.5601, Validation Loss: 1.6860, Validation F1: 0.8101\n",
      "Epoch 938, Train Loss: 1.5596, Validation Loss: 1.6809, Validation F1: 0.8112\n",
      "Epoch 939, Train Loss: 1.5597, Validation Loss: 1.6836, Validation F1: 0.8117\n",
      "Epoch 940, Train Loss: 1.5599, Validation Loss: 1.6888, Validation F1: 0.8112\n",
      "Epoch 941, Train Loss: 1.5585, Validation Loss: 1.6881, Validation F1: 0.8109\n",
      "Epoch 942, Train Loss: 1.5587, Validation Loss: 1.6831, Validation F1: 0.8097\n",
      "Epoch 943, Train Loss: 1.5588, Validation Loss: 1.6804, Validation F1: 0.8122\n",
      "Epoch 944, Train Loss: 1.5598, Validation Loss: 1.6817, Validation F1: 0.8098\n",
      "Epoch 945, Train Loss: 1.5614, Validation Loss: 1.6885, Validation F1: 0.8123\n",
      "Epoch 946, Train Loss: 1.5617, Validation Loss: 1.6829, Validation F1: 0.8107\n",
      "Epoch 947, Train Loss: 1.5593, Validation Loss: 1.6892, Validation F1: 0.8111\n",
      "Epoch 948, Train Loss: 1.5586, Validation Loss: 1.6890, Validation F1: 0.8126\n",
      "Epoch 949, Train Loss: 1.5584, Validation Loss: 1.6861, Validation F1: 0.8099\n",
      "Epoch 950, Train Loss: 1.5590, Validation Loss: 1.6849, Validation F1: 0.8124\n",
      "Epoch 951, Train Loss: 1.5600, Validation Loss: 1.6827, Validation F1: 0.8122\n",
      "Epoch 952, Train Loss: 1.5605, Validation Loss: 1.6844, Validation F1: 0.8112\n",
      "Epoch 953, Train Loss: 1.5616, Validation Loss: 1.6821, Validation F1: 0.8111\n",
      "Epoch 954, Train Loss: 1.5586, Validation Loss: 1.6812, Validation F1: 0.8122\n",
      "Epoch 955, Train Loss: 1.5589, Validation Loss: 1.6878, Validation F1: 0.8136\n",
      "Epoch 956, Train Loss: 1.5604, Validation Loss: 1.6851, Validation F1: 0.8109\n",
      "Epoch 957, Train Loss: 1.5602, Validation Loss: 1.6888, Validation F1: 0.8105\n",
      "Epoch 958, Train Loss: 1.5599, Validation Loss: 1.6844, Validation F1: 0.8111\n",
      "Epoch 959, Train Loss: 1.5601, Validation Loss: 1.6831, Validation F1: 0.8120\n",
      "Epoch 960, Train Loss: 1.5588, Validation Loss: 1.6808, Validation F1: 0.8116\n",
      "Epoch 961, Train Loss: 1.5617, Validation Loss: 1.6756, Validation F1: 0.8104\n",
      "Epoch 962, Train Loss: 1.5606, Validation Loss: 1.6897, Validation F1: 0.8123\n",
      "Epoch 963, Train Loss: 1.5598, Validation Loss: 1.6819, Validation F1: 0.8109\n",
      "Epoch 964, Train Loss: 1.5619, Validation Loss: 1.6840, Validation F1: 0.8104\n",
      "Epoch 965, Train Loss: 1.5603, Validation Loss: 1.6869, Validation F1: 0.8118\n",
      "Epoch 966, Train Loss: 1.5586, Validation Loss: 1.6905, Validation F1: 0.8104\n",
      "Epoch 967, Train Loss: 1.5597, Validation Loss: 1.6835, Validation F1: 0.8110\n",
      "Epoch 968, Train Loss: 1.5588, Validation Loss: 1.6873, Validation F1: 0.8111\n",
      "Epoch 969, Train Loss: 1.5589, Validation Loss: 1.6869, Validation F1: 0.8112\n",
      "Epoch 970, Train Loss: 1.5586, Validation Loss: 1.6884, Validation F1: 0.8104\n",
      "Epoch 971, Train Loss: 1.5596, Validation Loss: 1.6799, Validation F1: 0.8125\n",
      "Epoch 972, Train Loss: 1.5590, Validation Loss: 1.6882, Validation F1: 0.8110\n",
      "Epoch 973, Train Loss: 1.5591, Validation Loss: 1.6906, Validation F1: 0.8115\n",
      "Epoch 974, Train Loss: 1.5588, Validation Loss: 1.6834, Validation F1: 0.8114\n",
      "Epoch 975, Train Loss: 1.5588, Validation Loss: 1.6843, Validation F1: 0.8100\n",
      "Epoch 976, Train Loss: 1.5589, Validation Loss: 1.6837, Validation F1: 0.8129\n",
      "Epoch 977, Train Loss: 1.5586, Validation Loss: 1.6911, Validation F1: 0.8118\n",
      "Epoch 978, Train Loss: 1.5591, Validation Loss: 1.6825, Validation F1: 0.8117\n",
      "Epoch 979, Train Loss: 1.5599, Validation Loss: 1.6913, Validation F1: 0.8114\n",
      "Epoch 980, Train Loss: 1.5599, Validation Loss: 1.6798, Validation F1: 0.8102\n",
      "Epoch 981, Train Loss: 1.5602, Validation Loss: 1.6902, Validation F1: 0.8111\n",
      "Epoch 982, Train Loss: 1.5608, Validation Loss: 1.6881, Validation F1: 0.8104\n",
      "Epoch 983, Train Loss: 1.5601, Validation Loss: 1.6883, Validation F1: 0.8119\n",
      "Epoch 984, Train Loss: 1.5591, Validation Loss: 1.6872, Validation F1: 0.8108\n",
      "Epoch 985, Train Loss: 1.5589, Validation Loss: 1.6890, Validation F1: 0.8118\n",
      "Epoch 986, Train Loss: 1.5585, Validation Loss: 1.6866, Validation F1: 0.8110\n",
      "Epoch 987, Train Loss: 1.5614, Validation Loss: 1.6844, Validation F1: 0.8124\n",
      "Epoch 988, Train Loss: 1.5597, Validation Loss: 1.6806, Validation F1: 0.8126\n",
      "Epoch 989, Train Loss: 1.5588, Validation Loss: 1.6827, Validation F1: 0.8125\n",
      "Epoch 990, Train Loss: 1.5613, Validation Loss: 1.6915, Validation F1: 0.8101\n",
      "Epoch 991, Train Loss: 1.5600, Validation Loss: 1.6899, Validation F1: 0.8105\n",
      "Epoch 992, Train Loss: 1.5588, Validation Loss: 1.6819, Validation F1: 0.8107\n",
      "Epoch 993, Train Loss: 1.5591, Validation Loss: 1.6859, Validation F1: 0.8113\n",
      "Epoch 994, Train Loss: 1.5605, Validation Loss: 1.6829, Validation F1: 0.8117\n",
      "Epoch 995, Train Loss: 1.5595, Validation Loss: 1.6798, Validation F1: 0.8099\n",
      "Epoch 996, Train Loss: 1.5586, Validation Loss: 1.6850, Validation F1: 0.8112\n",
      "Epoch 997, Train Loss: 1.5627, Validation Loss: 1.6785, Validation F1: 0.8097\n",
      "Epoch 998, Train Loss: 1.5617, Validation Loss: 1.6859, Validation F1: 0.8105\n",
      "Epoch 999, Train Loss: 1.5594, Validation Loss: 1.6903, Validation F1: 0.8127\n",
      "Model training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Extract the best parameters from the grid search\n",
    "best_hidden_dim = 256  # Replace with the best hidden_dim found\n",
    "best_learning_rate = 0.005  # Replace with the best learning_rate found\n",
    "best_dropout = 0.3  # Replace with the best dropout found\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize the model with the best parameters\n",
    "model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                   edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                   hidden_channels=best_hidden_dim,\n",
    "                   dropout=best_dropout,\n",
    "                   out_channels=num_classes).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Compute class weights for the training dataset\n",
    "labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.unique(labels),\n",
    "                                                  y=labels)\n",
    "\n",
    "# Normalize class weights\n",
    "class_weights = th.FloatTensor(class_weights).to(device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "scheduler = th.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "# Move the graph data to the device\n",
    "G_pyg_train = G_pyg_train.to(device)\n",
    "G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "# ===== Load checkpoint if exists =====\n",
    "best_f1 = 0\n",
    "start_epoch = 0\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = th.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "    print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_f1_history = []\n",
    "saved_model_epochs = []\n",
    "\n",
    "train_loss_history_path = os.path.join(saves_path, 'train_loss_history.pkl')\n",
    "val_loss_history_path = os.path.join(saves_path, 'val_loss_history.pkl')\n",
    "val_f1_history_path = os.path.join(saves_path, 'val_f1_history.pkl')\n",
    "saved_model_epochs_path = os.path.join(saves_path, 'saved_model_epochs.pkl')\n",
    "\n",
    "if os.path.exists(train_loss_history_path) and os.path.exists(val_loss_history_path) and os.path.exists(val_f1_history_path) and os.path.exists(saved_model_epochs_path):\n",
    "    with open(train_loss_history_path, 'rb') as f:\n",
    "        train_loss_history = pickle.load(f)\n",
    "    with open(val_loss_history_path, 'rb') as f:\n",
    "        val_loss_history = pickle.load(f)\n",
    "    with open(val_f1_history_path, 'rb') as f:\n",
    "        val_f1_history = pickle.load(f)\n",
    "    with open(saved_model_epochs_path, 'rb') as f:\n",
    "        saved_model_epochs = pickle.load(f)\n",
    "\n",
    "# ===== Start Training =====\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    out = model(G_pyg_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(out, G_pyg_train.edge_label)\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        out = model(G_pyg_val)\n",
    "        loss = criterion(out, G_pyg_val.edge_label)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "    val_f1_micro = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='micro')\n",
    "    val_f1_macro = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='macro')\n",
    "\n",
    "    if epoch % 10 == 0 or val_f1 > best_f1:\n",
    "        # Save checkpoint\n",
    "        th.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_f1': best_f1\n",
    "        }, checkpoint_path)\n",
    "        with open(train_loss_history_path, 'wb') as f:\n",
    "            pickle.dump(train_loss_history, f)\n",
    "        with open(val_loss_history_path, 'wb') as f:\n",
    "            pickle.dump(val_loss_history, f)\n",
    "        with open(val_f1_history_path, 'wb') as f:\n",
    "            pickle.dump(val_f1_history, f)\n",
    "        with open(saved_model_epochs_path, 'wb') as f:\n",
    "            pickle.dump(saved_model_epochs, f)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1  # Update the best F1 score for this fold\n",
    "        best_model_state = model.state_dict()\n",
    "        th.save(best_model_state, best_model_path)\n",
    "        print(f\"Epoch {epoch} Saved best model. Best F1:\", best_f1)\n",
    "        saved_model_epochs.append(epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_f1_history.append((val_f1, val_f1_micro, val_f1_macro))\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Model training completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2376aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_process(train_losses, val_losses, val_f1, saved_model_epochs):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "    # Plot Train Loss\n",
    "    axs[0].plot(train_losses, label='Train Loss', color='blue')\n",
    "    axs[0].plot(val_losses, label='Validation Loss', color='red')\n",
    "    axs[0].set_ylabel('Train Loss')\n",
    "    axs[0].set_title('Training Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid()\n",
    "\n",
    "    val_f1_weighted_history = []\n",
    "    val_f1_micro_history = []\n",
    "    val_f1_macro_history = []\n",
    "\n",
    "    for val_f1_weighted, val_f1_micro, val_f1_macro in val_f1:\n",
    "        val_f1_weighted_history.append(val_f1_weighted)\n",
    "        val_f1_micro_history.append(val_f1_micro)\n",
    "        val_f1_macro_history.append(val_f1_macro)\n",
    "    \n",
    "    # Plot Validation F1\n",
    "\n",
    "    axs[1].plot(val_f1_weighted_history, label='Validation F1 Weighted', color='green')\n",
    "    axs[1].plot(val_f1_micro_history, label='Validation F1 Micro', color='blue')\n",
    "    axs[1].plot(val_f1_macro_history, label='Validation F1 Macro', color='red')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Validation F1')\n",
    "    axs[1].set_title('Validation F1 Score')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid()\n",
    "\n",
    "    # Add scatter for saved model epochs (e.g., F1 weighted here)\n",
    "    axs[1].scatter(saved_model_epochs, [val_f1_weighted_history[i] for i in saved_model_epochs],\n",
    "                   color='black', marker='o', label='Saved Model')\n",
    "    axs[1].legend()\n",
    "\n",
    "    print(len(train_losses))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df26e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_process(train_loss_history, val_loss_history, val_f1_history, saved_model_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8368fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 175\n",
      "Shape of node in G_pyg: torch.Size([175, 58])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 58])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "857f271a-612b-4cd6-a85a-e4236dec9d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_categorical_port/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[   41   100   110    87     0    13     0    22    25     4]\n",
      " [    5   100   119    64     2     7     0    19    12    21]\n",
      " [  264   379   823   453    47   108     0   138   107   134]\n",
      " [  658   459  1133  1288   238   485     0   979   676   763]\n",
      " [  179   226   215   764   311   280     0   687   445   530]\n",
      " [   56  1082   119   196    29 30503     0   127    97   113]\n",
      " [   30     3    12   117    27    58 32686   171    74   103]\n",
      " [  121    69   177   449   103   231     0   366   262   320]\n",
      " [   11     3     3    61    13    25     0    45    29    37]\n",
      " [    4     0     0     5     0     1     0     5     5     6]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0299    0.1020    0.0463       402\n",
      "     Backdoors     0.0413    0.2865    0.0722       349\n",
      "           DoS     0.3036    0.3355    0.3187      2453\n",
      "      Exploits     0.3697    0.1928    0.2535      6679\n",
      "       Fuzzers     0.4039    0.0855    0.1411      3637\n",
      "       Generic     0.9619    0.9437    0.9527     32322\n",
      "        Normal     1.0000    0.9821    0.9910     33281\n",
      "Reconnaissance     0.1430    0.1745    0.1572      2098\n",
      "     Shellcode     0.0167    0.1278    0.0296       227\n",
      "         Worms     0.0030    0.2308    0.0058        26\n",
      "\n",
      "      accuracy                         0.8120     81474\n",
      "     macro avg     0.3273    0.3461    0.2968     81474\n",
      "  weighted avg     0.8516    0.8120    0.8241     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def eval(G_pyg_test, adversarial=False):\n",
    "\n",
    "    G_pyg_test = G_pyg_test.to(device)\n",
    "    G_pyg_test.edge_label = G_pyg_test.edge_label.to(device)\n",
    "    G_pyg_test.edge_attr = G_pyg_test.edge_attr.to(device)\n",
    "\n",
    "    best_model = EGraphSAGE(node_in_channels=G_pyg_test.num_node_features, \n",
    "                       edge_in_channels=G_pyg_test.num_edge_features,\n",
    "                       hidden_channels=best_hidden_dim, \n",
    "                       out_channels=num_classes).to(device)\n",
    "\n",
    "    print(\"Loading model from\", best_model_path)\n",
    "    best_model.load_state_dict(th.load(best_model_path))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    best_model.eval()\n",
    "\n",
    "    print(\"inference start\")\n",
    "    with th.no_grad():\n",
    "            \n",
    "        try:\n",
    "            out = best_model(G_pyg_test)\n",
    "            \n",
    "        except Exception as forward_error:\n",
    "            print(f\"Error during forward/backward pass at {forward_error}\")\n",
    "\n",
    "    print(\"inference done\")\n",
    "\n",
    "    pred_labels = out.argmax(dim=1).cpu()\n",
    "    all_test_labels = G_pyg_test.edge_label.cpu()\n",
    "\n",
    "    if adversarial:\n",
    "\n",
    "        # Create a boolean mask where the label is NOT equal to the adversarial class\n",
    "        adversarial_mask = all_test_labels == ADVERSARIAL_CLASS_LABEL\n",
    "\n",
    "        # Print the class that the adversarial samples are classified as\n",
    "        cm_adversarial = confusion_matrix(all_test_labels[adversarial_mask], pred_labels[adversarial_mask], labels=range(len(class_map) + 1))\n",
    "        print(\"Adversarial confusion matrix:\", cm_adversarial)\n",
    "\n",
    "        # Apply the mask to both labels and predictions\n",
    "        all_test_labels = all_test_labels[~adversarial_mask]\n",
    "        pred_labels = pred_labels[~adversarial_mask]\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"class_map\", class_map)\n",
    "    # Generate a report\n",
    "    cm = confusion_matrix(all_test_labels, pred_labels, labels=range(len(class_map)))\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    report = classification_report(all_test_labels, pred_labels, target_names=class_map, digits=4)\n",
    "    print(report)\n",
    "\n",
    "    \n",
    "eval(G_pyg_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcb5e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_traffic_to_attacker(graph, ratio=0.1, num_injected_nodes=1, is_attack=False):\n",
    "    edge_index = graph.edge_index.clone()\n",
    "    edge_attr = graph.edge_attr.clone()\n",
    "    edge_label = graph.edge_label.clone()\n",
    "    x = graph.x.clone()\n",
    "\n",
    "    num_edges = edge_index.size(1)\n",
    "    feature_dim = graph.x.size(1)\n",
    "\n",
    "    # 1. Identify attacker nodes\n",
    "    attacker_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "    attacker_nodes = th.unique(edge_index[:, attacker_edges])\n",
    "    if attacker_nodes.numel() == 0:\n",
    "        raise ValueError(\"No attacker nodes found.\")\n",
    "\n",
    "    # 2. Sample benign edge feature pool\n",
    "    if is_attack:\n",
    "        attack_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[attack_edges]\n",
    "    else:\n",
    "        benign_edges = (edge_label == BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[benign_edges]\n",
    "\n",
    "    # 3. Inject new nodes\n",
    "    original_num_nodes = x.size(0)\n",
    "\n",
    "    new_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "    x = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "    # 4. Inject edges from injected nodes to attacker nodes\n",
    "    num_to_inject = max(1, int(ratio * num_edges))\n",
    "    new_edges = []\n",
    "    new_attrs = []\n",
    "    new_labels = []\n",
    "\n",
    "    \n",
    "    for _ in range(num_to_inject):\n",
    "        src = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\n",
    "        dst = attacker_nodes[random.randint(0, len(attacker_nodes) - 1)].item()\n",
    "\n",
    "        new_edges.append([src, dst])\n",
    "        attr = inject_edge_attr_pool[random.randint(0, len(inject_edge_attr_pool) - 1)]\n",
    "        new_attrs.append(attr)\n",
    "        new_labels.append(ADVERSARIAL_CLASS_LABEL)\n",
    "\n",
    "    # Create a new empty graph to store the injected edges\n",
    "    new_graph = Data()\n",
    "\n",
    "    # 5. Merge into graph\n",
    "    if new_edges:\n",
    "        new_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "        new_attrs = th.stack(new_attrs)\n",
    "        new_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "        new_graph.edge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "        new_graph.edge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "        new_graph.edge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "        new_graph.x = x\n",
    "\n",
    "        # new_graph.first_injected_node_idx = original_num_nodes # Store injected node indices\n",
    "\n",
    "    return new_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "632c5bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_categorical_port/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0 1047    0    0 2324  850 3770  156    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[   43     0    69   231     0     1     0    11    47     0]\n",
      " [   42     0    90   165     0     7     0    11    34     0]\n",
      " [  322     0   537  1191     0    96     0    46   261     0]\n",
      " [  328     0   823  3231     0   438     0   278  1581     0]\n",
      " [  104     0   139  1596     0   288     0   222  1288     0]\n",
      " [   46     0    93  1414     0 30500     0    49   220     0]\n",
      " [    4     0    28   271     0    53 32674    39   212     0]\n",
      " [   86     0   102   943     0   212     0   129   626     0]\n",
      " [    3     0     4   112     0    22     0    16    70     0]\n",
      " [    1     0     1    12     0     1     0     2     9     0]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0439    0.1070    0.0623       402\n",
      "     Backdoors     0.0000    0.0000    0.0000       349\n",
      "           DoS     0.2847    0.2189    0.2475      2453\n",
      "      Exploits     0.3525    0.4838    0.4078      6679\n",
      "       Fuzzers     0.0000    0.0000    0.0000      3637\n",
      "       Generic     0.9646    0.9436    0.9540     32322\n",
      "        Normal     1.0000    0.9818    0.9908     33281\n",
      "Reconnaissance     0.1606    0.0615    0.0889      2098\n",
      "     Shellcode     0.0161    0.3084    0.0306       227\n",
      "         Worms     0.0000    0.0000    0.0000        26\n",
      "\n",
      "      accuracy                         0.8246     81474\n",
      "     macro avg     0.2823    0.3105    0.2782     81474\n",
      "  weighted avg     0.8330    0.8246    0.8268     81474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Inject Attack Traffic to Attacker Nodes\n",
    "G_pyg_test = G_pyg_test.cpu()\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=True)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64c37b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_categorical_port/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0 1011    0  149 4216 1977  633  161    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[   42    33    37   224     0     1     0     2     1    62]\n",
      " [   40    49    41   151     0     5     0    11     9    43]\n",
      " [  302   247   296  1071     0    78    28    73    44   314]\n",
      " [  256   451   399  2427     0   353   179   493   246  1875]\n",
      " [   45   104    51  1092     0   231   158   307   227  1422]\n",
      " [   35    50    50  1311     0 30484    14    64    52   262]\n",
      " [    0    10    22   151     0    47 32697    62    17   275]\n",
      " [   53    56    55   642     0   180    79   177   100   756]\n",
      " [    0     2     4    72     0    19     8    24    12    86]\n",
      " [    0     0     1     8     0     1     0     2     0    14]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0543    0.1045    0.0715       402\n",
      "     Backdoors     0.0489    0.1404    0.0725       349\n",
      "           DoS     0.3096    0.1207    0.1737      2453\n",
      "      Exploits     0.3395    0.3634    0.3510      6679\n",
      "       Fuzzers     0.0000    0.0000    0.0000      3637\n",
      "       Generic     0.9709    0.9431    0.9568     32322\n",
      "        Normal     0.9859    0.9825    0.9842     33281\n",
      "Reconnaissance     0.1457    0.0844    0.1069      2098\n",
      "     Shellcode     0.0169    0.0529    0.0257       227\n",
      "         Worms     0.0027    0.5385    0.0055        26\n",
      "\n",
      "      accuracy                         0.8125     81474\n",
      "     macro avg     0.2875    0.3330    0.2748     81474\n",
      "  weighted avg     0.8293    0.8125    0.8191     81474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Inject BENIGN Traffic to Attacker Nodes\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=False)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bf0e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_random_nodes(graph, ratio=0.1, num_injected_nodes=1):\n",
    "\tedge_index = graph.edge_index.clone()\n",
    "\tedge_attr = graph.edge_attr.clone()\n",
    "\tedge_label = graph.edge_label.clone()\n",
    "\tx = graph.x.clone()\n",
    "\n",
    "\tnum_edges = edge_index.size(1)\n",
    "\tfeature_dim = graph.x.size(1)\n",
    "\n",
    "\t# 1. Inject new nodes\n",
    "\toriginal_num_nodes = x.size(0)\n",
    "\tnew_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "\tx = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "\t# 2. Inject random edges\n",
    "\tnum_to_inject = max(1, int(ratio * num_edges))\n",
    "\tnew_edges = []\n",
    "\tnew_attrs = []\n",
    "\tnew_labels = []\n",
    "\n",
    "\tfor _ in range(num_to_inject):\n",
    "\t\tsrc = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\t\tdst = random.randint(0, original_num_nodes - 1)  # to existing nodes\n",
    "\n",
    "\t\tnew_edges.append([src, dst])\n",
    "\t\tattr = edge_attr[random.randint(0, len(edge_attr) - 1)]  # Randomly sample edge attributes\n",
    "\t\tnew_attrs.append(attr)\n",
    "\t\tnew_labels.append(ADVERSARIAL_CLASS_LABEL)  # Assign benign class label to new edges\n",
    "\n",
    "\t# 3. Merge into graph\n",
    "\tif new_edges:\n",
    "\t\tnew_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "\t\tnew_attrs = th.stack(new_attrs)\n",
    "\t\tnew_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "\t\tedge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "\t\tedge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "\t\tedge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "\n",
    "\t# Create a new graph with the injected nodes and edges\n",
    "\tnew_graph = Data(\n",
    "\t\tedge_index=edge_index,\n",
    "\t\tedge_attr=edge_attr,\n",
    "\t\tedge_label=edge_label,\n",
    "\t\tx=x\n",
    "\t)\n",
    "\n",
    "\treturn new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abb63898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_categorical_port/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0  308   97   33 6292  353  291  773    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[   74    99    37   139     0     1     0     2    49     1]\n",
      " [   51   100    41    91     1     5     0    15    40     5]\n",
      " [  502   373   288   769    15    78     0    87   311    30]\n",
      " [ 1028   432   378  1855   111   353     0   604  1765   153]\n",
      " [  205   210    42  1037   124   231     0   343  1372    73]\n",
      " [   97  1075    45   262    17 30484     0    79   242    21]\n",
      " [   34     0     0   176     7    47 32686    71   245    15]\n",
      " [  148    60    40   624    38   180     0   215   743    50]\n",
      " [   10     1     0    73     7    19     0    32    77     8]\n",
      " [    3     0     0     8     0     1     0     2    12     0]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0344    0.1841    0.0579       402\n",
      "     Backdoors     0.0426    0.2865    0.0741       349\n",
      "           DoS     0.3307    0.1174    0.1733      2453\n",
      "      Exploits     0.3685    0.2777    0.3167      6679\n",
      "       Fuzzers     0.3875    0.0341    0.0627      3637\n",
      "       Generic     0.9709    0.9431    0.9568     32322\n",
      "        Normal     1.0000    0.9821    0.9910     33281\n",
      "Reconnaissance     0.1483    0.1025    0.1212      2098\n",
      "     Shellcode     0.0159    0.3392    0.0303       227\n",
      "         Worms     0.0000    0.0000    0.0000        26\n",
      "\n",
      "      accuracy                         0.8089     81474\n",
      "     macro avg     0.3299    0.3267    0.2784     81474\n",
      "  weighted avg     0.8553    0.8089    0.8222     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inject Random Nodes in the graph\n",
    "injected_graph = inject_random_nodes(G_pyg_test, 0.1, num_injected_nodes=1)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
