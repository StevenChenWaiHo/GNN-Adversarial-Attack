{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fb9e1b2-1c5f-49e3-82b4-3bfe52cabad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "=====Experiment=====\n",
    "Dataset: UNSW-NB15 dataset\n",
    "\n",
    "Training with whole graph\n",
    "Downsampled 90% normal traffic randomly\n",
    "Split train and test randomly\n",
    "\n",
    "IP as node\n",
    "Encode Ports in raw numbers\n",
    "'''\n",
    "\n",
    "from torch_geometric.utils import from_networkx, add_self_loops, degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "# import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Datasets.UNSW_NB15.UNSW_NB15_config import UNSW_NB15_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ef09a-d405-43b8-971e-fe9e6a592c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3555220/1634753847.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "Normal            221876\n",
      "Generic           215481\n",
      "Exploits           44525\n",
      "Fuzzers            24246\n",
      "DoS                16353\n",
      "Reconnaissance     13987\n",
      "Analysis            2677\n",
      "Backdoors           2329\n",
      "Shellcode           1511\n",
      "Worms                174\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    321283\n",
      "0    221876\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "csv_file_name = \"all_downsampled\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n",
    "\n",
    "DATASET_NAME = \"UNSW_NB15\"\n",
    "EXPERIMENT_NAME = \"whole_graph_raw_port\"\n",
    "\n",
    "SOURCE_IP_COL_NAME = UNSW_NB15_Config.SOURCE_IP_COL_NAME\n",
    "DESTINATION_IP_COL_NAME = UNSW_NB15_Config.DESTINATION_IP_COL_NAME\n",
    "SOURCE_PORT_COL_NAME = UNSW_NB15_Config.SOURCE_PORT_COL_NAME\n",
    "DESTINATION_PORT_COL_NAME = UNSW_NB15_Config.DESTINATION_PORT_COL_NAME\n",
    "\n",
    "ATTACK_CLASS_COL_NAME = UNSW_NB15_Config.ATTACK_CLASS_COL_NAME\n",
    "IS_ATTACK_COL_NAME = UNSW_NB15_Config.IS_ATTACK_COL_NAME\n",
    "\n",
    "BENIGN_CLASS_NAME = UNSW_NB15_Config.BENIGN_CLASS_NAME\n",
    "\n",
    "TIME_COLS = UNSW_NB15_Config.TIME_COL_NAMES\n",
    "\n",
    "CATEGORICAL_COLS = UNSW_NB15_Config.CATEGORICAL_COLS\n",
    "\n",
    "print(data[ATTACK_CLASS_COL_NAME].value_counts())\n",
    "print(data[IS_ATTACK_COL_NAME].value_counts())\n",
    "\n",
    "MULTICLASS = True\n",
    "\n",
    "if MULTICLASS:\n",
    "    label_col = ATTACK_CLASS_COL_NAME\n",
    "    data.drop(columns=[IS_ATTACK_COL_NAME], inplace=True)\n",
    "else:\n",
    "    label_col = IS_ATTACK_COL_NAME\n",
    "    data.drop(columns=[ATTACK_CLASS_COL_NAME], inplace=True)\n",
    "\n",
    "saves_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, EXPERIMENT_NAME)\n",
    "\n",
    "checkpoint_path = os.path.join(saves_path, f\"checkpoints_{csv_file_name}.pth\")\n",
    "best_model_path = os.path.join(saves_path, f\"best_model_{csv_file_name}.pth\")\n",
    "\n",
    "os.makedirs(saves_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a1af1-1d3d-4179-9628-7c2ec551ce0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srcip</th>\n",
       "      <th>sport</th>\n",
       "      <th>dstip</th>\n",
       "      <th>dsport</th>\n",
       "      <th>state</th>\n",
       "      <th>dur</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>source_file_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.40.85.1_0</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0.0.5_0</td>\n",
       "      <td>0</td>\n",
       "      <td>INT</td>\n",
       "      <td>50.004341</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.166.0.6_0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.4_0</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>132</td>\n",
       "      <td>164</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175.45.176.0_0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.16_0</td>\n",
       "      <td>80</td>\n",
       "      <td>FIN</td>\n",
       "      <td>2.390390</td>\n",
       "      <td>1362</td>\n",
       "      <td>268</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reconnaissance</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.166.0.3_0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.8_0</td>\n",
       "      <td>25</td>\n",
       "      <td>FIN</td>\n",
       "      <td>34.077175</td>\n",
       "      <td>37358</td>\n",
       "      <td>3380</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.40.170.2_0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.40.170.2_0</td>\n",
       "      <td>0</td>\n",
       "      <td>INT</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543154</th>\n",
       "      <td>175.45.176.1_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.11_3</td>\n",
       "      <td>6071</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.291164</td>\n",
       "      <td>732</td>\n",
       "      <td>468</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Generic</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543155</th>\n",
       "      <td>175.45.176.3_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.16_3</td>\n",
       "      <td>2140</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.011751</td>\n",
       "      <td>76</td>\n",
       "      <td>132</td>\n",
       "      <td>254</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Backdoors</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543156</th>\n",
       "      <td>59.166.0.2_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.4_3</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>146</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Normal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543157</th>\n",
       "      <td>175.45.176.1_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.11_3</td>\n",
       "      <td>5250</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.176514</td>\n",
       "      <td>10778</td>\n",
       "      <td>268</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Generic</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543158</th>\n",
       "      <td>59.166.0.2_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.2_3</td>\n",
       "      <td>8406</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.049598</td>\n",
       "      <td>2646</td>\n",
       "      <td>25564</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Normal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543159 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 srcip  sport             dstip dsport state        dur  \\\n",
       "0         10.40.85.1_0      0       224.0.0.5_0      0   INT  50.004341   \n",
       "1         59.166.0.6_0      0   149.171.126.4_0     53   CON   0.001134   \n",
       "2       175.45.176.0_0      0  149.171.126.16_0     80   FIN   2.390390   \n",
       "3         59.166.0.3_0      0   149.171.126.8_0     25   FIN  34.077175   \n",
       "4        10.40.170.2_0      0     10.40.170.2_0      0   INT   0.000000   \n",
       "...                ...    ...               ...    ...   ...        ...   \n",
       "543154  175.45.176.1_3      0  149.171.126.11_3   6071   FIN   0.291164   \n",
       "543155  175.45.176.3_3      0  149.171.126.16_3   2140   CON   0.011751   \n",
       "543156    59.166.0.2_3      0   149.171.126.4_3     53   CON   0.002410   \n",
       "543157  175.45.176.1_3      0  149.171.126.11_3   5250   FIN   0.176514   \n",
       "543158    59.166.0.2_3      0   149.171.126.2_3   8406   FIN   0.049598   \n",
       "\n",
       "        sbytes  dbytes  sttl  dttl  ...  ct_ftp_cmd  ct_srv_src  ct_srv_dst  \\\n",
       "0          384       0     1     0  ...         0.0           2           4   \n",
       "1          132     164    31    29  ...         0.0          12           7   \n",
       "2         1362     268   254   252  ...         0.0           5           2   \n",
       "3        37358    3380    31    29  ...         0.0           1           1   \n",
       "4           46       0     0     0  ...         0.0           2           2   \n",
       "...        ...     ...   ...   ...  ...         ...         ...         ...   \n",
       "543154     732     468   254   252  ...         NaN           1           1   \n",
       "543155      76     132   254    60  ...         NaN           1           1   \n",
       "543156     146     178    31    29  ...         NaN           3           5   \n",
       "543157   10778     268   254   252  ...         NaN           1           1   \n",
       "543158    2646   25564    31    29  ...         NaN           6           2   \n",
       "\n",
       "        ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  \\\n",
       "0                4           2                 2                 4   \n",
       "1                1           2                 2                 1   \n",
       "2                2           1                 1                 1   \n",
       "3               12          10                 1                 1   \n",
       "4                2           2                 2                 2   \n",
       "...            ...         ...               ...               ...   \n",
       "543154           1           1                 1                 1   \n",
       "543155           1           1                 1                 1   \n",
       "543156           3           2                 2                 2   \n",
       "543157           1           1                 1                 1   \n",
       "543158           4           7                 1                 1   \n",
       "\n",
       "        ct_dst_src_ltm      attack_cat  source_file_id  \n",
       "0                    2          Normal               0  \n",
       "1                    1          Normal               0  \n",
       "2                    1  Reconnaissance               0  \n",
       "3                    2          Normal               0  \n",
       "4                    2          Normal               0  \n",
       "...                ...             ...             ...  \n",
       "543154               2         Generic               3  \n",
       "543155               1       Backdoors               3  \n",
       "543156               4          Normal               3  \n",
       "543157               2         Generic               3  \n",
       "543158               3          Normal               3  \n",
       "\n",
       "[543159 rows x 45 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns=UNSW_NB15_Config.DROP_COLS,inplace=True)\n",
    "data.drop(columns=UNSW_NB15_Config.TIME_COL_NAMES)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c690c-86a4-49f7-aa9c-58f94529547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns = CATEGORICAL_COLS) # One Hot Encoding for categorical data\n",
    "\n",
    "converted_categorical_cols = [col for col in data.columns if col.startswith(tuple(CATEGORICAL_COLS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5651ef5b-0a9d-4641-aad7-5738092c46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                  srcip  sport             dstip  dsport        dur  sbytes  \\\n",
      "0         10.40.85.1_0      0       224.0.0.5_0       0  50.004341     384   \n",
      "1         59.166.0.6_0      0   149.171.126.4_0      83   0.001134     132   \n",
      "2       175.45.176.0_0      0  149.171.126.16_0     128   2.390390    1362   \n",
      "3         59.166.0.3_0      0   149.171.126.8_0      37  34.077175   37358   \n",
      "4        10.40.170.2_0      0     10.40.170.2_0       0   0.000000      46   \n",
      "...                ...    ...               ...     ...        ...     ...   \n",
      "543154  175.45.176.1_3      0  149.171.126.11_3   24689   0.291164     732   \n",
      "543155  175.45.176.3_3      0  149.171.126.16_3    8512   0.011751      76   \n",
      "543156    59.166.0.2_3      0   149.171.126.4_3      83   0.002410     146   \n",
      "543157  175.45.176.1_3      0  149.171.126.11_3   21072   0.176514   10778   \n",
      "543158    59.166.0.2_3      0   149.171.126.2_3   33798   0.049598    2646   \n",
      "\n",
      "        dbytes  sttl  dttl  sloss  ...  state_ECR  state_FIN  state_INT  \\\n",
      "0            0     1     0      0  ...      False      False       True   \n",
      "1          164    31    29      0  ...      False      False      False   \n",
      "2          268   254   252      6  ...      False       True      False   \n",
      "3         3380    31    29     18  ...      False       True      False   \n",
      "4            0     0     0      0  ...      False      False       True   \n",
      "...        ...   ...   ...    ...  ...        ...        ...        ...   \n",
      "543154     468   254   252      3  ...      False       True      False   \n",
      "543155     132   254    60      0  ...      False      False      False   \n",
      "543156     178    31    29      0  ...      False      False      False   \n",
      "543157     268   254   252      5  ...      False       True      False   \n",
      "543158   25564    31    29      7  ...      False       True      False   \n",
      "\n",
      "        state_MAS  state_PAR  state_REQ  state_RST  state_TST  state_TXD  \\\n",
      "0           False      False      False      False      False      False   \n",
      "1           False      False      False      False      False      False   \n",
      "2           False      False      False      False      False      False   \n",
      "3           False      False      False      False      False      False   \n",
      "4           False      False      False      False      False      False   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "543154      False      False      False      False      False      False   \n",
      "543155      False      False      False      False      False      False   \n",
      "543156      False      False      False      False      False      False   \n",
      "543157      False      False      False      False      False      False   \n",
      "543158      False      False      False      False      False      False   \n",
      "\n",
      "        state_URH  \n",
      "0           False  \n",
      "1           False  \n",
      "2           False  \n",
      "3           False  \n",
      "4           False  \n",
      "...           ...  \n",
      "543154      False  \n",
      "543155      False  \n",
      "543156      False  \n",
      "543157      False  \n",
      "543158      False  \n",
      "\n",
      "[543159 rows x 60 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2d96115-31f9-48cb-b3e6-7853d2d253cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                  srcip  sport             dstip  dsport        dur  sbytes  \\\n",
      "0         10.40.85.1_0      0       224.0.0.5_0       0  50.004341     384   \n",
      "1         59.166.0.6_0      0   149.171.126.4_0      83   0.001134     132   \n",
      "2       175.45.176.0_0      0  149.171.126.16_0     128   2.390390    1362   \n",
      "3         59.166.0.3_0      0   149.171.126.8_0      37  34.077175   37358   \n",
      "4        10.40.170.2_0      0     10.40.170.2_0       0   0.000000      46   \n",
      "...                ...    ...               ...     ...        ...     ...   \n",
      "543154  175.45.176.1_3      0  149.171.126.11_3   24689   0.291164     732   \n",
      "543155  175.45.176.3_3      0  149.171.126.16_3    8512   0.011751      76   \n",
      "543156    59.166.0.2_3      0   149.171.126.4_3      83   0.002410     146   \n",
      "543157  175.45.176.1_3      0  149.171.126.11_3   21072   0.176514   10778   \n",
      "543158    59.166.0.2_3      0   149.171.126.2_3   33798   0.049598    2646   \n",
      "\n",
      "        dbytes  sttl  dttl  sloss  ...  state_ECR  state_FIN  state_INT  \\\n",
      "0            0     1     0      0  ...      False      False       True   \n",
      "1          164    31    29      0  ...      False      False      False   \n",
      "2          268   254   252      6  ...      False       True      False   \n",
      "3         3380    31    29     18  ...      False       True      False   \n",
      "4            0     0     0      0  ...      False      False       True   \n",
      "...        ...   ...   ...    ...  ...        ...        ...        ...   \n",
      "543154     468   254   252      3  ...      False       True      False   \n",
      "543155     132   254    60      0  ...      False      False      False   \n",
      "543156     178    31    29      0  ...      False      False      False   \n",
      "543157     268   254   252      5  ...      False       True      False   \n",
      "543158   25564    31    29      7  ...      False       True      False   \n",
      "\n",
      "        state_MAS  state_PAR  state_REQ  state_RST  state_TST  state_TXD  \\\n",
      "0           False      False      False      False      False      False   \n",
      "1           False      False      False      False      False      False   \n",
      "2           False      False      False      False      False      False   \n",
      "3           False      False      False      False      False      False   \n",
      "4           False      False      False      False      False      False   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "543154      False      False      False      False      False      False   \n",
      "543155      False      False      False      False      False      False   \n",
      "543156      False      False      False      False      False      False   \n",
      "543157      False      False      False      False      False      False   \n",
      "543158      False      False      False      False      False      False   \n",
      "\n",
      "        state_URH  \n",
      "0           False  \n",
      "1           False  \n",
      "2           False  \n",
      "3           False  \n",
      "4           False  \n",
      "...           ...  \n",
      "543154      False  \n",
      "543155      False  \n",
      "543156      False  \n",
      "543157      False  \n",
      "543158      False  \n",
      "\n",
      "[543159 rows x 60 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "data.drop(columns=['index'],inplace=True)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df5c4c-70a2-4566-ae5e-ee3dcc6037a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 dur        sbytes        dbytes           sttl  \\\n",
      "count  543159.000000  5.431590e+05  5.431590e+05  543159.000000   \n",
      "mean        0.703562  5.129376e+03  1.912066e+04     157.223966   \n",
      "std        12.635598  1.202304e+05  1.382834e+05     108.429349   \n",
      "min         0.000000  0.000000e+00  0.000000e+00       0.000000   \n",
      "25%         0.000007  1.140000e+02  0.000000e+00      31.000000   \n",
      "50%         0.000010  2.000000e+02  0.000000e+00     254.000000   \n",
      "75%         0.070875  1.580000e+03  1.936000e+03     254.000000   \n",
      "max      8760.776367  1.435577e+07  1.465753e+07     255.000000   \n",
      "\n",
      "                dttl          sloss          dloss         Sload  \\\n",
      "count  543159.000000  543159.000000  543159.000000  5.431590e+05   \n",
      "mean       38.847354       3.789714       8.637535  6.901181e+07   \n",
      "std        77.059190      45.614073      49.869719  1.425974e+08   \n",
      "min         0.000000       0.000000       0.000000  0.000000e+00   \n",
      "25%         0.000000       0.000000       0.000000  3.760815e+05   \n",
      "50%         0.000000       0.000000       0.000000  4.560000e+07   \n",
      "75%        29.000000       3.000000       4.000000  8.888889e+07   \n",
      "max       254.000000    5319.000000    5507.000000  5.988000e+09   \n",
      "\n",
      "              Dload          Spkts  ...     ct_ftp_cmd     ct_srv_src  \\\n",
      "count  5.431590e+05  543159.000000  ...  543159.000000  543159.000000   \n",
      "mean   1.145602e+06      20.260456  ...       0.007661      15.025361   \n",
      "std    3.125320e+06     101.785929  ...       0.091356      14.239878   \n",
      "min    0.000000e+00       0.000000  ...       0.000000       1.000000   \n",
      "25%    0.000000e+00       2.000000  ...       0.000000       3.000000   \n",
      "50%    0.000000e+00       2.000000  ...       0.000000       9.000000   \n",
      "75%    4.080209e+05      14.000000  ...       0.000000      26.000000   \n",
      "max    2.248756e+07   10646.000000  ...       4.000000      67.000000   \n",
      "\n",
      "          ct_srv_dst     ct_dst_ltm     ct_src_ltm  ct_src_dport_ltm  \\\n",
      "count  543159.000000  543159.000000  543159.000000     543159.000000   \n",
      "mean       14.853214      10.321932      10.848566          9.357573   \n",
      "std        14.314732      10.996982      10.976383         11.399195   \n",
      "min         1.000000       1.000000       1.000000          1.000000   \n",
      "25%         3.000000       2.000000       2.000000          1.000000   \n",
      "50%         8.000000       5.000000       6.000000          2.000000   \n",
      "75%        26.000000      17.000000      17.000000         17.000000   \n",
      "max        67.000000      67.000000      67.000000         67.000000   \n",
      "\n",
      "       ct_dst_sport_ltm  ct_dst_src_ltm          sport         dsport  \n",
      "count     543159.000000   543159.000000  543159.000000  543159.000000  \n",
      "mean           7.219855       13.786578     792.052090   14131.372302  \n",
      "std            8.074346       14.983005    5931.378086   58591.257073  \n",
      "min            1.000000        1.000000       0.000000      -1.000000  \n",
      "25%            1.000000        1.000000       0.000000       0.000000  \n",
      "50%            2.000000        5.000000       0.000000      83.000000  \n",
      "75%           15.000000       26.000000       0.000000      83.000000  \n",
      "max           60.000000       67.000000   65531.000000  415029.000000  \n",
      "\n",
      "[8 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_norm = UNSW_NB15_Config.COLS_TO_NORM + [SOURCE_PORT_COL_NAME, DESTINATION_PORT_COL_NAME]\n",
    "print(data[cols_to_norm].describe()) # Check if there's any too large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ea95177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All other columns processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_numeric_issues(df, cols_to_norm):\n",
    "    for col in cols_to_norm:\n",
    "        try:\n",
    "            # Try to coerce to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Try to clip the column\n",
    "            df[col] = df[col].clip(lower=-1e9, upper=1e9)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Column '{col}' failed with error: {e}\")\n",
    "            print(f\"  - Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "            print(f\"  - Data type: {df[col].dtype}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n✅ All other columns processed successfully.\")\n",
    "\n",
    "check_numeric_issues(data, UNSW_NB15_Config.COLS_TO_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37256006-abc1-44aa-8e74-46d05dc6ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c6e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "Attack label mapping: {'Analysis': 0, 'Backdoors': 1, 'DoS': 2, 'Exploits': 3, 'Fuzzers': 4, 'Generic': 5, 'Normal': 6, 'Reconnaissance': 7, 'Shellcode': 8, 'Worms': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_classes = 2\n",
    "class_map = [0, 1]\n",
    "if MULTICLASS:\n",
    "    le = LabelEncoder()\n",
    "    attack_labels = le.fit_transform(data[ATTACK_CLASS_COL_NAME])\n",
    "    class_map = le.classes_\n",
    "    print(class_map)\n",
    "    print(\"Attack label mapping:\", dict(zip(class_map, range(len(class_map)))))\n",
    "    data[ATTACK_CLASS_COL_NAME] = attack_labels\n",
    "    num_classes = len(class_map)\n",
    "    class_dict = {le.inverse_transform([i])[0]: i for i in range(len(le.classes_))}\n",
    "\n",
    "BENIGN_CLASS_LABEL = le.transform([BENIGN_CLASS_NAME])[0] if MULTICLASS else 0\n",
    "ADVERSARIAL_CLASS_LABEL = len(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f4cdd-2716-431f-af50-b34cc3d2d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'sport', 'dsport', 'state_ACC', 'state_CLO', 'state_CON', 'state_ECO', 'state_ECR', 'state_FIN', 'state_INT', 'state_MAS', 'state_PAR', 'state_REQ', 'state_RST', 'state_TST', 'state_TXD', 'state_URH']\n",
      "Number of training samples: 461685\n",
      "attack_cat\n",
      "6    188595\n",
      "5    183159\n",
      "3     37846\n",
      "4     20609\n",
      "2     13900\n",
      "7     11889\n",
      "0      2275\n",
      "1      1980\n",
      "8      1284\n",
      "9       148\n",
      "Name: count, dtype: int64\n",
      "Number of test samples: 81474\n",
      "attack_cat\n",
      "6    33281\n",
      "5    32322\n",
      "3     6679\n",
      "4     3637\n",
      "2     2453\n",
      "7     2098\n",
      "0      402\n",
      "1      349\n",
      "8      227\n",
      "9       26\n",
      "Name: count, dtype: int64\n",
      "                 srcip     sport             dstip    dsport       dur  \\\n",
      "35798   175.45.176.0_0 -0.133536  149.171.126.11_0 -0.241186  0.024440   \n",
      "454500    59.166.0.2_3 -0.133536   149.171.126.1_3  3.763272 -0.052447   \n",
      "23187   175.45.176.0_0 -0.133536  149.171.126.18_0 -0.241186 -0.017379   \n",
      "489755  175.45.176.3_3 -0.133536  149.171.126.15_3 -0.240281 -0.055680   \n",
      "134305    59.166.0.1_1 -0.133536   149.171.126.4_1 -0.240759 -0.004966   \n",
      "\n",
      "          sbytes    dbytes      sttl      dttl     sloss     dloss     Sload  \\\n",
      "35798  -0.037756 -0.136334  0.892527  2.766092 -0.039236 -0.153150 -0.554662   \n",
      "454500 -0.010009  0.271236 -1.164114 -0.127790  0.070379  0.368209 -0.548462   \n",
      "23187  -0.033264 -0.136334  0.892527  2.766092 -0.039236 -0.153150 -0.554559   \n",
      "489755 -0.041715 -0.138272  0.892527 -0.504124 -0.083082 -0.173202 -0.018194   \n",
      "134305  0.269172 -0.113829 -1.164114 -0.127790  0.311533 -0.012784 -0.550916   \n",
      "\n",
      "           Dload     Spkts     Dpkts      swin      dwin     stcpb     dtcpb  \\\n",
      "35798  -0.365989 -0.100804 -0.158623  1.303104  1.304850  0.950180 -0.462170   \n",
      "454500  3.128362  0.449370  0.427016  1.303104  1.304850  0.274400  0.303777   \n",
      "23187  -0.365371 -0.100804 -0.158623  1.303104  1.304850  1.484216  1.484299   \n",
      "489755 -0.366555 -0.179401 -0.215298 -0.767401 -0.766374 -0.720896 -0.720905   \n",
      "134305 -0.353373  0.311827  0.181426  1.303104  1.304850  1.484216 -0.715505   \n",
      "\n",
      "         smeansz   dmeansz  trans_depth  res_bdy_len      Sjit      Djit  \\\n",
      "35798  -0.329716 -0.366711    -0.134792     -0.05343  0.153264 -0.076895   \n",
      "454500 -0.329716  2.437416    -0.134792     -0.05343 -0.080902 -0.175606   \n",
      "23187   0.017606 -0.366711    -0.134792     -0.05343  0.012423 -0.150894   \n",
      "489755 -0.342580 -0.526845    -0.134792     -0.05343 -0.080902 -0.189417   \n",
      "134305  3.928197 -0.242162    -0.134792     -0.05343 -0.043454 -0.177189   \n",
      "\n",
      "             Stime       Ltime   Sintpkt   Dintpkt    tcprtt    synack  \\\n",
      "35798   1421933745  1421933746 -0.008638  0.119318  3.717742  3.612803   \n",
      "454500  1424240533  1424240533 -0.060137 -0.054933 -0.278494 -0.242636   \n",
      "23187   1421931706  1421931707 -0.036975  0.019956  2.899294  2.974987   \n",
      "489755  1424245264  1424245264 -0.060450 -0.055530 -0.289971 -0.258558   \n",
      "134305  1424222454  1424222454 -0.053968 -0.040295 -0.277104 -0.239905   \n",
      "\n",
      "          ackdat  is_sm_ips_ports  ct_state_ttl  ct_flw_http_mthd  \\\n",
      "35798   3.416380        -0.028049     -0.098678         -0.156918   \n",
      "454500 -0.288018        -0.028049     -1.123769         -0.156918   \n",
      "23187   2.483664        -0.028049     -0.098678         -0.156918   \n",
      "489755 -0.293097        -0.028049      0.926413         -0.156918   \n",
      "134305 -0.288324        -0.028049     -1.123769         -0.156918   \n",
      "\n",
      "        is_ftp_login  ct_ftp_cmd  ct_srv_src  ct_srv_dst  ct_dst_ltm  \\\n",
      "35798      -0.104295   -0.083856   -0.633809   -0.618469   -0.756748   \n",
      "454500     -0.104295   -0.083856   -0.774260   -0.688327   -0.574879   \n",
      "23187      -0.104295   -0.083856   -0.984936   -0.967760   -0.483945   \n",
      "489755     -0.104295   -0.083856    0.489797    0.499261    1.061935   \n",
      "134305     -0.104295   -0.083856   -0.984936   -0.967760   -0.483945   \n",
      "\n",
      "        ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \\\n",
      "35798    -0.806147         -0.733173         -0.770324       -0.786664   \n",
      "454500   -0.715042         -0.733173         -0.770324       -0.786664   \n",
      "23187    -0.715042         -0.733173         -0.770324       -0.853406   \n",
      "489755    1.015949          1.109064          1.830509        0.548183   \n",
      "134305   -0.441727         -0.733173         -0.770324       -0.786664   \n",
      "\n",
      "        attack_cat  source_file_id  state_ACC  state_CLO  state_CON  \\\n",
      "35798            4               0      False      False      False   \n",
      "454500           6               3      False      False      False   \n",
      "23187            3               0      False      False      False   \n",
      "489755           5               3      False      False      False   \n",
      "134305           6               1      False      False      False   \n",
      "\n",
      "        state_ECO  state_ECR  state_FIN  state_INT  state_MAS  state_PAR  \\\n",
      "35798       False      False       True      False      False      False   \n",
      "454500      False      False       True      False      False      False   \n",
      "23187       False      False       True      False      False      False   \n",
      "489755      False      False      False       True      False      False   \n",
      "134305      False      False       True      False      False      False   \n",
      "\n",
      "        state_REQ  state_RST  state_TST  state_TXD  state_URH  \\\n",
      "35798       False      False      False      False      False   \n",
      "454500      False      False      False      False      False   \n",
      "23187       False      False      False      False      False   \n",
      "489755      False      False      False      False      False   \n",
      "134305      False      False      False      False      False   \n",
      "\n",
      "                                                        h  \n",
      "35798   [0.024439518249307186, -0.03775569116632746, -...  \n",
      "454500  [-0.05244682114681552, -0.01000892950063232, 0...  \n",
      "23187   [-0.01737919714689172, -0.03326430888231206, -...  \n",
      "489755  [-0.05568046603287485, -0.04171476147594104, -...  \n",
      "134305  [-0.004966318747785316, 0.269172066323925, -0....  \n"
     ]
    }
   ],
   "source": [
    "# 70% train, 15% validation, 15% test\n",
    "train_full_df, test_df = train_test_split(\n",
    "     data, test_size=0.15, random_state=42, stratify=data[label_col])\n",
    "\n",
    "\n",
    "feature_cols = cols_to_norm + converted_categorical_cols\n",
    "\n",
    "print('Feature Columns:', feature_cols)\n",
    "\n",
    "train_full_df['h'] = train_full_df[ feature_cols ].values.tolist()\n",
    "test_df['h'] = test_df[ feature_cols ].values.tolist()\n",
    "\n",
    "y_train = train_full_df[label_col]\n",
    "y_test = test_df[label_col]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_full_df))\n",
    "print(y_train.value_counts())\n",
    "print(\"Number of test samples:\", len(test_df))\n",
    "print(y_test.value_counts())\n",
    "\n",
    "print(train_full_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f83bd73-531f-42a0-9f73-e1fd98dce1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, source_ip_col, destination_ip_col, edge_attr, create_using=nx.MultiDiGraph(), **kwargs):\n",
    "    G_nx = nx.from_pandas_edgelist(df, source_ip_col, destination_ip_col, edge_attr, create_using=create_using, **kwargs)\n",
    "    G_pyg = from_networkx(G_nx)\n",
    "\n",
    "    num_nodes = G_pyg.num_nodes\n",
    "    num_edges = G_pyg.num_edges\n",
    "\n",
    "    G_pyg.x = th.ones(num_nodes, len(df['h'].iloc[0])) \n",
    "\n",
    "    edge_attr_list = []\n",
    "    edge_label_list = []\n",
    "\n",
    "    for u, v, key, data in G_nx.edges(keys=True, data=True):\n",
    "        edge_attr_list.append(data['h']) \n",
    "        edge_label_list.append(data[label_col]) \n",
    "\n",
    "    G_pyg.edge_attr = th.tensor(edge_attr_list, dtype=th.float32)\n",
    "    G_pyg.edge_label = th.tensor(edge_label_list, dtype=th.long)\n",
    "\n",
    "    print(\"Number of edges in G_pyg:\", num_edges)\n",
    "    print(\"Number of node in G_pyg:\", num_nodes)\n",
    "    print(\"Shape of node in G_pyg:\", G_pyg.x.shape)\n",
    "    print(\"Shape of edge attr in G_pyg:\", G_pyg.edge_attr.shape)\n",
    "    print(\"Shape of edge label in G_pyg:\", G_pyg.edge_label.shape)\n",
    "\n",
    "    return G_nx, G_pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c8f9cdb-1316-461a-a927-8d67d90d6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 175\n",
      "Shape of node in G_pyg: torch.Size([175, 54])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 54])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41795339-6036-468f-9b9d-2bb68d78ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGELayerPyG(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_dim, out_channels, activation=F.relu):\n",
    "        super().__init__(aggr='mean')  # mean aggregation\n",
    "        self.W_msg = nn.Linear(in_channels + edge_dim, out_channels)\n",
    "        self.W_apply = nn.Linear(in_channels + out_channels, out_channels)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: [num_nodes, in_channels]\n",
    "        # edge_attr: [num_edges, edge_dim]\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j: features of source nodes (neighbours)\n",
    "        msg_input = th.cat([x_j, edge_attr], dim=1)\n",
    "        return self.W_msg(msg_input)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out: [num_nodes, out_channels]\n",
    "        combined = th.cat([x, aggr_out], dim=1)\n",
    "        out = self.W_apply(combined)\n",
    "        return self.activation(out)\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.lin = nn.Linear(in_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, data, z):\n",
    "        row, col = data.edge_index\n",
    "        # Concatenate the features of source and target nodes for each edge\n",
    "        edge_feat = th.cat([z[row], z[col]], dim=1)\n",
    "        return self.lin(edge_feat)\n",
    "\n",
    "class EGraphSAGE(nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels, out_channels, dropout=0.2):\n",
    "        super(EGraphSAGE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGELayerPyG(node_in_channels, edge_in_channels, hidden_channels)\n",
    "        self.conv2 = SAGELayerPyG(hidden_channels, edge_in_channels, hidden_channels)\n",
    "        self.mlp_predictor = MLPPredictor(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return self.mlp_predictor(data, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bca25fef-29d9-40cf-8910-16b24d530693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cccdc850-b98d-4836-b82b-67aa4b9e1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89157faf-e24b-49d6-9c90-6f71dae515b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d37f0-713b-4abc-8d7a-3e768ae9a2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 307790\n",
      "Number of node in G_pyg: 178\n",
      "Shape of node in G_pyg: torch.Size([178, 54])\n",
      "Shape of edge attr in G_pyg: torch.Size([307790, 54])\n",
      "Shape of edge label in G_pyg: torch.Size([307790])\n",
      "Number of edges in G_pyg: 153895\n",
      "Number of node in G_pyg: 174\n",
      "Shape of node in G_pyg: torch.Size([174, 54])\n",
      "Shape of edge attr in G_pyg: torch.Size([153895, 54])\n",
      "Shape of edge label in G_pyg: torch.Size([153895])\n",
      "Number of edges in G_pyg: 307790\n",
      "Number of node in G_pyg: 177\n",
      "Shape of node in G_pyg: torch.Size([177, 54])\n",
      "Shape of edge attr in G_pyg: torch.Size([307790, 54])\n",
      "Shape of edge label in G_pyg: torch.Size([307790])\n",
      "Number of edges in G_pyg: 153895\n",
      "Number of node in G_pyg: 177\n",
      "Shape of node in G_pyg: torch.Size([177, 54])\n",
      "Shape of edge attr in G_pyg: torch.Size([153895, 54])\n",
      "Shape of edge label in G_pyg: torch.Size([153895])\n",
      "Number of edges in G_pyg: 307790\n",
      "Number of node in G_pyg: 178\n",
      "Shape of node in G_pyg: torch.Size([178, 54])\n",
      "Shape of edge attr in G_pyg: torch.Size([307790, 54])\n",
      "Shape of edge label in G_pyg: torch.Size([307790])\n",
      "Number of edges in G_pyg: 153895\n",
      "Number of node in G_pyg: 176\n",
      "Shape of node in G_pyg: torch.Size([176, 54])\n",
      "Shape of edge attr in G_pyg: torch.Size([153895, 54])\n",
      "Shape of edge label in G_pyg: torch.Size([153895])\n",
      "Testing with learning rate: 0.001, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1948, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5766, Validation Loss: 2.2806, Validation F1: 0.1948\n",
      "Best F1 Score at epoch 1: 0.6939, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7285, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7498, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7661, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7723, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7736, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.7807, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.7832, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.7904, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8126, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8160, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 33: 0.8198, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 48: 0.8222, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 50: 0.8239, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 59: 0.8241, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 66: 0.8242, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 94: 0.8278, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3053, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5929, Validation Loss: 2.3089, Validation F1: 0.3053\n",
      "Best F1 Score at epoch 2: 0.5419, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7112, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7494, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7790, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.7877, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8185, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 58: 0.8190, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 70: 0.8197, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 82: 0.8201, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 95: 0.8208, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2978, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5099, Validation Loss: 2.2572, Validation F1: 0.2978\n",
      "Best F1 Score at epoch 2: 0.6201, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7150, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7309, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7505, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7627, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.7790, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.7794, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.7897, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.7980, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8091, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 32: 0.8099, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 33: 0.8159, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 34: 0.8224, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 50: 0.8276, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.001, hidden_dim 128: 0.8254\n",
      "Testing with learning rate: 0.001, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2170, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.6311, Validation Loss: 2.3631, Validation F1: 0.2170\n",
      "Best F1 Score at epoch 1: 0.5264, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.6027, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7565, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7690, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.7703, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8022, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 29: 0.8156, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 48: 0.8196, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 57: 0.8256, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 78: 0.8298, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2498, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4488, Validation Loss: 2.2780, Validation F1: 0.2498\n",
      "Best F1 Score at epoch 1: 0.6304, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.6702, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7098, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7619, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7621, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7748, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.7781, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.7923, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8017, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8087, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8094, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8185, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 30: 0.8206, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 44: 0.8207, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 70: 0.8209, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2689, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5363, Validation Loss: 2.3501, Validation F1: 0.2689\n",
      "Best F1 Score at epoch 1: 0.6138, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.6396, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.6956, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7395, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7426, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7636, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.7685, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.7712, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.7755, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.7817, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.7878, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 30: 0.7899, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.8014, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 33: 0.8024, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 34: 0.8164, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8200, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 45: 0.8230, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.001, hidden_dim 128: 0.8246\n",
      "Testing with learning rate: 0.001, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2049, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5581, Validation Loss: 2.3723, Validation F1: 0.2049\n",
      "Best F1 Score at epoch 1: 0.4416, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.6905, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7229, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7824, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.7918, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.7984, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.8040, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8087, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 37: 0.8090, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 41: 0.8130, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 44: 0.8154, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 53: 0.8173, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 83: 0.8200, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3409, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5268, Validation Loss: 2.3330, Validation F1: 0.3409\n",
      "Best F1 Score at epoch 1: 0.5194, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.6680, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7019, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7527, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7562, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.7713, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7765, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.7804, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.7819, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.7847, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.7877, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.7977, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 35: 0.8014, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 36: 0.8100, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 37: 0.8152, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 38: 0.8165, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 63: 0.8165, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 65: 0.8251, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 82: 0.8255, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1213, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.6030, Validation Loss: 2.4387, Validation F1: 0.1213\n",
      "Best F1 Score at epoch 1: 0.3397, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.3854, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.6470, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.6722, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7411, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7606, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.7607, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.7611, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7834, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.7881, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.7939, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8126, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 32: 0.8156, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 46: 0.8193, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 83: 0.8198, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.001, hidden_dim 128: 0.8218\n",
      "Testing with learning rate: 0.001, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5157, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5289, Validation Loss: 2.3054, Validation F1: 0.5157\n",
      "Best F1 Score at epoch 1: 0.7436, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7520, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7779, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8011, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8018, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8077, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8104, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8167, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8171, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 36: 0.8182, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 37: 0.8215, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8234, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 39: 0.8237, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 44: 0.8258, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 45: 0.8302, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3173, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5965, Validation Loss: 2.3433, Validation F1: 0.3173\n",
      "Best F1 Score at epoch 1: 0.7055, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7530, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7586, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7738, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.7833, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.7942, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8130, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8199, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 42: 0.8249, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 43: 0.8273, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2166, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4918, Validation Loss: 2.4561, Validation F1: 0.2166\n",
      "Best F1 Score at epoch 1: 0.6988, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7409, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7420, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7582, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.7645, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.7695, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.7703, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.7824, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8010, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8016, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8073, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8124, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8133, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8188, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 40: 0.8257, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 42: 0.8264, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.001, hidden_dim 256: 0.8280\n",
      "Testing with learning rate: 0.001, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1471, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.6141, Validation Loss: 2.3207, Validation F1: 0.1471\n",
      "Best F1 Score at epoch 1: 0.6733, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7225, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7519, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7617, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7739, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7783, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.7867, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8026, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8063, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8096, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8126, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 33: 0.8139, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 34: 0.8232, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 97: 0.8235, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4809, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4772, Validation Loss: 2.3194, Validation F1: 0.4809\n",
      "Best F1 Score at epoch 1: 0.7449, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7581, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7733, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7786, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.7840, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8088, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8157, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8194, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 33: 0.8247, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 41: 0.8269, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3243, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5949, Validation Loss: 2.2900, Validation F1: 0.3243\n",
      "Best F1 Score at epoch 1: 0.4725, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7287, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7622, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.7729, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.7864, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.7884, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.7895, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8042, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8056, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8069, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8117, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8184, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 33: 0.8191, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 35: 0.8194, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 44: 0.8254, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 71: 0.8298, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.001, hidden_dim 256: 0.8267\n",
      "Testing with learning rate: 0.001, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0160, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4593, Validation Loss: 2.3961, Validation F1: 0.0160\n",
      "Best F1 Score at epoch 1: 0.3968, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.6746, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7221, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7535, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7554, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7666, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.7709, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.7763, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.7791, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.7883, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8032, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8184, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 49: 0.8199, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 51: 0.8208, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 52: 0.8276, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 54: 0.8287, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1628, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.6194, Validation Loss: 2.2804, Validation F1: 0.1628\n",
      "Best F1 Score at epoch 1: 0.4979, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7090, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7424, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7548, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.7606, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.7635, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.7702, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.7849, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8189, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8225, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 54: 0.8232, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 55: 0.8289, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6652, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4687, Validation Loss: 2.2954, Validation F1: 0.6652\n",
      "Best F1 Score at epoch 1: 0.7462, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7530, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7552, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7561, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.7723, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.7736, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.7939, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8176, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.8204, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.8263, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 95: 0.8279, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.001, hidden_dim 256: 0.8285\n",
      "Testing with learning rate: 0.001, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3495, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4066, Validation Loss: 2.5112, Validation F1: 0.3495\n",
      "Best F1 Score at epoch 1: 0.5050, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7712, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.7787, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7828, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.7916, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8228, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8263, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.7215, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.3896, Validation Loss: 2.5147, Validation F1: 0.7215\n",
      "Best F1 Score at epoch 2: 0.7372, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7412, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7519, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7699, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.7855, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.7981, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8058, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8068, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8087, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8128, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8156, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 33: 0.8180, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 36: 0.8185, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 42: 0.8200, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 43: 0.8208, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 50: 0.8216, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0153, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5145, Validation Loss: 2.7480, Validation F1: 0.0153\n",
      "Best F1 Score at epoch 1: 0.0153, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7369, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7558, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7567, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7666, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.7825, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.7846, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8044, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8046, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8070, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8102, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8197, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8223, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8247, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.001, hidden_dim 512: 0.8242\n",
      "Testing with learning rate: 0.001, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3735, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4724, Validation Loss: 2.6490, Validation F1: 0.3735\n",
      "Best F1 Score at epoch 1: 0.5856, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7320, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7548, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7882, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.7998, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8013, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8139, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8141, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8172, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 34: 0.8241, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.7625, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.3857, Validation Loss: 2.8044, Validation F1: 0.7625\n",
      "Best F1 Score at epoch 6: 0.7697, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7739, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8037, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8249, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8260, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8310, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1216, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5027, Validation Loss: 2.7375, Validation F1: 0.1216\n",
      "Best F1 Score at epoch 1: 0.7292, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7592, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.7600, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.7849, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8201, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8286, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.001, hidden_dim 512: 0.8279\n",
      "Testing with learning rate: 0.001, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3585, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4461, Validation Loss: 2.6251, Validation F1: 0.3585\n",
      "Best F1 Score at epoch 1: 0.7474, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7582, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.7698, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.7854, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.7973, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8022, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8090, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8169, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 38: 0.8183, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 40: 0.8194, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 44: 0.8199, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 48: 0.8214, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 50: 0.8246, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 68: 0.8250, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6260, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4400, Validation Loss: 2.5105, Validation F1: 0.6260\n",
      "Best F1 Score at epoch 1: 0.7567, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7687, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.7761, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8057, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8085, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8098, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8110, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8141, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.8166, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8196, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 42: 0.8216, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 76: 0.8299, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.7024, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4814, Validation Loss: 2.8914, Validation F1: 0.7024\n",
      "Best F1 Score at epoch 4: 0.7410, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7496, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7844, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.7895, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.7961, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8102, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8196, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 41: 0.8207, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 43: 0.8209, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 79: 0.8244, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.001, hidden_dim 512: 0.8264\n",
      "Best Parameters: {'learning_rate': 0.001, 'hidden_dim': 256, 'drop_out': 0.4}, Best F1 Score: 0.8285\n",
      "All results: {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}}\n",
      "Testing with learning rate: 0.005, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3792, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.8671, Validation Loss: 2.7494, Validation F1: 0.3792\n",
      "Best F1 Score at epoch 2: 0.4115, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7351, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7499, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7683, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.7724, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.7960, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8120, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8195, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 37: 0.8212, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8226, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 40: 0.8239, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 47: 0.8315, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6543, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4558, Validation Loss: 2.6011, Validation F1: 0.6543\n",
      "Best F1 Score at epoch 1: 0.7674, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7713, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7801, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.7905, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.7968, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8023, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8071, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8143, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8192, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 31: 0.8198, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 90: 0.8249, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2729, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5780, Validation Loss: 2.8781, Validation F1: 0.2729\n",
      "Best F1 Score at epoch 1: 0.7031, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7399, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7418, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7580, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7632, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7653, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.7707, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8163, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8190, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8194, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8215, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8221, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 39: 0.8239, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 41: 0.8239, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 45: 0.8256, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 57: 0.8258, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 63: 0.8288, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 68: 0.8339, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.005, hidden_dim 128: 0.8301\n",
      "Testing with learning rate: 0.005, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1752, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5356, Validation Loss: 2.8146, Validation F1: 0.1752\n",
      "Best F1 Score at epoch 1: 0.6712, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7292, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7458, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7577, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7626, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7850, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8075, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8120, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8141, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8226, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.8310, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0037, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.6377, Validation Loss: 3.6148, Validation F1: 0.0037\n",
      "Best F1 Score at epoch 1: 0.4190, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.6797, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7481, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7819, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7834, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.7906, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8036, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8049, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8054, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8139, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8144, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 33: 0.8157, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 35: 0.8160, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8174, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 63: 0.8224, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 64: 0.8308, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6276, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.7102, Validation Loss: 2.6608, Validation F1: 0.6276\n",
      "Best F1 Score at epoch 3: 0.7574, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7704, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.7903, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.7987, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8006, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8069, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8108, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8175, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8204, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 70: 0.8239, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.005, hidden_dim 128: 0.8286\n",
      "Testing with learning rate: 0.005, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2613, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5457, Validation Loss: 2.9580, Validation F1: 0.2613\n",
      "Best F1 Score at epoch 1: 0.3806, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7781, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.7810, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7885, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.7938, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8011, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8031, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8136, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8184, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 40: 0.8189, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 41: 0.8199, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 42: 0.8262, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6431, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5780, Validation Loss: 2.8589, Validation F1: 0.6431\n",
      "Best F1 Score at epoch 3: 0.6925, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7573, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7703, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7795, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.7956, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.7970, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.7981, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8000, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.8181, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 32: 0.8230, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 66: 0.8235, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.7063, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.6325, Validation Loss: 2.7937, Validation F1: 0.7063\n",
      "Best F1 Score at epoch 3: 0.7489, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7545, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7663, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7858, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.7942, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.7967, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8128, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8156, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.8193, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 63: 0.8215, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 64: 0.8252, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 65: 0.8271, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.005, hidden_dim 128: 0.8256\n",
      "Testing with learning rate: 0.005, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0472, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4412, Validation Loss: 4.4519, Validation F1: 0.0472\n",
      "Best F1 Score at epoch 1: 0.5165, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7223, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7650, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8144, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8187, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8194, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8208, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 27: 0.8252, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 37: 0.8261, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 41: 0.8270, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5411, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4600, Validation Loss: 3.2605, Validation F1: 0.5411\n",
      "Best F1 Score at epoch 2: 0.7140, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7420, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7879, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8103, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8155, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8179, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8181, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8184, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8193, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 43: 0.8230, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 59: 0.8246, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 65: 0.8249, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 73: 0.8289, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0104, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4401, Validation Loss: 5.0604, Validation F1: 0.0104\n",
      "Best F1 Score at epoch 1: 0.7140, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7471, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7511, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7595, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.7623, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.7671, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7738, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8248, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 39: 0.8263, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.005, hidden_dim 256: 0.8274\n",
      "Testing with learning rate: 0.005, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0032, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5511, Validation Loss: 3.5989, Validation F1: 0.0032\n",
      "Best F1 Score at epoch 1: 0.7537, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7553, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7772, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7789, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.7946, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.7976, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8105, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8151, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 29: 0.8156, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 30: 0.8188, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8220, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 37: 0.8239, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 42: 0.8249, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3021, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4913, Validation Loss: 4.1908, Validation F1: 0.3021\n",
      "Best F1 Score at epoch 1: 0.6715, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7028, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7280, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7482, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7695, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.7761, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.7855, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8197, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 40: 0.8210, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 41: 0.8223, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 47: 0.8249, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4376, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4489, Validation Loss: 4.0587, Validation F1: 0.4376\n",
      "Best F1 Score at epoch 1: 0.6063, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7192, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7374, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7465, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7603, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.7634, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.7639, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.7711, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.7761, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.7863, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8064, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8106, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8132, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8165, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 30: 0.8224, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 50: 0.8283, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 94: 0.8288, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.005, hidden_dim 256: 0.8262\n",
      "Testing with learning rate: 0.005, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0000, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4994, Validation Loss: 5.3246, Validation F1: 0.0000\n",
      "Best F1 Score at epoch 1: 0.6685, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.6873, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7412, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7656, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7722, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.7775, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.7794, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.7827, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.7920, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.7976, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8094, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8136, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8144, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.8148, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8168, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 32: 0.8180, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.8192, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 56: 0.8209, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 81: 0.8225, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0101, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4347, Validation Loss: 3.0060, Validation F1: 0.0101\n",
      "Best F1 Score at epoch 1: 0.7220, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7473, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7667, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.7750, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.7771, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.7830, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.7841, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8090, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8157, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8174, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8302, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0083, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.3976, Validation Loss: 5.1279, Validation F1: 0.0083\n",
      "Best F1 Score at epoch 1: 0.5979, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7450, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7664, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7685, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7767, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8053, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8088, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8097, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.8167, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8227, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 70: 0.8231, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 72: 0.8232, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 74: 0.8237, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.005, hidden_dim 256: 0.8255\n",
      "Testing with learning rate: 0.005, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0834, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4480, Validation Loss: 13.9314, Validation F1: 0.0834\n",
      "Best F1 Score at epoch 1: 0.4712, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7223, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.7386, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.7536, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7566, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.7811, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.7901, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8019, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8064, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8143, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8147, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8227, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 32: 0.8291, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2536, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.3854, Validation Loss: 9.5399, Validation F1: 0.2536\n",
      "Best F1 Score at epoch 1: 0.6999, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7354, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7647, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.7677, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.7732, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.7831, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.7857, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.7902, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8164, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8192, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 54: 0.8210, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 92: 0.8217, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 97: 0.8233, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3597, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4504, Validation Loss: 12.3736, Validation F1: 0.3597\n",
      "Best F1 Score at epoch 3: 0.7030, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7139, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.7203, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.7434, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.7546, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.7781, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.7836, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.7903, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8199, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 42: 0.8232, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 51: 0.8249, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 71: 0.8254, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.005, hidden_dim 512: 0.8259\n",
      "Testing with learning rate: 0.005, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2773, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4242, Validation Loss: 9.1504, Validation F1: 0.2773\n",
      "Best F1 Score at epoch 1: 0.7234, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7485, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.7678, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.7752, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.7836, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8017, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8071, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8075, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 30: 0.8120, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.8175, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8181, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 47: 0.8182, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 48: 0.8194, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 49: 0.8208, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 60: 0.8229, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 74: 0.8240, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 75: 0.8241, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0667, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5191, Validation Loss: 9.2591, Validation F1: 0.0667\n",
      "Best F1 Score at epoch 1: 0.3572, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7178, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7577, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.7597, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.7620, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.7895, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.7907, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.7978, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8037, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8073, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8075, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8078, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8158, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 35: 0.8160, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8163, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 38: 0.8175, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 39: 0.8186, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 52: 0.8188, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 53: 0.8193, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 65: 0.8207, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0022, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4632, Validation Loss: 13.9866, Validation F1: 0.0022\n",
      "Best F1 Score at epoch 1: 0.3839, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7100, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7444, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7728, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.7844, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.7916, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.7948, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8038, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8071, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8095, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8173, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8233, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.005, hidden_dim 512: 0.8227\n",
      "Testing with learning rate: 0.005, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4493, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4664, Validation Loss: 10.2757, Validation F1: 0.4493\n",
      "Best F1 Score at epoch 3: 0.7340, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7584, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.7728, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8030, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8091, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8302, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0037, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4253, Validation Loss: 12.0078, Validation F1: 0.0037\n",
      "Best F1 Score at epoch 1: 0.3814, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7095, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7301, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7404, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7934, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8077, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.8129, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.8277, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8278, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1667, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4912, Validation Loss: 9.6144, Validation F1: 0.1667\n",
      "Best F1 Score at epoch 1: 0.4674, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7576, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7779, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.7850, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.7954, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8025, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8072, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8106, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.8130, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8140, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 35: 0.8142, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 37: 0.8143, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 42: 0.8159, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 47: 0.8161, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}, (0.4, 0.005, 512): {'folds': [0.8301751423758913, 0.8278422609808436, 0.8160682860813335], 'avg_f1': 0.8246952298126895}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.005, hidden_dim 512: 0.8247\n",
      "Best Parameters: {'learning_rate': 0.005, 'hidden_dim': 128, 'drop_out': 0.2}, Best F1 Score: 0.8301\n",
      "All results: {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}, (0.4, 0.005, 512): {'folds': [0.8301751423758913, 0.8278422609808436, 0.8160682860813335], 'avg_f1': 0.8246952298126895}}\n",
      "Testing with learning rate: 0.01, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1381, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5588, Validation Loss: 3.2634, Validation F1: 0.1381\n",
      "Best F1 Score at epoch 1: 0.6604, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7438, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7474, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7499, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7648, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7662, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.7782, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.7901, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.7974, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8199, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8220, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8222, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8272, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8288, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3377, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5215, Validation Loss: 4.0108, Validation F1: 0.3377\n",
      "Best F1 Score at epoch 3: 0.7657, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.7666, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.7905, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.7933, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8108, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8237, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 83: 0.8250, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3168, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5284, Validation Loss: 3.2249, Validation F1: 0.3168\n",
      "Best F1 Score at epoch 1: 0.4832, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7291, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7671, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.7759, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.7819, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.7925, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8001, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8032, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8052, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8120, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8128, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 27: 0.8147, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8157, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 82: 0.8161, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}, (0.4, 0.005, 512): {'folds': [0.8301751423758913, 0.8278422609808436, 0.8160682860813335], 'avg_f1': 0.8246952298126895}, (0.2, 0.01, 128): {'folds': [0.8288387067860324, 0.8250489368359705, 0.816124116443652], 'avg_f1': 0.8233372533552182}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.01, hidden_dim 128: 0.8233\n",
      "Testing with learning rate: 0.01, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0951, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5946, Validation Loss: 4.1106, Validation F1: 0.0951\n",
      "Best F1 Score at epoch 1: 0.7140, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7317, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7347, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7727, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.7857, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.7922, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8050, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8186, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8194, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 37: 0.8202, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 42: 0.8203, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8230, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0038, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.6059, Validation Loss: 4.1691, Validation F1: 0.0038\n",
      "Best F1 Score at epoch 1: 0.0121, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.6223, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7797, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8061, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8094, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8151, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8185, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8199, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8201, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 34: 0.8210, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0011, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5276, Validation Loss: 4.7543, Validation F1: 0.0011\n",
      "Best F1 Score at epoch 1: 0.3744, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.5385, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7506, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7541, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7721, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.7769, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.7886, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.7962, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8079, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8086, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8107, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8219, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 53: 0.8227, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 60: 0.8229, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 61: 0.8246, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 65: 0.8249, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 77: 0.8263, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 91: 0.8283, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}, (0.4, 0.005, 512): {'folds': [0.8301751423758913, 0.8278422609808436, 0.8160682860813335], 'avg_f1': 0.8246952298126895}, (0.2, 0.01, 128): {'folds': [0.8288387067860324, 0.8250489368359705, 0.816124116443652], 'avg_f1': 0.8233372533552182}, (0.3, 0.01, 128): {'folds': [0.8230149069596757, 0.821025006795235, 0.828344483251847], 'avg_f1': 0.8241281323355859}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.01, hidden_dim 128: 0.8241\n",
      "Testing with learning rate: 0.01, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3546, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.6908, Validation Loss: 5.0944, Validation F1: 0.3546\n",
      "Best F1 Score at epoch 1: 0.7204, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7577, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7960, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.7990, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8000, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8055, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8061, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8150, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.8176, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8181, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8187, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8201, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8212, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 41: 0.8212, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 52: 0.8219, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2637, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.8653, Validation Loss: 4.5537, Validation F1: 0.2637\n",
      "Best F1 Score at epoch 2: 0.4080, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.6605, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7251, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7486, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7546, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.7676, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.7722, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.7728, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8120, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8145, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8151, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8159, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8178, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 45: 0.8179, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 50: 0.8202, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 57: 0.8208, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 62: 0.8268, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1780, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5555, Validation Loss: 3.4039, Validation F1: 0.1780\n",
      "Best F1 Score at epoch 1: 0.6981, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7610, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7845, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.7909, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8091, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8131, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8145, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8268, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}, (0.4, 0.005, 512): {'folds': [0.8301751423758913, 0.8278422609808436, 0.8160682860813335], 'avg_f1': 0.8246952298126895}, (0.2, 0.01, 128): {'folds': [0.8288387067860324, 0.8250489368359705, 0.816124116443652], 'avg_f1': 0.8233372533552182}, (0.3, 0.01, 128): {'folds': [0.8230149069596757, 0.821025006795235, 0.828344483251847], 'avg_f1': 0.8241281323355859}, (0.4, 0.01, 128): {'folds': [0.8219298987506557, 0.8267504134492719, 0.8268396403465241], 'avg_f1': 0.825173317515484}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.01, hidden_dim 128: 0.8252\n",
      "Testing with learning rate: 0.01, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0043, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5485, Validation Loss: 12.5904, Validation F1: 0.0043\n",
      "Best F1 Score at epoch 1: 0.6847, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.7615, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.7670, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.7821, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.7862, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.7974, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8087, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8098, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 27: 0.8143, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8213, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8281, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 31: 0.8286, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0000, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5653, Validation Loss: 14.6930, Validation F1: 0.0000\n",
      "Best F1 Score at epoch 1: 0.2372, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.3549, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.5596, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7265, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.7528, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7578, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.7608, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.7715, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.7734, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.7886, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8183, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 37: 0.8191, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8278, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0030, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4475, Validation Loss: 11.2656, Validation F1: 0.0030\n",
      "Best F1 Score at epoch 1: 0.7468, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7531, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7583, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7778, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.7945, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8044, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8107, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8173, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8209, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8275, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}, (0.4, 0.005, 512): {'folds': [0.8301751423758913, 0.8278422609808436, 0.8160682860813335], 'avg_f1': 0.8246952298126895}, (0.2, 0.01, 128): {'folds': [0.8288387067860324, 0.8250489368359705, 0.816124116443652], 'avg_f1': 0.8233372533552182}, (0.3, 0.01, 128): {'folds': [0.8230149069596757, 0.821025006795235, 0.828344483251847], 'avg_f1': 0.8241281323355859}, (0.4, 0.01, 128): {'folds': [0.8219298987506557, 0.8267504134492719, 0.8268396403465241], 'avg_f1': 0.825173317515484}, (0.2, 0.01, 256): {'folds': [0.8286345419228869, 0.8278172088206012, 0.8275465664705015], 'avg_f1': 0.8279994390713298}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.01, hidden_dim 256: 0.8280\n",
      "Testing with learning rate: 0.01, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1970, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5210, Validation Loss: 9.2585, Validation F1: 0.1970\n",
      "Best F1 Score at epoch 1: 0.6726, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7247, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7530, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7652, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.7894, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.7978, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8031, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8050, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8096, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8125, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 33: 0.8162, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 40: 0.8181, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 45: 0.8199, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0009, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4291, Validation Loss: 11.1661, Validation F1: 0.0009\n",
      "Best F1 Score at epoch 1: 0.6855, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7031, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7700, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.7800, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.7934, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8089, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8120, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8140, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8209, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8228, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 97: 0.8244, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0126, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5317, Validation Loss: 12.8702, Validation F1: 0.0126\n",
      "Best F1 Score at epoch 1: 0.3615, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.5368, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.5687, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.6879, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7632, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.7720, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.7729, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.7792, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8015, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8088, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8103, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8130, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 30: 0.8192, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 49: 0.8196, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 52: 0.8208, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}, (0.4, 0.005, 512): {'folds': [0.8301751423758913, 0.8278422609808436, 0.8160682860813335], 'avg_f1': 0.8246952298126895}, (0.2, 0.01, 128): {'folds': [0.8288387067860324, 0.8250489368359705, 0.816124116443652], 'avg_f1': 0.8233372533552182}, (0.3, 0.01, 128): {'folds': [0.8230149069596757, 0.821025006795235, 0.828344483251847], 'avg_f1': 0.8241281323355859}, (0.4, 0.01, 128): {'folds': [0.8219298987506557, 0.8267504134492719, 0.8268396403465241], 'avg_f1': 0.825173317515484}, (0.2, 0.01, 256): {'folds': [0.8286345419228869, 0.8278172088206012, 0.8275465664705015], 'avg_f1': 0.8279994390713298}, (0.3, 0.01, 256): {'folds': [0.8198941976686785, 0.8244118605806073, 0.8207566261419321], 'avg_f1': 0.8216875614637393}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.01, hidden_dim 256: 0.8217\n",
      "Testing with learning rate: 0.01, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2756, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4931, Validation Loss: 10.9442, Validation F1: 0.2756\n",
      "Best F1 Score at epoch 2: 0.7119, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7385, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.7426, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.7751, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.7781, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.7819, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8016, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8070, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8087, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8095, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8141, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.8167, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 68: 0.8168, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3027, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5617, Validation Loss: 7.4114, Validation F1: 0.3027\n",
      "Best F1 Score at epoch 1: 0.3653, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.3924, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.6868, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.6964, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.7522, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7742, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.7757, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.8112, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.8160, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 56: 0.8180, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2146, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4261, Validation Loss: 10.9766, Validation F1: 0.2146\n",
      "Best F1 Score at epoch 1: 0.2369, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.3444, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.3514, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7281, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7537, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7646, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.7831, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.7854, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8081, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8096, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 32: 0.8105, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.8122, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 41: 0.8132, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 42: 0.8140, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 43: 0.8161, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 45: 0.8166, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 57: 0.8170, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}, (0.4, 0.005, 512): {'folds': [0.8301751423758913, 0.8278422609808436, 0.8160682860813335], 'avg_f1': 0.8246952298126895}, (0.2, 0.01, 128): {'folds': [0.8288387067860324, 0.8250489368359705, 0.816124116443652], 'avg_f1': 0.8233372533552182}, (0.3, 0.01, 128): {'folds': [0.8230149069596757, 0.821025006795235, 0.828344483251847], 'avg_f1': 0.8241281323355859}, (0.4, 0.01, 128): {'folds': [0.8219298987506557, 0.8267504134492719, 0.8268396403465241], 'avg_f1': 0.825173317515484}, (0.2, 0.01, 256): {'folds': [0.8286345419228869, 0.8278172088206012, 0.8275465664705015], 'avg_f1': 0.8279994390713298}, (0.3, 0.01, 256): {'folds': [0.8198941976686785, 0.8244118605806073, 0.8207566261419321], 'avg_f1': 0.8216875614637393}, (0.4, 0.01, 256): {'folds': [0.8168360397428803, 0.817984784842766, 0.8170414381941337], 'avg_f1': 0.8172874209265933}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.01, hidden_dim 256: 0.8173\n",
      "Testing with learning rate: 0.01, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2750, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.3386, Validation Loss: 34.8561, Validation F1: 0.2750\n",
      "Best F1 Score at epoch 6: 0.3548, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.5066, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.5890, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7490, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.7653, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.7731, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.7785, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 32: 0.7797, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 33: 0.7983, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8032, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 36: 0.8067, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 39: 0.8113, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 46: 0.8141, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 48: 0.8144, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 54: 0.8163, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 65: 0.8183, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 66: 0.8187, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 87: 0.8194, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3586, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.3789, Validation Loss: 34.4104, Validation F1: 0.3586\n",
      "Best F1 Score at epoch 8: 0.4084, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.4864, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.5239, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.5840, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.6949, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.7716, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.7811, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 69: 0.7837, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 72: 0.7917, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 73: 0.7971, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 74: 0.8086, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 75: 0.8135, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 77: 0.8153, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 86: 0.8180, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 89: 0.8204, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0007, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.3533, Validation Loss: 36.0374, Validation F1: 0.0007\n",
      "Best F1 Score at epoch 1: 0.2367, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.3506, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.4957, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.7111, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.7187, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.7320, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.7674, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 60: 0.7887, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 66: 0.8036, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 76: 0.8067, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 77: 0.8087, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 80: 0.8137, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 87: 0.8149, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}, (0.4, 0.005, 512): {'folds': [0.8301751423758913, 0.8278422609808436, 0.8160682860813335], 'avg_f1': 0.8246952298126895}, (0.2, 0.01, 128): {'folds': [0.8288387067860324, 0.8250489368359705, 0.816124116443652], 'avg_f1': 0.8233372533552182}, (0.3, 0.01, 128): {'folds': [0.8230149069596757, 0.821025006795235, 0.828344483251847], 'avg_f1': 0.8241281323355859}, (0.4, 0.01, 128): {'folds': [0.8219298987506557, 0.8267504134492719, 0.8268396403465241], 'avg_f1': 0.825173317515484}, (0.2, 0.01, 256): {'folds': [0.8286345419228869, 0.8278172088206012, 0.8275465664705015], 'avg_f1': 0.8279994390713298}, (0.3, 0.01, 256): {'folds': [0.8198941976686785, 0.8244118605806073, 0.8207566261419321], 'avg_f1': 0.8216875614637393}, (0.4, 0.01, 256): {'folds': [0.8168360397428803, 0.817984784842766, 0.8170414381941337], 'avg_f1': 0.8172874209265933}, (0.2, 0.01, 512): {'folds': [0.8194188148785398, 0.8204061619548898, 0.8148613190364387], 'avg_f1': 0.8182287652899561}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.01, hidden_dim 512: 0.8182\n",
      "Testing with learning rate: 0.01, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3411, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4875, Validation Loss: 31.7456, Validation F1: 0.3411\n",
      "Best F1 Score at epoch 2: 0.6374, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.6558, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.7339, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.7520, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.7647, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.7764, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.7854, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 33: 0.7882, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 34: 0.7962, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 35: 0.8070, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8075, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 37: 0.8091, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 38: 0.8170, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3860, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4567, Validation Loss: 18.1227, Validation F1: 0.3860\n",
      "Best F1 Score at epoch 5: 0.4788, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.5794, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.7403, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.7492, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.7558, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.7588, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.7654, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 33: 0.7911, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 37: 0.8036, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 41: 0.8053, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 42: 0.8095, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8176, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 44: 0.8208, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 45: 0.8219, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 57: 0.8226, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0989, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4038, Validation Loss: 36.3379, Validation F1: 0.0989\n",
      "Best F1 Score at epoch 1: 0.2369, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.5704, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7074, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.7426, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.7490, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.7689, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.7951, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8054, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8083, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 40: 0.8086, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 41: 0.8112, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 45: 0.8137, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 60: 0.8180, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 62: 0.8187, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 92: 0.8188, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}, (0.4, 0.005, 512): {'folds': [0.8301751423758913, 0.8278422609808436, 0.8160682860813335], 'avg_f1': 0.8246952298126895}, (0.2, 0.01, 128): {'folds': [0.8288387067860324, 0.8250489368359705, 0.816124116443652], 'avg_f1': 0.8233372533552182}, (0.3, 0.01, 128): {'folds': [0.8230149069596757, 0.821025006795235, 0.828344483251847], 'avg_f1': 0.8241281323355859}, (0.4, 0.01, 128): {'folds': [0.8219298987506557, 0.8267504134492719, 0.8268396403465241], 'avg_f1': 0.825173317515484}, (0.2, 0.01, 256): {'folds': [0.8286345419228869, 0.8278172088206012, 0.8275465664705015], 'avg_f1': 0.8279994390713298}, (0.3, 0.01, 256): {'folds': [0.8198941976686785, 0.8244118605806073, 0.8207566261419321], 'avg_f1': 0.8216875614637393}, (0.4, 0.01, 256): {'folds': [0.8168360397428803, 0.817984784842766, 0.8170414381941337], 'avg_f1': 0.8172874209265933}, (0.2, 0.01, 512): {'folds': [0.8194188148785398, 0.8204061619548898, 0.8148613190364387], 'avg_f1': 0.8182287652899561}, (0.3, 0.01, 512): {'folds': [0.8170210126905887, 0.8226208920385498, 0.8187933885291951], 'avg_f1': 0.8194784310861113}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.01, hidden_dim 512: 0.8195\n",
      "Testing with learning rate: 0.01, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3704, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4593, Validation Loss: 27.7269, Validation F1: 0.3704\n",
      "Best F1 Score at epoch 1: 0.3751, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.5956, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.6472, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.6946, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7197, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.7469, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.7607, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.7613, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 37: 0.7678, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 38: 0.7871, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 43: 0.7924, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 44: 0.7972, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 45: 0.8040, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 46: 0.8073, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 47: 0.8105, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 48: 0.8114, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 61: 0.8118, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 64: 0.8123, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 65: 0.8125, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 67: 0.8125, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 71: 0.8139, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 72: 0.8151, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 80: 0.8164, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 81: 0.8168, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 92: 0.8170, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 93: 0.8175, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3208, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4998, Validation Loss: 31.2543, Validation F1: 0.3208\n",
      "Best F1 Score at epoch 4: 0.3582, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.6484, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7313, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.7422, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.7657, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.7690, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.7719, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.7952, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8002, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 32: 0.8095, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 40: 0.8105, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 41: 0.8201, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0247, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4111, Validation Loss: 30.9882, Validation F1: 0.0247\n",
      "Best F1 Score at epoch 1: 0.2366, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.4993, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7654, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.7816, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.7842, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 61: 0.7968, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 64: 0.7995, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 65: 0.8006, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 68: 0.8011, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 70: 0.8012, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 71: 0.8064, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 72: 0.8080, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 75: 0.8081, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 76: 0.8094, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 78: 0.8106, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 82: 0.8114, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 83: 0.8153, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}, (0.4, 0.005, 512): {'folds': [0.8301751423758913, 0.8278422609808436, 0.8160682860813335], 'avg_f1': 0.8246952298126895}, (0.2, 0.01, 128): {'folds': [0.8288387067860324, 0.8250489368359705, 0.816124116443652], 'avg_f1': 0.8233372533552182}, (0.3, 0.01, 128): {'folds': [0.8230149069596757, 0.821025006795235, 0.828344483251847], 'avg_f1': 0.8241281323355859}, (0.4, 0.01, 128): {'folds': [0.8219298987506557, 0.8267504134492719, 0.8268396403465241], 'avg_f1': 0.825173317515484}, (0.2, 0.01, 256): {'folds': [0.8286345419228869, 0.8278172088206012, 0.8275465664705015], 'avg_f1': 0.8279994390713298}, (0.3, 0.01, 256): {'folds': [0.8198941976686785, 0.8244118605806073, 0.8207566261419321], 'avg_f1': 0.8216875614637393}, (0.4, 0.01, 256): {'folds': [0.8168360397428803, 0.817984784842766, 0.8170414381941337], 'avg_f1': 0.8172874209265933}, (0.2, 0.01, 512): {'folds': [0.8194188148785398, 0.8204061619548898, 0.8148613190364387], 'avg_f1': 0.8182287652899561}, (0.3, 0.01, 512): {'folds': [0.8170210126905887, 0.8226208920385498, 0.8187933885291951], 'avg_f1': 0.8194784310861113}, (0.4, 0.01, 512): {'folds': [0.8175212401064007, 0.8200572871681646, 0.8153350078724184], 'avg_f1': 0.8176378450489946}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.01, hidden_dim 512: 0.8176\n",
      "Best Parameters: {'learning_rate': 0.005, 'hidden_dim': 128, 'drop_out': 0.2}, Best F1 Score: 0.8301\n",
      "All results: {(0.2, 0.001, 128): {'folds': [0.8278231600731365, 0.8207954702660658, 0.8276054286089558], 'avg_f1': 0.825408019649386}, (0.3, 0.001, 128): {'folds': [0.8298415363093985, 0.8209407115993557, 0.8229932041854587], 'avg_f1': 0.8245918173647376}, (0.4, 0.001, 128): {'folds': [0.8199893257744554, 0.8254510693533139, 0.8198438758891561], 'avg_f1': 0.8217614236723084}, (0.2, 0.001, 256): {'folds': [0.8302494171806841, 0.8272762517723906, 0.8264323986016109], 'avg_f1': 0.8279860225182286}, (0.3, 0.001, 256): {'folds': [0.8234638031040713, 0.8269140290601108, 0.8297637246274263], 'avg_f1': 0.8267138522638695}, (0.4, 0.001, 256): {'folds': [0.8286730704038723, 0.8288694664498674, 0.8278600811589193], 'avg_f1': 0.828467539337553}, (0.2, 0.001, 512): {'folds': [0.8262931647514515, 0.8215532152407363, 0.8246993887714634], 'avg_f1': 0.8241819229212171}, (0.3, 0.001, 512): {'folds': [0.8240888103351526, 0.831014075020828, 0.828558540871068], 'avg_f1': 0.8278871420756828}, (0.4, 0.001, 512): {'folds': [0.8249897064784192, 0.8299330180166163, 0.8243727993041489], 'avg_f1': 0.8264318412663948}, (0.2, 0.005, 128): {'folds': [0.831477578601971, 0.8248606441754652, 0.8338868742117825], 'avg_f1': 0.8300750323297397}, (0.3, 0.005, 128): {'folds': [0.8310141129903471, 0.830785571819617, 0.8239109312563426], 'avg_f1': 0.8285702053554355}, (0.4, 0.005, 128): {'folds': [0.8262074431766142, 0.8235454233864331, 0.8271347367011012], 'avg_f1': 0.8256292010880495}, (0.2, 0.005, 256): {'folds': [0.8270415415182919, 0.8289381902311743, 0.8262715815801679], 'avg_f1': 0.8274171044432114}, (0.3, 0.005, 256): {'folds': [0.8249251943881555, 0.8249168512216949, 0.8287790861085552], 'avg_f1': 0.8262070439061352}, (0.4, 0.005, 256): {'folds': [0.8224584265782592, 0.8301888132812373, 0.8237333411223536], 'avg_f1': 0.8254601936606166}, (0.2, 0.005, 512): {'folds': [0.8290753837328227, 0.8233053778468382, 0.8253860567771926], 'avg_f1': 0.8259222727856178}, (0.3, 0.005, 512): {'folds': [0.8240592188997239, 0.8206717277564334, 0.823307802444758], 'avg_f1': 0.8226795830336385}, (0.4, 0.005, 512): {'folds': [0.8301751423758913, 0.8278422609808436, 0.8160682860813335], 'avg_f1': 0.8246952298126895}, (0.2, 0.01, 128): {'folds': [0.8288387067860324, 0.8250489368359705, 0.816124116443652], 'avg_f1': 0.8233372533552182}, (0.3, 0.01, 128): {'folds': [0.8230149069596757, 0.821025006795235, 0.828344483251847], 'avg_f1': 0.8241281323355859}, (0.4, 0.01, 128): {'folds': [0.8219298987506557, 0.8267504134492719, 0.8268396403465241], 'avg_f1': 0.825173317515484}, (0.2, 0.01, 256): {'folds': [0.8286345419228869, 0.8278172088206012, 0.8275465664705015], 'avg_f1': 0.8279994390713298}, (0.3, 0.01, 256): {'folds': [0.8198941976686785, 0.8244118605806073, 0.8207566261419321], 'avg_f1': 0.8216875614637393}, (0.4, 0.01, 256): {'folds': [0.8168360397428803, 0.817984784842766, 0.8170414381941337], 'avg_f1': 0.8172874209265933}, (0.2, 0.01, 512): {'folds': [0.8194188148785398, 0.8204061619548898, 0.8148613190364387], 'avg_f1': 0.8182287652899561}, (0.3, 0.01, 512): {'folds': [0.8170210126905887, 0.8226208920385498, 0.8187933885291951], 'avg_f1': 0.8194784310861113}, (0.4, 0.01, 512): {'folds': [0.8175212401064007, 0.8200572871681646, 0.8153350078724184], 'avg_f1': 0.8176378450489946}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def grid_search(data, epochs, learning_rates, hidden_dims, drop_outs):\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_params = {}\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Precompute the train and validation graphs for all folds\n",
    "    folds = []\n",
    "    for train_idx, val_idx in skf.split(data, data[label_col]):\n",
    "        train_df = data.iloc[train_idx]\n",
    "        val_df = data.iloc[val_idx]\n",
    "\n",
    "        G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "        G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "\n",
    "        G_pyg_train = G_pyg_train.to(device)\n",
    "        G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "        G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "        G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "        G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "        G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "        folds.append((G_pyg_train, G_pyg_val))\n",
    "\n",
    "    params_results = {}\n",
    "    for lr in learning_rates:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for drop_out in drop_outs:\n",
    "                print(f\"Testing with learning rate: {lr}, hidden_dim: {hidden_dim}, drop_out: {drop_out}\")\n",
    "                fold_f1_scores = []\n",
    "\n",
    "                for fold, (G_pyg_train, G_pyg_val) in enumerate(folds):\n",
    "                    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "                    model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                                    edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                                    hidden_channels=hidden_dim,\n",
    "                                    dropout=drop_out,\n",
    "                                    out_channels=num_classes).to(device)\n",
    "\n",
    "                    model.apply(init_weights)\n",
    "\n",
    "                    labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "                    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                                    classes=np.unique(labels),\n",
    "                                                                    y=labels)\n",
    "\n",
    "                    # Normalize to stabilize training\n",
    "                    class_weights = th.FloatTensor(class_weights).to(device)\n",
    "                    print(\"Class weights:\", class_weights)\n",
    "\n",
    "                    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "                    optimizer = th.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    best_epoch_f1 = 0  # Track the best F1 score for this fold\n",
    "\n",
    "                    for epoch in range(epochs):\n",
    "                        train_loss = 0\n",
    "                        val_loss = 0\n",
    "\n",
    "                        try:\n",
    "                            model.train()\n",
    "                            out = model(G_pyg_train)\n",
    "                            \n",
    "                            optimizer.zero_grad()\n",
    "                            loss = criterion(out, G_pyg_train.edge_label)\n",
    "                            train_loss = loss.item()\n",
    "\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                            model.eval()\n",
    "                            with th.no_grad():\n",
    "                                out = model(G_pyg_val)\n",
    "                                loss = criterion(out, G_pyg_val.edge_label)\n",
    "                                val_loss = loss.item()\n",
    "\n",
    "                            val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "\n",
    "                            if val_f1 > best_epoch_f1:\n",
    "                                best_epoch_f1 = val_f1  # Update the best F1 score for this fold\n",
    "                                print(f\"Best F1 Score at epoch {epoch}: {best_epoch_f1:.4f}, Parameters: lr={lr}, hidden_dim{hidden_dim}, drop_out={drop_out}\")\n",
    "\n",
    "                            if epoch % 100 == 0:\n",
    "                                print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred at epoch {epoch}: {str(e)}\")\n",
    "                            break\n",
    "\n",
    "                    fold_f1_scores.append(best_epoch_f1)  # Append the best F1 score for this fold\n",
    "                \n",
    "                avg_f1 = sum(fold_f1_scores) / len(fold_f1_scores)\n",
    "                params_results[(drop_out, lr, hidden_dim)] = {'folds': fold_f1_scores, 'avg_f1': avg_f1}\n",
    "                print(\"Current Results: \", params_results)\n",
    "                print(f\"Average F1 Score for dropout {drop_out}, learning rate {lr}, hidden_dim {hidden_dim}: {avg_f1:.4f}\")\n",
    "\n",
    "                if avg_f1 > best_f1:\n",
    "                    best_f1 = avg_f1\n",
    "                    best_params = {'learning_rate': lr, 'hidden_dim': hidden_dim, 'drop_out': drop_out}\n",
    "\n",
    "        print(f\"Best Parameters: {best_params}, Best F1 Score: {best_f1:.4f}\")\n",
    "        print(\"All results:\", params_results)\n",
    "\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "hidden_dims = [128, 256, 512]\n",
    "drop_outs = [0.2, 0.3, 0.4]\n",
    "\n",
    "grid_search(train_full_df, epochs=100, learning_rates=learning_rates, hidden_dims=hidden_dims, drop_outs=drop_outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f52b2fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 392432\n",
      "Number of node in G_pyg: 179\n",
      "Shape of node in G_pyg: torch.Size([179, 54])\n",
      "Shape of edge attr in G_pyg: torch.Size([392432, 54])\n",
      "Shape of edge label in G_pyg: torch.Size([392432])\n",
      "Number of edges in G_pyg: 69253\n",
      "Number of node in G_pyg: 173\n",
      "Shape of node in G_pyg: torch.Size([173, 54])\n",
      "Shape of edge attr in G_pyg: torch.Size([69253, 54])\n",
      "Shape of edge label in G_pyg: torch.Size([69253])\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "     train_full_df, test_size=0.15, random_state=42, stratify=train_full_df[label_col])\n",
    "\n",
    "G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([2.0291e+01, 2.3317e+01, 3.3215e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8832e+00, 3.5970e+01, 3.1145e+02], device='cuda:0')\n",
      "Epoch 0 Saved best model. Best F1: 0.38647278374906024\n",
      "Epoch 0, Train Loss: 2.3972, Validation Loss: 2.2887, Validation F1: 0.3865\n",
      "Epoch 1 Saved best model. Best F1: 0.6976993576265106\n",
      "Epoch 1, Train Loss: 2.2982, Validation Loss: 2.0968, Validation F1: 0.6977\n",
      "Epoch 2 Saved best model. Best F1: 0.7275851801291849\n",
      "Epoch 2, Train Loss: 2.0794, Validation Loss: 2.0138, Validation F1: 0.7276\n",
      "Epoch 3, Train Loss: 1.9582, Validation Loss: 1.9722, Validation F1: 0.7245\n",
      "Epoch 4 Saved best model. Best F1: 0.7429064071393331\n",
      "Epoch 4, Train Loss: 1.9154, Validation Loss: 1.9481, Validation F1: 0.7429\n",
      "Epoch 5 Saved best model. Best F1: 0.7550305313494655\n",
      "Epoch 5, Train Loss: 1.8967, Validation Loss: 1.8789, Validation F1: 0.7550\n",
      "Epoch 6, Train Loss: 1.8391, Validation Loss: 1.8607, Validation F1: 0.7542\n",
      "Epoch 7, Train Loss: 1.8110, Validation Loss: 1.8609, Validation F1: 0.7518\n",
      "Epoch 8 Saved best model. Best F1: 0.7684509571347911\n",
      "Epoch 8, Train Loss: 1.8111, Validation Loss: 1.8176, Validation F1: 0.7685\n",
      "Epoch 9, Train Loss: 1.7961, Validation Loss: 1.8094, Validation F1: 0.7557\n",
      "Epoch 10, Train Loss: 1.7784, Validation Loss: 1.8036, Validation F1: 0.7523\n",
      "Epoch 11 Saved best model. Best F1: 0.7704088632467526\n",
      "Epoch 11, Train Loss: 1.7704, Validation Loss: 1.7868, Validation F1: 0.7704\n",
      "Epoch 12, Train Loss: 1.7426, Validation Loss: 1.7815, Validation F1: 0.7534\n",
      "Epoch 13 Saved best model. Best F1: 0.7744983158975691\n",
      "Epoch 13, Train Loss: 1.7445, Validation Loss: 1.7666, Validation F1: 0.7745\n",
      "Epoch 14 Saved best model. Best F1: 0.789025548676092\n",
      "Epoch 14, Train Loss: 1.7224, Validation Loss: 1.7511, Validation F1: 0.7890\n",
      "Epoch 15 Saved best model. Best F1: 0.8024747155616724\n",
      "Epoch 15, Train Loss: 1.7103, Validation Loss: 1.7499, Validation F1: 0.8025\n",
      "Epoch 16, Train Loss: 1.7072, Validation Loss: 1.7478, Validation F1: 0.7939\n",
      "Epoch 17 Saved best model. Best F1: 0.8069254811771169\n",
      "Epoch 17, Train Loss: 1.7054, Validation Loss: 1.7506, Validation F1: 0.8069\n",
      "Epoch 18 Saved best model. Best F1: 0.8169107383234867\n",
      "Epoch 18, Train Loss: 1.6977, Validation Loss: 1.7496, Validation F1: 0.8169\n",
      "Epoch 19, Train Loss: 1.6829, Validation Loss: 1.7528, Validation F1: 0.8152\n",
      "Epoch 20, Train Loss: 1.6839, Validation Loss: 1.7551, Validation F1: 0.8115\n",
      "Epoch 21, Train Loss: 1.6747, Validation Loss: 1.7279, Validation F1: 0.8077\n",
      "Epoch 22, Train Loss: 1.6801, Validation Loss: 1.7329, Validation F1: 0.8071\n",
      "Epoch 23, Train Loss: 1.6758, Validation Loss: 1.7197, Validation F1: 0.7996\n",
      "Epoch 24, Train Loss: 1.6612, Validation Loss: 1.7149, Validation F1: 0.8047\n",
      "Epoch 25, Train Loss: 1.6632, Validation Loss: 1.7169, Validation F1: 0.8063\n",
      "Epoch 26, Train Loss: 1.6635, Validation Loss: 1.7201, Validation F1: 0.8158\n",
      "Epoch 27, Train Loss: 1.6553, Validation Loss: 1.7123, Validation F1: 0.8168\n",
      "Epoch 28, Train Loss: 1.6554, Validation Loss: 1.7054, Validation F1: 0.8077\n",
      "Epoch 29, Train Loss: 1.6608, Validation Loss: 1.7187, Validation F1: 0.8122\n",
      "Epoch 30, Train Loss: 1.6501, Validation Loss: 1.7130, Validation F1: 0.8028\n",
      "Epoch 31, Train Loss: 1.6535, Validation Loss: 1.7058, Validation F1: 0.8062\n",
      "Epoch 32, Train Loss: 1.6426, Validation Loss: 1.7033, Validation F1: 0.8047\n",
      "Epoch 33, Train Loss: 1.6398, Validation Loss: 1.7063, Validation F1: 0.8048\n",
      "Epoch 34, Train Loss: 1.6484, Validation Loss: 1.7120, Validation F1: 0.8051\n",
      "Epoch 35, Train Loss: 1.6434, Validation Loss: 1.7014, Validation F1: 0.8134\n",
      "Epoch 36, Train Loss: 1.6397, Validation Loss: 1.7064, Validation F1: 0.8070\n",
      "Epoch 37, Train Loss: 1.6397, Validation Loss: 1.7122, Validation F1: 0.8089\n",
      "Epoch 38, Train Loss: 1.6418, Validation Loss: 1.7047, Validation F1: 0.8100\n",
      "Epoch 39 Saved best model. Best F1: 0.820502346329121\n",
      "Epoch 39, Train Loss: 1.6401, Validation Loss: 1.7022, Validation F1: 0.8205\n",
      "Epoch 40 Saved best model. Best F1: 0.8232500866862094\n",
      "Epoch 40, Train Loss: 1.6382, Validation Loss: 1.7000, Validation F1: 0.8233\n",
      "Epoch 41, Train Loss: 1.6329, Validation Loss: 1.6959, Validation F1: 0.8171\n",
      "Epoch 42, Train Loss: 1.6394, Validation Loss: 1.6860, Validation F1: 0.8208\n",
      "Epoch 43, Train Loss: 1.6325, Validation Loss: 1.6923, Validation F1: 0.8194\n",
      "Epoch 44, Train Loss: 1.6333, Validation Loss: 1.6933, Validation F1: 0.8093\n",
      "Epoch 45, Train Loss: 1.6309, Validation Loss: 1.7022, Validation F1: 0.8131\n",
      "Epoch 46, Train Loss: 1.6299, Validation Loss: 1.6929, Validation F1: 0.8130\n",
      "Epoch 47, Train Loss: 1.6303, Validation Loss: 1.7020, Validation F1: 0.8109\n",
      "Epoch 48, Train Loss: 1.6232, Validation Loss: 1.6925, Validation F1: 0.8185\n",
      "Epoch 49, Train Loss: 1.6199, Validation Loss: 1.6823, Validation F1: 0.8152\n",
      "Epoch 50 Saved best model. Best F1: 0.8364410625097133\n",
      "Epoch 50, Train Loss: 1.6267, Validation Loss: 1.6924, Validation F1: 0.8364\n",
      "Epoch 51, Train Loss: 1.6176, Validation Loss: 1.6957, Validation F1: 0.8180\n",
      "Epoch 52, Train Loss: 1.6210, Validation Loss: 1.6814, Validation F1: 0.8057\n",
      "Epoch 53, Train Loss: 1.6209, Validation Loss: 1.6917, Validation F1: 0.8081\n",
      "Epoch 54, Train Loss: 1.6183, Validation Loss: 1.6791, Validation F1: 0.8191\n",
      "Epoch 55, Train Loss: 1.6185, Validation Loss: 1.6874, Validation F1: 0.8123\n",
      "Epoch 56, Train Loss: 1.6173, Validation Loss: 1.6868, Validation F1: 0.8149\n",
      "Epoch 57, Train Loss: 1.6181, Validation Loss: 1.6782, Validation F1: 0.8212\n",
      "Epoch 58, Train Loss: 1.6165, Validation Loss: 1.6880, Validation F1: 0.8259\n",
      "Epoch 59, Train Loss: 1.6173, Validation Loss: 1.6789, Validation F1: 0.8195\n",
      "Epoch 60, Train Loss: 1.6145, Validation Loss: 1.6821, Validation F1: 0.8141\n",
      "Epoch 61, Train Loss: 1.6097, Validation Loss: 1.6944, Validation F1: 0.8120\n",
      "Epoch 62, Train Loss: 1.6123, Validation Loss: 1.6857, Validation F1: 0.8103\n",
      "Epoch 63, Train Loss: 1.6120, Validation Loss: 1.6803, Validation F1: 0.8137\n",
      "Epoch 64, Train Loss: 1.6112, Validation Loss: 1.6808, Validation F1: 0.8167\n",
      "Epoch 65, Train Loss: 1.6074, Validation Loss: 1.6848, Validation F1: 0.8181\n",
      "Epoch 66, Train Loss: 1.6087, Validation Loss: 1.6757, Validation F1: 0.8179\n",
      "Epoch 67, Train Loss: 1.6074, Validation Loss: 1.6831, Validation F1: 0.8227\n",
      "Epoch 68, Train Loss: 1.6091, Validation Loss: 1.6760, Validation F1: 0.8114\n",
      "Epoch 69, Train Loss: 1.6082, Validation Loss: 1.6769, Validation F1: 0.8124\n",
      "Epoch 70, Train Loss: 1.6034, Validation Loss: 1.6810, Validation F1: 0.8137\n",
      "Epoch 71, Train Loss: 1.6030, Validation Loss: 1.6817, Validation F1: 0.8117\n",
      "Epoch 72, Train Loss: 1.6072, Validation Loss: 1.6819, Validation F1: 0.8108\n",
      "Epoch 73, Train Loss: 1.6044, Validation Loss: 1.6750, Validation F1: 0.8104\n",
      "Epoch 74, Train Loss: 1.6076, Validation Loss: 1.6731, Validation F1: 0.8159\n",
      "Epoch 75, Train Loss: 1.6073, Validation Loss: 1.6711, Validation F1: 0.8156\n",
      "Epoch 76, Train Loss: 1.6022, Validation Loss: 1.6785, Validation F1: 0.8157\n",
      "Epoch 77, Train Loss: 1.6069, Validation Loss: 1.6696, Validation F1: 0.8144\n",
      "Epoch 78, Train Loss: 1.6026, Validation Loss: 1.6841, Validation F1: 0.8123\n",
      "Epoch 79, Train Loss: 1.6027, Validation Loss: 1.6814, Validation F1: 0.8129\n",
      "Epoch 80, Train Loss: 1.6042, Validation Loss: 1.6781, Validation F1: 0.8274\n",
      "Epoch 81, Train Loss: 1.6021, Validation Loss: 1.6827, Validation F1: 0.8230\n",
      "Epoch 82, Train Loss: 1.6032, Validation Loss: 1.6802, Validation F1: 0.8254\n",
      "Epoch 83, Train Loss: 1.6030, Validation Loss: 1.6685, Validation F1: 0.8216\n",
      "Epoch 84, Train Loss: 1.6001, Validation Loss: 1.6761, Validation F1: 0.8177\n",
      "Epoch 85, Train Loss: 1.6016, Validation Loss: 1.6738, Validation F1: 0.8163\n",
      "Epoch 86, Train Loss: 1.6031, Validation Loss: 1.6749, Validation F1: 0.8176\n",
      "Epoch 87, Train Loss: 1.6002, Validation Loss: 1.6710, Validation F1: 0.8202\n",
      "Epoch 88, Train Loss: 1.6020, Validation Loss: 1.6710, Validation F1: 0.8185\n",
      "Epoch 89, Train Loss: 1.6002, Validation Loss: 1.6796, Validation F1: 0.8235\n",
      "Epoch 90, Train Loss: 1.5981, Validation Loss: 1.6725, Validation F1: 0.8198\n",
      "Epoch 91, Train Loss: 1.5997, Validation Loss: 1.6725, Validation F1: 0.8179\n",
      "Epoch 92, Train Loss: 1.6008, Validation Loss: 1.6711, Validation F1: 0.8115\n",
      "Epoch 93, Train Loss: 1.5991, Validation Loss: 1.6700, Validation F1: 0.8076\n",
      "Epoch 94, Train Loss: 1.5968, Validation Loss: 1.6627, Validation F1: 0.8110\n",
      "Epoch 95, Train Loss: 1.6005, Validation Loss: 1.6662, Validation F1: 0.8098\n",
      "Epoch 96, Train Loss: 1.5996, Validation Loss: 1.6734, Validation F1: 0.8118\n",
      "Epoch 97, Train Loss: 1.5986, Validation Loss: 1.6725, Validation F1: 0.8240\n",
      "Epoch 98, Train Loss: 1.5966, Validation Loss: 1.6753, Validation F1: 0.8209\n",
      "Epoch 99, Train Loss: 1.5972, Validation Loss: 1.6756, Validation F1: 0.8181\n",
      "Epoch 100, Train Loss: 1.5971, Validation Loss: 1.6734, Validation F1: 0.8230\n",
      "Epoch 101, Train Loss: 1.5968, Validation Loss: 1.6738, Validation F1: 0.8192\n",
      "Epoch 102, Train Loss: 1.5967, Validation Loss: 1.6624, Validation F1: 0.8125\n",
      "Epoch 103, Train Loss: 1.5982, Validation Loss: 1.6708, Validation F1: 0.8134\n",
      "Epoch 104, Train Loss: 1.5998, Validation Loss: 1.6760, Validation F1: 0.8160\n",
      "Epoch 105, Train Loss: 1.5960, Validation Loss: 1.6710, Validation F1: 0.8144\n",
      "Epoch 106, Train Loss: 1.5990, Validation Loss: 1.6720, Validation F1: 0.8111\n",
      "Epoch 107, Train Loss: 1.5971, Validation Loss: 1.6803, Validation F1: 0.8207\n",
      "Epoch 108, Train Loss: 1.5982, Validation Loss: 1.6758, Validation F1: 0.8236\n",
      "Epoch 109, Train Loss: 1.5978, Validation Loss: 1.6766, Validation F1: 0.8225\n",
      "Epoch 110, Train Loss: 1.5945, Validation Loss: 1.6935, Validation F1: 0.8213\n",
      "Epoch 111, Train Loss: 1.5948, Validation Loss: 1.6765, Validation F1: 0.8209\n",
      "Epoch 112, Train Loss: 1.5970, Validation Loss: 1.6693, Validation F1: 0.8118\n",
      "Epoch 113, Train Loss: 1.5948, Validation Loss: 1.6742, Validation F1: 0.8133\n",
      "Epoch 114, Train Loss: 1.5975, Validation Loss: 1.6804, Validation F1: 0.8147\n",
      "Epoch 115, Train Loss: 1.5956, Validation Loss: 1.6652, Validation F1: 0.8142\n",
      "Epoch 116, Train Loss: 1.5927, Validation Loss: 1.6630, Validation F1: 0.8181\n",
      "Epoch 117, Train Loss: 1.5929, Validation Loss: 1.6688, Validation F1: 0.8169\n",
      "Epoch 118, Train Loss: 1.5925, Validation Loss: 1.6611, Validation F1: 0.8176\n",
      "Epoch 119, Train Loss: 1.5940, Validation Loss: 1.6646, Validation F1: 0.8194\n",
      "Epoch 120, Train Loss: 1.5919, Validation Loss: 1.6615, Validation F1: 0.8177\n",
      "Epoch 121, Train Loss: 1.5940, Validation Loss: 1.6701, Validation F1: 0.8179\n",
      "Epoch 122, Train Loss: 1.5916, Validation Loss: 1.6735, Validation F1: 0.8102\n",
      "Epoch 123, Train Loss: 1.5929, Validation Loss: 1.6660, Validation F1: 0.8100\n",
      "Epoch 124, Train Loss: 1.5946, Validation Loss: 1.6707, Validation F1: 0.8173\n",
      "Epoch 125, Train Loss: 1.5948, Validation Loss: 1.6763, Validation F1: 0.8209\n",
      "Epoch 126, Train Loss: 1.5934, Validation Loss: 1.6790, Validation F1: 0.8206\n",
      "Epoch 127, Train Loss: 1.5922, Validation Loss: 1.6713, Validation F1: 0.8176\n",
      "Epoch 128, Train Loss: 1.5915, Validation Loss: 1.6721, Validation F1: 0.8196\n",
      "Epoch 129, Train Loss: 1.5928, Validation Loss: 1.6750, Validation F1: 0.8240\n",
      "Epoch 130, Train Loss: 1.5939, Validation Loss: 1.6628, Validation F1: 0.8229\n",
      "Epoch 131, Train Loss: 1.5930, Validation Loss: 1.6728, Validation F1: 0.8229\n",
      "Epoch 132, Train Loss: 1.5897, Validation Loss: 1.6612, Validation F1: 0.8188\n",
      "Epoch 133, Train Loss: 1.5925, Validation Loss: 1.6649, Validation F1: 0.8139\n",
      "Epoch 134, Train Loss: 1.5916, Validation Loss: 1.6700, Validation F1: 0.8130\n",
      "Epoch 135, Train Loss: 1.5932, Validation Loss: 1.6679, Validation F1: 0.8184\n",
      "Epoch 136, Train Loss: 1.5907, Validation Loss: 1.6637, Validation F1: 0.8177\n",
      "Epoch 137, Train Loss: 1.5904, Validation Loss: 1.6805, Validation F1: 0.8142\n",
      "Epoch 138, Train Loss: 1.5893, Validation Loss: 1.6759, Validation F1: 0.8234\n",
      "Epoch 139, Train Loss: 1.5918, Validation Loss: 1.6639, Validation F1: 0.8280\n",
      "Epoch 140, Train Loss: 1.5905, Validation Loss: 1.6645, Validation F1: 0.8152\n",
      "Epoch 141, Train Loss: 1.5885, Validation Loss: 1.6617, Validation F1: 0.8184\n",
      "Epoch 142, Train Loss: 1.5888, Validation Loss: 1.6629, Validation F1: 0.8153\n",
      "Epoch 143, Train Loss: 1.5924, Validation Loss: 1.6603, Validation F1: 0.8142\n",
      "Epoch 144, Train Loss: 1.5895, Validation Loss: 1.6675, Validation F1: 0.8175\n",
      "Epoch 145, Train Loss: 1.5878, Validation Loss: 1.6702, Validation F1: 0.8153\n",
      "Epoch 146, Train Loss: 1.5914, Validation Loss: 1.6724, Validation F1: 0.8173\n",
      "Epoch 147, Train Loss: 1.5875, Validation Loss: 1.6716, Validation F1: 0.8262\n",
      "Epoch 148, Train Loss: 1.5888, Validation Loss: 1.6608, Validation F1: 0.8175\n",
      "Epoch 149, Train Loss: 1.5888, Validation Loss: 1.6584, Validation F1: 0.8222\n",
      "Epoch 150, Train Loss: 1.5890, Validation Loss: 1.6588, Validation F1: 0.8130\n",
      "Epoch 151, Train Loss: 1.5896, Validation Loss: 1.6595, Validation F1: 0.8178\n",
      "Epoch 152, Train Loss: 1.5888, Validation Loss: 1.6703, Validation F1: 0.8123\n",
      "Epoch 153, Train Loss: 1.5903, Validation Loss: 1.6819, Validation F1: 0.8068\n",
      "Epoch 154, Train Loss: 1.5911, Validation Loss: 1.6662, Validation F1: 0.8181\n",
      "Epoch 155, Train Loss: 1.5873, Validation Loss: 1.6708, Validation F1: 0.8194\n",
      "Epoch 156, Train Loss: 1.5925, Validation Loss: 1.6642, Validation F1: 0.8194\n",
      "Epoch 157, Train Loss: 1.5863, Validation Loss: 1.6648, Validation F1: 0.8174\n",
      "Epoch 158, Train Loss: 1.5884, Validation Loss: 1.6650, Validation F1: 0.8142\n",
      "Epoch 159, Train Loss: 1.5871, Validation Loss: 1.6608, Validation F1: 0.8206\n",
      "Epoch 160, Train Loss: 1.5873, Validation Loss: 1.6647, Validation F1: 0.8138\n",
      "Epoch 161, Train Loss: 1.5890, Validation Loss: 1.6645, Validation F1: 0.8207\n",
      "Epoch 162, Train Loss: 1.5942, Validation Loss: 1.6619, Validation F1: 0.8156\n",
      "Epoch 163, Train Loss: 1.5874, Validation Loss: 1.6693, Validation F1: 0.8131\n",
      "Epoch 164, Train Loss: 1.5861, Validation Loss: 1.6750, Validation F1: 0.8095\n",
      "Epoch 165, Train Loss: 1.5940, Validation Loss: 1.6634, Validation F1: 0.8153\n",
      "Epoch 166, Train Loss: 1.5853, Validation Loss: 1.6776, Validation F1: 0.8208\n",
      "Epoch 167, Train Loss: 1.5888, Validation Loss: 1.6743, Validation F1: 0.8193\n",
      "Epoch 168, Train Loss: 1.5914, Validation Loss: 1.6724, Validation F1: 0.8189\n",
      "Epoch 169, Train Loss: 1.5888, Validation Loss: 1.6732, Validation F1: 0.8269\n",
      "Epoch 170, Train Loss: 1.5881, Validation Loss: 1.6706, Validation F1: 0.8046\n",
      "Epoch 171, Train Loss: 1.5892, Validation Loss: 1.6761, Validation F1: 0.8124\n",
      "Epoch 172, Train Loss: 1.5900, Validation Loss: 1.6772, Validation F1: 0.8119\n",
      "Epoch 173, Train Loss: 1.5864, Validation Loss: 1.6630, Validation F1: 0.8180\n",
      "Epoch 174, Train Loss: 1.5881, Validation Loss: 1.6583, Validation F1: 0.8153\n",
      "Epoch 175, Train Loss: 1.5882, Validation Loss: 1.6629, Validation F1: 0.8169\n",
      "Epoch 176, Train Loss: 1.5856, Validation Loss: 1.6716, Validation F1: 0.8117\n",
      "Epoch 177, Train Loss: 1.5857, Validation Loss: 1.6728, Validation F1: 0.8175\n",
      "Epoch 178, Train Loss: 1.5880, Validation Loss: 1.6776, Validation F1: 0.8178\n",
      "Epoch 179, Train Loss: 1.5861, Validation Loss: 1.6736, Validation F1: 0.8192\n",
      "Epoch 180, Train Loss: 1.5886, Validation Loss: 1.6604, Validation F1: 0.8191\n",
      "Epoch 181, Train Loss: 1.5856, Validation Loss: 1.6621, Validation F1: 0.8210\n",
      "Epoch 182, Train Loss: 1.5868, Validation Loss: 1.6598, Validation F1: 0.8218\n",
      "Epoch 183, Train Loss: 1.5889, Validation Loss: 1.6634, Validation F1: 0.8210\n",
      "Epoch 184, Train Loss: 1.5879, Validation Loss: 1.6688, Validation F1: 0.8110\n",
      "Epoch 185, Train Loss: 1.5861, Validation Loss: 1.6588, Validation F1: 0.8176\n",
      "Epoch 186, Train Loss: 1.5877, Validation Loss: 1.6744, Validation F1: 0.8142\n",
      "Epoch 187, Train Loss: 1.5874, Validation Loss: 1.6677, Validation F1: 0.8172\n",
      "Epoch 188, Train Loss: 1.5846, Validation Loss: 1.6680, Validation F1: 0.8184\n",
      "Epoch 189, Train Loss: 1.5862, Validation Loss: 1.6630, Validation F1: 0.8186\n",
      "Epoch 190, Train Loss: 1.5862, Validation Loss: 1.6663, Validation F1: 0.8187\n",
      "Epoch 191, Train Loss: 1.5854, Validation Loss: 1.6679, Validation F1: 0.8150\n",
      "Epoch 192, Train Loss: 1.5842, Validation Loss: 1.6608, Validation F1: 0.8147\n",
      "Epoch 193, Train Loss: 1.5840, Validation Loss: 1.6659, Validation F1: 0.8189\n",
      "Epoch 194, Train Loss: 1.5828, Validation Loss: 1.6657, Validation F1: 0.8158\n",
      "Epoch 195, Train Loss: 1.5840, Validation Loss: 1.6649, Validation F1: 0.8159\n",
      "Epoch 196, Train Loss: 1.5835, Validation Loss: 1.6621, Validation F1: 0.8176\n",
      "Epoch 197, Train Loss: 1.5850, Validation Loss: 1.6696, Validation F1: 0.8198\n",
      "Epoch 198, Train Loss: 1.5826, Validation Loss: 1.6687, Validation F1: 0.8205\n",
      "Epoch 199, Train Loss: 1.5830, Validation Loss: 1.6580, Validation F1: 0.8208\n",
      "Epoch 200, Train Loss: 1.5842, Validation Loss: 1.6654, Validation F1: 0.8263\n",
      "Epoch 201, Train Loss: 1.5833, Validation Loss: 1.6638, Validation F1: 0.8194\n",
      "Epoch 202, Train Loss: 1.5826, Validation Loss: 1.6629, Validation F1: 0.8197\n",
      "Epoch 203, Train Loss: 1.5798, Validation Loss: 1.6636, Validation F1: 0.8179\n",
      "Epoch 204, Train Loss: 1.5837, Validation Loss: 1.6726, Validation F1: 0.8181\n",
      "Epoch 205, Train Loss: 1.5825, Validation Loss: 1.6566, Validation F1: 0.8182\n",
      "Epoch 206, Train Loss: 1.5849, Validation Loss: 1.6677, Validation F1: 0.8202\n",
      "Epoch 207, Train Loss: 1.5818, Validation Loss: 1.6747, Validation F1: 0.8202\n",
      "Epoch 208, Train Loss: 1.5837, Validation Loss: 1.6655, Validation F1: 0.8217\n",
      "Epoch 209, Train Loss: 1.5830, Validation Loss: 1.6658, Validation F1: 0.8225\n",
      "Epoch 210, Train Loss: 1.5831, Validation Loss: 1.6624, Validation F1: 0.8240\n",
      "Epoch 211, Train Loss: 1.5865, Validation Loss: 1.6658, Validation F1: 0.8193\n",
      "Epoch 212, Train Loss: 1.5812, Validation Loss: 1.6623, Validation F1: 0.8153\n",
      "Epoch 213, Train Loss: 1.5819, Validation Loss: 1.6683, Validation F1: 0.8090\n",
      "Epoch 214, Train Loss: 1.5828, Validation Loss: 1.6663, Validation F1: 0.8141\n",
      "Epoch 215, Train Loss: 1.5830, Validation Loss: 1.6598, Validation F1: 0.8117\n",
      "Epoch 216, Train Loss: 1.5843, Validation Loss: 1.6697, Validation F1: 0.8158\n",
      "Epoch 217, Train Loss: 1.5837, Validation Loss: 1.6719, Validation F1: 0.8206\n",
      "Epoch 218, Train Loss: 1.5839, Validation Loss: 1.6621, Validation F1: 0.8177\n",
      "Epoch 219, Train Loss: 1.5805, Validation Loss: 1.6702, Validation F1: 0.8215\n",
      "Epoch 220, Train Loss: 1.5828, Validation Loss: 1.6817, Validation F1: 0.8190\n",
      "Epoch 221, Train Loss: 1.5815, Validation Loss: 1.6701, Validation F1: 0.8161\n",
      "Epoch 222, Train Loss: 1.5817, Validation Loss: 1.6596, Validation F1: 0.8112\n",
      "Epoch 223, Train Loss: 1.5805, Validation Loss: 1.6593, Validation F1: 0.8092\n",
      "Epoch 224, Train Loss: 1.5828, Validation Loss: 1.6630, Validation F1: 0.8168\n",
      "Epoch 225, Train Loss: 1.5805, Validation Loss: 1.6595, Validation F1: 0.8189\n",
      "Epoch 226, Train Loss: 1.5807, Validation Loss: 1.6665, Validation F1: 0.8211\n",
      "Epoch 227, Train Loss: 1.5787, Validation Loss: 1.6790, Validation F1: 0.8191\n",
      "Epoch 228, Train Loss: 1.5802, Validation Loss: 1.6688, Validation F1: 0.8186\n",
      "Epoch 229, Train Loss: 1.5799, Validation Loss: 1.6663, Validation F1: 0.8161\n",
      "Epoch 230, Train Loss: 1.5830, Validation Loss: 1.6699, Validation F1: 0.8172\n",
      "Epoch 231, Train Loss: 1.5795, Validation Loss: 1.6653, Validation F1: 0.8170\n",
      "Epoch 232, Train Loss: 1.5836, Validation Loss: 1.6635, Validation F1: 0.8179\n",
      "Epoch 233, Train Loss: 1.5790, Validation Loss: 1.6598, Validation F1: 0.8157\n",
      "Epoch 234, Train Loss: 1.5789, Validation Loss: 1.6706, Validation F1: 0.8165\n",
      "Epoch 235, Train Loss: 1.5815, Validation Loss: 1.6558, Validation F1: 0.8150\n",
      "Epoch 236, Train Loss: 1.5782, Validation Loss: 1.6649, Validation F1: 0.8159\n",
      "Epoch 237, Train Loss: 1.5799, Validation Loss: 1.6649, Validation F1: 0.8152\n",
      "Epoch 238, Train Loss: 1.5800, Validation Loss: 1.6687, Validation F1: 0.8182\n",
      "Epoch 239, Train Loss: 1.5801, Validation Loss: 1.6642, Validation F1: 0.8179\n",
      "Epoch 240, Train Loss: 1.5801, Validation Loss: 1.6689, Validation F1: 0.8180\n",
      "Epoch 241, Train Loss: 1.5784, Validation Loss: 1.6663, Validation F1: 0.8188\n",
      "Epoch 242, Train Loss: 1.5793, Validation Loss: 1.6792, Validation F1: 0.8188\n",
      "Epoch 243, Train Loss: 1.5793, Validation Loss: 1.6615, Validation F1: 0.8171\n",
      "Epoch 244, Train Loss: 1.5807, Validation Loss: 1.6700, Validation F1: 0.8179\n",
      "Epoch 245, Train Loss: 1.5787, Validation Loss: 1.6628, Validation F1: 0.8157\n",
      "Epoch 246, Train Loss: 1.5805, Validation Loss: 1.6645, Validation F1: 0.8174\n",
      "Epoch 247, Train Loss: 1.5794, Validation Loss: 1.6709, Validation F1: 0.8151\n",
      "Epoch 248, Train Loss: 1.5831, Validation Loss: 1.6699, Validation F1: 0.8226\n",
      "Epoch 249, Train Loss: 1.5777, Validation Loss: 1.6691, Validation F1: 0.8216\n",
      "Epoch 250, Train Loss: 1.5813, Validation Loss: 1.6679, Validation F1: 0.8168\n",
      "Epoch 251, Train Loss: 1.5790, Validation Loss: 1.6626, Validation F1: 0.8168\n",
      "Epoch 252, Train Loss: 1.5764, Validation Loss: 1.6736, Validation F1: 0.8184\n",
      "Epoch 253, Train Loss: 1.5807, Validation Loss: 1.6823, Validation F1: 0.8185\n",
      "Epoch 254, Train Loss: 1.5769, Validation Loss: 1.6711, Validation F1: 0.8190\n",
      "Epoch 255, Train Loss: 1.5788, Validation Loss: 1.6609, Validation F1: 0.8215\n",
      "Epoch 256, Train Loss: 1.5758, Validation Loss: 1.6691, Validation F1: 0.8223\n",
      "Epoch 257, Train Loss: 1.5776, Validation Loss: 1.6550, Validation F1: 0.8256\n",
      "Epoch 258, Train Loss: 1.5817, Validation Loss: 1.6611, Validation F1: 0.8153\n",
      "Epoch 259, Train Loss: 1.5758, Validation Loss: 1.6648, Validation F1: 0.8208\n",
      "Epoch 260, Train Loss: 1.5810, Validation Loss: 1.6657, Validation F1: 0.8154\n",
      "Epoch 261, Train Loss: 1.5791, Validation Loss: 1.6603, Validation F1: 0.8213\n",
      "Epoch 262, Train Loss: 1.5757, Validation Loss: 1.6609, Validation F1: 0.8204\n",
      "Epoch 263, Train Loss: 1.5803, Validation Loss: 1.6584, Validation F1: 0.8201\n",
      "Epoch 264, Train Loss: 1.5786, Validation Loss: 1.6667, Validation F1: 0.8177\n",
      "Epoch 265, Train Loss: 1.5795, Validation Loss: 1.6623, Validation F1: 0.8206\n",
      "Epoch 266, Train Loss: 1.5758, Validation Loss: 1.6779, Validation F1: 0.8190\n",
      "Epoch 267, Train Loss: 1.5776, Validation Loss: 1.6692, Validation F1: 0.8197\n",
      "Epoch 268, Train Loss: 1.5775, Validation Loss: 1.6672, Validation F1: 0.8143\n",
      "Epoch 269, Train Loss: 1.5788, Validation Loss: 1.6710, Validation F1: 0.8172\n",
      "Epoch 270, Train Loss: 1.5780, Validation Loss: 1.6936, Validation F1: 0.8196\n",
      "Epoch 271, Train Loss: 1.5777, Validation Loss: 1.6764, Validation F1: 0.8180\n",
      "Epoch 272, Train Loss: 1.5752, Validation Loss: 1.6877, Validation F1: 0.8255\n",
      "Epoch 273, Train Loss: 1.5770, Validation Loss: 1.6720, Validation F1: 0.8194\n",
      "Epoch 274, Train Loss: 1.5792, Validation Loss: 1.6731, Validation F1: 0.8182\n",
      "Epoch 275, Train Loss: 1.5757, Validation Loss: 1.6676, Validation F1: 0.8154\n",
      "Epoch 276, Train Loss: 1.5755, Validation Loss: 1.6747, Validation F1: 0.8192\n",
      "Epoch 277, Train Loss: 1.5768, Validation Loss: 1.6701, Validation F1: 0.8158\n",
      "Epoch 278, Train Loss: 1.5753, Validation Loss: 1.6651, Validation F1: 0.8180\n",
      "Epoch 279, Train Loss: 1.5775, Validation Loss: 1.6697, Validation F1: 0.8170\n",
      "Epoch 280, Train Loss: 1.5743, Validation Loss: 1.6765, Validation F1: 0.8186\n",
      "Epoch 281, Train Loss: 1.5756, Validation Loss: 1.6743, Validation F1: 0.8219\n",
      "Epoch 282, Train Loss: 1.5747, Validation Loss: 1.6709, Validation F1: 0.8171\n",
      "Epoch 283, Train Loss: 1.5765, Validation Loss: 1.6679, Validation F1: 0.8171\n",
      "Epoch 284, Train Loss: 1.5751, Validation Loss: 1.6687, Validation F1: 0.8181\n",
      "Epoch 285, Train Loss: 1.5780, Validation Loss: 1.6548, Validation F1: 0.8170\n",
      "Epoch 286, Train Loss: 1.5796, Validation Loss: 1.6750, Validation F1: 0.8167\n",
      "Epoch 287, Train Loss: 1.5775, Validation Loss: 1.6886, Validation F1: 0.8183\n",
      "Epoch 288, Train Loss: 1.5795, Validation Loss: 1.6833, Validation F1: 0.8213\n",
      "Epoch 289, Train Loss: 1.5769, Validation Loss: 1.6605, Validation F1: 0.8176\n",
      "Epoch 290, Train Loss: 1.5789, Validation Loss: 1.6602, Validation F1: 0.8156\n",
      "Epoch 291, Train Loss: 1.5780, Validation Loss: 1.6608, Validation F1: 0.8155\n",
      "Epoch 292, Train Loss: 1.5780, Validation Loss: 1.6625, Validation F1: 0.8110\n",
      "Epoch 293, Train Loss: 1.5757, Validation Loss: 1.6791, Validation F1: 0.8117\n",
      "Epoch 294, Train Loss: 1.5811, Validation Loss: 1.6825, Validation F1: 0.8201\n",
      "Epoch 295, Train Loss: 1.5761, Validation Loss: 1.6717, Validation F1: 0.8227\n",
      "Epoch 296, Train Loss: 1.5770, Validation Loss: 1.6655, Validation F1: 0.8254\n",
      "Epoch 297, Train Loss: 1.5779, Validation Loss: 1.6623, Validation F1: 0.8192\n",
      "Epoch 298, Train Loss: 1.5755, Validation Loss: 1.6617, Validation F1: 0.8160\n",
      "Epoch 299, Train Loss: 1.5758, Validation Loss: 1.6708, Validation F1: 0.8154\n",
      "Epoch 300, Train Loss: 1.5772, Validation Loss: 1.6615, Validation F1: 0.8163\n",
      "Epoch 301, Train Loss: 1.5770, Validation Loss: 1.6718, Validation F1: 0.8225\n",
      "Epoch 302, Train Loss: 1.5745, Validation Loss: 1.6608, Validation F1: 0.8209\n",
      "Epoch 303, Train Loss: 1.5762, Validation Loss: 1.6701, Validation F1: 0.8257\n",
      "Epoch 304, Train Loss: 1.5764, Validation Loss: 1.6662, Validation F1: 0.8228\n",
      "Epoch 305, Train Loss: 1.5718, Validation Loss: 1.6668, Validation F1: 0.8186\n",
      "Epoch 306, Train Loss: 1.5756, Validation Loss: 1.6751, Validation F1: 0.8195\n",
      "Epoch 307, Train Loss: 1.5784, Validation Loss: 1.6689, Validation F1: 0.8191\n",
      "Epoch 308, Train Loss: 1.5748, Validation Loss: 1.6632, Validation F1: 0.8215\n",
      "Epoch 309, Train Loss: 1.5790, Validation Loss: 1.6682, Validation F1: 0.8184\n",
      "Epoch 310, Train Loss: 1.5754, Validation Loss: 1.6659, Validation F1: 0.8176\n",
      "Epoch 311, Train Loss: 1.5740, Validation Loss: 1.6623, Validation F1: 0.8164\n",
      "Epoch 312, Train Loss: 1.5756, Validation Loss: 1.6591, Validation F1: 0.8225\n",
      "Epoch 313, Train Loss: 1.5735, Validation Loss: 1.6653, Validation F1: 0.8203\n",
      "Epoch 314, Train Loss: 1.5752, Validation Loss: 1.6629, Validation F1: 0.8176\n",
      "Epoch 315, Train Loss: 1.5754, Validation Loss: 1.6658, Validation F1: 0.8199\n",
      "Epoch 316, Train Loss: 1.5729, Validation Loss: 1.6736, Validation F1: 0.8164\n",
      "Epoch 317, Train Loss: 1.5757, Validation Loss: 1.6709, Validation F1: 0.8168\n",
      "Epoch 318, Train Loss: 1.5790, Validation Loss: 1.6656, Validation F1: 0.8197\n",
      "Epoch 319, Train Loss: 1.5733, Validation Loss: 1.6567, Validation F1: 0.8206\n",
      "Epoch 320, Train Loss: 1.5762, Validation Loss: 1.6702, Validation F1: 0.8216\n",
      "Epoch 321, Train Loss: 1.5770, Validation Loss: 1.6733, Validation F1: 0.8196\n",
      "Epoch 322, Train Loss: 1.5754, Validation Loss: 1.6770, Validation F1: 0.8168\n",
      "Epoch 323, Train Loss: 1.5735, Validation Loss: 1.6965, Validation F1: 0.8107\n",
      "Epoch 324, Train Loss: 1.5778, Validation Loss: 1.6734, Validation F1: 0.8195\n",
      "Epoch 325, Train Loss: 1.5774, Validation Loss: 1.6559, Validation F1: 0.8200\n",
      "Epoch 326, Train Loss: 1.5766, Validation Loss: 1.6550, Validation F1: 0.8180\n",
      "Epoch 327, Train Loss: 1.5818, Validation Loss: 1.6555, Validation F1: 0.8190\n",
      "Epoch 328, Train Loss: 1.5753, Validation Loss: 1.6509, Validation F1: 0.8175\n",
      "Epoch 329, Train Loss: 1.5815, Validation Loss: 1.6684, Validation F1: 0.8183\n",
      "Epoch 330, Train Loss: 1.5744, Validation Loss: 1.6872, Validation F1: 0.8179\n",
      "Epoch 331, Train Loss: 1.5783, Validation Loss: 1.6681, Validation F1: 0.8167\n",
      "Epoch 332, Train Loss: 1.5777, Validation Loss: 1.6711, Validation F1: 0.8234\n",
      "Epoch 333, Train Loss: 1.5761, Validation Loss: 1.6650, Validation F1: 0.8224\n",
      "Epoch 334, Train Loss: 1.5753, Validation Loss: 1.6709, Validation F1: 0.8217\n",
      "Epoch 335, Train Loss: 1.5766, Validation Loss: 1.6704, Validation F1: 0.8187\n",
      "Epoch 336, Train Loss: 1.5758, Validation Loss: 1.6795, Validation F1: 0.8119\n",
      "Epoch 337, Train Loss: 1.5801, Validation Loss: 1.6771, Validation F1: 0.8188\n",
      "Epoch 338, Train Loss: 1.5825, Validation Loss: 1.6734, Validation F1: 0.8140\n",
      "Epoch 339, Train Loss: 1.5756, Validation Loss: 1.6596, Validation F1: 0.8153\n",
      "Epoch 340, Train Loss: 1.5798, Validation Loss: 1.6665, Validation F1: 0.8194\n",
      "Epoch 341, Train Loss: 1.5775, Validation Loss: 1.6708, Validation F1: 0.8193\n",
      "Epoch 342, Train Loss: 1.5742, Validation Loss: 1.6713, Validation F1: 0.8204\n",
      "Epoch 343, Train Loss: 1.5773, Validation Loss: 1.6902, Validation F1: 0.8167\n",
      "Epoch 344, Train Loss: 1.5774, Validation Loss: 1.6740, Validation F1: 0.8177\n",
      "Epoch 345, Train Loss: 1.5755, Validation Loss: 1.6680, Validation F1: 0.8155\n",
      "Epoch 346, Train Loss: 1.5748, Validation Loss: 1.6724, Validation F1: 0.8160\n",
      "Epoch 347, Train Loss: 1.5790, Validation Loss: 1.6667, Validation F1: 0.8193\n",
      "Epoch 348, Train Loss: 1.5759, Validation Loss: 1.6719, Validation F1: 0.8185\n",
      "Epoch 349, Train Loss: 1.5754, Validation Loss: 1.6818, Validation F1: 0.8215\n",
      "Epoch 350, Train Loss: 1.5733, Validation Loss: 1.6699, Validation F1: 0.8220\n",
      "Epoch 351, Train Loss: 1.5728, Validation Loss: 1.6720, Validation F1: 0.8177\n",
      "Epoch 352, Train Loss: 1.5729, Validation Loss: 1.6613, Validation F1: 0.8173\n",
      "Epoch 353, Train Loss: 1.5729, Validation Loss: 1.6563, Validation F1: 0.8165\n",
      "Epoch 354, Train Loss: 1.5726, Validation Loss: 1.6589, Validation F1: 0.8182\n",
      "Epoch 355, Train Loss: 1.5749, Validation Loss: 1.6733, Validation F1: 0.8181\n",
      "Epoch 356, Train Loss: 1.5743, Validation Loss: 1.6748, Validation F1: 0.8193\n",
      "Epoch 357, Train Loss: 1.5723, Validation Loss: 1.6846, Validation F1: 0.8204\n",
      "Epoch 358, Train Loss: 1.5731, Validation Loss: 1.6772, Validation F1: 0.8238\n",
      "Epoch 359, Train Loss: 1.5727, Validation Loss: 1.6721, Validation F1: 0.8212\n",
      "Epoch 360, Train Loss: 1.5724, Validation Loss: 1.6753, Validation F1: 0.8205\n",
      "Epoch 361, Train Loss: 1.5736, Validation Loss: 1.6667, Validation F1: 0.8169\n",
      "Epoch 362, Train Loss: 1.5736, Validation Loss: 1.6706, Validation F1: 0.8195\n",
      "Epoch 363, Train Loss: 1.5730, Validation Loss: 1.6870, Validation F1: 0.8166\n",
      "Epoch 364, Train Loss: 1.5700, Validation Loss: 1.6847, Validation F1: 0.8198\n",
      "Epoch 365, Train Loss: 1.5719, Validation Loss: 1.7095, Validation F1: 0.8149\n",
      "Epoch 366, Train Loss: 1.5718, Validation Loss: 1.6971, Validation F1: 0.8147\n",
      "Epoch 367, Train Loss: 1.5755, Validation Loss: 1.6780, Validation F1: 0.8194\n",
      "Epoch 368, Train Loss: 1.5747, Validation Loss: 1.6694, Validation F1: 0.8215\n",
      "Epoch 369, Train Loss: 1.5744, Validation Loss: 1.6642, Validation F1: 0.8172\n",
      "Epoch 370, Train Loss: 1.5713, Validation Loss: 1.6714, Validation F1: 0.8128\n",
      "Epoch 371, Train Loss: 1.5715, Validation Loss: 1.6741, Validation F1: 0.8139\n",
      "Epoch 372, Train Loss: 1.5709, Validation Loss: 1.6853, Validation F1: 0.8156\n",
      "Epoch 373, Train Loss: 1.5722, Validation Loss: 1.6723, Validation F1: 0.8169\n",
      "Epoch 374, Train Loss: 1.5707, Validation Loss: 1.6672, Validation F1: 0.8203\n",
      "Epoch 375, Train Loss: 1.5731, Validation Loss: 1.6621, Validation F1: 0.8205\n",
      "Epoch 376, Train Loss: 1.5734, Validation Loss: 1.6662, Validation F1: 0.8232\n",
      "Epoch 377, Train Loss: 1.5723, Validation Loss: 1.6645, Validation F1: 0.8213\n",
      "Epoch 378, Train Loss: 1.5720, Validation Loss: 1.6646, Validation F1: 0.8180\n",
      "Epoch 379, Train Loss: 1.5705, Validation Loss: 1.6626, Validation F1: 0.8189\n",
      "Epoch 380, Train Loss: 1.5716, Validation Loss: 1.6613, Validation F1: 0.8205\n",
      "Epoch 381, Train Loss: 1.5721, Validation Loss: 1.6647, Validation F1: 0.8171\n",
      "Epoch 382, Train Loss: 1.5714, Validation Loss: 1.6634, Validation F1: 0.8167\n",
      "Epoch 383, Train Loss: 1.5724, Validation Loss: 1.6709, Validation F1: 0.8187\n",
      "Epoch 384, Train Loss: 1.5704, Validation Loss: 1.6738, Validation F1: 0.8191\n",
      "Epoch 385, Train Loss: 1.5707, Validation Loss: 1.6751, Validation F1: 0.8145\n",
      "Epoch 386, Train Loss: 1.5702, Validation Loss: 1.6829, Validation F1: 0.8184\n",
      "Epoch 387, Train Loss: 1.5712, Validation Loss: 1.6804, Validation F1: 0.8185\n",
      "Epoch 388, Train Loss: 1.5756, Validation Loss: 1.6695, Validation F1: 0.8167\n",
      "Epoch 389, Train Loss: 1.5766, Validation Loss: 1.6550, Validation F1: 0.8199\n",
      "Epoch 390, Train Loss: 1.5733, Validation Loss: 1.6648, Validation F1: 0.8210\n",
      "Epoch 391, Train Loss: 1.5719, Validation Loss: 1.6811, Validation F1: 0.8205\n",
      "Epoch 392, Train Loss: 1.5750, Validation Loss: 1.6818, Validation F1: 0.8198\n",
      "Epoch 393, Train Loss: 1.5787, Validation Loss: 1.6702, Validation F1: 0.8187\n",
      "Epoch 394, Train Loss: 1.5733, Validation Loss: 1.6702, Validation F1: 0.8186\n",
      "Epoch 395, Train Loss: 1.5716, Validation Loss: 1.6630, Validation F1: 0.8180\n",
      "Epoch 396, Train Loss: 1.5714, Validation Loss: 1.6623, Validation F1: 0.8162\n",
      "Epoch 397, Train Loss: 1.5711, Validation Loss: 1.6650, Validation F1: 0.8225\n",
      "Epoch 398, Train Loss: 1.5735, Validation Loss: 1.6904, Validation F1: 0.8239\n",
      "Epoch 399, Train Loss: 1.5711, Validation Loss: 1.6726, Validation F1: 0.8212\n",
      "Epoch 400, Train Loss: 1.5697, Validation Loss: 1.6637, Validation F1: 0.8190\n",
      "Epoch 401, Train Loss: 1.5702, Validation Loss: 1.6793, Validation F1: 0.8171\n",
      "Epoch 402, Train Loss: 1.5692, Validation Loss: 1.6648, Validation F1: 0.8200\n",
      "Epoch 403, Train Loss: 1.5716, Validation Loss: 1.6740, Validation F1: 0.8172\n",
      "Epoch 404, Train Loss: 1.5717, Validation Loss: 1.6617, Validation F1: 0.8209\n",
      "Epoch 405, Train Loss: 1.5707, Validation Loss: 1.6682, Validation F1: 0.8193\n",
      "Epoch 406, Train Loss: 1.5708, Validation Loss: 1.6749, Validation F1: 0.8207\n",
      "Epoch 407, Train Loss: 1.5743, Validation Loss: 1.6711, Validation F1: 0.8181\n",
      "Epoch 408, Train Loss: 1.5716, Validation Loss: 1.6726, Validation F1: 0.8195\n",
      "Epoch 409, Train Loss: 1.5709, Validation Loss: 1.6772, Validation F1: 0.8147\n",
      "Epoch 410, Train Loss: 1.5720, Validation Loss: 1.6786, Validation F1: 0.8170\n",
      "Epoch 411, Train Loss: 1.5698, Validation Loss: 1.6621, Validation F1: 0.8184\n",
      "Epoch 412, Train Loss: 1.5713, Validation Loss: 1.6620, Validation F1: 0.8200\n",
      "Epoch 413, Train Loss: 1.5732, Validation Loss: 1.6514, Validation F1: 0.8213\n",
      "Epoch 414, Train Loss: 1.5698, Validation Loss: 1.6617, Validation F1: 0.8214\n",
      "Epoch 415, Train Loss: 1.5705, Validation Loss: 1.6810, Validation F1: 0.8170\n",
      "Epoch 416, Train Loss: 1.5711, Validation Loss: 1.6901, Validation F1: 0.8210\n",
      "Epoch 417, Train Loss: 1.5712, Validation Loss: 1.6713, Validation F1: 0.8177\n",
      "Epoch 418, Train Loss: 1.5699, Validation Loss: 1.6834, Validation F1: 0.8192\n",
      "Epoch 419, Train Loss: 1.5701, Validation Loss: 1.6754, Validation F1: 0.8202\n",
      "Epoch 420, Train Loss: 1.5697, Validation Loss: 1.6705, Validation F1: 0.8170\n",
      "Epoch 421, Train Loss: 1.5715, Validation Loss: 1.6658, Validation F1: 0.8185\n",
      "Epoch 422, Train Loss: 1.5693, Validation Loss: 1.6780, Validation F1: 0.8171\n",
      "Epoch 423, Train Loss: 1.5695, Validation Loss: 1.6748, Validation F1: 0.8172\n",
      "Epoch 424, Train Loss: 1.5685, Validation Loss: 1.6718, Validation F1: 0.8211\n",
      "Epoch 425, Train Loss: 1.5688, Validation Loss: 1.6651, Validation F1: 0.8229\n",
      "Epoch 426, Train Loss: 1.5702, Validation Loss: 1.6676, Validation F1: 0.8204\n",
      "Epoch 427, Train Loss: 1.5702, Validation Loss: 1.6671, Validation F1: 0.8196\n",
      "Epoch 428, Train Loss: 1.5702, Validation Loss: 1.6600, Validation F1: 0.8198\n",
      "Epoch 429, Train Loss: 1.5722, Validation Loss: 1.6562, Validation F1: 0.8174\n",
      "Epoch 430, Train Loss: 1.5695, Validation Loss: 1.6558, Validation F1: 0.8185\n",
      "Epoch 431, Train Loss: 1.5709, Validation Loss: 1.6709, Validation F1: 0.8167\n",
      "Epoch 432, Train Loss: 1.5692, Validation Loss: 1.6708, Validation F1: 0.8191\n",
      "Epoch 433, Train Loss: 1.5696, Validation Loss: 1.6796, Validation F1: 0.8199\n",
      "Epoch 434, Train Loss: 1.5705, Validation Loss: 1.6659, Validation F1: 0.8210\n",
      "Epoch 435, Train Loss: 1.5697, Validation Loss: 1.6678, Validation F1: 0.8222\n",
      "Epoch 436, Train Loss: 1.5703, Validation Loss: 1.6676, Validation F1: 0.8171\n",
      "Epoch 437, Train Loss: 1.5714, Validation Loss: 1.6800, Validation F1: 0.8188\n",
      "Epoch 438, Train Loss: 1.5704, Validation Loss: 1.6886, Validation F1: 0.8137\n",
      "Epoch 439, Train Loss: 1.5689, Validation Loss: 1.6902, Validation F1: 0.8152\n",
      "Epoch 440, Train Loss: 1.5694, Validation Loss: 1.6757, Validation F1: 0.8205\n",
      "Epoch 441, Train Loss: 1.5713, Validation Loss: 1.6791, Validation F1: 0.8211\n",
      "Epoch 442, Train Loss: 1.5682, Validation Loss: 1.6838, Validation F1: 0.8198\n",
      "Epoch 443, Train Loss: 1.5689, Validation Loss: 1.6762, Validation F1: 0.8165\n",
      "Epoch 444, Train Loss: 1.5696, Validation Loss: 1.6903, Validation F1: 0.8178\n",
      "Epoch 445, Train Loss: 1.5692, Validation Loss: 1.6729, Validation F1: 0.8219\n",
      "Epoch 446, Train Loss: 1.5704, Validation Loss: 1.6711, Validation F1: 0.8192\n",
      "Epoch 447, Train Loss: 1.5679, Validation Loss: 1.6570, Validation F1: 0.8175\n",
      "Epoch 448, Train Loss: 1.5733, Validation Loss: 1.6609, Validation F1: 0.8209\n",
      "Epoch 449, Train Loss: 1.5690, Validation Loss: 1.6723, Validation F1: 0.8201\n",
      "Epoch 450, Train Loss: 1.5713, Validation Loss: 1.6704, Validation F1: 0.8186\n",
      "Epoch 451, Train Loss: 1.5714, Validation Loss: 1.6697, Validation F1: 0.8196\n",
      "Epoch 452, Train Loss: 1.5691, Validation Loss: 1.6805, Validation F1: 0.8156\n",
      "Epoch 453, Train Loss: 1.5690, Validation Loss: 1.6661, Validation F1: 0.8171\n",
      "Epoch 454, Train Loss: 1.5699, Validation Loss: 1.6621, Validation F1: 0.8196\n",
      "Epoch 455, Train Loss: 1.5711, Validation Loss: 1.6577, Validation F1: 0.8210\n",
      "Epoch 456, Train Loss: 1.5701, Validation Loss: 1.6728, Validation F1: 0.8233\n",
      "Epoch 457, Train Loss: 1.5685, Validation Loss: 1.6644, Validation F1: 0.8229\n",
      "Epoch 458, Train Loss: 1.5704, Validation Loss: 1.6797, Validation F1: 0.8198\n",
      "Epoch 459, Train Loss: 1.5751, Validation Loss: 1.6613, Validation F1: 0.8203\n",
      "Epoch 460, Train Loss: 1.5687, Validation Loss: 1.6614, Validation F1: 0.8223\n",
      "Epoch 461, Train Loss: 1.5731, Validation Loss: 1.6552, Validation F1: 0.8193\n",
      "Epoch 462, Train Loss: 1.5707, Validation Loss: 1.6639, Validation F1: 0.8192\n",
      "Epoch 463, Train Loss: 1.5707, Validation Loss: 1.6707, Validation F1: 0.8181\n",
      "Epoch 464, Train Loss: 1.5685, Validation Loss: 1.6584, Validation F1: 0.8185\n",
      "Epoch 465, Train Loss: 1.5685, Validation Loss: 1.6626, Validation F1: 0.8192\n",
      "Epoch 466, Train Loss: 1.5690, Validation Loss: 1.6735, Validation F1: 0.8193\n",
      "Epoch 467, Train Loss: 1.5697, Validation Loss: 1.6748, Validation F1: 0.8213\n",
      "Epoch 468, Train Loss: 1.5697, Validation Loss: 1.6666, Validation F1: 0.8206\n",
      "Epoch 469, Train Loss: 1.5690, Validation Loss: 1.6659, Validation F1: 0.8188\n",
      "Epoch 470, Train Loss: 1.5694, Validation Loss: 1.6822, Validation F1: 0.8184\n",
      "Epoch 471, Train Loss: 1.5682, Validation Loss: 1.6769, Validation F1: 0.8184\n",
      "Epoch 472, Train Loss: 1.5704, Validation Loss: 1.6624, Validation F1: 0.8158\n",
      "Epoch 473, Train Loss: 1.5710, Validation Loss: 1.6647, Validation F1: 0.8194\n",
      "Epoch 474, Train Loss: 1.5694, Validation Loss: 1.6596, Validation F1: 0.8194\n",
      "Epoch 475, Train Loss: 1.5705, Validation Loss: 1.6588, Validation F1: 0.8218\n",
      "Epoch 476, Train Loss: 1.5690, Validation Loss: 1.6702, Validation F1: 0.8184\n",
      "Epoch 477, Train Loss: 1.5699, Validation Loss: 1.6784, Validation F1: 0.8202\n",
      "Epoch 478, Train Loss: 1.5712, Validation Loss: 1.6689, Validation F1: 0.8183\n",
      "Epoch 479, Train Loss: 1.5676, Validation Loss: 1.6580, Validation F1: 0.8169\n",
      "Epoch 480, Train Loss: 1.5671, Validation Loss: 1.6752, Validation F1: 0.8175\n",
      "Epoch 481, Train Loss: 1.5698, Validation Loss: 1.6706, Validation F1: 0.8191\n",
      "Epoch 482, Train Loss: 1.5690, Validation Loss: 1.6665, Validation F1: 0.8214\n",
      "Epoch 483, Train Loss: 1.5676, Validation Loss: 1.6809, Validation F1: 0.8213\n",
      "Epoch 484, Train Loss: 1.5694, Validation Loss: 1.6861, Validation F1: 0.8190\n",
      "Epoch 485, Train Loss: 1.5685, Validation Loss: 1.6881, Validation F1: 0.8161\n",
      "Epoch 486, Train Loss: 1.5709, Validation Loss: 1.6759, Validation F1: 0.8176\n",
      "Epoch 487, Train Loss: 1.5698, Validation Loss: 1.6876, Validation F1: 0.8177\n",
      "Epoch 488, Train Loss: 1.5692, Validation Loss: 1.6747, Validation F1: 0.8182\n",
      "Epoch 489, Train Loss: 1.5697, Validation Loss: 1.6545, Validation F1: 0.8183\n",
      "Epoch 490, Train Loss: 1.5692, Validation Loss: 1.6505, Validation F1: 0.8180\n",
      "Epoch 491, Train Loss: 1.5685, Validation Loss: 1.6539, Validation F1: 0.8204\n",
      "Epoch 492, Train Loss: 1.5700, Validation Loss: 1.6688, Validation F1: 0.8200\n",
      "Epoch 493, Train Loss: 1.5688, Validation Loss: 1.6725, Validation F1: 0.8211\n",
      "Epoch 494, Train Loss: 1.5712, Validation Loss: 1.6685, Validation F1: 0.8199\n",
      "Epoch 495, Train Loss: 1.5679, Validation Loss: 1.6696, Validation F1: 0.8221\n",
      "Epoch 496, Train Loss: 1.5688, Validation Loss: 1.6529, Validation F1: 0.8204\n",
      "Epoch 497, Train Loss: 1.5673, Validation Loss: 1.6577, Validation F1: 0.8245\n",
      "Epoch 498, Train Loss: 1.5682, Validation Loss: 1.6562, Validation F1: 0.8204\n",
      "Epoch 499, Train Loss: 1.5685, Validation Loss: 1.6518, Validation F1: 0.8192\n",
      "Epoch 500, Train Loss: 1.5671, Validation Loss: 1.6704, Validation F1: 0.8169\n",
      "Epoch 501, Train Loss: 1.5678, Validation Loss: 1.6733, Validation F1: 0.8180\n",
      "Epoch 502, Train Loss: 1.5675, Validation Loss: 1.6647, Validation F1: 0.8206\n",
      "Epoch 503, Train Loss: 1.5682, Validation Loss: 1.6723, Validation F1: 0.8220\n",
      "Epoch 504, Train Loss: 1.5680, Validation Loss: 1.6675, Validation F1: 0.8206\n",
      "Epoch 505, Train Loss: 1.5679, Validation Loss: 1.6903, Validation F1: 0.8177\n",
      "Epoch 506, Train Loss: 1.5687, Validation Loss: 1.6744, Validation F1: 0.8186\n",
      "Epoch 507, Train Loss: 1.5665, Validation Loss: 1.6765, Validation F1: 0.8173\n",
      "Epoch 508, Train Loss: 1.5661, Validation Loss: 1.6663, Validation F1: 0.8169\n",
      "Epoch 509, Train Loss: 1.5668, Validation Loss: 1.6747, Validation F1: 0.8198\n",
      "Epoch 510, Train Loss: 1.5679, Validation Loss: 1.6661, Validation F1: 0.8174\n",
      "Epoch 511, Train Loss: 1.5669, Validation Loss: 1.6667, Validation F1: 0.8191\n",
      "Epoch 512, Train Loss: 1.5666, Validation Loss: 1.6582, Validation F1: 0.8233\n",
      "Epoch 513, Train Loss: 1.5675, Validation Loss: 1.6659, Validation F1: 0.8197\n",
      "Epoch 514, Train Loss: 1.5654, Validation Loss: 1.6650, Validation F1: 0.8183\n",
      "Epoch 515, Train Loss: 1.5678, Validation Loss: 1.6644, Validation F1: 0.8187\n",
      "Epoch 516, Train Loss: 1.5680, Validation Loss: 1.6741, Validation F1: 0.8194\n",
      "Epoch 517, Train Loss: 1.5675, Validation Loss: 1.6576, Validation F1: 0.8170\n",
      "Epoch 518, Train Loss: 1.5662, Validation Loss: 1.6520, Validation F1: 0.8200\n",
      "Epoch 519, Train Loss: 1.5665, Validation Loss: 1.6546, Validation F1: 0.8191\n",
      "Epoch 520, Train Loss: 1.5674, Validation Loss: 1.6635, Validation F1: 0.8199\n",
      "Epoch 521, Train Loss: 1.5667, Validation Loss: 1.6770, Validation F1: 0.8196\n",
      "Epoch 522, Train Loss: 1.5679, Validation Loss: 1.6729, Validation F1: 0.8211\n",
      "Epoch 523, Train Loss: 1.5671, Validation Loss: 1.6600, Validation F1: 0.8206\n",
      "Epoch 524, Train Loss: 1.5665, Validation Loss: 1.6670, Validation F1: 0.8211\n",
      "Epoch 525, Train Loss: 1.5682, Validation Loss: 1.6581, Validation F1: 0.8213\n",
      "Epoch 526, Train Loss: 1.5677, Validation Loss: 1.6580, Validation F1: 0.8197\n",
      "Epoch 527, Train Loss: 1.5672, Validation Loss: 1.6578, Validation F1: 0.8198\n",
      "Epoch 528, Train Loss: 1.5659, Validation Loss: 1.6583, Validation F1: 0.8215\n",
      "Epoch 529, Train Loss: 1.5674, Validation Loss: 1.6660, Validation F1: 0.8228\n",
      "Epoch 530, Train Loss: 1.5663, Validation Loss: 1.6677, Validation F1: 0.8200\n",
      "Epoch 531, Train Loss: 1.5674, Validation Loss: 1.6655, Validation F1: 0.8196\n",
      "Epoch 532, Train Loss: 1.5663, Validation Loss: 1.6663, Validation F1: 0.8211\n",
      "Epoch 533, Train Loss: 1.5664, Validation Loss: 1.6658, Validation F1: 0.8213\n",
      "Epoch 534, Train Loss: 1.5688, Validation Loss: 1.6669, Validation F1: 0.8181\n",
      "Epoch 535, Train Loss: 1.5680, Validation Loss: 1.6666, Validation F1: 0.8200\n",
      "Epoch 536, Train Loss: 1.5656, Validation Loss: 1.6561, Validation F1: 0.8196\n",
      "Epoch 537, Train Loss: 1.5661, Validation Loss: 1.6678, Validation F1: 0.8208\n",
      "Epoch 538, Train Loss: 1.5664, Validation Loss: 1.6726, Validation F1: 0.8196\n",
      "Epoch 539, Train Loss: 1.5670, Validation Loss: 1.6716, Validation F1: 0.8177\n",
      "Epoch 540, Train Loss: 1.5653, Validation Loss: 1.6693, Validation F1: 0.8157\n",
      "Epoch 541, Train Loss: 1.5666, Validation Loss: 1.6840, Validation F1: 0.8217\n",
      "Epoch 542, Train Loss: 1.5654, Validation Loss: 1.6861, Validation F1: 0.8156\n",
      "Epoch 543, Train Loss: 1.5657, Validation Loss: 1.6724, Validation F1: 0.8187\n",
      "Epoch 544, Train Loss: 1.5656, Validation Loss: 1.6733, Validation F1: 0.8178\n",
      "Epoch 545, Train Loss: 1.5724, Validation Loss: 1.6746, Validation F1: 0.8204\n",
      "Epoch 546, Train Loss: 1.5676, Validation Loss: 1.6700, Validation F1: 0.8178\n",
      "Epoch 547, Train Loss: 1.5677, Validation Loss: 1.6676, Validation F1: 0.8186\n",
      "Epoch 548, Train Loss: 1.5690, Validation Loss: 1.6625, Validation F1: 0.8204\n",
      "Epoch 549, Train Loss: 1.5671, Validation Loss: 1.6654, Validation F1: 0.8206\n",
      "Epoch 550, Train Loss: 1.5683, Validation Loss: 1.6585, Validation F1: 0.8216\n",
      "Epoch 551, Train Loss: 1.5670, Validation Loss: 1.6647, Validation F1: 0.8227\n",
      "Epoch 552, Train Loss: 1.5663, Validation Loss: 1.6613, Validation F1: 0.8194\n",
      "Epoch 553, Train Loss: 1.5697, Validation Loss: 1.6654, Validation F1: 0.8161\n",
      "Epoch 554, Train Loss: 1.5678, Validation Loss: 1.6549, Validation F1: 0.8190\n",
      "Epoch 555, Train Loss: 1.5669, Validation Loss: 1.6573, Validation F1: 0.8206\n",
      "Epoch 556, Train Loss: 1.5686, Validation Loss: 1.6607, Validation F1: 0.8227\n",
      "Epoch 557, Train Loss: 1.5663, Validation Loss: 1.6598, Validation F1: 0.8230\n",
      "Epoch 558, Train Loss: 1.5664, Validation Loss: 1.6717, Validation F1: 0.8217\n",
      "Epoch 559, Train Loss: 1.5657, Validation Loss: 1.6688, Validation F1: 0.8172\n",
      "Epoch 560, Train Loss: 1.5659, Validation Loss: 1.6705, Validation F1: 0.8169\n",
      "Epoch 561, Train Loss: 1.5655, Validation Loss: 1.6676, Validation F1: 0.8202\n",
      "Epoch 562, Train Loss: 1.5657, Validation Loss: 1.6663, Validation F1: 0.8175\n",
      "Epoch 563, Train Loss: 1.5655, Validation Loss: 1.6579, Validation F1: 0.8218\n",
      "Epoch 564, Train Loss: 1.5661, Validation Loss: 1.6796, Validation F1: 0.8183\n",
      "Epoch 565, Train Loss: 1.5653, Validation Loss: 1.6898, Validation F1: 0.8151\n",
      "Epoch 566, Train Loss: 1.5664, Validation Loss: 1.6844, Validation F1: 0.8151\n",
      "Epoch 567, Train Loss: 1.5674, Validation Loss: 1.6759, Validation F1: 0.8175\n",
      "Epoch 568, Train Loss: 1.5657, Validation Loss: 1.6659, Validation F1: 0.8198\n",
      "Epoch 569, Train Loss: 1.5662, Validation Loss: 1.6595, Validation F1: 0.8183\n",
      "Epoch 570, Train Loss: 1.5652, Validation Loss: 1.6673, Validation F1: 0.8211\n",
      "Epoch 571, Train Loss: 1.5653, Validation Loss: 1.6705, Validation F1: 0.8213\n",
      "Epoch 572, Train Loss: 1.5663, Validation Loss: 1.6591, Validation F1: 0.8204\n",
      "Epoch 573, Train Loss: 1.5661, Validation Loss: 1.6560, Validation F1: 0.8201\n",
      "Epoch 574, Train Loss: 1.5668, Validation Loss: 1.6624, Validation F1: 0.8160\n",
      "Epoch 575, Train Loss: 1.5658, Validation Loss: 1.6625, Validation F1: 0.8183\n",
      "Epoch 576, Train Loss: 1.5644, Validation Loss: 1.6554, Validation F1: 0.8203\n",
      "Epoch 577, Train Loss: 1.5664, Validation Loss: 1.6689, Validation F1: 0.8167\n",
      "Epoch 578, Train Loss: 1.5638, Validation Loss: 1.6724, Validation F1: 0.8195\n",
      "Epoch 579, Train Loss: 1.5658, Validation Loss: 1.6749, Validation F1: 0.8173\n",
      "Epoch 580, Train Loss: 1.5653, Validation Loss: 1.6699, Validation F1: 0.8200\n",
      "Epoch 581, Train Loss: 1.5650, Validation Loss: 1.6738, Validation F1: 0.8203\n",
      "Epoch 582, Train Loss: 1.5663, Validation Loss: 1.6841, Validation F1: 0.8198\n",
      "Epoch 583, Train Loss: 1.5720, Validation Loss: 1.6705, Validation F1: 0.8182\n",
      "Epoch 584, Train Loss: 1.5657, Validation Loss: 1.6731, Validation F1: 0.8182\n",
      "Epoch 585, Train Loss: 1.5652, Validation Loss: 1.6688, Validation F1: 0.8183\n",
      "Epoch 586, Train Loss: 1.5663, Validation Loss: 1.6673, Validation F1: 0.8194\n",
      "Epoch 587, Train Loss: 1.5657, Validation Loss: 1.6533, Validation F1: 0.8219\n",
      "Epoch 588, Train Loss: 1.5647, Validation Loss: 1.6648, Validation F1: 0.8220\n",
      "Epoch 589, Train Loss: 1.5672, Validation Loss: 1.6537, Validation F1: 0.8210\n",
      "Epoch 590, Train Loss: 1.5656, Validation Loss: 1.6490, Validation F1: 0.8193\n",
      "Epoch 591, Train Loss: 1.5651, Validation Loss: 1.6594, Validation F1: 0.8184\n",
      "Epoch 592, Train Loss: 1.5676, Validation Loss: 1.6675, Validation F1: 0.8194\n",
      "Epoch 593, Train Loss: 1.5661, Validation Loss: 1.6646, Validation F1: 0.8172\n",
      "Epoch 594, Train Loss: 1.5659, Validation Loss: 1.6615, Validation F1: 0.8185\n",
      "Epoch 595, Train Loss: 1.5659, Validation Loss: 1.6600, Validation F1: 0.8186\n",
      "Epoch 596, Train Loss: 1.5651, Validation Loss: 1.6583, Validation F1: 0.8175\n",
      "Epoch 597, Train Loss: 1.5662, Validation Loss: 1.6607, Validation F1: 0.8203\n",
      "Epoch 598, Train Loss: 1.5659, Validation Loss: 1.6619, Validation F1: 0.8209\n",
      "Epoch 599, Train Loss: 1.5662, Validation Loss: 1.6816, Validation F1: 0.8167\n",
      "Epoch 600, Train Loss: 1.5645, Validation Loss: 1.6908, Validation F1: 0.8148\n",
      "Epoch 601, Train Loss: 1.5670, Validation Loss: 1.6867, Validation F1: 0.8175\n",
      "Epoch 602, Train Loss: 1.5650, Validation Loss: 1.6626, Validation F1: 0.8166\n",
      "Epoch 603, Train Loss: 1.5652, Validation Loss: 1.6620, Validation F1: 0.8180\n",
      "Epoch 604, Train Loss: 1.5665, Validation Loss: 1.6590, Validation F1: 0.8165\n",
      "Epoch 605, Train Loss: 1.5645, Validation Loss: 1.6683, Validation F1: 0.8153\n",
      "Epoch 606, Train Loss: 1.5648, Validation Loss: 1.6694, Validation F1: 0.8166\n",
      "Epoch 607, Train Loss: 1.5645, Validation Loss: 1.6744, Validation F1: 0.8174\n",
      "Epoch 608, Train Loss: 1.5660, Validation Loss: 1.6617, Validation F1: 0.8178\n",
      "Epoch 609, Train Loss: 1.5650, Validation Loss: 1.6689, Validation F1: 0.8231\n",
      "Epoch 610, Train Loss: 1.5663, Validation Loss: 1.6648, Validation F1: 0.8215\n",
      "Epoch 611, Train Loss: 1.5660, Validation Loss: 1.6604, Validation F1: 0.8213\n",
      "Epoch 612, Train Loss: 1.5656, Validation Loss: 1.6578, Validation F1: 0.8179\n",
      "Epoch 613, Train Loss: 1.5652, Validation Loss: 1.6642, Validation F1: 0.8183\n",
      "Epoch 614, Train Loss: 1.5654, Validation Loss: 1.6621, Validation F1: 0.8185\n",
      "Epoch 615, Train Loss: 1.5667, Validation Loss: 1.6618, Validation F1: 0.8179\n",
      "Epoch 616, Train Loss: 1.5649, Validation Loss: 1.6605, Validation F1: 0.8199\n",
      "Epoch 617, Train Loss: 1.5651, Validation Loss: 1.6701, Validation F1: 0.8219\n",
      "Epoch 618, Train Loss: 1.5638, Validation Loss: 1.6624, Validation F1: 0.8193\n",
      "Epoch 619, Train Loss: 1.5642, Validation Loss: 1.6671, Validation F1: 0.8181\n",
      "Epoch 620, Train Loss: 1.5650, Validation Loss: 1.6580, Validation F1: 0.8190\n",
      "Epoch 621, Train Loss: 1.5668, Validation Loss: 1.6671, Validation F1: 0.8205\n",
      "Epoch 622, Train Loss: 1.5653, Validation Loss: 1.6553, Validation F1: 0.8190\n",
      "Epoch 623, Train Loss: 1.5664, Validation Loss: 1.6610, Validation F1: 0.8205\n",
      "Epoch 624, Train Loss: 1.5667, Validation Loss: 1.6675, Validation F1: 0.8208\n",
      "Epoch 625, Train Loss: 1.5658, Validation Loss: 1.6613, Validation F1: 0.8201\n",
      "Epoch 626, Train Loss: 1.5647, Validation Loss: 1.6612, Validation F1: 0.8208\n",
      "Epoch 627, Train Loss: 1.5658, Validation Loss: 1.6634, Validation F1: 0.8220\n",
      "Epoch 628, Train Loss: 1.5647, Validation Loss: 1.6579, Validation F1: 0.8195\n",
      "Epoch 629, Train Loss: 1.5655, Validation Loss: 1.6601, Validation F1: 0.8186\n",
      "Epoch 630, Train Loss: 1.5644, Validation Loss: 1.6633, Validation F1: 0.8175\n",
      "Epoch 631, Train Loss: 1.5668, Validation Loss: 1.6551, Validation F1: 0.8198\n",
      "Epoch 632, Train Loss: 1.5656, Validation Loss: 1.6576, Validation F1: 0.8205\n",
      "Epoch 633, Train Loss: 1.5659, Validation Loss: 1.6557, Validation F1: 0.8200\n",
      "Epoch 634, Train Loss: 1.5680, Validation Loss: 1.6490, Validation F1: 0.8191\n",
      "Epoch 635, Train Loss: 1.5646, Validation Loss: 1.6704, Validation F1: 0.8197\n",
      "Epoch 636, Train Loss: 1.5671, Validation Loss: 1.6793, Validation F1: 0.8172\n",
      "Epoch 637, Train Loss: 1.5666, Validation Loss: 1.6700, Validation F1: 0.8164\n",
      "Epoch 638, Train Loss: 1.5653, Validation Loss: 1.6629, Validation F1: 0.8170\n",
      "Epoch 639, Train Loss: 1.5658, Validation Loss: 1.6560, Validation F1: 0.8213\n",
      "Epoch 640, Train Loss: 1.5666, Validation Loss: 1.6684, Validation F1: 0.8239\n",
      "Epoch 641, Train Loss: 1.5669, Validation Loss: 1.6707, Validation F1: 0.8260\n",
      "Epoch 642, Train Loss: 1.5670, Validation Loss: 1.6693, Validation F1: 0.8238\n",
      "Epoch 643, Train Loss: 1.5654, Validation Loss: 1.6599, Validation F1: 0.8175\n",
      "Epoch 644, Train Loss: 1.5658, Validation Loss: 1.6620, Validation F1: 0.8141\n",
      "Epoch 645, Train Loss: 1.5644, Validation Loss: 1.6620, Validation F1: 0.8171\n",
      "Epoch 646, Train Loss: 1.5651, Validation Loss: 1.6610, Validation F1: 0.8206\n",
      "Epoch 647, Train Loss: 1.5663, Validation Loss: 1.6548, Validation F1: 0.8195\n",
      "Epoch 648, Train Loss: 1.5645, Validation Loss: 1.6716, Validation F1: 0.8220\n",
      "Epoch 649, Train Loss: 1.5646, Validation Loss: 1.6616, Validation F1: 0.8208\n",
      "Epoch 650, Train Loss: 1.5641, Validation Loss: 1.6733, Validation F1: 0.8146\n",
      "Epoch 651, Train Loss: 1.5659, Validation Loss: 1.6692, Validation F1: 0.8160\n",
      "Epoch 652, Train Loss: 1.5642, Validation Loss: 1.6621, Validation F1: 0.8190\n",
      "Epoch 653, Train Loss: 1.5635, Validation Loss: 1.6589, Validation F1: 0.8196\n",
      "Epoch 654, Train Loss: 1.5651, Validation Loss: 1.6528, Validation F1: 0.8225\n",
      "Epoch 655, Train Loss: 1.5654, Validation Loss: 1.6618, Validation F1: 0.8216\n",
      "Epoch 656, Train Loss: 1.5640, Validation Loss: 1.6538, Validation F1: 0.8192\n",
      "Epoch 657, Train Loss: 1.5644, Validation Loss: 1.6643, Validation F1: 0.8177\n",
      "Epoch 658, Train Loss: 1.5640, Validation Loss: 1.6594, Validation F1: 0.8165\n",
      "Epoch 659, Train Loss: 1.5639, Validation Loss: 1.6602, Validation F1: 0.8206\n",
      "Epoch 660, Train Loss: 1.5648, Validation Loss: 1.6686, Validation F1: 0.8202\n",
      "Epoch 661, Train Loss: 1.5637, Validation Loss: 1.6671, Validation F1: 0.8182\n",
      "Epoch 662, Train Loss: 1.5638, Validation Loss: 1.6642, Validation F1: 0.8183\n",
      "Epoch 663, Train Loss: 1.5646, Validation Loss: 1.6782, Validation F1: 0.8201\n",
      "Epoch 664, Train Loss: 1.5656, Validation Loss: 1.6709, Validation F1: 0.8191\n",
      "Epoch 665, Train Loss: 1.5652, Validation Loss: 1.6617, Validation F1: 0.8205\n",
      "Epoch 666, Train Loss: 1.5643, Validation Loss: 1.6703, Validation F1: 0.8203\n",
      "Epoch 667, Train Loss: 1.5639, Validation Loss: 1.6649, Validation F1: 0.8211\n",
      "Epoch 668, Train Loss: 1.5630, Validation Loss: 1.6637, Validation F1: 0.8180\n",
      "Epoch 669, Train Loss: 1.5635, Validation Loss: 1.6599, Validation F1: 0.8192\n",
      "Epoch 670, Train Loss: 1.5624, Validation Loss: 1.6605, Validation F1: 0.8213\n",
      "Epoch 671, Train Loss: 1.5635, Validation Loss: 1.6672, Validation F1: 0.8201\n",
      "Epoch 672, Train Loss: 1.5640, Validation Loss: 1.6744, Validation F1: 0.8205\n",
      "Epoch 673, Train Loss: 1.5638, Validation Loss: 1.6658, Validation F1: 0.8203\n",
      "Epoch 674, Train Loss: 1.5639, Validation Loss: 1.6665, Validation F1: 0.8197\n",
      "Epoch 675, Train Loss: 1.5631, Validation Loss: 1.6705, Validation F1: 0.8160\n",
      "Epoch 676, Train Loss: 1.5643, Validation Loss: 1.6682, Validation F1: 0.8152\n",
      "Epoch 677, Train Loss: 1.5631, Validation Loss: 1.6698, Validation F1: 0.8191\n",
      "Epoch 678, Train Loss: 1.5635, Validation Loss: 1.6814, Validation F1: 0.8182\n",
      "Epoch 679, Train Loss: 1.5628, Validation Loss: 1.6692, Validation F1: 0.8201\n",
      "Epoch 680, Train Loss: 1.5634, Validation Loss: 1.6683, Validation F1: 0.8203\n",
      "Epoch 681, Train Loss: 1.5637, Validation Loss: 1.6699, Validation F1: 0.8202\n",
      "Epoch 682, Train Loss: 1.5642, Validation Loss: 1.6720, Validation F1: 0.8192\n",
      "Epoch 683, Train Loss: 1.5644, Validation Loss: 1.6777, Validation F1: 0.8179\n",
      "Epoch 684, Train Loss: 1.5624, Validation Loss: 1.6728, Validation F1: 0.8182\n",
      "Epoch 685, Train Loss: 1.5645, Validation Loss: 1.6706, Validation F1: 0.8187\n",
      "Epoch 686, Train Loss: 1.5640, Validation Loss: 1.6720, Validation F1: 0.8202\n",
      "Epoch 687, Train Loss: 1.5642, Validation Loss: 1.6813, Validation F1: 0.8191\n",
      "Epoch 688, Train Loss: 1.5638, Validation Loss: 1.6723, Validation F1: 0.8224\n",
      "Epoch 689, Train Loss: 1.5651, Validation Loss: 1.6799, Validation F1: 0.8207\n",
      "Epoch 690, Train Loss: 1.5648, Validation Loss: 1.6695, Validation F1: 0.8224\n",
      "Epoch 691, Train Loss: 1.5626, Validation Loss: 1.6612, Validation F1: 0.8192\n",
      "Epoch 692, Train Loss: 1.5644, Validation Loss: 1.6663, Validation F1: 0.8189\n",
      "Epoch 693, Train Loss: 1.5641, Validation Loss: 1.6699, Validation F1: 0.8219\n",
      "Epoch 694, Train Loss: 1.5637, Validation Loss: 1.6671, Validation F1: 0.8213\n",
      "Epoch 695, Train Loss: 1.5632, Validation Loss: 1.6583, Validation F1: 0.8214\n",
      "Epoch 696, Train Loss: 1.5637, Validation Loss: 1.6629, Validation F1: 0.8216\n",
      "Epoch 697, Train Loss: 1.5658, Validation Loss: 1.6626, Validation F1: 0.8205\n",
      "Epoch 698, Train Loss: 1.5629, Validation Loss: 1.6671, Validation F1: 0.8187\n",
      "Epoch 699, Train Loss: 1.5653, Validation Loss: 1.6610, Validation F1: 0.8167\n",
      "Epoch 700, Train Loss: 1.5642, Validation Loss: 1.6589, Validation F1: 0.8176\n",
      "Epoch 701, Train Loss: 1.5626, Validation Loss: 1.6597, Validation F1: 0.8195\n",
      "Epoch 702, Train Loss: 1.5662, Validation Loss: 1.6582, Validation F1: 0.8178\n",
      "Epoch 703, Train Loss: 1.5646, Validation Loss: 1.6602, Validation F1: 0.8200\n",
      "Epoch 704, Train Loss: 1.5647, Validation Loss: 1.6618, Validation F1: 0.8209\n",
      "Epoch 705, Train Loss: 1.5631, Validation Loss: 1.6654, Validation F1: 0.8213\n",
      "Epoch 706, Train Loss: 1.5639, Validation Loss: 1.6610, Validation F1: 0.8191\n",
      "Epoch 707, Train Loss: 1.5654, Validation Loss: 1.6587, Validation F1: 0.8182\n",
      "Epoch 708, Train Loss: 1.5646, Validation Loss: 1.6595, Validation F1: 0.8191\n",
      "Epoch 709, Train Loss: 1.5628, Validation Loss: 1.6710, Validation F1: 0.8192\n",
      "Epoch 710, Train Loss: 1.5634, Validation Loss: 1.6644, Validation F1: 0.8199\n",
      "Epoch 711, Train Loss: 1.5652, Validation Loss: 1.6758, Validation F1: 0.8198\n",
      "Epoch 712, Train Loss: 1.5636, Validation Loss: 1.6574, Validation F1: 0.8183\n",
      "Epoch 713, Train Loss: 1.5653, Validation Loss: 1.6815, Validation F1: 0.8173\n",
      "Epoch 714, Train Loss: 1.5643, Validation Loss: 1.6761, Validation F1: 0.8171\n",
      "Epoch 715, Train Loss: 1.5623, Validation Loss: 1.6849, Validation F1: 0.8183\n",
      "Epoch 716, Train Loss: 1.5648, Validation Loss: 1.6627, Validation F1: 0.8181\n",
      "Epoch 717, Train Loss: 1.5632, Validation Loss: 1.6691, Validation F1: 0.8187\n",
      "Epoch 718, Train Loss: 1.5636, Validation Loss: 1.6735, Validation F1: 0.8187\n",
      "Epoch 719, Train Loss: 1.5626, Validation Loss: 1.6660, Validation F1: 0.8184\n",
      "Epoch 720, Train Loss: 1.5634, Validation Loss: 1.6647, Validation F1: 0.8198\n",
      "Epoch 721, Train Loss: 1.5647, Validation Loss: 1.6583, Validation F1: 0.8194\n",
      "Epoch 722, Train Loss: 1.5629, Validation Loss: 1.6724, Validation F1: 0.8208\n",
      "Epoch 723, Train Loss: 1.5630, Validation Loss: 1.6799, Validation F1: 0.8227\n",
      "Epoch 724, Train Loss: 1.5636, Validation Loss: 1.6662, Validation F1: 0.8216\n",
      "Epoch 725, Train Loss: 1.5635, Validation Loss: 1.6646, Validation F1: 0.8235\n",
      "Epoch 726, Train Loss: 1.5622, Validation Loss: 1.6659, Validation F1: 0.8211\n",
      "Epoch 727, Train Loss: 1.5625, Validation Loss: 1.6625, Validation F1: 0.8176\n",
      "Epoch 728, Train Loss: 1.5621, Validation Loss: 1.6606, Validation F1: 0.8215\n",
      "Epoch 729, Train Loss: 1.5612, Validation Loss: 1.6766, Validation F1: 0.8210\n",
      "Epoch 730, Train Loss: 1.5617, Validation Loss: 1.6754, Validation F1: 0.8192\n",
      "Epoch 731, Train Loss: 1.5620, Validation Loss: 1.6611, Validation F1: 0.8217\n",
      "Epoch 732, Train Loss: 1.5630, Validation Loss: 1.6625, Validation F1: 0.8220\n",
      "Epoch 733, Train Loss: 1.5625, Validation Loss: 1.6653, Validation F1: 0.8221\n",
      "Epoch 734, Train Loss: 1.5623, Validation Loss: 1.6663, Validation F1: 0.8207\n",
      "Epoch 735, Train Loss: 1.5627, Validation Loss: 1.6699, Validation F1: 0.8198\n",
      "Epoch 736, Train Loss: 1.5629, Validation Loss: 1.6721, Validation F1: 0.8200\n",
      "Epoch 737, Train Loss: 1.5626, Validation Loss: 1.6787, Validation F1: 0.8186\n",
      "Epoch 738, Train Loss: 1.5659, Validation Loss: 1.6654, Validation F1: 0.8198\n",
      "Epoch 739, Train Loss: 1.5637, Validation Loss: 1.6594, Validation F1: 0.8209\n",
      "Epoch 740, Train Loss: 1.5633, Validation Loss: 1.6591, Validation F1: 0.8223\n",
      "Epoch 741, Train Loss: 1.5659, Validation Loss: 1.6578, Validation F1: 0.8182\n",
      "Epoch 742, Train Loss: 1.5636, Validation Loss: 1.6649, Validation F1: 0.8162\n",
      "Epoch 743, Train Loss: 1.5634, Validation Loss: 1.6802, Validation F1: 0.8117\n",
      "Epoch 744, Train Loss: 1.5645, Validation Loss: 1.6728, Validation F1: 0.8174\n",
      "Epoch 745, Train Loss: 1.5630, Validation Loss: 1.6733, Validation F1: 0.8187\n",
      "Epoch 746, Train Loss: 1.5635, Validation Loss: 1.6572, Validation F1: 0.8219\n",
      "Epoch 747, Train Loss: 1.5643, Validation Loss: 1.6597, Validation F1: 0.8198\n",
      "Epoch 748, Train Loss: 1.5647, Validation Loss: 1.6593, Validation F1: 0.8196\n",
      "Epoch 749, Train Loss: 1.5625, Validation Loss: 1.6696, Validation F1: 0.8195\n",
      "Epoch 750, Train Loss: 1.5637, Validation Loss: 1.6815, Validation F1: 0.8177\n",
      "Epoch 751, Train Loss: 1.5656, Validation Loss: 1.6705, Validation F1: 0.8184\n",
      "Epoch 752, Train Loss: 1.5632, Validation Loss: 1.6639, Validation F1: 0.8196\n",
      "Epoch 753, Train Loss: 1.5638, Validation Loss: 1.6665, Validation F1: 0.8208\n",
      "Epoch 754, Train Loss: 1.5651, Validation Loss: 1.6670, Validation F1: 0.8225\n",
      "Epoch 755, Train Loss: 1.5628, Validation Loss: 1.6600, Validation F1: 0.8184\n",
      "Epoch 756, Train Loss: 1.5638, Validation Loss: 1.6776, Validation F1: 0.8179\n",
      "Epoch 757, Train Loss: 1.5649, Validation Loss: 1.6757, Validation F1: 0.8171\n",
      "Epoch 758, Train Loss: 1.5645, Validation Loss: 1.6647, Validation F1: 0.8142\n",
      "Epoch 759, Train Loss: 1.5645, Validation Loss: 1.6583, Validation F1: 0.8174\n",
      "Epoch 760, Train Loss: 1.5627, Validation Loss: 1.6643, Validation F1: 0.8200\n",
      "Epoch 761, Train Loss: 1.5640, Validation Loss: 1.6659, Validation F1: 0.8215\n",
      "Epoch 762, Train Loss: 1.5642, Validation Loss: 1.6625, Validation F1: 0.8197\n",
      "Epoch 763, Train Loss: 1.5622, Validation Loss: 1.6845, Validation F1: 0.8182\n",
      "Epoch 764, Train Loss: 1.5636, Validation Loss: 1.6784, Validation F1: 0.8180\n",
      "Epoch 765, Train Loss: 1.5636, Validation Loss: 1.6689, Validation F1: 0.8184\n",
      "Epoch 766, Train Loss: 1.5628, Validation Loss: 1.6607, Validation F1: 0.8182\n",
      "Epoch 767, Train Loss: 1.5632, Validation Loss: 1.6622, Validation F1: 0.8206\n",
      "Epoch 768, Train Loss: 1.5628, Validation Loss: 1.6521, Validation F1: 0.8192\n",
      "Epoch 769, Train Loss: 1.5638, Validation Loss: 1.6676, Validation F1: 0.8207\n",
      "Epoch 770, Train Loss: 1.5640, Validation Loss: 1.6614, Validation F1: 0.8202\n",
      "Epoch 771, Train Loss: 1.5622, Validation Loss: 1.6653, Validation F1: 0.8197\n",
      "Epoch 772, Train Loss: 1.5629, Validation Loss: 1.6693, Validation F1: 0.8194\n",
      "Epoch 773, Train Loss: 1.5618, Validation Loss: 1.6733, Validation F1: 0.8218\n",
      "Epoch 774, Train Loss: 1.5636, Validation Loss: 1.6616, Validation F1: 0.8219\n",
      "Epoch 775, Train Loss: 1.5632, Validation Loss: 1.6711, Validation F1: 0.8188\n",
      "Epoch 776, Train Loss: 1.5631, Validation Loss: 1.6670, Validation F1: 0.8226\n",
      "Epoch 777, Train Loss: 1.5615, Validation Loss: 1.6609, Validation F1: 0.8186\n",
      "Epoch 778, Train Loss: 1.5620, Validation Loss: 1.6689, Validation F1: 0.8188\n",
      "Epoch 779, Train Loss: 1.5637, Validation Loss: 1.6650, Validation F1: 0.8196\n",
      "Epoch 780, Train Loss: 1.5632, Validation Loss: 1.6646, Validation F1: 0.8191\n",
      "Epoch 781, Train Loss: 1.5624, Validation Loss: 1.6577, Validation F1: 0.8203\n",
      "Epoch 782, Train Loss: 1.5622, Validation Loss: 1.6578, Validation F1: 0.8220\n",
      "Epoch 783, Train Loss: 1.5637, Validation Loss: 1.6553, Validation F1: 0.8206\n",
      "Epoch 784, Train Loss: 1.5634, Validation Loss: 1.6641, Validation F1: 0.8200\n",
      "Epoch 785, Train Loss: 1.5629, Validation Loss: 1.6656, Validation F1: 0.8193\n",
      "Epoch 786, Train Loss: 1.5626, Validation Loss: 1.6729, Validation F1: 0.8204\n",
      "Epoch 787, Train Loss: 1.5632, Validation Loss: 1.6805, Validation F1: 0.8204\n",
      "Epoch 788, Train Loss: 1.5624, Validation Loss: 1.6752, Validation F1: 0.8186\n",
      "Epoch 789, Train Loss: 1.5628, Validation Loss: 1.6794, Validation F1: 0.8192\n",
      "Epoch 790, Train Loss: 1.5635, Validation Loss: 1.6705, Validation F1: 0.8200\n",
      "Epoch 791, Train Loss: 1.5614, Validation Loss: 1.6613, Validation F1: 0.8230\n",
      "Epoch 792, Train Loss: 1.5646, Validation Loss: 1.6680, Validation F1: 0.8185\n",
      "Epoch 793, Train Loss: 1.5626, Validation Loss: 1.6736, Validation F1: 0.8185\n",
      "Epoch 794, Train Loss: 1.5619, Validation Loss: 1.6722, Validation F1: 0.8208\n",
      "Epoch 795, Train Loss: 1.5617, Validation Loss: 1.6765, Validation F1: 0.8204\n",
      "Epoch 796, Train Loss: 1.5650, Validation Loss: 1.6672, Validation F1: 0.8209\n",
      "Epoch 797, Train Loss: 1.5636, Validation Loss: 1.6630, Validation F1: 0.8195\n",
      "Epoch 798, Train Loss: 1.5631, Validation Loss: 1.6621, Validation F1: 0.8193\n",
      "Epoch 799, Train Loss: 1.5624, Validation Loss: 1.6655, Validation F1: 0.8207\n",
      "Epoch 800, Train Loss: 1.5639, Validation Loss: 1.6616, Validation F1: 0.8195\n",
      "Epoch 801, Train Loss: 1.5631, Validation Loss: 1.6632, Validation F1: 0.8185\n",
      "Epoch 802, Train Loss: 1.5631, Validation Loss: 1.6556, Validation F1: 0.8180\n",
      "Epoch 803, Train Loss: 1.5630, Validation Loss: 1.6660, Validation F1: 0.8188\n",
      "Epoch 804, Train Loss: 1.5629, Validation Loss: 1.6627, Validation F1: 0.8199\n",
      "Epoch 805, Train Loss: 1.5614, Validation Loss: 1.6690, Validation F1: 0.8210\n",
      "Epoch 806, Train Loss: 1.5617, Validation Loss: 1.6648, Validation F1: 0.8197\n",
      "Epoch 807, Train Loss: 1.5628, Validation Loss: 1.6637, Validation F1: 0.8197\n",
      "Epoch 808, Train Loss: 1.5616, Validation Loss: 1.6680, Validation F1: 0.8185\n",
      "Epoch 809, Train Loss: 1.5624, Validation Loss: 1.6714, Validation F1: 0.8185\n",
      "Epoch 810, Train Loss: 1.5628, Validation Loss: 1.6589, Validation F1: 0.8198\n",
      "Epoch 811, Train Loss: 1.5622, Validation Loss: 1.6752, Validation F1: 0.8216\n",
      "Epoch 812, Train Loss: 1.5623, Validation Loss: 1.6735, Validation F1: 0.8189\n",
      "Epoch 813, Train Loss: 1.5617, Validation Loss: 1.6541, Validation F1: 0.8210\n",
      "Epoch 814, Train Loss: 1.5618, Validation Loss: 1.6640, Validation F1: 0.8212\n",
      "Epoch 815, Train Loss: 1.5627, Validation Loss: 1.6615, Validation F1: 0.8191\n",
      "Epoch 816, Train Loss: 1.5614, Validation Loss: 1.6591, Validation F1: 0.8181\n",
      "Epoch 817, Train Loss: 1.5613, Validation Loss: 1.6689, Validation F1: 0.8174\n",
      "Epoch 818, Train Loss: 1.5624, Validation Loss: 1.6626, Validation F1: 0.8172\n",
      "Epoch 819, Train Loss: 1.5623, Validation Loss: 1.6579, Validation F1: 0.8183\n",
      "Epoch 820, Train Loss: 1.5618, Validation Loss: 1.6657, Validation F1: 0.8207\n",
      "Epoch 821, Train Loss: 1.5636, Validation Loss: 1.6612, Validation F1: 0.8195\n",
      "Epoch 822, Train Loss: 1.5612, Validation Loss: 1.6714, Validation F1: 0.8211\n",
      "Epoch 823, Train Loss: 1.5631, Validation Loss: 1.6614, Validation F1: 0.8206\n",
      "Epoch 824, Train Loss: 1.5617, Validation Loss: 1.6571, Validation F1: 0.8209\n",
      "Epoch 825, Train Loss: 1.5611, Validation Loss: 1.6580, Validation F1: 0.8212\n",
      "Epoch 826, Train Loss: 1.5621, Validation Loss: 1.6553, Validation F1: 0.8218\n",
      "Epoch 827, Train Loss: 1.5614, Validation Loss: 1.6576, Validation F1: 0.8203\n",
      "Epoch 828, Train Loss: 1.5620, Validation Loss: 1.6597, Validation F1: 0.8224\n",
      "Epoch 829, Train Loss: 1.5620, Validation Loss: 1.6559, Validation F1: 0.8208\n",
      "Epoch 830, Train Loss: 1.5621, Validation Loss: 1.6614, Validation F1: 0.8198\n",
      "Epoch 831, Train Loss: 1.5627, Validation Loss: 1.6564, Validation F1: 0.8208\n",
      "Epoch 832, Train Loss: 1.5614, Validation Loss: 1.6673, Validation F1: 0.8198\n",
      "Epoch 833, Train Loss: 1.5630, Validation Loss: 1.6647, Validation F1: 0.8202\n",
      "Epoch 834, Train Loss: 1.5613, Validation Loss: 1.6689, Validation F1: 0.8221\n",
      "Epoch 835, Train Loss: 1.5624, Validation Loss: 1.6674, Validation F1: 0.8199\n",
      "Epoch 836, Train Loss: 1.5612, Validation Loss: 1.6800, Validation F1: 0.8190\n",
      "Epoch 837, Train Loss: 1.5615, Validation Loss: 1.6683, Validation F1: 0.8188\n",
      "Epoch 838, Train Loss: 1.5624, Validation Loss: 1.6736, Validation F1: 0.8166\n",
      "Epoch 839, Train Loss: 1.5618, Validation Loss: 1.6724, Validation F1: 0.8192\n",
      "Epoch 840, Train Loss: 1.5620, Validation Loss: 1.6641, Validation F1: 0.8203\n",
      "Epoch 841, Train Loss: 1.5619, Validation Loss: 1.6694, Validation F1: 0.8207\n",
      "Epoch 842, Train Loss: 1.5608, Validation Loss: 1.6632, Validation F1: 0.8196\n",
      "Epoch 843, Train Loss: 1.5611, Validation Loss: 1.6685, Validation F1: 0.8198\n",
      "Epoch 844, Train Loss: 1.5621, Validation Loss: 1.6646, Validation F1: 0.8199\n",
      "Epoch 845, Train Loss: 1.5605, Validation Loss: 1.6597, Validation F1: 0.8208\n",
      "Epoch 846, Train Loss: 1.5608, Validation Loss: 1.6525, Validation F1: 0.8202\n",
      "Epoch 847, Train Loss: 1.5607, Validation Loss: 1.6554, Validation F1: 0.8221\n",
      "Epoch 848, Train Loss: 1.5613, Validation Loss: 1.6527, Validation F1: 0.8178\n",
      "Epoch 849, Train Loss: 1.5599, Validation Loss: 1.6611, Validation F1: 0.8226\n",
      "Epoch 850, Train Loss: 1.5605, Validation Loss: 1.6667, Validation F1: 0.8195\n",
      "Epoch 851, Train Loss: 1.5626, Validation Loss: 1.6716, Validation F1: 0.8207\n",
      "Epoch 852, Train Loss: 1.5614, Validation Loss: 1.6719, Validation F1: 0.8201\n",
      "Epoch 853, Train Loss: 1.5613, Validation Loss: 1.6726, Validation F1: 0.8198\n",
      "Epoch 854, Train Loss: 1.5611, Validation Loss: 1.6804, Validation F1: 0.8187\n",
      "Epoch 855, Train Loss: 1.5634, Validation Loss: 1.6748, Validation F1: 0.8186\n",
      "Epoch 856, Train Loss: 1.5617, Validation Loss: 1.6679, Validation F1: 0.8170\n",
      "Epoch 857, Train Loss: 1.5615, Validation Loss: 1.6759, Validation F1: 0.8148\n",
      "Epoch 858, Train Loss: 1.5620, Validation Loss: 1.6679, Validation F1: 0.8206\n",
      "Epoch 859, Train Loss: 1.5622, Validation Loss: 1.6682, Validation F1: 0.8206\n",
      "Epoch 860, Train Loss: 1.5626, Validation Loss: 1.6688, Validation F1: 0.8198\n",
      "Epoch 861, Train Loss: 1.5611, Validation Loss: 1.6658, Validation F1: 0.8208\n",
      "Epoch 862, Train Loss: 1.5612, Validation Loss: 1.6576, Validation F1: 0.8189\n",
      "Epoch 863, Train Loss: 1.5613, Validation Loss: 1.6499, Validation F1: 0.8179\n",
      "Epoch 864, Train Loss: 1.5633, Validation Loss: 1.6578, Validation F1: 0.8205\n",
      "Epoch 865, Train Loss: 1.5613, Validation Loss: 1.6538, Validation F1: 0.8206\n",
      "Epoch 866, Train Loss: 1.5622, Validation Loss: 1.6628, Validation F1: 0.8196\n",
      "Epoch 867, Train Loss: 1.5632, Validation Loss: 1.6660, Validation F1: 0.8215\n",
      "Epoch 868, Train Loss: 1.5610, Validation Loss: 1.6570, Validation F1: 0.8190\n",
      "Epoch 869, Train Loss: 1.5606, Validation Loss: 1.6625, Validation F1: 0.8201\n",
      "Epoch 870, Train Loss: 1.5618, Validation Loss: 1.6615, Validation F1: 0.8181\n",
      "Epoch 871, Train Loss: 1.5607, Validation Loss: 1.6579, Validation F1: 0.8187\n",
      "Epoch 872, Train Loss: 1.5605, Validation Loss: 1.6660, Validation F1: 0.8211\n",
      "Epoch 873, Train Loss: 1.5625, Validation Loss: 1.6636, Validation F1: 0.8204\n",
      "Epoch 874, Train Loss: 1.5608, Validation Loss: 1.6735, Validation F1: 0.8212\n",
      "Epoch 875, Train Loss: 1.5614, Validation Loss: 1.6798, Validation F1: 0.8204\n",
      "Epoch 876, Train Loss: 1.5605, Validation Loss: 1.6738, Validation F1: 0.8223\n",
      "Epoch 877, Train Loss: 1.5613, Validation Loss: 1.6833, Validation F1: 0.8208\n",
      "Epoch 878, Train Loss: 1.5609, Validation Loss: 1.6774, Validation F1: 0.8220\n",
      "Epoch 879, Train Loss: 1.5621, Validation Loss: 1.6687, Validation F1: 0.8176\n",
      "Epoch 880, Train Loss: 1.5624, Validation Loss: 1.6653, Validation F1: 0.8205\n",
      "Epoch 881, Train Loss: 1.5619, Validation Loss: 1.6680, Validation F1: 0.8199\n",
      "Epoch 882, Train Loss: 1.5617, Validation Loss: 1.6650, Validation F1: 0.8185\n",
      "Epoch 883, Train Loss: 1.5603, Validation Loss: 1.6711, Validation F1: 0.8191\n",
      "Epoch 884, Train Loss: 1.5611, Validation Loss: 1.6763, Validation F1: 0.8196\n",
      "Epoch 885, Train Loss: 1.5617, Validation Loss: 1.6632, Validation F1: 0.8189\n",
      "Epoch 886, Train Loss: 1.5648, Validation Loss: 1.6580, Validation F1: 0.8199\n",
      "Epoch 887, Train Loss: 1.5624, Validation Loss: 1.6578, Validation F1: 0.8205\n",
      "Epoch 888, Train Loss: 1.5629, Validation Loss: 1.6567, Validation F1: 0.8230\n",
      "Epoch 889, Train Loss: 1.5602, Validation Loss: 1.6558, Validation F1: 0.8218\n",
      "Epoch 890, Train Loss: 1.5604, Validation Loss: 1.6554, Validation F1: 0.8204\n",
      "Epoch 891, Train Loss: 1.5612, Validation Loss: 1.6571, Validation F1: 0.8234\n",
      "Epoch 892, Train Loss: 1.5615, Validation Loss: 1.6574, Validation F1: 0.8207\n",
      "Epoch 893, Train Loss: 1.5609, Validation Loss: 1.6594, Validation F1: 0.8209\n",
      "Epoch 894, Train Loss: 1.5605, Validation Loss: 1.6575, Validation F1: 0.8195\n",
      "Epoch 895, Train Loss: 1.5615, Validation Loss: 1.6552, Validation F1: 0.8220\n",
      "Epoch 896, Train Loss: 1.5610, Validation Loss: 1.6568, Validation F1: 0.8227\n",
      "Epoch 897, Train Loss: 1.5608, Validation Loss: 1.6646, Validation F1: 0.8216\n",
      "Epoch 898, Train Loss: 1.5611, Validation Loss: 1.6560, Validation F1: 0.8210\n",
      "Epoch 899, Train Loss: 1.5605, Validation Loss: 1.6593, Validation F1: 0.8179\n",
      "Epoch 900, Train Loss: 1.5613, Validation Loss: 1.6594, Validation F1: 0.8201\n",
      "Epoch 901, Train Loss: 1.5611, Validation Loss: 1.6550, Validation F1: 0.8202\n",
      "Epoch 902, Train Loss: 1.5614, Validation Loss: 1.6573, Validation F1: 0.8196\n",
      "Epoch 903, Train Loss: 1.5611, Validation Loss: 1.6615, Validation F1: 0.8226\n",
      "Epoch 904, Train Loss: 1.5609, Validation Loss: 1.6593, Validation F1: 0.8219\n",
      "Epoch 905, Train Loss: 1.5601, Validation Loss: 1.6707, Validation F1: 0.8211\n",
      "Epoch 906, Train Loss: 1.5605, Validation Loss: 1.6652, Validation F1: 0.8203\n",
      "Epoch 907, Train Loss: 1.5621, Validation Loss: 1.6639, Validation F1: 0.8193\n",
      "Epoch 908, Train Loss: 1.5602, Validation Loss: 1.6608, Validation F1: 0.8170\n",
      "Epoch 909, Train Loss: 1.5613, Validation Loss: 1.6621, Validation F1: 0.8207\n",
      "Epoch 910, Train Loss: 1.5625, Validation Loss: 1.6561, Validation F1: 0.8193\n",
      "Epoch 911, Train Loss: 1.5611, Validation Loss: 1.6581, Validation F1: 0.8202\n",
      "Epoch 912, Train Loss: 1.5611, Validation Loss: 1.6508, Validation F1: 0.8180\n",
      "Epoch 913, Train Loss: 1.5611, Validation Loss: 1.6593, Validation F1: 0.8211\n",
      "Epoch 914, Train Loss: 1.5604, Validation Loss: 1.6607, Validation F1: 0.8181\n",
      "Epoch 915, Train Loss: 1.5611, Validation Loss: 1.6619, Validation F1: 0.8208\n",
      "Epoch 916, Train Loss: 1.5604, Validation Loss: 1.6700, Validation F1: 0.8196\n",
      "Epoch 917, Train Loss: 1.5608, Validation Loss: 1.6622, Validation F1: 0.8189\n",
      "Epoch 918, Train Loss: 1.5618, Validation Loss: 1.6620, Validation F1: 0.8213\n",
      "Epoch 919, Train Loss: 1.5609, Validation Loss: 1.6616, Validation F1: 0.8202\n",
      "Epoch 920, Train Loss: 1.5615, Validation Loss: 1.6633, Validation F1: 0.8207\n",
      "Epoch 921, Train Loss: 1.5611, Validation Loss: 1.6623, Validation F1: 0.8180\n",
      "Epoch 922, Train Loss: 1.5606, Validation Loss: 1.6695, Validation F1: 0.8176\n",
      "Epoch 923, Train Loss: 1.5617, Validation Loss: 1.6616, Validation F1: 0.8219\n",
      "Epoch 924, Train Loss: 1.5617, Validation Loss: 1.6572, Validation F1: 0.8195\n",
      "Epoch 925, Train Loss: 1.5612, Validation Loss: 1.6669, Validation F1: 0.8220\n",
      "Epoch 926, Train Loss: 1.5623, Validation Loss: 1.6590, Validation F1: 0.8213\n",
      "Epoch 927, Train Loss: 1.5618, Validation Loss: 1.6642, Validation F1: 0.8216\n",
      "Epoch 928, Train Loss: 1.5608, Validation Loss: 1.6642, Validation F1: 0.8209\n",
      "Epoch 929, Train Loss: 1.5613, Validation Loss: 1.6615, Validation F1: 0.8186\n",
      "Epoch 930, Train Loss: 1.5614, Validation Loss: 1.6650, Validation F1: 0.8193\n",
      "Epoch 931, Train Loss: 1.5606, Validation Loss: 1.6544, Validation F1: 0.8185\n",
      "Epoch 932, Train Loss: 1.5601, Validation Loss: 1.6618, Validation F1: 0.8192\n",
      "Epoch 933, Train Loss: 1.5608, Validation Loss: 1.6609, Validation F1: 0.8219\n",
      "Epoch 934, Train Loss: 1.5604, Validation Loss: 1.6607, Validation F1: 0.8233\n",
      "Epoch 935, Train Loss: 1.5619, Validation Loss: 1.6571, Validation F1: 0.8208\n",
      "Epoch 936, Train Loss: 1.5613, Validation Loss: 1.6544, Validation F1: 0.8207\n",
      "Epoch 937, Train Loss: 1.5608, Validation Loss: 1.6664, Validation F1: 0.8219\n",
      "Epoch 938, Train Loss: 1.5604, Validation Loss: 1.6573, Validation F1: 0.8211\n",
      "Epoch 939, Train Loss: 1.5610, Validation Loss: 1.6660, Validation F1: 0.8212\n",
      "Epoch 940, Train Loss: 1.5604, Validation Loss: 1.6642, Validation F1: 0.8179\n",
      "Epoch 941, Train Loss: 1.5603, Validation Loss: 1.6602, Validation F1: 0.8209\n",
      "Epoch 942, Train Loss: 1.5595, Validation Loss: 1.6618, Validation F1: 0.8207\n",
      "Epoch 943, Train Loss: 1.5604, Validation Loss: 1.6630, Validation F1: 0.8209\n",
      "Epoch 944, Train Loss: 1.5600, Validation Loss: 1.6674, Validation F1: 0.8169\n",
      "Epoch 945, Train Loss: 1.5611, Validation Loss: 1.6576, Validation F1: 0.8183\n",
      "Epoch 946, Train Loss: 1.5599, Validation Loss: 1.6600, Validation F1: 0.8178\n",
      "Epoch 947, Train Loss: 1.5609, Validation Loss: 1.6732, Validation F1: 0.8195\n",
      "Epoch 948, Train Loss: 1.5599, Validation Loss: 1.6593, Validation F1: 0.8203\n",
      "Epoch 949, Train Loss: 1.5605, Validation Loss: 1.6583, Validation F1: 0.8205\n",
      "Epoch 950, Train Loss: 1.5598, Validation Loss: 1.6571, Validation F1: 0.8200\n",
      "Epoch 951, Train Loss: 1.5620, Validation Loss: 1.6529, Validation F1: 0.8201\n",
      "Epoch 952, Train Loss: 1.5607, Validation Loss: 1.6592, Validation F1: 0.8202\n",
      "Epoch 953, Train Loss: 1.5605, Validation Loss: 1.6583, Validation F1: 0.8205\n",
      "Epoch 954, Train Loss: 1.5607, Validation Loss: 1.6702, Validation F1: 0.8189\n",
      "Epoch 955, Train Loss: 1.5601, Validation Loss: 1.6647, Validation F1: 0.8204\n",
      "Epoch 956, Train Loss: 1.5614, Validation Loss: 1.6694, Validation F1: 0.8202\n",
      "Epoch 957, Train Loss: 1.5616, Validation Loss: 1.6585, Validation F1: 0.8203\n",
      "Epoch 958, Train Loss: 1.5596, Validation Loss: 1.6536, Validation F1: 0.8210\n",
      "Epoch 959, Train Loss: 1.5611, Validation Loss: 1.6553, Validation F1: 0.8203\n",
      "Epoch 960, Train Loss: 1.5599, Validation Loss: 1.6577, Validation F1: 0.8216\n",
      "Epoch 961, Train Loss: 1.5625, Validation Loss: 1.6623, Validation F1: 0.8221\n",
      "Epoch 962, Train Loss: 1.5608, Validation Loss: 1.6635, Validation F1: 0.8201\n",
      "Epoch 963, Train Loss: 1.5607, Validation Loss: 1.6671, Validation F1: 0.8206\n",
      "Epoch 964, Train Loss: 1.5614, Validation Loss: 1.6600, Validation F1: 0.8197\n",
      "Epoch 965, Train Loss: 1.5624, Validation Loss: 1.6590, Validation F1: 0.8201\n",
      "Epoch 966, Train Loss: 1.5625, Validation Loss: 1.6605, Validation F1: 0.8210\n",
      "Epoch 967, Train Loss: 1.5612, Validation Loss: 1.6632, Validation F1: 0.8196\n",
      "Epoch 968, Train Loss: 1.5604, Validation Loss: 1.6611, Validation F1: 0.8155\n",
      "Epoch 969, Train Loss: 1.5618, Validation Loss: 1.6630, Validation F1: 0.8193\n",
      "Epoch 970, Train Loss: 1.5616, Validation Loss: 1.6605, Validation F1: 0.8192\n",
      "Epoch 971, Train Loss: 1.5607, Validation Loss: 1.6646, Validation F1: 0.8209\n",
      "Epoch 972, Train Loss: 1.5603, Validation Loss: 1.6703, Validation F1: 0.8201\n",
      "Epoch 973, Train Loss: 1.5601, Validation Loss: 1.6577, Validation F1: 0.8196\n",
      "Epoch 974, Train Loss: 1.5604, Validation Loss: 1.6561, Validation F1: 0.8194\n",
      "Epoch 975, Train Loss: 1.5599, Validation Loss: 1.6629, Validation F1: 0.8183\n",
      "Epoch 976, Train Loss: 1.5618, Validation Loss: 1.6698, Validation F1: 0.8188\n",
      "Epoch 977, Train Loss: 1.5609, Validation Loss: 1.6616, Validation F1: 0.8200\n",
      "Epoch 978, Train Loss: 1.5619, Validation Loss: 1.6592, Validation F1: 0.8202\n",
      "Epoch 979, Train Loss: 1.5607, Validation Loss: 1.6617, Validation F1: 0.8199\n",
      "Epoch 980, Train Loss: 1.5615, Validation Loss: 1.6517, Validation F1: 0.8207\n",
      "Epoch 981, Train Loss: 1.5614, Validation Loss: 1.6559, Validation F1: 0.8209\n",
      "Epoch 982, Train Loss: 1.5607, Validation Loss: 1.6608, Validation F1: 0.8201\n",
      "Epoch 983, Train Loss: 1.5606, Validation Loss: 1.6547, Validation F1: 0.8183\n",
      "Epoch 984, Train Loss: 1.5609, Validation Loss: 1.6612, Validation F1: 0.8209\n",
      "Epoch 985, Train Loss: 1.5610, Validation Loss: 1.6554, Validation F1: 0.8204\n",
      "Epoch 986, Train Loss: 1.5609, Validation Loss: 1.6531, Validation F1: 0.8202\n",
      "Epoch 987, Train Loss: 1.5597, Validation Loss: 1.6566, Validation F1: 0.8199\n",
      "Epoch 988, Train Loss: 1.5607, Validation Loss: 1.6600, Validation F1: 0.8193\n",
      "Epoch 989, Train Loss: 1.5603, Validation Loss: 1.6641, Validation F1: 0.8185\n",
      "Epoch 990, Train Loss: 1.5607, Validation Loss: 1.6613, Validation F1: 0.8210\n",
      "Epoch 991, Train Loss: 1.5603, Validation Loss: 1.6688, Validation F1: 0.8189\n",
      "Epoch 992, Train Loss: 1.5597, Validation Loss: 1.6674, Validation F1: 0.8194\n",
      "Epoch 993, Train Loss: 1.5600, Validation Loss: 1.6628, Validation F1: 0.8195\n",
      "Epoch 994, Train Loss: 1.5601, Validation Loss: 1.6607, Validation F1: 0.8211\n",
      "Epoch 995, Train Loss: 1.5604, Validation Loss: 1.6600, Validation F1: 0.8181\n",
      "Epoch 996, Train Loss: 1.5607, Validation Loss: 1.6596, Validation F1: 0.8209\n",
      "Epoch 997, Train Loss: 1.5599, Validation Loss: 1.6603, Validation F1: 0.8191\n",
      "Epoch 998, Train Loss: 1.5608, Validation Loss: 1.6666, Validation F1: 0.8214\n",
      "Epoch 999, Train Loss: 1.5606, Validation Loss: 1.6651, Validation F1: 0.8186\n",
      "Model training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Extract the best parameters from the grid search\n",
    "best_hidden_dim = 256  # Replace with the best hidden_dim found\n",
    "best_learning_rate = 0.001  # Replace with the best learning_rate found\n",
    "best_dropout = 0.3  # Replace with the best dropout found\n",
    "\n",
    "# Initialize the model with the best parameters\n",
    "model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                   edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                   hidden_channels=best_hidden_dim,\n",
    "                   dropout=best_dropout,\n",
    "                   out_channels=num_classes).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Compute class weights for the training dataset\n",
    "labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.unique(labels),\n",
    "                                                  y=labels)\n",
    "\n",
    "# Normalize class weights\n",
    "class_weights = th.FloatTensor(class_weights).to(device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# Move the graph data to the device\n",
    "G_pyg_train = G_pyg_train.to(device)\n",
    "G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "# ===== Load checkpoint if exists =====\n",
    "best_f1 = 0\n",
    "start_epoch = 0\n",
    "epochs = 5000\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = th.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "    print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_f1_history = []\n",
    "saved_model_epochs = []\n",
    "\n",
    "train_loss_history_path = os.path.join(saves_path, 'train_loss_history.pkl')\n",
    "val_loss_history_path = os.path.join(saves_path, 'val_loss_history.pkl')\n",
    "val_f1_history_path = os.path.join(saves_path, 'val_f1_history.pkl')\n",
    "saved_model_epochs_path = os.path.join(saves_path, 'saved_model_epochs.pkl')\n",
    "\n",
    "if os.path.exists(train_loss_history_path) and os.path.exists(val_loss_history_path) and os.path.exists(val_f1_history_path) and os.path.exists(saved_model_epochs_path):\n",
    "    with open(train_loss_history_path, 'rb') as f:\n",
    "        train_loss_history = pickle.load(f)\n",
    "    with open(val_loss_history_path, 'rb') as f:\n",
    "        val_loss_history = pickle.load(f)\n",
    "    with open(val_f1_history_path, 'rb') as f:\n",
    "        val_f1_history = pickle.load(f)\n",
    "    with open(saved_model_epochs_path, 'rb') as f:\n",
    "        saved_model_epochs = pickle.load(f)\n",
    "\n",
    "# ===== Start Training =====\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    out = model(G_pyg_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(out, G_pyg_train.edge_label)\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        out = model(G_pyg_val)\n",
    "        loss = criterion(out, G_pyg_val.edge_label)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "    val_f1_micro = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='micro')\n",
    "    val_f1_macro = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='macro')\n",
    "\n",
    "    if epoch % 10 == 0 or val_f1 > best_f1:\n",
    "        # Save checkpoint\n",
    "        th.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_f1': best_f1\n",
    "        }, checkpoint_path)\n",
    "        with open(train_loss_history_path, 'wb') as f:\n",
    "            pickle.dump(train_loss_history, f)\n",
    "        with open(val_loss_history_path, 'wb') as f:\n",
    "            pickle.dump(val_loss_history, f)\n",
    "        with open(val_f1_history_path, 'wb') as f:\n",
    "            pickle.dump(val_f1_history, f)\n",
    "        with open(saved_model_epochs_path, 'wb') as f:\n",
    "            pickle.dump(saved_model_epochs, f)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1  # Update the best F1 score for this fold\n",
    "        best_model_state = model.state_dict()\n",
    "        th.save(best_model_state, best_model_path)\n",
    "        print(f\"Epoch {epoch} Saved best model. Best F1:\", best_f1)\n",
    "        saved_model_epochs.append(epoch)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_f1_history.append((val_f1, val_f1_micro, val_f1_macro))\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Model training completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_process(train_losses, val_losses, val_f1, saved_model_epochs):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "    # Plot Train Loss\n",
    "    axs[0].plot(train_losses, label='Train Loss', color='blue')\n",
    "    axs[0].plot(val_losses, label='Validation Loss', color='red')\n",
    "    axs[0].set_ylabel('Train Loss')\n",
    "    axs[0].set_title('Training Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid()\n",
    "\n",
    "    val_f1_weighted_history = []\n",
    "    val_f1_micro_history = []\n",
    "    val_f1_macro_history = []\n",
    "\n",
    "    for val_f1_weighted, val_f1_micro, val_f1_macro in val_f1:\n",
    "        val_f1_weighted_history.append(val_f1_weighted)\n",
    "        val_f1_micro_history.append(val_f1_micro)\n",
    "        val_f1_macro_history.append(val_f1_macro)\n",
    "    \n",
    "    # Plot Validation F1\n",
    "    axs[1].plot(val_f1_weighted_history, label='Validation F1 Weighted', color='green')\n",
    "    axs[1].plot(val_f1_micro_history, label='Validation F1 Micro', color='blue')\n",
    "    axs[1].plot(val_f1_macro_history, label='Validation F1 Macro', color='red')\n",
    "    average_val_f1 = np.mean(val_f1)\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Validation F1')\n",
    "    axs[1].set_title('Validation F1 Score')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid()\n",
    "\n",
    "    print(len(train_losses))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df26e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_process(train_loss_history, val_loss_history, val_f1_history, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8368fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 175\n",
      "Shape of node in G_pyg: torch.Size([175, 54])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 54])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f271a-612b-4cd6-a85a-e4236dec9d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_raw_port/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[   98    42   111    75    19     1     0     3    38    15]\n",
      " [   67    40   119    45     5     5     0    18    34    16]\n",
      " [  335   302   844   380   105    84     0   111   170   122]\n",
      " [  857   256  1207   814   628   414     0   739  1115   649]\n",
      " [  356    45   245   390   510   251     0   423   958   459]\n",
      " [   63    35   134   127    86 31525     0   101   154    97]\n",
      " [   25     0    29    60    72    53 32686   105   154    97]\n",
      " [  125    53   209   272   229   195     0   249   480   286]\n",
      " [   10     0     9    34    31    26     0    38    48    31]\n",
      " [    4     0     1     2     4     1     0     1     5     8]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0505    0.2438    0.0837       402\n",
      "     Backdoors     0.0517    0.1146    0.0713       349\n",
      "           DoS     0.2902    0.3441    0.3149      2453\n",
      "      Exploits     0.3702    0.1219    0.1834      6679\n",
      "       Fuzzers     0.3020    0.1402    0.1915      3637\n",
      "       Generic     0.9684    0.9753    0.9718     32322\n",
      "        Normal     1.0000    0.9821    0.9910     33281\n",
      "Reconnaissance     0.1393    0.1187    0.1282      2098\n",
      "     Shellcode     0.0152    0.2115    0.0284       227\n",
      "         Worms     0.0045    0.3077    0.0089        26\n",
      "\n",
      "      accuracy                         0.8202     81474\n",
      "     macro avg     0.3192    0.3560    0.2973     81474\n",
      "  weighted avg     0.8493    0.8202    0.8275     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def eval(G_pyg_test, adversarial=False):\n",
    "\n",
    "    G_pyg_test = G_pyg_test.to(device)\n",
    "    G_pyg_test.edge_label = G_pyg_test.edge_label.to(device)\n",
    "    G_pyg_test.edge_attr = G_pyg_test.edge_attr.to(device)\n",
    "\n",
    "    best_model = EGraphSAGE(node_in_channels=G_pyg_test.num_node_features, \n",
    "                       edge_in_channels=G_pyg_test.num_edge_features,\n",
    "                       hidden_channels=best_hidden_dim, \n",
    "                       out_channels=num_classes).to(device)\n",
    "\n",
    "    print(\"Loading model from\", best_model_path)\n",
    "    best_model.load_state_dict(th.load(best_model_path))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    best_model.eval()\n",
    "\n",
    "    print(\"inference start\")\n",
    "    with th.no_grad():\n",
    "            \n",
    "        try:\n",
    "            out = best_model(G_pyg_test)\n",
    "            \n",
    "        except Exception as forward_error:\n",
    "            print(f\"Error during forward/backward pass at {forward_error}\")\n",
    "\n",
    "    print(\"inference done\")\n",
    "    \n",
    "    pred_labels = out.argmax(dim=1).cpu()\n",
    "    all_test_labels = G_pyg_test.edge_label.cpu()\n",
    "\n",
    "    if adversarial:\n",
    "\n",
    "        # Create a boolean mask where the label is NOT equal to the adversarial class\n",
    "        adversarial_mask = all_test_labels == ADVERSARIAL_CLASS_LABEL\n",
    "\n",
    "        # Print the class that the adversarial samples are classified as\n",
    "        cm_adversarial = confusion_matrix(all_test_labels[adversarial_mask], pred_labels[adversarial_mask], labels=range(len(class_map) + 1))\n",
    "        print(\"Adversarial confusion matrix:\", cm_adversarial)\n",
    "\n",
    "        # Apply the mask to both labels and predictions\n",
    "        all_test_labels = all_test_labels[~adversarial_mask]\n",
    "        pred_labels = pred_labels[~adversarial_mask]\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"class_map\", class_map)\n",
    "    # Generate a report\n",
    "    cm = confusion_matrix(all_test_labels, pred_labels, labels=range(len(class_map)))\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    report = classification_report(all_test_labels, pred_labels, target_names=class_map, digits=4)\n",
    "    print(report)\n",
    "\n",
    "    \n",
    "eval(G_pyg_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bcb5e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_traffic_to_attacker(graph, ratio=0.1, num_injected_nodes=1, is_attack=False):\n",
    "    edge_index = graph.edge_index.clone()\n",
    "    edge_attr = graph.edge_attr.clone()\n",
    "    edge_label = graph.edge_label.clone()\n",
    "    x = graph.x.clone()\n",
    "\n",
    "    num_edges = edge_index.size(1)\n",
    "    feature_dim = graph.x.size(1)\n",
    "\n",
    "    # 1. Identify attacker nodes\n",
    "    attacker_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "    attacker_nodes = th.unique(edge_index[:, attacker_edges])\n",
    "    if attacker_nodes.numel() == 0:\n",
    "        raise ValueError(\"No attacker nodes found.\")\n",
    "\n",
    "    # 2. Sample benign edge feature pool\n",
    "    if is_attack:\n",
    "        attack_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[attack_edges]\n",
    "    else:\n",
    "        benign_edges = (edge_label == BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[benign_edges]\n",
    "\n",
    "    # 3. Inject new nodes\n",
    "    original_num_nodes = x.size(0)\n",
    "\n",
    "    new_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "    x = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "    # 4. Inject edges from injected nodes to attacker nodes\n",
    "    num_to_inject = max(1, int(ratio * num_edges))\n",
    "    new_edges = []\n",
    "    new_attrs = []\n",
    "    new_labels = []\n",
    "\n",
    "    \n",
    "    for _ in range(num_to_inject):\n",
    "        src = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\n",
    "        dst = attacker_nodes[random.randint(0, len(attacker_nodes) - 1)].item()\n",
    "\n",
    "        new_edges.append([src, dst])\n",
    "        attr = inject_edge_attr_pool[random.randint(0, len(inject_edge_attr_pool) - 1)]\n",
    "        new_attrs.append(attr)\n",
    "        new_labels.append(ADVERSARIAL_CLASS_LABEL)\n",
    "\n",
    "    # Create a new empty graph to store the injected edges\n",
    "    new_graph = Data()\n",
    "\n",
    "    # 5. Merge into graph\n",
    "    if new_edges:\n",
    "        new_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "        new_attrs = th.stack(new_attrs)\n",
    "        new_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "        new_graph.edge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "        new_graph.edge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "        new_graph.edge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "        new_graph.x = x\n",
    "\n",
    "        # new_graph.first_injected_node_idx = original_num_nodes # Store injected node indices\n",
    "\n",
    "    return new_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "632c5bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_raw_port/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 135    0    0  619  585  151 2347 2469 1400  441    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[   45    57   196    29    15    13     0    20     0    27]\n",
      " [    9    60   198     8     4    13     0    22     1    34]\n",
      " [  307    70  1437    59    72   153     0   123     7   225]\n",
      " [  883   157  1803   420   435   761     0   712    47  1461]\n",
      " [  338   151   357   326   511   467     0   460    47   980]\n",
      " [   89     3   220    70    53 31581     0    98     8   200]\n",
      " [   37     0    39    39    68    79 32686   103    11   219]\n",
      " [  195     3   340   159   181   347     0   315    20   538]\n",
      " [   24     0    11    26    21    38     0    39     1    67]\n",
      " [    4     0     2     1     3     3     0     4     0     9]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0233    0.1119    0.0386       402\n",
      "     Backdoors     0.1198    0.1719    0.1412       349\n",
      "           DoS     0.3122    0.5858    0.4073      2453\n",
      "      Exploits     0.3694    0.0629    0.1075      6679\n",
      "       Fuzzers     0.3749    0.1405    0.2044      3637\n",
      "       Generic     0.9440    0.9771    0.9602     32322\n",
      "        Normal     1.0000    0.9821    0.9910     33281\n",
      "Reconnaissance     0.1661    0.1501    0.1577      2098\n",
      "     Shellcode     0.0070    0.0044    0.0054       227\n",
      "         Worms     0.0024    0.3462    0.0048        26\n",
      "\n",
      "      accuracy                         0.8231     81474\n",
      "     macro avg     0.3319    0.3533    0.3018     81474\n",
      "  weighted avg     0.8443    0.8231    0.8208     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inject Attack Traffic to Attacker Nodes\n",
    "G_pyg_test = G_pyg_test.cpu()\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=True)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64c37b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_raw_port/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 281    0    0  458  142  133 3476  299 3358    0    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[   38   175    81     0     0    13     8    87     0     0]\n",
      " [   46   190    29     2     0    13     5    64     0     0]\n",
      " [  311   939   533    28     4   132    75   431     0     0]\n",
      " [  511  1400   831   181    20   655   448  2633     0     0]\n",
      " [  120   488   140   111    30   386   373  1989     0     0]\n",
      " [   59  1194    71    22     2 30530    46   398     0     0]\n",
      " [    6    33    26    10     2    74 32729   401     0     0]\n",
      " [   97   254   145    53    15   294   179  1061     0     0]\n",
      " [    5    12     8    12     0    34    16   140     0     0]\n",
      " [    1     3     2     0     0     3     2    15     0     0]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0318    0.0945    0.0476       402\n",
      "     Backdoors     0.0405    0.5444    0.0754       349\n",
      "           DoS     0.2856    0.2173    0.2468      2453\n",
      "      Exploits     0.4320    0.0271    0.0510      6679\n",
      "       Fuzzers     0.4110    0.0082    0.0162      3637\n",
      "       Generic     0.9501    0.9446    0.9473     32322\n",
      "        Normal     0.9660    0.9834    0.9746     33281\n",
      "Reconnaissance     0.1470    0.5057    0.2278      2098\n",
      "     Shellcode     0.0000    0.0000    0.0000       227\n",
      "         Worms     0.0000    0.0000    0.0000        26\n",
      "\n",
      "      accuracy                         0.8014     81474\n",
      "     macro avg     0.3264    0.3325    0.2587     81474\n",
      "  weighted avg     0.8380    0.8014    0.7927     81474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Inject BENIGN Traffic to Attacker Nodes\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=False)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bf0e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_random_nodes(graph, ratio=0.1, num_injected_nodes=1):\n",
    "\tedge_index = graph.edge_index.clone()\n",
    "\tedge_attr = graph.edge_attr.clone()\n",
    "\tedge_label = graph.edge_label.clone()\n",
    "\tx = graph.x.clone()\n",
    "\n",
    "\tnum_edges = edge_index.size(1)\n",
    "\tfeature_dim = graph.x.size(1)\n",
    "\n",
    "\t# 1. Inject new nodes\n",
    "\toriginal_num_nodes = x.size(0)\n",
    "\tnew_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "\tx = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "\t# 2. Inject random edges\n",
    "\tnum_to_inject = max(1, int(ratio * num_edges))\n",
    "\tnew_edges = []\n",
    "\tnew_attrs = []\n",
    "\tnew_labels = []\n",
    "\n",
    "\tfor _ in range(num_to_inject):\n",
    "\t\tsrc = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\t\tdst = random.randint(0, original_num_nodes - 1)  # to existing nodes\n",
    "\n",
    "\t\tnew_edges.append([src, dst])\n",
    "\t\tattr = edge_attr[random.randint(0, len(edge_attr) - 1)]  # Randomly sample edge attributes\n",
    "\t\tnew_attrs.append(attr)\n",
    "\t\tnew_labels.append(ADVERSARIAL_CLASS_LABEL)  # Assign benign class label to new edges\n",
    "\n",
    "\t# 3. Merge into graph\n",
    "\tif new_edges:\n",
    "\t\tnew_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "\t\tnew_attrs = th.stack(new_attrs)\n",
    "\t\tnew_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "\t\tedge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "\t\tedge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "\t\tedge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "\n",
    "\t# Create a new graph with the injected nodes and edges\n",
    "\tnew_graph = Data(\n",
    "\t\tedge_index=edge_index,\n",
    "\t\tedge_attr=edge_attr,\n",
    "\t\tedge_label=edge_label,\n",
    "\t\tx=x\n",
    "\t)\n",
    "\n",
    "\treturn new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "abb63898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_raw_port/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 105    0   42  144  227   44 6324   44 1173   44    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[  198    86    42     1    39    13     0     2    11    10]\n",
      " [  158    80    30     2    31    14     0    20     8     6]\n",
      " [  887   611   320    11   231   165     0   114    83    31]\n",
      " [ 1753   701   430    97  1341   819     0   741   584   213]\n",
      " [  506   252   121   101  1116   499     0   409   479   154]\n",
      " [  165   109    47    12   174 31590     0   100    93    32]\n",
      " [   41    24    24    10   216    84 32686    72    79    45]\n",
      " [  262   159   139    27   528   359     0   271   252   101]\n",
      " [   16    12     9     3    62    42     0    41    29    13]\n",
      " [    4     1     2     1     5     3     0     1     7     2]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0496    0.4925    0.0902       402\n",
      "     Backdoors     0.0393    0.2292    0.0671       349\n",
      "           DoS     0.2749    0.1305    0.1769      2453\n",
      "      Exploits     0.3660    0.0145    0.0279      6679\n",
      "       Fuzzers     0.2982    0.3068    0.3024      3637\n",
      "       Generic     0.9405    0.9774    0.9586     32322\n",
      "        Normal     1.0000    0.9821    0.9910     33281\n",
      "Reconnaissance     0.1530    0.1292    0.1401      2098\n",
      "     Shellcode     0.0178    0.1278    0.0313       227\n",
      "         Worms     0.0033    0.0769    0.0063        26\n",
      "\n",
      "      accuracy                         0.8148     81474\n",
      "     macro avg     0.3143    0.3467    0.2792     81474\n",
      "  weighted avg     0.8376    0.8148    0.8106     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inject Random Nodes in the graph\n",
    "injected_graph = inject_random_nodes(G_pyg_test, 0.1, num_injected_nodes=1)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
