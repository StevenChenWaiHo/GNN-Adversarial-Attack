{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb9e1b2-1c5f-49e3-82b4-3bfe52cabad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "=====Experiment=====\n",
    "Dataset: UNSW-NB15 dataset\n",
    "\n",
    "Training with whole graph\n",
    "Downsampled 90% normal traffic randomly\n",
    "Split train and test randomly\n",
    "\n",
    "Combined IP and Port features\n",
    "'''\n",
    "\n",
    "from torch_geometric.utils import from_networkx, add_self_loops, degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "# import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Datasets.UNSW_NB15.UNSW_NB15_config import UNSW_NB15_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9ef09a-d405-43b8-971e-fe9e6a592c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1414208/3491705315.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "Normal            221876\n",
      "Generic           215481\n",
      "Exploits           44525\n",
      "Fuzzers            24246\n",
      "DoS                16353\n",
      "Reconnaissance     13987\n",
      "Analysis            2677\n",
      "Backdoor            1795\n",
      "Shellcode           1511\n",
      "Backdoors            534\n",
      "Worms                174\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    321283\n",
      "0    221876\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "csv_file_name = \"all_raw_downsampled\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n",
    "\n",
    "DATASET_NAME = \"UNSW_NB15\"\n",
    "\n",
    "SOURCE_IP_COL_NAME = UNSW_NB15_Config.SOURCE_IP_COL_NAME\n",
    "DESTINATION_IP_COL_NAME = UNSW_NB15_Config.DESTINATION_IP_COL_NAME\n",
    "SOURCE_PORT_COL_NAME = UNSW_NB15_Config.SOURCE_PORT_COL_NAME\n",
    "DESTINATION_PORT_COL_NAME = UNSW_NB15_Config.DESTINATION_PORT_COL_NAME\n",
    "\n",
    "ATTACK_CLASS_COL_NAME = UNSW_NB15_Config.ATTACK_CLASS_COL_NAME\n",
    "IS_ATTACK_COL_NAME = UNSW_NB15_Config.IS_ATTACK_COL_NAME\n",
    "\n",
    "BENIGN_CLASS_NAME = UNSW_NB15_Config.BENIGN_CLASS_NAME\n",
    "\n",
    "TIME_COLS = UNSW_NB15_Config.TIME_COL_NAMES\n",
    "\n",
    "MULTICLASS = True\n",
    "label_col = ATTACK_CLASS_COL_NAME if MULTICLASS else IS_ATTACK_COL_NAME\n",
    "\n",
    "print(data[ATTACK_CLASS_COL_NAME].value_counts())\n",
    "print(data[IS_ATTACK_COL_NAME].value_counts())\n",
    "\n",
    "if MULTICLASS:\n",
    "    data.drop(columns=[IS_ATTACK_COL_NAME], inplace=True)\n",
    "else:\n",
    "    data.drop(columns=[ATTACK_CLASS_COL_NAME], inplace=True)\n",
    "\n",
    "checkpoint_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, f\"whole_graph_combined_ports/checkpoints_{csv_file_name}.pth\")\n",
    "best_model_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, f\"whole_graph_combined_ports/best_model_{csv_file_name}.pth\")\n",
    "\n",
    "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(best_model_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449a1af1-1d3d-4179-9628-7c2ec551ce0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srcip</th>\n",
       "      <th>sport</th>\n",
       "      <th>dstip</th>\n",
       "      <th>dsport</th>\n",
       "      <th>state</th>\n",
       "      <th>dur</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>...</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>attack_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.40.85.1</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0.0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>INT</td>\n",
       "      <td>50.004341</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.166.0.6</td>\n",
       "      <td>2142</td>\n",
       "      <td>149.171.126.4</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>132</td>\n",
       "      <td>164</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175.45.176.0</td>\n",
       "      <td>13284</td>\n",
       "      <td>149.171.126.16</td>\n",
       "      <td>80</td>\n",
       "      <td>FIN</td>\n",
       "      <td>2.390390</td>\n",
       "      <td>1362</td>\n",
       "      <td>268</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reconnaissance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.166.0.3</td>\n",
       "      <td>42587</td>\n",
       "      <td>149.171.126.8</td>\n",
       "      <td>25</td>\n",
       "      <td>FIN</td>\n",
       "      <td>34.077175</td>\n",
       "      <td>37358</td>\n",
       "      <td>3380</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.40.170.2</td>\n",
       "      <td>0</td>\n",
       "      <td>10.40.170.2</td>\n",
       "      <td>0</td>\n",
       "      <td>INT</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543154</th>\n",
       "      <td>175.45.176.0</td>\n",
       "      <td>47439</td>\n",
       "      <td>149.171.126.10</td>\n",
       "      <td>53</td>\n",
       "      <td>INT</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>Generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543155</th>\n",
       "      <td>175.45.176.0</td>\n",
       "      <td>47439</td>\n",
       "      <td>149.171.126.10</td>\n",
       "      <td>53</td>\n",
       "      <td>INT</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>Generic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543156</th>\n",
       "      <td>59.166.0.5</td>\n",
       "      <td>53521</td>\n",
       "      <td>149.171.126.7</td>\n",
       "      <td>21</td>\n",
       "      <td>CON</td>\n",
       "      <td>1.086072</td>\n",
       "      <td>1940</td>\n",
       "      <td>2404</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543157</th>\n",
       "      <td>175.45.176.0</td>\n",
       "      <td>17293</td>\n",
       "      <td>149.171.126.17</td>\n",
       "      <td>110</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.942984</td>\n",
       "      <td>574</td>\n",
       "      <td>676</td>\n",
       "      <td>62</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Exploits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543158</th>\n",
       "      <td>175.45.176.0</td>\n",
       "      <td>17293</td>\n",
       "      <td>149.171.126.17</td>\n",
       "      <td>110</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.942984</td>\n",
       "      <td>574</td>\n",
       "      <td>676</td>\n",
       "      <td>62</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Exploits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543159 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               srcip  sport           dstip dsport state        dur  sbytes  \\\n",
       "0         10.40.85.1      0       224.0.0.5      0   INT  50.004341     384   \n",
       "1         59.166.0.6   2142   149.171.126.4     53   CON   0.001134     132   \n",
       "2       175.45.176.0  13284  149.171.126.16     80   FIN   2.390390    1362   \n",
       "3         59.166.0.3  42587   149.171.126.8     25   FIN  34.077175   37358   \n",
       "4        10.40.170.2      0     10.40.170.2      0   INT   0.000000      46   \n",
       "...              ...    ...             ...    ...   ...        ...     ...   \n",
       "543154  175.45.176.0  47439  149.171.126.10     53   INT   0.000001     114   \n",
       "543155  175.45.176.0  47439  149.171.126.10     53   INT   0.000001     114   \n",
       "543156    59.166.0.5  53521   149.171.126.7     21   CON   1.086072    1940   \n",
       "543157  175.45.176.0  17293  149.171.126.17    110   CON   0.942984     574   \n",
       "543158  175.45.176.0  17293  149.171.126.17    110   CON   0.942984     574   \n",
       "\n",
       "        dbytes  sttl  dttl  ...  is_ftp_login  ct_ftp_cmd  ct_srv_src  \\\n",
       "0            0     1     0  ...           0.0         0.0           2   \n",
       "1          164    31    29  ...           0.0         0.0          12   \n",
       "2          268   254   252  ...           0.0         0.0           5   \n",
       "3         3380    31    29  ...           0.0         0.0           1   \n",
       "4            0     0     0  ...           0.0         0.0           2   \n",
       "...        ...   ...   ...  ...           ...         ...         ...   \n",
       "543154       0   254     0  ...           0.0         NaN          15   \n",
       "543155       0   254     0  ...           0.0         NaN          15   \n",
       "543156    2404    31    29  ...           2.0         2.0           2   \n",
       "543157     676    62   252  ...           0.0         NaN           2   \n",
       "543158     676    62   252  ...           0.0         NaN           1   \n",
       "\n",
       "        ct_srv_dst  ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  \\\n",
       "0                4           4           2                 2   \n",
       "1                7           1           2                 2   \n",
       "2                2           2           1                 1   \n",
       "3                1          12          10                 1   \n",
       "4                2           2           2                 2   \n",
       "...            ...         ...         ...               ...   \n",
       "543154          15          15          15                15   \n",
       "543155          15          15          15                15   \n",
       "543156           2           3           3                 2   \n",
       "543157           1           2           4                 2   \n",
       "543158           1           2           4                 2   \n",
       "\n",
       "        ct_dst_sport_ltm  ct_dst_src_ltm      attack_cat  \n",
       "0                      4               2          Normal  \n",
       "1                      1               1          Normal  \n",
       "2                      1               1  Reconnaissance  \n",
       "3                      1               2          Normal  \n",
       "4                      2               2          Normal  \n",
       "...                  ...             ...             ...  \n",
       "543154                15              15         Generic  \n",
       "543155                15              15         Generic  \n",
       "543156                 2               3          Normal  \n",
       "543157                 2               2        Exploits  \n",
       "543158                 2               2        Exploits  \n",
       "\n",
       "[543159 rows x 44 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns=UNSW_NB15_Config.DROP_COLS,inplace=True)\n",
    "data.drop(columns=UNSW_NB15_Config.TIME_COL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2c690c-86a4-49f7-aa9c-58f94529547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME].apply(str)\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME].apply(str)\n",
    "\n",
    "# # Combine Port and IP\n",
    "data[SOURCE_PORT_COL_NAME] = data[SOURCE_PORT_COL_NAME].apply(str)\n",
    "data[DESTINATION_PORT_COL_NAME] = data[DESTINATION_PORT_COL_NAME].apply(str)\n",
    "\n",
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME] + ':' + data[SOURCE_PORT_COL_NAME]\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME] + ':' + data[DESTINATION_PORT_COL_NAME]\n",
    "data.drop(columns=[SOURCE_PORT_COL_NAME,DESTINATION_PORT_COL_NAME],inplace=True)\n",
    "\n",
    "# data[SOURCE_PORT_COL_NAME] = pd.to_numeric(data[SOURCE_PORT_COL_NAME], errors='coerce').fillna(0).astype(int)\n",
    "# data[DESTINATION_PORT_COL_NAME] = pd.to_numeric(data[DESTINATION_PORT_COL_NAME], errors='coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5651ef5b-0a9d-4641-aad7-5738092c46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                      srcip               dstip state        dur  sbytes  \\\n",
      "0             10.40.85.1:0         224.0.0.5:0   INT  50.004341     384   \n",
      "1          59.166.0.6:2142    149.171.126.4:53   CON   0.001134     132   \n",
      "2       175.45.176.0:13284   149.171.126.16:80   FIN   2.390390    1362   \n",
      "3         59.166.0.3:42587    149.171.126.8:25   FIN  34.077175   37358   \n",
      "4            10.40.170.2:0       10.40.170.2:0   INT   0.000000      46   \n",
      "...                    ...                 ...   ...        ...     ...   \n",
      "543154  175.45.176.0:47439   149.171.126.10:53   INT   0.000001     114   \n",
      "543155  175.45.176.0:47439   149.171.126.10:53   INT   0.000001     114   \n",
      "543156    59.166.0.5:53521    149.171.126.7:21   CON   1.086072    1940   \n",
      "543157  175.45.176.0:17293  149.171.126.17:110   CON   0.942984     574   \n",
      "543158  175.45.176.0:17293  149.171.126.17:110   CON   0.942984     574   \n",
      "\n",
      "        dbytes  sttl  dttl  sloss  dloss  ...  is_ftp_login  ct_ftp_cmd  \\\n",
      "0            0     1     0      0      0  ...           0.0         0.0   \n",
      "1          164    31    29      0      0  ...           0.0         0.0   \n",
      "2          268   254   252      6      1  ...           0.0         0.0   \n",
      "3         3380    31    29     18      8  ...           0.0         0.0   \n",
      "4            0     0     0      0      0  ...           0.0         0.0   \n",
      "...        ...   ...   ...    ...    ...  ...           ...         ...   \n",
      "543154       0   254     0      0      0  ...           0.0         NaN   \n",
      "543155       0   254     0      0      0  ...           0.0         NaN   \n",
      "543156    2404    31    29      8     10  ...           2.0         2.0   \n",
      "543157     676    62   252      5      6  ...           0.0         NaN   \n",
      "543158     676    62   252      5      6  ...           0.0         NaN   \n",
      "\n",
      "        ct_srv_src  ct_srv_dst  ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  \\\n",
      "0                2           4           4           2                 2   \n",
      "1               12           7           1           2                 2   \n",
      "2                5           2           2           1                 1   \n",
      "3                1           1          12          10                 1   \n",
      "4                2           2           2           2                 2   \n",
      "...            ...         ...         ...         ...               ...   \n",
      "543154          15          15          15          15                15   \n",
      "543155          15          15          15          15                15   \n",
      "543156           2           2           3           3                 2   \n",
      "543157           2           1           2           4                 2   \n",
      "543158           1           1           2           4                 2   \n",
      "\n",
      "        ct_dst_sport_ltm  ct_dst_src_ltm      attack_cat  \n",
      "0                      4               2          Normal  \n",
      "1                      1               1          Normal  \n",
      "2                      1               1  Reconnaissance  \n",
      "3                      1               2          Normal  \n",
      "4                      2               2          Normal  \n",
      "...                  ...             ...             ...  \n",
      "543154                15              15         Generic  \n",
      "543155                15              15         Generic  \n",
      "543156                 2               3          Normal  \n",
      "543157                 2               2        Exploits  \n",
      "543158                 2               2        Exploits  \n",
      "\n",
      "[543159 rows x 44 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7fb458-ca34-42ca-a8af-f8e1609aff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns = UNSW_NB15_Config.CATEGORICAL_COLS) # One Hot Encoding for categorical data\n",
    "converted_categorical_cols = [col for col in data.columns if col.startswith(tuple(UNSW_NB15_Config.CATEGORICAL_COLS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d96115-31f9-48cb-b3e6-7853d2d253cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                      srcip               dstip        dur  sbytes  dbytes  \\\n",
      "0             10.40.85.1:0         224.0.0.5:0  50.004341     384       0   \n",
      "1          59.166.0.6:2142    149.171.126.4:53   0.001134     132     164   \n",
      "2       175.45.176.0:13284   149.171.126.16:80   2.390390    1362     268   \n",
      "3         59.166.0.3:42587    149.171.126.8:25  34.077175   37358    3380   \n",
      "4            10.40.170.2:0       10.40.170.2:0   0.000000      46       0   \n",
      "...                    ...                 ...        ...     ...     ...   \n",
      "543154  175.45.176.0:47439   149.171.126.10:53   0.000001     114       0   \n",
      "543155  175.45.176.0:47439   149.171.126.10:53   0.000001     114       0   \n",
      "543156    59.166.0.5:53521    149.171.126.7:21   1.086072    1940    2404   \n",
      "543157  175.45.176.0:17293  149.171.126.17:110   0.942984     574     676   \n",
      "543158  175.45.176.0:17293  149.171.126.17:110   0.942984     574     676   \n",
      "\n",
      "        sttl  dttl  sloss  dloss         Sload  ...  state_ECO  state_FIN  \\\n",
      "0          1     0      0      0  5.119556e+01  ...      False      False   \n",
      "1         31    29      0      0  4.656085e+05  ...      False      False   \n",
      "2        254   252      6      1  4.233619e+03  ...      False       True   \n",
      "3         31    29     18      8  8.601652e+03  ...      False       True   \n",
      "4          0     0      0      0  0.000000e+00  ...      False      False   \n",
      "...      ...   ...    ...    ...           ...  ...        ...        ...   \n",
      "543154   254     0      0      0  4.560000e+08  ...      False      False   \n",
      "543155   254     0      0      0  4.560000e+08  ...      False      False   \n",
      "543156    31    29      8     10  1.387017e+04  ...      False      False   \n",
      "543157    62   252      5      6  4.470914e+03  ...      False      False   \n",
      "543158    62   252      5      6  4.470914e+03  ...      False      False   \n",
      "\n",
      "        state_INT  state_MAS  state_PAR  state_REQ  state_RST  state_TST  \\\n",
      "0            True      False      False      False      False      False   \n",
      "1           False      False      False      False      False      False   \n",
      "2           False      False      False      False      False      False   \n",
      "3           False      False      False      False      False      False   \n",
      "4            True      False      False      False      False      False   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "543154       True      False      False      False      False      False   \n",
      "543155       True      False      False      False      False      False   \n",
      "543156      False      False      False      False      False      False   \n",
      "543157      False      False      False      False      False      False   \n",
      "543158      False      False      False      False      False      False   \n",
      "\n",
      "        state_TXD  state_URH  \n",
      "0           False      False  \n",
      "1           False      False  \n",
      "2           False      False  \n",
      "3           False      False  \n",
      "4           False      False  \n",
      "...           ...        ...  \n",
      "543154      False      False  \n",
      "543155      False      False  \n",
      "543156      False      False  \n",
      "543157      False      False  \n",
      "543158      False      False  \n",
      "\n",
      "[543159 rows x 56 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "data.drop(columns=['index'],inplace=True)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1df5c4c-70a2-4566-ae5e-ee3dcc6037a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 dur        sbytes        dbytes           sttl  \\\n",
      "count  543159.000000  5.431590e+05  5.431590e+05  543159.000000   \n",
      "mean        0.706760  5.136572e+03  1.936909e+04     157.197364   \n",
      "std        12.637229  1.202311e+05  1.390925e+05     108.452474   \n",
      "min         0.000000  0.000000e+00  0.000000e+00       0.000000   \n",
      "25%         0.000007  1.140000e+02  0.000000e+00      31.000000   \n",
      "50%         0.000011  2.000000e+02  0.000000e+00     254.000000   \n",
      "75%         0.072088  1.580000e+03  1.940000e+03     254.000000   \n",
      "max      8760.776367  1.435577e+07  1.465753e+07     255.000000   \n",
      "\n",
      "                dttl          sloss          dloss         Sload  \\\n",
      "count  543159.000000  543159.000000  543159.000000  5.431590e+05   \n",
      "mean       38.850764       3.800661       8.729770  6.877595e+07   \n",
      "std        77.034389      45.616565      50.136204  1.420534e+08   \n",
      "min         0.000000       0.000000       0.000000  0.000000e+00   \n",
      "25%         0.000000       0.000000       0.000000  3.705672e+05   \n",
      "50%         0.000000       0.000000       0.000000  4.560000e+07   \n",
      "75%        29.000000       3.000000       4.000000  8.888889e+07   \n",
      "max       254.000000    5319.000000    5507.000000  5.988000e+09   \n",
      "\n",
      "              Dload          Spkts  ...  ct_flw_http_mthd   is_ftp_login  \\\n",
      "count  5.431590e+05  543159.000000  ...     543159.000000  543159.000000   \n",
      "mean   1.148111e+06      20.369921  ...          0.088724       0.011490   \n",
      "std    3.127653e+06     101.923505  ...          0.566327       0.109623   \n",
      "min    0.000000e+00       0.000000  ...          0.000000       0.000000   \n",
      "25%    0.000000e+00       2.000000  ...          0.000000       0.000000   \n",
      "50%    0.000000e+00       2.000000  ...          0.000000       0.000000   \n",
      "75%    4.087816e+05      14.000000  ...          0.000000       0.000000   \n",
      "max    2.274587e+07   10646.000000  ...         36.000000       4.000000   \n",
      "\n",
      "          ct_ftp_cmd     ct_srv_src     ct_srv_dst     ct_dst_ltm  \\\n",
      "count  543159.000000  543159.000000  543159.000000  543159.000000   \n",
      "mean        0.013029      15.008556      14.833110      10.306825   \n",
      "std         0.141497      14.229735      14.305878      10.989668   \n",
      "min         0.000000       1.000000       1.000000       1.000000   \n",
      "25%         0.000000       3.000000       3.000000       2.000000   \n",
      "50%         0.000000       9.000000       8.000000       5.000000   \n",
      "75%         0.000000      26.000000      26.000000      17.000000   \n",
      "max         8.000000      67.000000      67.000000      67.000000   \n",
      "\n",
      "          ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \n",
      "count  543159.000000     543159.000000     543159.000000   543159.000000  \n",
      "mean       10.837838          9.343288          7.209839       13.766985  \n",
      "std        10.967546         11.391238          8.069018       14.972826  \n",
      "min         1.000000          1.000000          1.000000        1.000000  \n",
      "25%         2.000000          1.000000          1.000000        1.000000  \n",
      "50%         6.000000          2.000000          2.000000        5.000000  \n",
      "75%        17.000000         17.000000         15.000000       26.000000  \n",
      "max        67.000000         67.000000         60.000000       67.000000  \n",
      "\n",
      "[8 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_norm = UNSW_NB15_Config.COLS_TO_NORM\n",
    "print(data[cols_to_norm].describe()) # Check if there's any too large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea95177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All other columns processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_numeric_issues(df, cols_to_norm):\n",
    "    for col in cols_to_norm:\n",
    "        try:\n",
    "            # Try to coerce to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Try to clip the column\n",
    "            df[col] = df[col].clip(lower=-1e9, upper=1e9)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Column '{col}' failed with error: {e}\")\n",
    "            print(f\"  - Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "            print(f\"  - Data type: {df[col].dtype}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n✅ All other columns processed successfully.\")\n",
    "\n",
    "check_numeric_issues(data, UNSW_NB15_Config.COLS_TO_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37256006-abc1-44aa-8e74-46d05dc6ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c6e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis' 'Backdoor' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic'\n",
      " 'Normal' 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "Attack label mapping: {'Analysis': 0, 'Backdoor': 1, 'Backdoors': 2, 'DoS': 3, 'Exploits': 4, 'Fuzzers': 5, 'Generic': 6, 'Normal': 7, 'Reconnaissance': 8, 'Shellcode': 9, 'Worms': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_classes = 2\n",
    "class_map = [0, 1]\n",
    "if MULTICLASS:\n",
    "    le = LabelEncoder()\n",
    "    attack_labels = le.fit_transform(data[ATTACK_CLASS_COL_NAME])\n",
    "    class_map = le.classes_\n",
    "    print(class_map)\n",
    "    print(\"Attack label mapping:\", dict(zip(class_map, range(len(class_map)))))\n",
    "    data[ATTACK_CLASS_COL_NAME] = attack_labels\n",
    "    num_classes = len(class_map)\n",
    "    class_dict = {le.inverse_transform([i])[0]: i for i in range(len(le.classes_))}\n",
    "\n",
    "BENIGN_CLASS_LABEL = le.transform([BENIGN_CLASS_NAME])[0] if MULTICLASS else 0\n",
    "ADVERSARIAL_CLASS_LABEL = len(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d35f4cdd-2716-431f-af50-b34cc3d2d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'state_ACC', 'state_CLO', 'state_CON', 'state_ECO', 'state_FIN', 'state_INT', 'state_MAS', 'state_PAR', 'state_REQ', 'state_RST', 'state_TST', 'state_TXD', 'state_URH']\n",
      "Number of training samples: 461685\n",
      "attack_cat\n",
      "7     188595\n",
      "6     183159\n",
      "4      37846\n",
      "5      20609\n",
      "3      13900\n",
      "8      11889\n",
      "0       2275\n",
      "1       1526\n",
      "9       1284\n",
      "2        454\n",
      "10       148\n",
      "Name: count, dtype: int64\n",
      "Number of test samples: 81474\n",
      "attack_cat\n",
      "7     33281\n",
      "6     32322\n",
      "4      6679\n",
      "5      3637\n",
      "3      2453\n",
      "8      2098\n",
      "0       402\n",
      "1       269\n",
      "9       227\n",
      "2        80\n",
      "10       26\n",
      "Name: count, dtype: int64\n",
      "                      srcip                dstip       dur    sbytes  \\\n",
      "35798    175.45.176.0:10819   149.171.126.11:445  0.024183 -0.037815   \n",
      "447994  149.171.126.14:1043      175.45.176.1:53 -0.055926 -0.040527   \n",
      "23187    175.45.176.0:15079    149.171.126.18:80 -0.017630 -0.033324   \n",
      "477690   175.45.176.3:50624  149.171.126.16:5060 -0.055926 -0.030962   \n",
      "134305     59.166.0.1:44316     149.171.126.4:25 -0.005219  0.269111   \n",
      "\n",
      "          dbytes      sttl      dttl     sloss     dloss     Sload     Dload  \\\n",
      "35798  -0.137327  0.892582  2.766939 -0.039474 -0.154176 -0.555127 -0.366518   \n",
      "447994 -0.139253 -0.896222 -0.504331 -0.083318 -0.174121  0.692417 -0.367084   \n",
      "23187  -0.137327  0.892582  2.766939 -0.039474 -0.154176 -0.555023 -0.365900   \n",
      "477690 -0.139253  0.892582 -0.504331 -0.083318 -0.174121  4.122315 -0.367084   \n",
      "134305 -0.114953 -1.163620 -0.127875  0.311276 -0.014556 -0.551365 -0.353912   \n",
      "\n",
      "           Spkts     Dpkts      swin      dwin     stcpb     dtcpb   smeansz  \\\n",
      "35798  -0.101742 -0.159740  1.301681  1.303441  0.949065 -0.462943 -0.329273   \n",
      "447994 -0.180233 -0.216121 -0.768239 -0.767201 -0.721514 -0.721619  0.140425   \n",
      "23187  -0.101742 -0.159740  1.301681  1.303441  1.482943  1.483089  0.018175   \n",
      "477690 -0.180233 -0.216121 -0.768239 -0.767201 -0.721514 -0.721619  3.840098   \n",
      "134305  0.310332  0.178542  1.301681  1.303441  1.482943 -0.716221  3.930177   \n",
      "\n",
      "         dmeansz  trans_depth  res_bdy_len      Sjit      Djit       Stime  \\\n",
      "35798  -0.367140    -0.134353    -0.054182  0.153349 -0.076737  1421933745   \n",
      "447994 -0.526921    -0.134353    -0.054182 -0.080649 -0.189023  1424252930   \n",
      "23187  -0.367140    -0.134353    -0.054182  0.012609 -0.150581  1421931706   \n",
      "477690 -0.526921    -0.134353    -0.054182 -0.080649 -0.189023  1424255007   \n",
      "134305 -0.242867    -0.134353    -0.054182 -0.043228 -0.176821  1424222454   \n",
      "\n",
      "             Ltime   Sintpkt   Dintpkt    tcprtt    synack    ackdat  \\\n",
      "35798   1421933746 -0.008957  0.117394  3.760093  3.793317  3.354919   \n",
      "447994  1424252930 -0.060833 -0.055697 -0.293130 -0.270319 -0.288797   \n",
      "23187   1421931707 -0.037329  0.019030  2.932351  3.123823  2.438738   \n",
      "477690  1424255007 -0.060832 -0.055697 -0.293130 -0.270319 -0.288797   \n",
      "134305  1424222454 -0.054344 -0.040616 -0.280117 -0.250740 -0.284108   \n",
      "\n",
      "        is_sm_ips_ports  ct_state_ttl  ct_flw_http_mthd  is_ftp_login  \\\n",
      "35798         -0.027818     -0.098490         -0.156665     -0.104815   \n",
      "447994        -0.027818     -1.123447         -0.156665     -0.104815   \n",
      "23187         -0.027818     -0.098490         -0.156665     -0.104815   \n",
      "477690        -0.027818      0.926467         -0.156665     -0.104815   \n",
      "134305        -0.027818     -1.123447         -0.156665     -0.104815   \n",
      "\n",
      "        ct_ftp_cmd  ct_srv_src  ct_srv_dst  ct_dst_ltm  ct_src_ltm  \\\n",
      "35798    -0.092082   -0.633080   -0.617447   -0.755877   -0.805818   \n",
      "447994   -0.092082    1.545459    1.549497    2.337941    2.294240   \n",
      "23187    -0.092082   -0.984457   -0.966954   -0.482893   -0.714640   \n",
      "477690   -0.092082   -0.914182   -0.966954   -0.846871   -0.896996   \n",
      "134305   -0.092082   -0.984457   -0.966954   -0.482893   -0.441105   \n",
      "\n",
      "        ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  attack_cat  \\\n",
      "35798          -0.732431         -0.769591       -0.785890           5   \n",
      "447994          2.340109          1.709028        1.551680           7   \n",
      "23187          -0.732431         -0.769591       -0.852678           4   \n",
      "477690         -0.732431         -0.769591       -0.852678           6   \n",
      "134305         -0.732431         -0.769591       -0.785890           7   \n",
      "\n",
      "        state_ACC  state_CLO  state_CON  state_ECO  state_FIN  state_INT  \\\n",
      "35798       False      False      False      False       True      False   \n",
      "447994      False      False      False      False      False       True   \n",
      "23187       False      False      False      False       True      False   \n",
      "477690      False      False      False      False      False       True   \n",
      "134305      False      False      False      False       True      False   \n",
      "\n",
      "        state_MAS  state_PAR  state_REQ  state_RST  state_TST  state_TXD  \\\n",
      "35798       False      False      False      False      False      False   \n",
      "447994      False      False      False      False      False      False   \n",
      "23187       False      False      False      False      False      False   \n",
      "477690      False      False      False      False      False      False   \n",
      "134305      False      False      False      False      False      False   \n",
      "\n",
      "        state_URH                                                  h  \n",
      "35798       False  [0.024183325690751246, -0.037815308455400876, ...  \n",
      "447994      False  [-0.05592631735495768, -0.04052675608212099, -...  \n",
      "23187       False  [-0.017629992085505666, -0.03332395349089518, ...  \n",
      "477690      False  [-0.055926079960930894, -0.030961833472525516,...  \n",
      "134305      False  [-0.0052187158398693, 0.26911058210050526, -0....  \n"
     ]
    }
   ],
   "source": [
    "# 70% train, 15% validation, 15% test\n",
    "train_full_df, test_df = train_test_split(\n",
    "     data, test_size=0.15, random_state=42, stratify=data[label_col])\n",
    "\n",
    "\n",
    "# # Maintain the order of the rows in the original dataframe\n",
    "# train_full_df = train_full_df.sort_values(by=UNSW_NB15_Config.TIME_COL_NAMES)\n",
    "# test_df = test_df.sort_values(by=UNSW_NB15_Config.TIME_COL_NAMES)\n",
    "\n",
    "feature_cols = UNSW_NB15_Config.COLS_TO_NORM + converted_categorical_cols\n",
    "\n",
    "print('Feature Columns:', feature_cols)\n",
    "\n",
    "train_full_df['h'] = train_full_df[ feature_cols ].values.tolist()\n",
    "test_df['h'] = test_df[ feature_cols ].values.tolist()\n",
    "\n",
    "# X_train = train_full_df.drop(columns=[label_col])\n",
    "# X_val = val_df.drop(columns=[label_col])\n",
    "# X_test = test_df.drop(columns=[label_col])\n",
    "\n",
    "y_train = train_full_df[label_col]\n",
    "y_test = test_df[label_col]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_full_df))\n",
    "print(y_train.value_counts())\n",
    "print(\"Number of test samples:\", len(test_df))\n",
    "print(y_test.value_counts())\n",
    "\n",
    "print(train_full_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f83bd73-531f-42a0-9f73-e1fd98dce1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, source_ip_col, destination_ip_col, edge_attr, create_using=nx.MultiDiGraph(), **kwargs):\n",
    "    G_nx = nx.from_pandas_edgelist(df, source_ip_col, destination_ip_col, edge_attr, create_using=create_using, **kwargs)\n",
    "    G_pyg = from_networkx(G_nx)\n",
    "\n",
    "    num_nodes = G_pyg.num_nodes\n",
    "    num_edges = G_pyg.num_edges\n",
    "\n",
    "    G_pyg.x = th.ones(num_nodes, len(df['h'].iloc[0])) \n",
    "\n",
    "    edge_attr_list = []\n",
    "    edge_label_list = []\n",
    "\n",
    "    for u, v, key, data in G_nx.edges(keys=True, data=True):\n",
    "        edge_attr_list.append(data['h']) \n",
    "        edge_label_list.append(data[label_col]) \n",
    "\n",
    "    G_pyg.edge_attr = th.tensor(edge_attr_list, dtype=th.float32)\n",
    "    G_pyg.edge_label = th.tensor(edge_label_list, dtype=th.long)\n",
    "\n",
    "    print(\"Number of edges in G_pyg:\", num_edges)\n",
    "    print(\"Number of node in G_pyg:\", num_nodes)\n",
    "    print(\"Shape of node in G_pyg:\", G_pyg.x.shape)\n",
    "    print(\"Shape of edge attr in G_pyg:\", G_pyg.edge_attr.shape)\n",
    "    print(\"Shape of edge label in G_pyg:\", G_pyg.edge_label.shape)\n",
    "\n",
    "    return G_nx, G_pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8f9cdb-1316-461a-a927-8d67d90d6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 52521\n",
      "Shape of node in G_pyg: torch.Size([52521, 51])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 51])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41795339-6036-468f-9b9d-2bb68d78ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGELayerPyG(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_dim, out_channels, activation=F.relu):\n",
    "        super().__init__(aggr='mean')  # mean aggregation\n",
    "        self.W_msg = nn.Linear(in_channels + edge_dim, out_channels)\n",
    "        self.W_apply = nn.Linear(in_channels + out_channels, out_channels)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: [num_nodes, in_channels]\n",
    "        # edge_attr: [num_edges, edge_dim]\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j: features of source nodes (neighbours)\n",
    "        msg_input = th.cat([x_j, edge_attr], dim=1)\n",
    "        return self.W_msg(msg_input)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out: [num_nodes, out_channels]\n",
    "        combined = th.cat([x, aggr_out], dim=1)\n",
    "        out = self.W_apply(combined)\n",
    "        return self.activation(out)\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.lin = nn.Linear(in_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, data, z):\n",
    "        row, col = data.edge_index\n",
    "        # Concatenate the features of source and target nodes for each edge\n",
    "        edge_feat = th.cat([z[row], z[col]], dim=1)\n",
    "        return self.lin(edge_feat)\n",
    "\n",
    "class EGraphSAGE(nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels, out_channels, dropout=0.2):\n",
    "        super(EGraphSAGE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGELayerPyG(node_in_channels, edge_in_channels, hidden_channels)\n",
    "        self.conv2 = SAGELayerPyG(hidden_channels, edge_in_channels, hidden_channels)\n",
    "        self.mlp_predictor = MLPPredictor(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return self.mlp_predictor(data, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bca25fef-29d9-40cf-8910-16b24d530693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cccdc850-b98d-4836-b82b-67aa4b9e1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89157faf-e24b-49d6-9c90-6f71dae515b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d37f0-713b-4abc-8d7a-3e768ae9a2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 307790\n",
      "Number of node in G_pyg: 182726\n",
      "Shape of node in G_pyg: torch.Size([182726, 51])\n",
      "Shape of edge attr in G_pyg: torch.Size([307790, 51])\n",
      "Shape of edge label in G_pyg: torch.Size([307790])\n",
      "Number of edges in G_pyg: 153895\n",
      "Number of node in G_pyg: 96060\n",
      "Shape of node in G_pyg: torch.Size([96060, 51])\n",
      "Shape of edge attr in G_pyg: torch.Size([153895, 51])\n",
      "Shape of edge label in G_pyg: torch.Size([153895])\n",
      "Number of edges in G_pyg: 307790\n",
      "Number of node in G_pyg: 182512\n",
      "Shape of node in G_pyg: torch.Size([182512, 51])\n",
      "Shape of edge attr in G_pyg: torch.Size([307790, 51])\n",
      "Shape of edge label in G_pyg: torch.Size([307790])\n",
      "Number of edges in G_pyg: 153895\n",
      "Number of node in G_pyg: 96341\n",
      "Shape of node in G_pyg: torch.Size([96341, 51])\n",
      "Shape of edge attr in G_pyg: torch.Size([153895, 51])\n",
      "Shape of edge label in G_pyg: torch.Size([153895])\n",
      "Number of edges in G_pyg: 307790\n",
      "Number of node in G_pyg: 182467\n",
      "Shape of node in G_pyg: torch.Size([182467, 51])\n",
      "Shape of edge attr in G_pyg: torch.Size([307790, 51])\n",
      "Shape of edge label in G_pyg: torch.Size([307790])\n",
      "Number of edges in G_pyg: 153895\n",
      "Number of node in G_pyg: 96245\n",
      "Shape of node in G_pyg: torch.Size([96245, 51])\n",
      "Shape of edge attr in G_pyg: torch.Size([153895, 51])\n",
      "Shape of edge label in G_pyg: torch.Size([153895])\n",
      "Testing with learning rate: 0.001, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0960, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.7520, Validation Loss: 2.4429, Validation F1: 0.0960\n",
      "Best F1 Score at epoch 1: 0.3833, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7570, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7710, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7752, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8052, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8131, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8200, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8244, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8258, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8289, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8313, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8330, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8358, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8372, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8386, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8519, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 27: 0.8581, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8633, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8644, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8659, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 31: 0.8819, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 40: 0.8875, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 47: 0.8920, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 54: 0.8946, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 82: 0.8969, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2061, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5400, Validation Loss: 2.2722, Validation F1: 0.2061\n",
      "Best F1 Score at epoch 1: 0.5443, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.6704, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7220, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7455, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7874, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7896, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8035, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8094, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8166, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8189, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8289, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8334, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8356, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8385, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8411, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8547, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8572, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8578, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 27: 0.8579, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8588, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8775, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8795, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 32: 0.8828, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 33: 0.8879, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8882, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 41: 0.8887, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 43: 0.8904, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 59: 0.8907, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 61: 0.8925, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 71: 0.8925, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 81: 0.8939, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 87: 0.8939, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 88: 0.8950, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 95: 0.8967, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0075, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.7243, Validation Loss: 2.4853, Validation F1: 0.0075\n",
      "Best F1 Score at epoch 1: 0.0857, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.0869, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.3743, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.5157, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.6036, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.6740, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.7346, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.7700, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7856, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.7998, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8044, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8086, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8144, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8188, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8229, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8281, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8301, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8363, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8435, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 32: 0.8480, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 37: 0.8597, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8726, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 42: 0.8754, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 43: 0.8757, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 45: 0.8793, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 65: 0.8827, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 98: 0.8831, Parameters: lr=0.001, hidden_dim128, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.001, hidden_dim 128: 0.8922\n",
      "Testing with learning rate: 0.001, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3429, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4683, Validation Loss: 2.3288, Validation F1: 0.3429\n",
      "Best F1 Score at epoch 1: 0.4555, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.6194, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.6766, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7121, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7440, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7756, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7878, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8032, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8193, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8290, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8290, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8369, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8430, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8445, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8543, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8559, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8624, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.8650, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 34: 0.8766, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 35: 0.8800, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 44: 0.8843, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 54: 0.8866, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 60: 0.8876, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 65: 0.8897, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 80: 0.8904, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 89: 0.8935, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3736, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5484, Validation Loss: 2.2947, Validation F1: 0.3736\n",
      "Best F1 Score at epoch 1: 0.6727, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7283, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7605, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8017, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8047, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8220, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8241, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8246, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8318, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8419, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8466, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 29: 0.8546, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8609, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 35: 0.8782, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 42: 0.8873, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8895, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 62: 0.8920, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 64: 0.8926, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 67: 0.8929, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 75: 0.8931, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 80: 0.8939, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 86: 0.8939, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 90: 0.8959, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 92: 0.8961, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0642, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5979, Validation Loss: 2.4623, Validation F1: 0.0642\n",
      "Best F1 Score at epoch 1: 0.1836, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.3715, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.4740, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.5813, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.6716, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7339, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7355, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.7598, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.7985, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8036, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8060, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8090, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8127, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8139, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8190, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8312, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8368, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8532, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8651, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 29: 0.8657, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8721, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8728, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 38: 0.8742, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 42: 0.8753, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8761, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 44: 0.8779, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 52: 0.8792, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 56: 0.8793, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 58: 0.8803, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 64: 0.8861, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 92: 0.8940, Parameters: lr=0.001, hidden_dim128, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.001, hidden_dim 128: 0.8946\n",
      "Testing with learning rate: 0.001, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2348, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5756, Validation Loss: 2.3963, Validation F1: 0.2348\n",
      "Best F1 Score at epoch 2: 0.5690, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7908, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7927, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7988, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8007, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8037, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8160, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8233, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8245, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8301, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8436, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8493, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8505, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8620, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 32: 0.8692, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.8747, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8814, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 40: 0.8816, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 44: 0.8852, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 51: 0.8863, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 56: 0.8868, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 58: 0.8900, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 81: 0.8947, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2937, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.6583, Validation Loss: 2.4962, Validation F1: 0.2937\n",
      "Best F1 Score at epoch 1: 0.4400, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.4606, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.6670, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7435, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7580, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7899, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.7996, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8214, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8255, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8265, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.8316, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8373, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8472, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8584, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.8737, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 38: 0.8831, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 41: 0.8858, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 45: 0.8871, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 53: 0.8893, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 69: 0.8899, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 73: 0.8901, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 75: 0.8917, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 89: 0.8936, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 91: 0.8938, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2193, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4613, Validation Loss: 2.3740, Validation F1: 0.2193\n",
      "Best F1 Score at epoch 2: 0.5325, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.6814, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7489, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7518, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7761, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8008, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8021, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8125, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8180, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8204, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.8288, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8306, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8426, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8476, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.8479, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8518, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.8521, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8576, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 35: 0.8654, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 38: 0.8688, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 40: 0.8726, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 41: 0.8769, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 46: 0.8769, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 51: 0.8780, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 52: 0.8787, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 60: 0.8804, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 61: 0.8859, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 70: 0.8892, Parameters: lr=0.001, hidden_dim128, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.001, hidden_dim 128: 0.8926\n",
      "Testing with learning rate: 0.001, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5932, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4512, Validation Loss: 2.1927, Validation F1: 0.5932\n",
      "Best F1 Score at epoch 1: 0.7693, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7895, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.8135, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.8265, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8287, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8299, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8450, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8604, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8755, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8843, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8855, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8894, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 39: 0.8905, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 51: 0.8906, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 60: 0.8907, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 61: 0.8912, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 62: 0.8935, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 79: 0.8944, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 91: 0.8956, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 92: 0.8975, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2406, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5148, Validation Loss: 2.2105, Validation F1: 0.2406\n",
      "Best F1 Score at epoch 1: 0.7151, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7781, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7889, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8114, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8307, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8325, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8539, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8588, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8622, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8840, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8864, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8872, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8876, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8893, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8903, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 39: 0.8910, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 40: 0.8924, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 45: 0.8941, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 53: 0.8951, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 54: 0.8961, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 62: 0.8970, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 76: 0.8973, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 77: 0.8974, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 80: 0.8986, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 96: 0.9004, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5748, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5268, Validation Loss: 2.2799, Validation F1: 0.5748\n",
      "Best F1 Score at epoch 1: 0.7178, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7731, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7748, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.8056, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8092, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8170, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8186, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8363, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8544, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8698, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8709, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8746, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8798, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 34: 0.8803, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 42: 0.8815, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 43: 0.8824, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 49: 0.8861, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 71: 0.8906, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 77: 0.8922, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 88: 0.8929, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 92: 0.8977, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 94: 0.8989, Parameters: lr=0.001, hidden_dim256, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.001, hidden_dim 256: 0.8989\n",
      "Testing with learning rate: 0.001, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4012, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5002, Validation Loss: 2.2377, Validation F1: 0.4012\n",
      "Best F1 Score at epoch 1: 0.6075, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7660, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7964, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.8114, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8124, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8201, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8267, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8383, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8394, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8539, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8623, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8715, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8716, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8724, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8741, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8759, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8779, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8790, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8851, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8912, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 54: 0.8941, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 69: 0.8954, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 81: 0.8961, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 88: 0.8971, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 89: 0.8990, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2144, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4643, Validation Loss: 2.2334, Validation F1: 0.2144\n",
      "Best F1 Score at epoch 1: 0.4940, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.6284, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7978, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7987, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.8034, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8090, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8226, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8320, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8458, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8498, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8579, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8721, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8846, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8910, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8913, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 49: 0.8914, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 50: 0.8967, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 70: 0.8969, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 71: 0.8979, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 89: 0.8980, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3963, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4931, Validation Loss: 2.3363, Validation F1: 0.3963\n",
      "Best F1 Score at epoch 1: 0.6378, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7432, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7907, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7952, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7969, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8142, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8203, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8237, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.8254, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8293, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8331, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8469, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8494, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8533, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8569, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8595, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8612, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8681, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8766, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8783, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 30: 0.8793, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 38: 0.8807, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 39: 0.8830, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 40: 0.8870, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 68: 0.8898, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 81: 0.8959, Parameters: lr=0.001, hidden_dim256, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.001, hidden_dim 256: 0.8976\n",
      "Testing with learning rate: 0.001, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4854, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5382, Validation Loss: 2.2163, Validation F1: 0.4854\n",
      "Best F1 Score at epoch 1: 0.5282, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7347, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7792, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.8048, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8054, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.8192, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8286, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8310, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8331, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8452, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8580, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8627, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8684, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8792, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8814, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.8855, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 37: 0.8919, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 62: 0.8934, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 74: 0.8943, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 75: 0.8955, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 80: 0.8958, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 85: 0.8986, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3882, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5067, Validation Loss: 2.2128, Validation F1: 0.3882\n",
      "Best F1 Score at epoch 1: 0.7208, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7468, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7872, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7983, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8168, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8195, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8243, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8320, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8381, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8660, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8699, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8796, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8849, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8908, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 36: 0.8912, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 41: 0.8924, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 42: 0.8925, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 52: 0.8933, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 58: 0.8945, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 59: 0.8959, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 70: 0.8967, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 80: 0.8974, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 86: 0.8980, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 89: 0.8986, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 90: 0.8988, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5253, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4673, Validation Loss: 2.2461, Validation F1: 0.5253\n",
      "Best F1 Score at epoch 1: 0.5967, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7151, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7797, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7808, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7927, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8074, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8262, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8276, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8285, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8402, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8409, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8474, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8513, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.8655, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8735, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8746, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8750, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8775, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 37: 0.8782, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 39: 0.8783, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 43: 0.8784, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 44: 0.8799, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 49: 0.8808, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 50: 0.8823, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 57: 0.8830, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 58: 0.8832, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 63: 0.8832, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 65: 0.8846, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 74: 0.8852, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 75: 0.8877, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 87: 0.8888, Parameters: lr=0.001, hidden_dim256, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.001, hidden_dim 256: 0.8954\n",
      "Testing with learning rate: 0.001, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1656, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4551, Validation Loss: 2.1186, Validation F1: 0.1656\n",
      "Best F1 Score at epoch 1: 0.7848, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.8225, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.8236, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.8329, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8380, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8405, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8635, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8635, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8887, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8896, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8932, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8950, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8953, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 36: 0.8954, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 42: 0.8985, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 43: 0.8997, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 61: 0.9005, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 69: 0.9012, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.7455, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.6205, Validation Loss: 2.1418, Validation F1: 0.7455\n",
      "Best F1 Score at epoch 1: 0.8225, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8231, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8456, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8483, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8613, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8680, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8688, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8892, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8893, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8902, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8918, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8919, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8934, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 32: 0.8936, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 34: 0.8945, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8959, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 36: 0.8962, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 41: 0.8963, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 42: 0.8965, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 51: 0.8966, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 52: 0.8970, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 53: 0.8976, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 55: 0.8977, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 61: 0.8990, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 64: 0.8997, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 69: 0.9001, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 70: 0.9015, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 99: 0.9016, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4263, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4271, Validation Loss: 2.2025, Validation F1: 0.4263\n",
      "Best F1 Score at epoch 1: 0.7542, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.8227, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8239, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8317, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8394, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8526, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8709, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8735, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8746, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8746, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8760, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8766, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8772, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8783, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8799, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8830, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8849, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 36: 0.8869, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 45: 0.8882, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 60: 0.8909, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 71: 0.8937, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 77: 0.8985, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 92: 0.8987, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 97: 0.9003, Parameters: lr=0.001, hidden_dim512, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.001, hidden_dim 512: 0.9010\n",
      "Testing with learning rate: 0.001, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4693, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4315, Validation Loss: 2.1043, Validation F1: 0.4693\n",
      "Best F1 Score at epoch 1: 0.7663, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.8152, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.8233, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.8290, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8363, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.8566, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8573, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8759, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8796, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8840, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8907, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 35: 0.8930, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8935, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 42: 0.8946, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8990, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 86: 0.9012, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3015, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4845, Validation Loss: 2.1461, Validation F1: 0.3015\n",
      "Best F1 Score at epoch 1: 0.8058, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.8103, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.8211, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8464, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8560, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8584, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8706, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8855, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8880, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8903, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8919, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8944, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 40: 0.8957, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 53: 0.8968, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 60: 0.8969, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 64: 0.8984, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 71: 0.8985, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 78: 0.8986, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 79: 0.8993, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 82: 0.8999, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 87: 0.9010, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6875, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5176, Validation Loss: 2.1838, Validation F1: 0.6875\n",
      "Best F1 Score at epoch 1: 0.7616, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7678, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.8073, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.8186, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8238, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8527, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8543, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8549, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8718, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8731, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8746, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8757, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8763, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8788, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8791, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8805, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8818, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 29: 0.8820, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8868, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 58: 0.8890, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 68: 0.8914, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 69: 0.8922, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 78: 0.8925, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 83: 0.8946, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 90: 0.8992, Parameters: lr=0.001, hidden_dim512, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.001, hidden_dim 512: 0.9005\n",
      "Testing with learning rate: 0.001, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6723, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5437, Validation Loss: 2.1188, Validation F1: 0.6723\n",
      "Best F1 Score at epoch 1: 0.8017, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.8029, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.8260, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8374, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8461, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8657, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8677, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8679, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8772, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8829, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8830, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8844, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8862, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8880, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.8899, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8911, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 39: 0.8931, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 59: 0.8933, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 60: 0.8936, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 61: 0.8962, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 82: 0.8967, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6112, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4844, Validation Loss: 2.0792, Validation F1: 0.6112\n",
      "Best F1 Score at epoch 1: 0.8068, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.8246, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8481, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8576, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8792, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8901, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.8905, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 35: 0.8917, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 36: 0.8941, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 37: 0.8955, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 45: 0.8955, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 47: 0.8958, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 53: 0.8971, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 54: 0.8973, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 65: 0.8978, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 72: 0.8991, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 81: 0.8991, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 86: 0.9000, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 88: 0.9010, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 97: 0.9015, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5410, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4477, Validation Loss: 2.2324, Validation F1: 0.5410\n",
      "Best F1 Score at epoch 1: 0.7832, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.8052, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8116, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.8217, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8414, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8424, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8471, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8595, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8692, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8713, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8739, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8763, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8775, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8782, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 32: 0.8802, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.8811, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 37: 0.8823, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 39: 0.8834, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 53: 0.8852, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 61: 0.8890, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 63: 0.8914, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 83: 0.8917, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 84: 0.8960, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 85: 0.8962, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 97: 0.8978, Parameters: lr=0.001, hidden_dim512, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.001, hidden_dim 512: 0.8987\n",
      "Best Parameters: {'learning_rate': 0.001, 'hidden_dim': 512, 'drop_out': 0.2}, Best F1 Score: 0.9010\n",
      "All results: {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}}\n",
      "Testing with learning rate: 0.005, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.7490, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5037, Validation Loss: 2.1760, Validation F1: 0.7490\n",
      "Best F1 Score at epoch 1: 0.7805, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.8150, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.8400, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8567, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8750, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8759, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8771, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8782, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8839, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8852, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8874, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 36: 0.8895, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 37: 0.8903, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8905, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 43: 0.8911, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 44: 0.8913, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 45: 0.8920, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 46: 0.8924, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 47: 0.8925, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 52: 0.8926, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 53: 0.8941, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 54: 0.8969, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 56: 0.8983, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 88: 0.8987, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 89: 0.9000, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5853, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5226, Validation Loss: 2.2196, Validation F1: 0.5853\n",
      "Best F1 Score at epoch 1: 0.7301, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7814, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.8157, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.8308, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8365, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8615, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8738, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8780, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8867, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8869, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8955, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8957, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 42: 0.8963, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 46: 0.8982, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 47: 0.8997, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 54: 0.9001, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 55: 0.9007, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 74: 0.9012, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 75: 0.9029, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2376, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5274, Validation Loss: 2.5164, Validation F1: 0.2376\n",
      "Best F1 Score at epoch 1: 0.5405, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7353, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7681, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.8050, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8256, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8307, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8334, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8505, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8519, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8716, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8719, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8738, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8747, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8769, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8785, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8796, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8812, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8812, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 31: 0.8830, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 32: 0.8834, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 33: 0.8846, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 45: 0.8878, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 58: 0.8904, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 67: 0.8935, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 73: 0.8973, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 88: 0.8984, Parameters: lr=0.005, hidden_dim128, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.005, hidden_dim 128: 0.9004\n",
      "Testing with learning rate: 0.005, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3710, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4360, Validation Loss: 2.5296, Validation F1: 0.3710\n",
      "Best F1 Score at epoch 1: 0.4923, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7982, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8036, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8201, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8379, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8437, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8623, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8792, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8877, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 29: 0.8882, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 33: 0.8896, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8923, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 48: 0.8939, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 58: 0.8988, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 70: 0.9000, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 71: 0.9007, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6783, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5298, Validation Loss: 2.2666, Validation F1: 0.6783\n",
      "Best F1 Score at epoch 3: 0.7223, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7750, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7869, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7871, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8111, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8221, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8611, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8767, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8849, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8869, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8907, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8931, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 33: 0.8934, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 38: 0.8961, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 39: 0.8964, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 45: 0.8966, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 52: 0.8975, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 53: 0.9009, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1384, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5640, Validation Loss: 2.2950, Validation F1: 0.1384\n",
      "Best F1 Score at epoch 1: 0.6632, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7837, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7900, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.8075, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8345, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8543, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8562, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8691, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8698, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8771, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8781, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8790, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8794, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8811, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8835, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 37: 0.8840, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8843, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 50: 0.8845, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 51: 0.8848, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 55: 0.8849, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 57: 0.8892, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 60: 0.8895, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 77: 0.8950, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 80: 0.8959, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 81: 0.8981, Parameters: lr=0.005, hidden_dim128, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.005, hidden_dim 128: 0.8999\n",
      "Testing with learning rate: 0.005, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4635, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5423, Validation Loss: 2.4321, Validation F1: 0.4635\n",
      "Best F1 Score at epoch 1: 0.7522, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7746, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8474, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8484, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8589, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8738, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8789, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8815, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8849, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8882, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 36: 0.8887, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 38: 0.8917, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 39: 0.8942, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 56: 0.8974, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 65: 0.9005, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4491, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.7170, Validation Loss: 2.3151, Validation F1: 0.4491\n",
      "Best F1 Score at epoch 1: 0.7456, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7801, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.8009, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.8072, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8158, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.8187, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8319, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8651, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8654, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8773, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8795, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8900, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8907, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8913, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8923, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 32: 0.8926, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.8941, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 42: 0.8956, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 45: 0.8964, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 56: 0.8977, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 67: 0.8979, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 70: 0.8989, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 81: 0.8990, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 82: 0.9017, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4729, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.6495, Validation Loss: 2.3261, Validation F1: 0.4729\n",
      "Best F1 Score at epoch 1: 0.6755, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7066, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7922, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.8093, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8234, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.8271, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8303, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8684, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8727, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8750, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.8764, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.8771, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8773, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8798, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8813, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8818, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 36: 0.8852, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 37: 0.8877, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 39: 0.8912, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 48: 0.8947, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 69: 0.8976, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 81: 0.8984, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 83: 0.9000, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 87: 0.9010, Parameters: lr=0.005, hidden_dim128, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.005, hidden_dim 128: 0.9011\n",
      "Testing with learning rate: 0.005, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3327, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4339, Validation Loss: 2.5391, Validation F1: 0.3327\n",
      "Best F1 Score at epoch 1: 0.7580, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7971, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.8081, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8331, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8549, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8751, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8935, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8961, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 53: 0.8978, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 76: 0.9001, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1038, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5088, Validation Loss: 2.7096, Validation F1: 0.1038\n",
      "Best F1 Score at epoch 1: 0.7560, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7991, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.8122, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8392, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8412, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8465, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8614, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8781, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8834, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8883, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8906, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8922, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8956, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8957, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8971, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 40: 0.8988, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 41: 0.8992, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 49: 0.8999, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 52: 0.9012, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 53: 0.9019, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 57: 0.9023, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 71: 0.9025, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.7738, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5054, Validation Loss: 2.5990, Validation F1: 0.7738\n",
      "Best F1 Score at epoch 3: 0.7848, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.8052, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8070, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8108, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8237, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8281, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8369, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8540, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8750, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8754, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8771, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8772, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8805, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8842, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8849, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8869, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 36: 0.8938, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 52: 0.8992, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 63: 0.8992, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 68: 0.9001, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 69: 0.9003, Parameters: lr=0.005, hidden_dim256, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.005, hidden_dim 256: 0.9010\n",
      "Testing with learning rate: 0.005, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4156, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4774, Validation Loss: 2.7018, Validation F1: 0.4156\n",
      "Best F1 Score at epoch 1: 0.7295, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.8537, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8613, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8984, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 77: 0.9021, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4315, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4289, Validation Loss: 2.5061, Validation F1: 0.4315\n",
      "Best F1 Score at epoch 1: 0.6568, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7502, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.8203, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8316, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8560, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8659, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8789, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8848, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8882, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8905, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8908, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 29: 0.8923, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 30: 0.8960, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 38: 0.8971, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 45: 0.8978, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 61: 0.8998, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 62: 0.9002, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 63: 0.9012, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 70: 0.9028, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5844, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4774, Validation Loss: 2.5970, Validation F1: 0.5844\n",
      "Best F1 Score at epoch 1: 0.7224, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7886, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7979, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8103, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8345, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8553, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.8639, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8689, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8712, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8752, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8780, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8789, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8805, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8818, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8818, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 30: 0.8827, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.8847, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8853, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8896, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 37: 0.8938, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 57: 0.8972, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 64: 0.8988, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 79: 0.8993, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 83: 0.8996, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 90: 0.9003, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 95: 0.9006, Parameters: lr=0.005, hidden_dim256, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.005, hidden_dim 256: 0.9018\n",
      "Testing with learning rate: 0.005, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2996, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5019, Validation Loss: 2.6979, Validation F1: 0.2996\n",
      "Best F1 Score at epoch 1: 0.6228, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7949, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.8169, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8199, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.8221, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8327, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8452, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8745, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8763, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8798, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8882, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.8891, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8900, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8908, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 38: 0.8991, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 44: 0.8999, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 55: 0.9011, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 56: 0.9014, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.7351, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5183, Validation Loss: 2.6453, Validation F1: 0.7351\n",
      "Best F1 Score at epoch 2: 0.7965, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.8015, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.8230, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8304, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8441, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8523, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8771, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8798, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8876, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8917, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 36: 0.8922, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 40: 0.8931, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 45: 0.8961, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 47: 0.8961, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 48: 0.8967, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 50: 0.8973, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 55: 0.8999, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 64: 0.9004, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 73: 0.9009, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 81: 0.9015, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 84: 0.9015, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 90: 0.9046, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3661, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5921, Validation Loss: 2.7428, Validation F1: 0.3661\n",
      "Best F1 Score at epoch 1: 0.7310, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7542, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7966, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8200, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.8470, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8600, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8750, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8758, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8841, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8855, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 47: 0.8884, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 51: 0.8938, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 60: 0.8990, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 78: 0.9005, Parameters: lr=0.005, hidden_dim256, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.005, hidden_dim 256: 0.9022\n",
      "Testing with learning rate: 0.005, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0223, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4568, Validation Loss: 5.3574, Validation F1: 0.0223\n",
      "Best F1 Score at epoch 1: 0.6024, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7538, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7743, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7890, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8207, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8412, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8624, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8785, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8818, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8829, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8839, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8926, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 34: 0.8953, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 53: 0.8996, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 70: 0.9024, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6298, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4579, Validation Loss: 4.4092, Validation F1: 0.6298\n",
      "Best F1 Score at epoch 1: 0.7498, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.8347, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8512, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8575, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8796, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8828, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8868, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8898, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8908, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8925, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8951, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 33: 0.8960, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 39: 0.8976, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 45: 0.8981, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 46: 0.8992, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 50: 0.8994, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 51: 0.9001, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 53: 0.9002, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 55: 0.9017, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 89: 0.9031, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3456, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5299, Validation Loss: 4.1115, Validation F1: 0.3456\n",
      "Best F1 Score at epoch 1: 0.6727, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7748, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7951, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8265, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8275, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8539, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8736, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8753, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8782, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8797, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8801, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8830, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8833, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8839, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8845, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 31: 0.8880, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 47: 0.8904, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 55: 0.8909, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 57: 0.8933, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 58: 0.8943, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 59: 0.8947, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 66: 0.8984, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 68: 0.8984, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 70: 0.9006, Parameters: lr=0.005, hidden_dim512, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.005, hidden_dim 512: 0.9021\n",
      "Testing with learning rate: 0.005, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5815, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4545, Validation Loss: 4.2568, Validation F1: 0.5815\n",
      "Best F1 Score at epoch 4: 0.8086, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8504, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8532, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.8622, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8797, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8827, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8863, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8885, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8903, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8917, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 38: 0.8932, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 44: 0.8935, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 46: 0.8975, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 47: 0.8983, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5611, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.3960, Validation Loss: 3.0813, Validation F1: 0.5611\n",
      "Best F1 Score at epoch 1: 0.7125, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7360, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7747, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7993, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.8156, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8268, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8349, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8458, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8521, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.8768, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8775, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8784, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8869, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8873, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8898, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8909, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8929, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 30: 0.8929, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 46: 0.8952, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 48: 0.8967, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 54: 0.8970, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 55: 0.8978, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 56: 0.8987, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 71: 0.8999, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 72: 0.9005, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 74: 0.9011, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6451, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4656, Validation Loss: 4.3529, Validation F1: 0.6451\n",
      "Best F1 Score at epoch 4: 0.7865, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8172, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8467, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8607, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8713, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8763, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8804, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8822, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.8836, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8842, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 37: 0.8858, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 45: 0.8893, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 46: 0.8902, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 49: 0.8942, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 52: 0.8972, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 53: 0.8984, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 63: 0.8991, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 64: 0.9004, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 73: 0.9005, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 89: 0.9006, Parameters: lr=0.005, hidden_dim512, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.005, hidden_dim 512: 0.9000\n",
      "Testing with learning rate: 0.005, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2729, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4389, Validation Loss: 5.0008, Validation F1: 0.2729\n",
      "Best F1 Score at epoch 1: 0.3337, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.3826, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.3978, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7789, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7912, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8176, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8457, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8529, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8536, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8775, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8831, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8887, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8898, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8917, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 40: 0.8924, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 41: 0.8934, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 42: 0.8965, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 61: 0.8985, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 98: 0.8992, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 99: 0.9007, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3819, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4875, Validation Loss: 3.7412, Validation F1: 0.3819\n",
      "Best F1 Score at epoch 1: 0.7947, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7967, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.8143, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8395, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8468, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8590, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8722, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.8787, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.8830, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8873, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8878, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8910, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.8923, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8928, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 42: 0.8968, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 52: 0.8984, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 62: 0.8994, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 78: 0.9009, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 79: 0.9011, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 94: 0.9013, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0994, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4474, Validation Loss: 4.3975, Validation F1: 0.0994\n",
      "Best F1 Score at epoch 1: 0.4301, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.6050, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7815, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7948, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8036, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8264, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8675, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8740, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8753, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8755, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8757, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8769, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8773, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8788, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8791, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8810, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8817, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8835, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 35: 0.8837, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 36: 0.8842, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 40: 0.8859, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 49: 0.8861, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 50: 0.8905, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 51: 0.8913, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 68: 0.8932, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 71: 0.8936, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 72: 0.8944, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 73: 0.8944, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 78: 0.8948, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 79: 0.8979, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 84: 0.8983, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 85: 0.8984, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 89: 0.8998, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 98: 0.9016, Parameters: lr=0.005, hidden_dim512, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}, (0.4, 0.005, 512): {'folds': [0.9006590700899209, 0.901347428500428, 0.9015984327762246], 'avg_f1': 0.9012016437888578}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.005, hidden_dim 512: 0.9012\n",
      "Best Parameters: {'learning_rate': 0.005, 'hidden_dim': 256, 'drop_out': 0.4}, Best F1 Score: 0.9022\n",
      "All results: {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}, (0.4, 0.005, 512): {'folds': [0.9006590700899209, 0.901347428500428, 0.9015984327762246], 'avg_f1': 0.9012016437888578}}\n",
      "Testing with learning rate: 0.01, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0447, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4874, Validation Loss: 3.9464, Validation F1: 0.0447\n",
      "Best F1 Score at epoch 1: 0.6917, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7482, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8111, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8382, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8613, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8675, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8697, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8765, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8777, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8845, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8852, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8878, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8896, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 33: 0.8901, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 36: 0.8905, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 37: 0.8913, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8949, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 48: 0.8962, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 59: 0.8963, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 60: 0.9008, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 66: 0.9013, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 96: 0.9019, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6091, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.3915, Validation Loss: 2.6799, Validation F1: 0.6091\n",
      "Best F1 Score at epoch 3: 0.6962, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.8045, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8203, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8245, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8300, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8326, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8500, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8605, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8610, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8737, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8852, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8895, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8911, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 27: 0.8921, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 31: 0.8937, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8941, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 36: 0.8985, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 47: 0.8986, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 55: 0.9000, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 65: 0.9035, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 74: 0.9035, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 86: 0.9040, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5654, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4380, Validation Loss: 2.7128, Validation F1: 0.5654\n",
      "Best F1 Score at epoch 2: 0.7542, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7655, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7713, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7926, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8365, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8520, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8646, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8689, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8733, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8750, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8833, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8920, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8923, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 48: 0.8955, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 51: 0.8966, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 63: 0.8998, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 96: 0.9000, Parameters: lr=0.01, hidden_dim128, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}, (0.4, 0.005, 512): {'folds': [0.9006590700899209, 0.901347428500428, 0.9015984327762246], 'avg_f1': 0.9012016437888578}, (0.2, 0.01, 128): {'folds': [0.901870319191538, 0.9040393612174754, 0.8999763380225619], 'avg_f1': 0.9019620061438585}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.01, hidden_dim 128: 0.9020\n",
      "Testing with learning rate: 0.01, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0226, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.6494, Validation Loss: 2.6045, Validation F1: 0.0226\n",
      "Best F1 Score at epoch 1: 0.2918, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.6413, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7317, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.8208, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.8311, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8347, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8352, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8393, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.8642, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8827, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8843, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8893, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.8925, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 37: 0.8992, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 81: 0.9027, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0091, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5811, Validation Loss: 3.0838, Validation F1: 0.0091\n",
      "Best F1 Score at epoch 1: 0.3741, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.6331, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7389, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7691, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7883, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8050, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8260, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8356, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8445, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8539, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8765, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8789, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8796, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8806, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8836, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8874, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8899, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8912, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.8938, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8953, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 39: 0.8955, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 40: 0.8971, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8973, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 48: 0.8974, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 50: 0.8999, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 59: 0.9005, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 60: 0.9011, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 62: 0.9017, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 70: 0.9029, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 87: 0.9031, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 88: 0.9034, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3190, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.6366, Validation Loss: 2.7755, Validation F1: 0.3190\n",
      "Best F1 Score at epoch 1: 0.7235, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7801, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.8021, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.8136, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8261, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8557, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8563, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.8688, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8755, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8797, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8800, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8800, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8806, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8841, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8954, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 37: 0.8963, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8983, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 44: 0.8985, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 52: 0.8988, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 58: 0.8994, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 63: 0.8999, Parameters: lr=0.01, hidden_dim128, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}, (0.4, 0.005, 512): {'folds': [0.9006590700899209, 0.901347428500428, 0.9015984327762246], 'avg_f1': 0.9012016437888578}, (0.2, 0.01, 128): {'folds': [0.901870319191538, 0.9040393612174754, 0.8999763380225619], 'avg_f1': 0.9019620061438585}, (0.3, 0.01, 128): {'folds': [0.9027219629511057, 0.9034388144047479, 0.8999139256261134], 'avg_f1': 0.902024900993989}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.01, hidden_dim 128: 0.9020\n",
      "Testing with learning rate: 0.01, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6074, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.6185, Validation Loss: 2.5410, Validation F1: 0.6074\n",
      "Best F1 Score at epoch 4: 0.7915, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7923, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8083, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8331, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8351, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8381, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8579, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8799, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8836, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8892, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8905, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8913, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 41: 0.8926, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 42: 0.8950, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 48: 0.8958, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 49: 0.8965, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 56: 0.8965, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 60: 0.9021, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 66: 0.9031, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2752, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.6223, Validation Loss: 3.2122, Validation F1: 0.2752\n",
      "Best F1 Score at epoch 1: 0.5696, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7653, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.8021, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.8212, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8316, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8549, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8739, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8787, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8851, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8859, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.8872, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8910, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8916, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.8986, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 36: 0.8994, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 38: 0.9002, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 56: 0.9004, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 60: 0.9007, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 71: 0.9011, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 72: 0.9016, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 84: 0.9021, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 98: 0.9024, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3760, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.6540, Validation Loss: 2.7530, Validation F1: 0.3760\n",
      "Best F1 Score at epoch 1: 0.7548, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7724, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.8121, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8234, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.8302, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8430, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8483, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8529, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8600, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8818, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8843, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.8844, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 41: 0.8867, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 42: 0.8879, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 43: 0.8885, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 44: 0.8887, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 47: 0.8893, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 52: 0.8916, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 53: 0.8929, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 54: 0.8990, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 62: 0.8991, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 71: 0.8994, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 80: 0.8999, Parameters: lr=0.01, hidden_dim128, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}, (0.4, 0.005, 512): {'folds': [0.9006590700899209, 0.901347428500428, 0.9015984327762246], 'avg_f1': 0.9012016437888578}, (0.2, 0.01, 128): {'folds': [0.901870319191538, 0.9040393612174754, 0.8999763380225619], 'avg_f1': 0.9019620061438585}, (0.3, 0.01, 128): {'folds': [0.9027219629511057, 0.9034388144047479, 0.8999139256261134], 'avg_f1': 0.902024900993989}, (0.4, 0.01, 128): {'folds': [0.9030675210138166, 0.9024192664068831, 0.8999098590340374], 'avg_f1': 0.9017988821515791}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.01, hidden_dim 128: 0.9018\n",
      "Testing with learning rate: 0.01, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0196, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.6976, Validation Loss: 4.3318, Validation F1: 0.0196\n",
      "Best F1 Score at epoch 1: 0.7278, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7347, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8067, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8587, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8770, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8776, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8878, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8896, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 27: 0.8922, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8925, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 32: 0.8937, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 37: 0.8946, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8959, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 39: 0.8989, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 55: 0.9010, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 61: 0.9033, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3292, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5060, Validation Loss: 4.3922, Validation F1: 0.3292\n",
      "Best F1 Score at epoch 3: 0.7100, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7680, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8213, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8510, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8537, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8735, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8744, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8842, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8893, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8996, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 46: 0.8998, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 65: 0.9010, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 74: 0.9022, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 88: 0.9030, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0038, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4930, Validation Loss: 5.3115, Validation F1: 0.0038\n",
      "Best F1 Score at epoch 1: 0.0816, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7802, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7805, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8076, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8271, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8536, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8690, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8724, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8761, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8794, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8811, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 27: 0.8833, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8861, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8895, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 46: 0.8896, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 62: 0.8899, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 63: 0.8964, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 85: 0.8987, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 88: 0.9002, Parameters: lr=0.01, hidden_dim256, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}, (0.4, 0.005, 512): {'folds': [0.9006590700899209, 0.901347428500428, 0.9015984327762246], 'avg_f1': 0.9012016437888578}, (0.2, 0.01, 128): {'folds': [0.901870319191538, 0.9040393612174754, 0.8999763380225619], 'avg_f1': 0.9019620061438585}, (0.3, 0.01, 128): {'folds': [0.9027219629511057, 0.9034388144047479, 0.8999139256261134], 'avg_f1': 0.902024900993989}, (0.4, 0.01, 128): {'folds': [0.9030675210138166, 0.9024192664068831, 0.8999098590340374], 'avg_f1': 0.9017988821515791}, (0.2, 0.01, 256): {'folds': [0.9032519807412079, 0.9030196782539881, 0.9002162321565016], 'avg_f1': 0.9021626303838991}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.01, hidden_dim 256: 0.9022\n",
      "Testing with learning rate: 0.01, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0151, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5062, Validation Loss: 3.8249, Validation F1: 0.0151\n",
      "Best F1 Score at epoch 1: 0.2767, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.3670, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.4969, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7112, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7766, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8282, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8481, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8516, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8806, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8875, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8879, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8882, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8892, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 34: 0.8922, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 39: 0.8977, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 47: 0.8983, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 50: 0.8986, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 60: 0.8997, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 72: 0.8998, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 77: 0.9016, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 78: 0.9016, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6708, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4860, Validation Loss: 3.8258, Validation F1: 0.6708\n",
      "Best F1 Score at epoch 3: 0.7596, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8073, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8101, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8265, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.8288, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8435, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8592, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8667, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8689, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8711, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8735, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8739, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8899, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 29: 0.8926, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 35: 0.8938, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8963, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 37: 0.8963, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 52: 0.8975, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 55: 0.9011, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 74: 0.9020, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0124, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4769, Validation Loss: 6.1552, Validation F1: 0.0124\n",
      "Best F1 Score at epoch 1: 0.7158, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7678, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7803, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7958, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8088, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.8211, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8532, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8697, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8707, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8716, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8777, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8821, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8830, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8836, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8839, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8863, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 29: 0.8868, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 37: 0.8868, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 50: 0.8891, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 51: 0.8895, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 52: 0.8899, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 53: 0.8900, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 56: 0.8940, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 57: 0.9005, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 62: 0.9010, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 70: 0.9032, Parameters: lr=0.01, hidden_dim256, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}, (0.4, 0.005, 512): {'folds': [0.9006590700899209, 0.901347428500428, 0.9015984327762246], 'avg_f1': 0.9012016437888578}, (0.2, 0.01, 128): {'folds': [0.901870319191538, 0.9040393612174754, 0.8999763380225619], 'avg_f1': 0.9019620061438585}, (0.3, 0.01, 128): {'folds': [0.9027219629511057, 0.9034388144047479, 0.8999139256261134], 'avg_f1': 0.902024900993989}, (0.4, 0.01, 128): {'folds': [0.9030675210138166, 0.9024192664068831, 0.8999098590340374], 'avg_f1': 0.9017988821515791}, (0.2, 0.01, 256): {'folds': [0.9032519807412079, 0.9030196782539881, 0.9002162321565016], 'avg_f1': 0.9021626303838991}, (0.3, 0.01, 256): {'folds': [0.9015976286580346, 0.9020191847096248, 0.9032051447266346], 'avg_f1': 0.9022739860314314}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.01, hidden_dim 256: 0.9023\n",
      "Testing with learning rate: 0.01, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0166, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.6207, Validation Loss: 6.0643, Validation F1: 0.0166\n",
      "Best F1 Score at epoch 1: 0.6615, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.6855, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7951, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8467, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8645, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8682, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8775, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8801, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8877, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8917, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8952, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.8955, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 40: 0.8995, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 59: 0.9006, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 65: 0.9011, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 99: 0.9014, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0047, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4476, Validation Loss: 4.8859, Validation F1: 0.0047\n",
      "Best F1 Score at epoch 1: 0.7463, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7930, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.8225, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8340, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8565, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8779, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8845, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8871, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8893, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.8910, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8921, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 37: 0.8973, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 40: 0.8978, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 46: 0.8990, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 52: 0.8991, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 56: 0.9012, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 63: 0.9019, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 79: 0.9019, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 82: 0.9022, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 83: 0.9023, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 84: 0.9025, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 93: 0.9032, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2368, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.3962, Validation Loss: 7.9910, Validation F1: 0.2368\n",
      "Best F1 Score at epoch 1: 0.2809, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.3443, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.4365, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7400, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7469, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.7634, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.7701, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.7836, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7936, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.7995, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8059, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8207, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8258, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8373, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8474, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8749, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.8766, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8810, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8812, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8825, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.8829, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.8840, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8884, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 54: 0.8905, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 55: 0.8916, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 60: 0.8918, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 61: 0.8976, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 69: 0.8978, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 71: 0.8980, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 72: 0.8983, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 76: 0.8995, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 77: 0.9003, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 82: 0.9010, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 83: 0.9012, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 89: 0.9013, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 90: 0.9014, Parameters: lr=0.01, hidden_dim256, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}, (0.4, 0.005, 512): {'folds': [0.9006590700899209, 0.901347428500428, 0.9015984327762246], 'avg_f1': 0.9012016437888578}, (0.2, 0.01, 128): {'folds': [0.901870319191538, 0.9040393612174754, 0.8999763380225619], 'avg_f1': 0.9019620061438585}, (0.3, 0.01, 128): {'folds': [0.9027219629511057, 0.9034388144047479, 0.8999139256261134], 'avg_f1': 0.902024900993989}, (0.4, 0.01, 128): {'folds': [0.9030675210138166, 0.9024192664068831, 0.8999098590340374], 'avg_f1': 0.9017988821515791}, (0.2, 0.01, 256): {'folds': [0.9032519807412079, 0.9030196782539881, 0.9002162321565016], 'avg_f1': 0.9021626303838991}, (0.3, 0.01, 256): {'folds': [0.9015976286580346, 0.9020191847096248, 0.9032051447266346], 'avg_f1': 0.9022739860314314}, (0.4, 0.01, 256): {'folds': [0.9013818756127574, 0.9031789537606468, 0.9014197070694532], 'avg_f1': 0.9019935121476191}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.01, hidden_dim 256: 0.9020\n",
      "Testing with learning rate: 0.01, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0717, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4697, Validation Loss: 11.2001, Validation F1: 0.0717\n",
      "Best F1 Score at epoch 1: 0.7054, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.7380, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.7495, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.7647, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8206, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8296, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8411, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8582, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8736, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8778, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8928, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 47: 0.8930, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 67: 0.8940, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 77: 0.8985, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3393, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4253, Validation Loss: 15.4500, Validation F1: 0.3393\n",
      "Best F1 Score at epoch 2: 0.6539, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.6656, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7786, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8035, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8056, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8250, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8439, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8646, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8704, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8798, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 32: 0.8817, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 33: 0.8867, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8891, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 39: 0.8956, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 65: 0.8961, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 70: 0.8966, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 81: 0.8976, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 92: 0.9027, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0041, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4586, Validation Loss: 10.8106, Validation F1: 0.0041\n",
      "Best F1 Score at epoch 1: 0.6744, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7143, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.7583, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.7638, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8116, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8128, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8321, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8611, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8644, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8671, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8685, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8704, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8729, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 34: 0.8732, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8763, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 36: 0.8764, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 37: 0.8765, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 39: 0.8804, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 40: 0.8812, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 42: 0.8816, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 43: 0.8818, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 44: 0.8823, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 48: 0.8825, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 49: 0.8829, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 50: 0.8836, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 53: 0.8851, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 54: 0.8851, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 64: 0.8853, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 65: 0.8862, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 74: 0.8864, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 77: 0.8874, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 88: 0.8877, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 89: 0.8901, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 90: 0.9002, Parameters: lr=0.01, hidden_dim512, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}, (0.4, 0.005, 512): {'folds': [0.9006590700899209, 0.901347428500428, 0.9015984327762246], 'avg_f1': 0.9012016437888578}, (0.2, 0.01, 128): {'folds': [0.901870319191538, 0.9040393612174754, 0.8999763380225619], 'avg_f1': 0.9019620061438585}, (0.3, 0.01, 128): {'folds': [0.9027219629511057, 0.9034388144047479, 0.8999139256261134], 'avg_f1': 0.902024900993989}, (0.4, 0.01, 128): {'folds': [0.9030675210138166, 0.9024192664068831, 0.8999098590340374], 'avg_f1': 0.9017988821515791}, (0.2, 0.01, 256): {'folds': [0.9032519807412079, 0.9030196782539881, 0.9002162321565016], 'avg_f1': 0.9021626303838991}, (0.3, 0.01, 256): {'folds': [0.9015976286580346, 0.9020191847096248, 0.9032051447266346], 'avg_f1': 0.9022739860314314}, (0.4, 0.01, 256): {'folds': [0.9013818756127574, 0.9031789537606468, 0.9014197070694532], 'avg_f1': 0.9019935121476191}, (0.2, 0.01, 512): {'folds': [0.898467250384832, 0.9026611268936877, 0.9002020737204186], 'avg_f1': 0.9004434836663128}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.01, hidden_dim 512: 0.9004\n",
      "Testing with learning rate: 0.01, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2368, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5057, Validation Loss: 21.9234, Validation F1: 0.2368\n",
      "Best F1 Score at epoch 2: 0.3880, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7226, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.7241, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.7735, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.7851, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.7879, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.7947, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.7984, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8051, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8335, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8492, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8574, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8747, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 61: 0.8774, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 62: 0.8785, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 63: 0.8827, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 64: 0.8844, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 65: 0.8846, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 69: 0.8860, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 81: 0.8864, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 83: 0.8866, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 95: 0.8898, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 98: 0.8934, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4427, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4927, Validation Loss: 13.7883, Validation F1: 0.4427\n",
      "Best F1 Score at epoch 1: 0.5769, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.6784, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7398, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7776, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8082, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8411, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8574, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8575, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8581, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8703, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8785, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 34: 0.8842, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 35: 0.8906, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8909, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 44: 0.8924, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 47: 0.8932, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 49: 0.8933, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 54: 0.8946, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 58: 0.8962, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 59: 0.8967, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 72: 0.8969, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 73: 0.8993, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 77: 0.9007, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 79: 0.9027, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2635, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.5002, Validation Loss: 10.2144, Validation F1: 0.2635\n",
      "Best F1 Score at epoch 2: 0.3974, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.6852, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.7615, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8142, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8571, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8707, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8785, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8788, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 33: 0.8794, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 34: 0.8821, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 41: 0.8839, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8849, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 44: 0.8860, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 54: 0.8863, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 60: 0.8872, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 69: 0.8880, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 70: 0.8909, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 80: 0.8911, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 81: 0.8913, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 93: 0.8989, Parameters: lr=0.01, hidden_dim512, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}, (0.4, 0.005, 512): {'folds': [0.9006590700899209, 0.901347428500428, 0.9015984327762246], 'avg_f1': 0.9012016437888578}, (0.2, 0.01, 128): {'folds': [0.901870319191538, 0.9040393612174754, 0.8999763380225619], 'avg_f1': 0.9019620061438585}, (0.3, 0.01, 128): {'folds': [0.9027219629511057, 0.9034388144047479, 0.8999139256261134], 'avg_f1': 0.902024900993989}, (0.4, 0.01, 128): {'folds': [0.9030675210138166, 0.9024192664068831, 0.8999098590340374], 'avg_f1': 0.9017988821515791}, (0.2, 0.01, 256): {'folds': [0.9032519807412079, 0.9030196782539881, 0.9002162321565016], 'avg_f1': 0.9021626303838991}, (0.3, 0.01, 256): {'folds': [0.9015976286580346, 0.9020191847096248, 0.9032051447266346], 'avg_f1': 0.9022739860314314}, (0.4, 0.01, 256): {'folds': [0.9013818756127574, 0.9031789537606468, 0.9014197070694532], 'avg_f1': 0.9019935121476191}, (0.2, 0.01, 512): {'folds': [0.898467250384832, 0.9026611268936877, 0.9002020737204186], 'avg_f1': 0.9004434836663128}, (0.3, 0.01, 512): {'folds': [0.8934013528018699, 0.9026545352165455, 0.8988983049739071], 'avg_f1': 0.8983180643307742}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.01, hidden_dim 512: 0.8983\n",
      "Testing with learning rate: 0.01, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([1.8457e+01, 2.7513e+01, 9.2346e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4903, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4841, Validation Loss: 12.6062, Validation F1: 0.4903\n",
      "Best F1 Score at epoch 7: 0.5737, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.6796, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.7273, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.7618, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8190, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8303, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.8465, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8674, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.8752, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 36: 0.8790, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 38: 0.8866, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 45: 0.8911, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 71: 0.8927, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 77: 0.8947, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 91: 0.9003, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([1.8445e+01, 2.7513e+01, 9.2652e+01, 3.0194e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8264e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2805, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4519, Validation Loss: 15.5296, Validation F1: 0.2805\n",
      "Best F1 Score at epoch 1: 0.3605, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.3890, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.6273, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.6710, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7766, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8022, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8071, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8170, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8390, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8602, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8684, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.8697, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8763, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8777, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 33: 0.8790, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8802, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 35: 0.8802, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 36: 0.8812, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 37: 0.8910, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 38: 0.8915, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 44: 0.8948, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 46: 0.8957, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 57: 0.8964, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 86: 0.8977, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 90: 0.9004, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([1.8445e+01, 2.7486e+01, 9.2346e+01, 3.0197e+00, 1.1090e+00, 2.0365e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5303e+00, 3.2688e+01, 2.8552e+02],\n",
      "       device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0124, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.5409, Validation Loss: 12.4279, Validation F1: 0.0124\n",
      "Best F1 Score at epoch 1: 0.6976, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7061, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.7536, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.7992, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8547, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8560, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8606, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8659, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8660, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8699, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.8756, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.8798, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8821, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 46: 0.8833, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 48: 0.8848, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 54: 0.8852, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 55: 0.8868, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 57: 0.8870, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 58: 0.8872, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 59: 0.8874, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 63: 0.8902, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 69: 0.8949, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 76: 0.9029, Parameters: lr=0.01, hidden_dim512, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}, (0.4, 0.005, 512): {'folds': [0.9006590700899209, 0.901347428500428, 0.9015984327762246], 'avg_f1': 0.9012016437888578}, (0.2, 0.01, 128): {'folds': [0.901870319191538, 0.9040393612174754, 0.8999763380225619], 'avg_f1': 0.9019620061438585}, (0.3, 0.01, 128): {'folds': [0.9027219629511057, 0.9034388144047479, 0.8999139256261134], 'avg_f1': 0.902024900993989}, (0.4, 0.01, 128): {'folds': [0.9030675210138166, 0.9024192664068831, 0.8999098590340374], 'avg_f1': 0.9017988821515791}, (0.2, 0.01, 256): {'folds': [0.9032519807412079, 0.9030196782539881, 0.9002162321565016], 'avg_f1': 0.9021626303838991}, (0.3, 0.01, 256): {'folds': [0.9015976286580346, 0.9020191847096248, 0.9032051447266346], 'avg_f1': 0.9022739860314314}, (0.4, 0.01, 256): {'folds': [0.9013818756127574, 0.9031789537606468, 0.9014197070694532], 'avg_f1': 0.9019935121476191}, (0.2, 0.01, 512): {'folds': [0.898467250384832, 0.9026611268936877, 0.9002020737204186], 'avg_f1': 0.9004434836663128}, (0.3, 0.01, 512): {'folds': [0.8934013528018699, 0.9026545352165455, 0.8988983049739071], 'avg_f1': 0.8983180643307742}, (0.4, 0.01, 512): {'folds': [0.9002786936879846, 0.9004407826705525, 0.9028687123820461], 'avg_f1': 0.9011960629135277}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.01, hidden_dim 512: 0.9012\n",
      "Best Parameters: {'learning_rate': 0.01, 'hidden_dim': 256, 'drop_out': 0.3}, Best F1 Score: 0.9023\n",
      "All results: {(0.2, 0.001, 128): {'folds': [0.8968577463399633, 0.896690711626011, 0.883092822622029], 'avg_f1': 0.8922137601960012}, (0.3, 0.001, 128): {'folds': [0.8935392772988573, 0.8960971348851793, 0.89402113096031], 'avg_f1': 0.8945525143814489}, (0.4, 0.001, 128): {'folds': [0.8947100126121516, 0.8938211975802075, 0.8891981714079995], 'avg_f1': 0.8925764605334529}, (0.2, 0.001, 256): {'folds': [0.8975156615345858, 0.9004451313550933, 0.8988609048239212], 'avg_f1': 0.8989405659045335}, (0.3, 0.001, 256): {'folds': [0.899002759525854, 0.8979973640597733, 0.8958958320097248], 'avg_f1': 0.8976319851984508}, (0.4, 0.001, 256): {'folds': [0.8986179446832488, 0.8988391645264462, 0.8887698957447585], 'avg_f1': 0.8954090016514845}, (0.2, 0.001, 512): {'folds': [0.9012083527164816, 0.901601138458137, 0.900278072515808], 'avg_f1': 0.901029187896809}, (0.3, 0.001, 512): {'folds': [0.9012098941667989, 0.9010168659613965, 0.8992458571855055], 'avg_f1': 0.9004908724379003}, (0.4, 0.001, 512): {'folds': [0.8967264716041199, 0.9014811433148544, 0.8977838744014929], 'avg_f1': 0.898663829773489}, (0.2, 0.005, 128): {'folds': [0.9000229190092077, 0.9028692375146407, 0.8984341818917454], 'avg_f1': 0.9004421128051979}, (0.3, 0.005, 128): {'folds': [0.900711583801615, 0.9008924011989246, 0.8980807595670652], 'avg_f1': 0.8998949148558683}, (0.4, 0.005, 128): {'folds': [0.9005398194480148, 0.9017335169244084, 0.9009625626605935], 'avg_f1': 0.9010786330110055}, (0.2, 0.005, 256): {'folds': [0.9000729649186112, 0.9025434744752675, 0.9003099206637535], 'avg_f1': 0.9009754533525441}, (0.3, 0.005, 256): {'folds': [0.9020896436642751, 0.9028234855504793, 0.90060624023804], 'avg_f1': 0.9018397898175982}, (0.4, 0.005, 256): {'folds': [0.9014376803471746, 0.904616803291014, 0.9005072524234226], 'avg_f1': 0.9021872453538705}, (0.2, 0.005, 512): {'folds': [0.9024498384586813, 0.9031165373978257, 0.9006047543660932], 'avg_f1': 0.9020570434075333}, (0.3, 0.005, 512): {'folds': [0.8982955254854459, 0.9011110606276802, 0.9005931232288987], 'avg_f1': 0.8999999031140082}, (0.4, 0.005, 512): {'folds': [0.9006590700899209, 0.901347428500428, 0.9015984327762246], 'avg_f1': 0.9012016437888578}, (0.2, 0.01, 128): {'folds': [0.901870319191538, 0.9040393612174754, 0.8999763380225619], 'avg_f1': 0.9019620061438585}, (0.3, 0.01, 128): {'folds': [0.9027219629511057, 0.9034388144047479, 0.8999139256261134], 'avg_f1': 0.902024900993989}, (0.4, 0.01, 128): {'folds': [0.9030675210138166, 0.9024192664068831, 0.8999098590340374], 'avg_f1': 0.9017988821515791}, (0.2, 0.01, 256): {'folds': [0.9032519807412079, 0.9030196782539881, 0.9002162321565016], 'avg_f1': 0.9021626303838991}, (0.3, 0.01, 256): {'folds': [0.9015976286580346, 0.9020191847096248, 0.9032051447266346], 'avg_f1': 0.9022739860314314}, (0.4, 0.01, 256): {'folds': [0.9013818756127574, 0.9031789537606468, 0.9014197070694532], 'avg_f1': 0.9019935121476191}, (0.2, 0.01, 512): {'folds': [0.898467250384832, 0.9026611268936877, 0.9002020737204186], 'avg_f1': 0.9004434836663128}, (0.3, 0.01, 512): {'folds': [0.8934013528018699, 0.9026545352165455, 0.8988983049739071], 'avg_f1': 0.8983180643307742}, (0.4, 0.01, 512): {'folds': [0.9002786936879846, 0.9004407826705525, 0.9028687123820461], 'avg_f1': 0.9011960629135277}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def grid_search(data, epochs, learning_rates, hidden_dims, drop_outs):\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_params = {}\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Precompute the train and validation graphs for all folds\n",
    "    folds = []\n",
    "    for train_idx, val_idx in skf.split(data, data[label_col]):\n",
    "        train_df = data.iloc[train_idx]\n",
    "        val_df = data.iloc[val_idx]\n",
    "\n",
    "        G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "        G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "\n",
    "        G_pyg_train = G_pyg_train.to(device)\n",
    "        G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "        G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "        G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "        G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "        G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "        folds.append((G_pyg_train, G_pyg_val))\n",
    "\n",
    "    params_results = {}\n",
    "    for lr in learning_rates:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for drop_out in drop_outs:\n",
    "                print(f\"Testing with learning rate: {lr}, hidden_dim: {hidden_dim}\")\n",
    "                fold_f1_scores = []\n",
    "\n",
    "                for fold, (G_pyg_train, G_pyg_val) in enumerate(folds):\n",
    "                    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "                    model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                                    edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                                    hidden_channels=hidden_dim,\n",
    "                                    dropout=drop_out,\n",
    "                                    out_channels=num_classes).to(device)\n",
    "\n",
    "                    model.apply(init_weights)\n",
    "\n",
    "                    labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "                    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                                    classes=np.unique(labels),\n",
    "                                                                    y=labels)\n",
    "\n",
    "                    # Normalize to stabilize training\n",
    "                    class_weights = th.FloatTensor(class_weights).to(device)\n",
    "                    print(\"Class weights:\", class_weights)\n",
    "\n",
    "                    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "                    optimizer = th.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    best_epoch_f1 = 0  # Track the best F1 score for this fold\n",
    "\n",
    "                    for epoch in range(epochs):\n",
    "                        train_loss = 0\n",
    "                        val_loss = 0\n",
    "\n",
    "                        try:\n",
    "                            model.train()\n",
    "                            out = model(G_pyg_train)\n",
    "                            \n",
    "                            optimizer.zero_grad()\n",
    "                            loss = criterion(out, G_pyg_train.edge_label)\n",
    "                            train_loss = loss.item()\n",
    "\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                            model.eval()\n",
    "                            with th.no_grad():\n",
    "                                out = model(G_pyg_val)\n",
    "                                loss = criterion(out, G_pyg_val.edge_label)\n",
    "                                val_loss = loss.item()\n",
    "\n",
    "                            val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "\n",
    "                            if val_f1 > best_epoch_f1:\n",
    "                                best_epoch_f1 = val_f1  # Update the best F1 score for this fold\n",
    "                                print(f\"Best F1 Score at epoch {epoch}: {best_epoch_f1:.4f}, Parameters: lr={lr}, hidden_dim{hidden_dim}, drop_out={drop_out}\")\n",
    "\n",
    "                            if epoch % 100 == 0:\n",
    "                                print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred at epoch {epoch}: {str(e)}\")\n",
    "                            break\n",
    "\n",
    "                    fold_f1_scores.append(best_epoch_f1)  # Append the best F1 score for this fold\n",
    "                \n",
    "                avg_f1 = sum(fold_f1_scores) / len(fold_f1_scores)\n",
    "                params_results[(drop_out, lr, hidden_dim)] = {'folds': fold_f1_scores, 'avg_f1': avg_f1}\n",
    "                print(\"Current Results: \", params_results)\n",
    "                print(f\"Average F1 Score for dropout {drop_out} learning rate {lr}, hidden_dim {hidden_dim}: {avg_f1:.4f}\")\n",
    "\n",
    "                if avg_f1 > best_f1:\n",
    "                    best_f1 = avg_f1\n",
    "                    best_params = {'learning_rate': lr, 'hidden_dim': hidden_dim, 'drop_out': drop_out}\n",
    "\n",
    "        print(f\"Best Parameters: {best_params}, Best F1 Score: {best_f1:.4f}\")\n",
    "        print(\"All results:\", params_results)\n",
    "\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "hidden_dims = [128, 256, 512]\n",
    "drop_outs = [0.2, 0.3, 0.4]\n",
    "\n",
    "# grid_search(train_full_df, epochs=100, learning_rates=learning_rates, hidden_dims=hidden_dims, drop_outs=drop_outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f52b2fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 392432\n",
      "Number of node in G_pyg: 226626\n",
      "Shape of node in G_pyg: torch.Size([226626, 51])\n",
      "Shape of edge attr in G_pyg: torch.Size([392432, 51])\n",
      "Shape of edge label in G_pyg: torch.Size([392432])\n",
      "Number of edges in G_pyg: 69253\n",
      "Number of node in G_pyg: 44952\n",
      "Shape of node in G_pyg: torch.Size([44952, 51])\n",
      "Shape of edge attr in G_pyg: torch.Size([69253, 51])\n",
      "Shape of edge label in G_pyg: torch.Size([69253])\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "     train_full_df, test_size=0.15, random_state=42, stratify=train_full_df[label_col])\n",
    "\n",
    "G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ab34f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_process(train_losses, val_losses, val_f1, saved_model_epochs):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "    # plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "    plt.plot(val_f1, label='Validation F1', color='green')\n",
    "    plt.scatter(saved_model_epochs, [val_f1[epoch] for epoch in saved_model_epochs], color='red', label='Saved Model', zorder=5)\n",
    "    plt.title('Train and Validation Metrics')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88bdffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([1.8447e+01, 2.7506e+01, 9.2424e+01, 3.0195e+00, 1.1090e+00, 2.0366e+00,\n",
      "        2.2915e-01, 2.2255e-01, 3.5301e+00, 3.2700e+01, 2.8314e+02],\n",
      "       device='cuda:0')\n",
      "Resumed training from epoch 33\n",
      "Epoch 33 Saved best model. Best F1: 0.8819925129967131\n",
      "Epoch 33, Train Loss: 1.2345, Validation Loss: 1.6728, Validation F1: 0.8820\n",
      "Epoch 34, Train Loss: 1.1682, Validation Loss: 1.6842, Validation F1: 0.8783\n",
      "Epoch 35, Train Loss: 1.2187, Validation Loss: 1.7162, Validation F1: 0.8802\n",
      "Epoch 36, Train Loss: 1.1892, Validation Loss: 1.7086, Validation F1: 0.8769\n",
      "Epoch 37, Train Loss: 1.1777, Validation Loss: 1.7755, Validation F1: 0.8778\n",
      "Epoch 38, Train Loss: 1.1583, Validation Loss: 1.5798, Validation F1: 0.8780\n",
      "Epoch 39, Train Loss: 1.1415, Validation Loss: 1.6470, Validation F1: 0.8752\n",
      "Epoch 40, Train Loss: 1.1930, Validation Loss: 1.6558, Validation F1: 0.8804\n",
      "Epoch 41 Saved best model. Best F1: 0.8839981468763695\n",
      "Epoch 41, Train Loss: 1.1473, Validation Loss: 1.6802, Validation F1: 0.8840\n",
      "Epoch 42, Train Loss: 1.1378, Validation Loss: 1.6282, Validation F1: 0.8828\n",
      "Epoch 43, Train Loss: 1.1312, Validation Loss: 1.6451, Validation F1: 0.8772\n",
      "Epoch 44 Saved best model. Best F1: 0.8869046113100176\n",
      "Epoch 44, Train Loss: 1.1051, Validation Loss: 1.6449, Validation F1: 0.8869\n",
      "Epoch 45, Train Loss: 1.1242, Validation Loss: 1.6554, Validation F1: 0.8819\n",
      "Epoch 46, Train Loss: 1.1315, Validation Loss: 1.6590, Validation F1: 0.8804\n",
      "Epoch 47, Train Loss: 1.1036, Validation Loss: 1.6288, Validation F1: 0.8815\n",
      "Epoch 48, Train Loss: 1.1146, Validation Loss: 1.6225, Validation F1: 0.8840\n",
      "Epoch 49, Train Loss: 1.1310, Validation Loss: 1.7911, Validation F1: 0.8827\n",
      "Epoch 50, Train Loss: 1.1179, Validation Loss: 1.7068, Validation F1: 0.8819\n",
      "Epoch 51, Train Loss: 1.0862, Validation Loss: 1.6226, Validation F1: 0.8807\n",
      "Epoch 52, Train Loss: 1.1140, Validation Loss: 1.6721, Validation F1: 0.8810\n",
      "Epoch 53, Train Loss: 1.2013, Validation Loss: 1.6104, Validation F1: 0.8821\n",
      "Epoch 54, Train Loss: 1.1741, Validation Loss: 1.5802, Validation F1: 0.8800\n",
      "Epoch 55, Train Loss: 1.1108, Validation Loss: 1.6847, Validation F1: 0.8867\n",
      "Epoch 56, Train Loss: 1.0685, Validation Loss: 1.5210, Validation F1: 0.8855\n",
      "Epoch 57, Train Loss: 1.1130, Validation Loss: 1.6569, Validation F1: 0.8834\n",
      "Epoch 58, Train Loss: 1.1194, Validation Loss: 1.6467, Validation F1: 0.8825\n",
      "Epoch 59, Train Loss: 1.0631, Validation Loss: 1.6644, Validation F1: 0.8832\n",
      "Epoch 60, Train Loss: 1.0692, Validation Loss: 1.6496, Validation F1: 0.8831\n",
      "Epoch 61, Train Loss: 1.0940, Validation Loss: 1.5738, Validation F1: 0.8832\n",
      "Epoch 62, Train Loss: 1.1018, Validation Loss: 1.5295, Validation F1: 0.8857\n",
      "Epoch 63, Train Loss: 1.0496, Validation Loss: 1.5665, Validation F1: 0.8842\n",
      "Epoch 64, Train Loss: 1.0763, Validation Loss: 1.5588, Validation F1: 0.8849\n",
      "Epoch 65, Train Loss: 1.0637, Validation Loss: 1.4723, Validation F1: 0.8813\n",
      "Epoch 66, Train Loss: 1.0635, Validation Loss: 1.4662, Validation F1: 0.8810\n",
      "Epoch 67, Train Loss: 1.0804, Validation Loss: 1.6150, Validation F1: 0.8825\n",
      "Epoch 68, Train Loss: 1.0400, Validation Loss: 1.6444, Validation F1: 0.8821\n",
      "Epoch 69, Train Loss: 1.0531, Validation Loss: 1.6931, Validation F1: 0.8831\n",
      "Epoch 70, Train Loss: 1.0324, Validation Loss: 1.6976, Validation F1: 0.8859\n",
      "Epoch 71, Train Loss: 1.0678, Validation Loss: 1.6272, Validation F1: 0.8849\n",
      "Epoch 72, Train Loss: 1.0521, Validation Loss: 1.6518, Validation F1: 0.8825\n",
      "Epoch 73, Train Loss: 1.0267, Validation Loss: 1.5197, Validation F1: 0.8857\n",
      "Epoch 74, Train Loss: 1.0378, Validation Loss: 1.4574, Validation F1: 0.8819\n",
      "Epoch 75, Train Loss: 1.0158, Validation Loss: 1.5047, Validation F1: 0.8867\n",
      "Epoch 76, Train Loss: 1.0198, Validation Loss: 1.5366, Validation F1: 0.8839\n",
      "Epoch 77, Train Loss: 1.0121, Validation Loss: 1.5300, Validation F1: 0.8829\n",
      "Epoch 78, Train Loss: 1.0123, Validation Loss: 1.5159, Validation F1: 0.8843\n",
      "Epoch 79, Train Loss: 1.0254, Validation Loss: 1.5071, Validation F1: 0.8817\n",
      "Epoch 80 Saved best model. Best F1: 0.8910635382101831\n",
      "Epoch 80, Train Loss: 1.0614, Validation Loss: 1.4986, Validation F1: 0.8911\n",
      "Epoch 81, Train Loss: 1.0077, Validation Loss: 1.5409, Validation F1: 0.8842\n",
      "Epoch 82, Train Loss: 1.0098, Validation Loss: 1.5594, Validation F1: 0.8862\n",
      "Epoch 83, Train Loss: 1.0070, Validation Loss: 1.5987, Validation F1: 0.8842\n",
      "Epoch 84, Train Loss: 1.0212, Validation Loss: 1.5192, Validation F1: 0.8871\n",
      "Epoch 85, Train Loss: 1.0140, Validation Loss: 1.4785, Validation F1: 0.8852\n",
      "Epoch 86, Train Loss: 1.0125, Validation Loss: 1.5321, Validation F1: 0.8891\n",
      "Epoch 87, Train Loss: 1.0016, Validation Loss: 1.4450, Validation F1: 0.8865\n",
      "Epoch 88, Train Loss: 0.9920, Validation Loss: 1.5210, Validation F1: 0.8848\n",
      "Epoch 89, Train Loss: 1.0005, Validation Loss: 1.5368, Validation F1: 0.8904\n",
      "Epoch 90, Train Loss: 0.9798, Validation Loss: 1.6012, Validation F1: 0.8846\n",
      "Epoch 91, Train Loss: 1.0307, Validation Loss: 1.5058, Validation F1: 0.8857\n",
      "Epoch 92, Train Loss: 1.0094, Validation Loss: 1.4753, Validation F1: 0.8867\n",
      "Epoch 93 Saved best model. Best F1: 0.8948855759015638\n",
      "Epoch 93, Train Loss: 0.9991, Validation Loss: 1.4255, Validation F1: 0.8949\n",
      "Epoch 94, Train Loss: 0.9891, Validation Loss: 1.5774, Validation F1: 0.8832\n",
      "Epoch 95, Train Loss: 0.9769, Validation Loss: 1.4966, Validation F1: 0.8860\n",
      "Epoch 96, Train Loss: 0.9865, Validation Loss: 1.5870, Validation F1: 0.8849\n",
      "Epoch 97, Train Loss: 1.0277, Validation Loss: 1.5198, Validation F1: 0.8856\n",
      "Epoch 98, Train Loss: 0.9911, Validation Loss: 1.4769, Validation F1: 0.8865\n",
      "Epoch 99, Train Loss: 1.0238, Validation Loss: 1.5228, Validation F1: 0.8897\n",
      "Epoch 100, Train Loss: 0.9746, Validation Loss: 1.4899, Validation F1: 0.8881\n",
      "Epoch 101, Train Loss: 1.0192, Validation Loss: 1.4855, Validation F1: 0.8863\n",
      "Epoch 102, Train Loss: 0.9758, Validation Loss: 1.4296, Validation F1: 0.8865\n",
      "Epoch 103, Train Loss: 0.9965, Validation Loss: 1.4696, Validation F1: 0.8835\n",
      "Epoch 104, Train Loss: 0.9911, Validation Loss: 1.4504, Validation F1: 0.8886\n",
      "Epoch 105 Saved best model. Best F1: 0.899925646056199\n",
      "Epoch 105, Train Loss: 0.9927, Validation Loss: 1.3916, Validation F1: 0.8999\n",
      "Epoch 106, Train Loss: 0.9703, Validation Loss: 1.3840, Validation F1: 0.8931\n",
      "Epoch 107 Saved best model. Best F1: 0.9013085341453655\n",
      "Epoch 107, Train Loss: 0.9752, Validation Loss: 1.4490, Validation F1: 0.9013\n",
      "Epoch 108, Train Loss: 0.9912, Validation Loss: 1.5027, Validation F1: 0.8944\n",
      "Epoch 109, Train Loss: 1.0052, Validation Loss: 1.4473, Validation F1: 0.8984\n",
      "Epoch 110, Train Loss: 0.9645, Validation Loss: 1.4768, Validation F1: 0.8986\n",
      "Epoch 111, Train Loss: 0.9756, Validation Loss: 1.4582, Validation F1: 0.8800\n",
      "Epoch 112, Train Loss: 0.9660, Validation Loss: 1.4188, Validation F1: 0.8827\n",
      "Epoch 113, Train Loss: 0.9803, Validation Loss: 1.4276, Validation F1: 0.8853\n",
      "Epoch 114, Train Loss: 0.9692, Validation Loss: 1.4547, Validation F1: 0.8942\n",
      "Epoch 115, Train Loss: 0.9634, Validation Loss: 1.4643, Validation F1: 0.8883\n",
      "Epoch 116, Train Loss: 0.9517, Validation Loss: 1.4521, Validation F1: 0.8976\n",
      "Epoch 117, Train Loss: 1.0016, Validation Loss: 1.4422, Validation F1: 0.8995\n",
      "Epoch 118, Train Loss: 0.9648, Validation Loss: 1.4784, Validation F1: 0.8845\n",
      "Epoch 119, Train Loss: 0.9530, Validation Loss: 1.4401, Validation F1: 0.8984\n",
      "Epoch 120, Train Loss: 0.9718, Validation Loss: 1.4615, Validation F1: 0.8815\n",
      "Epoch 121, Train Loss: 0.9594, Validation Loss: 1.4469, Validation F1: 0.8902\n",
      "Epoch 122, Train Loss: 0.9574, Validation Loss: 1.4296, Validation F1: 0.8846\n",
      "Epoch 123, Train Loss: 0.9635, Validation Loss: 1.4150, Validation F1: 0.8925\n",
      "Epoch 124, Train Loss: 0.9620, Validation Loss: 1.4710, Validation F1: 0.8922\n",
      "Epoch 125, Train Loss: 0.9544, Validation Loss: 1.4229, Validation F1: 0.8949\n",
      "Epoch 126, Train Loss: 0.9508, Validation Loss: 1.4174, Validation F1: 0.8949\n",
      "Epoch 127, Train Loss: 0.9486, Validation Loss: 1.4025, Validation F1: 0.8891\n",
      "Epoch 128, Train Loss: 0.9603, Validation Loss: 1.4390, Validation F1: 0.8995\n",
      "Epoch 129, Train Loss: 0.9577, Validation Loss: 1.4165, Validation F1: 0.8852\n",
      "Epoch 130, Train Loss: 0.9528, Validation Loss: 1.3443, Validation F1: 0.8965\n",
      "Epoch 131, Train Loss: 0.9553, Validation Loss: 1.3959, Validation F1: 0.8957\n",
      "Epoch 132, Train Loss: 0.9562, Validation Loss: 1.3920, Validation F1: 0.8981\n",
      "Epoch 133, Train Loss: 0.9443, Validation Loss: 1.3827, Validation F1: 0.8985\n",
      "Epoch 134, Train Loss: 0.9435, Validation Loss: 1.4014, Validation F1: 0.8915\n",
      "Epoch 135, Train Loss: 0.9453, Validation Loss: 1.3727, Validation F1: 0.8989\n",
      "Epoch 136, Train Loss: 0.9479, Validation Loss: 1.4020, Validation F1: 0.9011\n",
      "Epoch 137 Saved best model. Best F1: 0.9015124653853155\n",
      "Epoch 137, Train Loss: 0.9385, Validation Loss: 1.3456, Validation F1: 0.9015\n",
      "Epoch 138, Train Loss: 0.9388, Validation Loss: 1.3818, Validation F1: 0.8894\n",
      "Epoch 139, Train Loss: 0.9397, Validation Loss: 1.4251, Validation F1: 0.8948\n",
      "Epoch 140, Train Loss: 0.9415, Validation Loss: 1.4126, Validation F1: 0.8941\n",
      "Epoch 141, Train Loss: 0.9324, Validation Loss: 1.4699, Validation F1: 0.8904\n",
      "Epoch 142, Train Loss: 0.9515, Validation Loss: 1.4107, Validation F1: 0.8877\n",
      "Epoch 143, Train Loss: 0.9563, Validation Loss: 1.4250, Validation F1: 0.8964\n",
      "Epoch 144, Train Loss: 0.9340, Validation Loss: 1.4801, Validation F1: 0.9015\n",
      "Epoch 145, Train Loss: 0.9366, Validation Loss: 1.3994, Validation F1: 0.8986\n",
      "Epoch 146, Train Loss: 0.9377, Validation Loss: 1.3664, Validation F1: 0.8926\n",
      "Epoch 147, Train Loss: 0.9375, Validation Loss: 1.4458, Validation F1: 0.8915\n",
      "Epoch 148, Train Loss: 0.9322, Validation Loss: 1.5017, Validation F1: 0.8856\n",
      "Epoch 149, Train Loss: 0.9364, Validation Loss: 1.5367, Validation F1: 0.8825\n",
      "Epoch 150, Train Loss: 0.9243, Validation Loss: 1.4593, Validation F1: 0.8796\n",
      "Epoch 151, Train Loss: 0.9281, Validation Loss: 1.5118, Validation F1: 0.8864\n",
      "Epoch 152, Train Loss: 0.9314, Validation Loss: 1.5057, Validation F1: 0.8806\n",
      "Epoch 153, Train Loss: 0.9331, Validation Loss: 1.4346, Validation F1: 0.8842\n",
      "Epoch 154, Train Loss: 0.9249, Validation Loss: 1.4472, Validation F1: 0.8975\n",
      "Epoch 155, Train Loss: 0.9314, Validation Loss: 1.3938, Validation F1: 0.8906\n",
      "Epoch 156, Train Loss: 0.9271, Validation Loss: 1.4524, Validation F1: 0.8921\n",
      "Epoch 157, Train Loss: 0.9296, Validation Loss: 1.4178, Validation F1: 0.8956\n",
      "Epoch 158, Train Loss: 0.9360, Validation Loss: 1.3920, Validation F1: 0.8956\n",
      "Epoch 159, Train Loss: 0.9242, Validation Loss: 1.4223, Validation F1: 0.8921\n",
      "Epoch 160, Train Loss: 0.9249, Validation Loss: 1.4159, Validation F1: 0.9010\n",
      "Epoch 161, Train Loss: 0.9402, Validation Loss: 1.4128, Validation F1: 0.9007\n",
      "Epoch 162, Train Loss: 0.9295, Validation Loss: 1.4034, Validation F1: 0.8992\n",
      "Epoch 163, Train Loss: 0.9188, Validation Loss: 1.3988, Validation F1: 0.8923\n",
      "Epoch 164, Train Loss: 0.9249, Validation Loss: 1.4055, Validation F1: 0.8931\n",
      "Epoch 165, Train Loss: 0.9148, Validation Loss: 1.4688, Validation F1: 0.8962\n",
      "Epoch 166, Train Loss: 0.9249, Validation Loss: 1.5176, Validation F1: 0.8841\n",
      "Epoch 167, Train Loss: 0.9263, Validation Loss: 1.4122, Validation F1: 0.8836\n",
      "Epoch 168, Train Loss: 0.9205, Validation Loss: 1.3682, Validation F1: 0.8932\n",
      "Epoch 169, Train Loss: 0.9198, Validation Loss: 1.3802, Validation F1: 0.8895\n",
      "Epoch 170, Train Loss: 0.9253, Validation Loss: 1.4131, Validation F1: 0.8919\n",
      "Epoch 171, Train Loss: 0.9243, Validation Loss: 1.4052, Validation F1: 0.8949\n",
      "Epoch 172, Train Loss: 0.9181, Validation Loss: 1.3418, Validation F1: 0.8960\n",
      "Epoch 173, Train Loss: 0.9276, Validation Loss: 1.3955, Validation F1: 0.9004\n",
      "Epoch 174, Train Loss: 0.9125, Validation Loss: 1.4192, Validation F1: 0.9015\n",
      "Epoch 175, Train Loss: 0.9138, Validation Loss: 1.4085, Validation F1: 0.9003\n",
      "Epoch 176, Train Loss: 0.9177, Validation Loss: 1.4742, Validation F1: 0.8974\n",
      "Epoch 177, Train Loss: 0.9119, Validation Loss: 1.3927, Validation F1: 0.9003\n",
      "Epoch 178, Train Loss: 0.9096, Validation Loss: 1.4782, Validation F1: 0.8950\n",
      "Epoch 179, Train Loss: 0.9433, Validation Loss: 1.4962, Validation F1: 0.8955\n",
      "Epoch 180, Train Loss: 0.9155, Validation Loss: 1.4405, Validation F1: 0.8897\n",
      "Epoch 181, Train Loss: 0.9089, Validation Loss: 1.4175, Validation F1: 0.8966\n",
      "Epoch 182, Train Loss: 0.9184, Validation Loss: 1.4087, Validation F1: 0.8954\n",
      "Epoch 183, Train Loss: 0.9159, Validation Loss: 1.4568, Validation F1: 0.8891\n",
      "Epoch 184, Train Loss: 0.9213, Validation Loss: 1.3844, Validation F1: 0.8995\n",
      "Epoch 185, Train Loss: 0.9166, Validation Loss: 1.3803, Validation F1: 0.8917\n",
      "Epoch 186, Train Loss: 0.9215, Validation Loss: 1.3985, Validation F1: 0.8834\n",
      "Epoch 187 Saved best model. Best F1: 0.9032495858491331\n",
      "Epoch 187, Train Loss: 0.9193, Validation Loss: 1.4051, Validation F1: 0.9032\n",
      "Epoch 188, Train Loss: 0.9285, Validation Loss: 1.3534, Validation F1: 0.8920\n",
      "Epoch 189, Train Loss: 0.9536, Validation Loss: 1.4210, Validation F1: 0.8869\n",
      "Epoch 190, Train Loss: 0.9154, Validation Loss: 1.3796, Validation F1: 0.8874\n",
      "Epoch 191, Train Loss: 0.9110, Validation Loss: 1.4008, Validation F1: 0.8897\n",
      "Epoch 192, Train Loss: 0.9306, Validation Loss: 1.4028, Validation F1: 0.8935\n",
      "Epoch 193, Train Loss: 0.9218, Validation Loss: 1.3510, Validation F1: 0.8883\n",
      "Epoch 194, Train Loss: 0.9160, Validation Loss: 1.3585, Validation F1: 0.8888\n",
      "Epoch 195, Train Loss: 0.9100, Validation Loss: 1.3892, Validation F1: 0.8935\n",
      "Epoch 196, Train Loss: 0.9157, Validation Loss: 1.3458, Validation F1: 0.8907\n",
      "Epoch 197, Train Loss: 0.9186, Validation Loss: 1.3473, Validation F1: 0.8997\n",
      "Epoch 198, Train Loss: 0.9092, Validation Loss: 1.3334, Validation F1: 0.8957\n",
      "Epoch 199, Train Loss: 0.9087, Validation Loss: 1.3763, Validation F1: 0.8951\n",
      "Epoch 200, Train Loss: 0.9077, Validation Loss: 1.3754, Validation F1: 0.8934\n",
      "Epoch 201, Train Loss: 0.9098, Validation Loss: 1.3550, Validation F1: 0.8951\n",
      "Epoch 202, Train Loss: 0.9104, Validation Loss: 1.3444, Validation F1: 0.8969\n",
      "Epoch 203, Train Loss: 0.9096, Validation Loss: 1.3831, Validation F1: 0.9005\n",
      "Epoch 204, Train Loss: 0.9019, Validation Loss: 1.3194, Validation F1: 0.8985\n",
      "Epoch 205, Train Loss: 0.9004, Validation Loss: 1.3592, Validation F1: 0.9019\n",
      "Epoch 206, Train Loss: 0.9025, Validation Loss: 1.4345, Validation F1: 0.9010\n",
      "Epoch 207, Train Loss: 0.9083, Validation Loss: 1.4254, Validation F1: 0.9026\n",
      "Epoch 208, Train Loss: 0.9066, Validation Loss: 1.3961, Validation F1: 0.8962\n",
      "Epoch 209, Train Loss: 0.9023, Validation Loss: 1.4067, Validation F1: 0.8889\n",
      "Epoch 210, Train Loss: 0.8962, Validation Loss: 1.4150, Validation F1: 0.9011\n",
      "Epoch 211, Train Loss: 0.8957, Validation Loss: 1.4257, Validation F1: 0.8876\n",
      "Epoch 212, Train Loss: 0.8941, Validation Loss: 1.4693, Validation F1: 0.8791\n",
      "Epoch 213, Train Loss: 0.9047, Validation Loss: 1.4274, Validation F1: 0.8868\n",
      "Epoch 214, Train Loss: 0.8947, Validation Loss: 1.3629, Validation F1: 0.8944\n",
      "Epoch 215, Train Loss: 0.8965, Validation Loss: 1.4104, Validation F1: 0.8899\n",
      "Epoch 216, Train Loss: 0.9000, Validation Loss: 1.3580, Validation F1: 0.8995\n",
      "Epoch 217, Train Loss: 0.8997, Validation Loss: 1.4145, Validation F1: 0.8988\n",
      "Epoch 218, Train Loss: 0.9022, Validation Loss: 1.4529, Validation F1: 0.8954\n",
      "Epoch 219, Train Loss: 0.8938, Validation Loss: 1.4322, Validation F1: 0.8960\n",
      "Epoch 220, Train Loss: 0.8962, Validation Loss: 1.3791, Validation F1: 0.8939\n",
      "Epoch 221, Train Loss: 0.9017, Validation Loss: 1.4430, Validation F1: 0.8893\n",
      "Epoch 222, Train Loss: 0.8937, Validation Loss: 1.4862, Validation F1: 0.8887\n",
      "Epoch 223, Train Loss: 0.9057, Validation Loss: 1.4035, Validation F1: 0.8889\n",
      "Epoch 224, Train Loss: 0.8956, Validation Loss: 1.4366, Validation F1: 0.8880\n",
      "Epoch 225, Train Loss: 0.8899, Validation Loss: 1.4835, Validation F1: 0.8913\n",
      "Epoch 226, Train Loss: 0.9052, Validation Loss: 1.4225, Validation F1: 0.8883\n",
      "Epoch 227, Train Loss: 0.8925, Validation Loss: 1.4331, Validation F1: 0.8955\n",
      "Epoch 228, Train Loss: 0.9041, Validation Loss: 1.4586, Validation F1: 0.8953\n",
      "Epoch 229, Train Loss: 0.8998, Validation Loss: 1.4330, Validation F1: 0.8815\n",
      "Epoch 230, Train Loss: 0.9122, Validation Loss: 1.4366, Validation F1: 0.8841\n",
      "Epoch 231, Train Loss: 0.8932, Validation Loss: 1.5586, Validation F1: 0.8802\n",
      "Epoch 232, Train Loss: 0.8979, Validation Loss: 1.6651, Validation F1: 0.8789\n",
      "Epoch 233, Train Loss: 0.9045, Validation Loss: 1.4308, Validation F1: 0.8883\n",
      "Epoch 234, Train Loss: 0.9072, Validation Loss: 1.4413, Validation F1: 0.8926\n",
      "Epoch 235, Train Loss: 0.8938, Validation Loss: 1.4417, Validation F1: 0.8929\n",
      "Epoch 236, Train Loss: 0.8941, Validation Loss: 1.4458, Validation F1: 0.8973\n",
      "Epoch 237, Train Loss: 0.8959, Validation Loss: 1.3804, Validation F1: 0.8966\n",
      "Epoch 238, Train Loss: 0.8952, Validation Loss: 1.3639, Validation F1: 0.8959\n",
      "Epoch 239, Train Loss: 0.8903, Validation Loss: 1.3661, Validation F1: 0.9001\n",
      "Epoch 240, Train Loss: 0.8914, Validation Loss: 1.4990, Validation F1: 0.9008\n",
      "Epoch 241, Train Loss: 0.8888, Validation Loss: 1.3438, Validation F1: 0.9002\n",
      "Epoch 242, Train Loss: 0.8906, Validation Loss: 1.4223, Validation F1: 0.9014\n",
      "Epoch 243, Train Loss: 0.8861, Validation Loss: 1.4317, Validation F1: 0.9031\n",
      "Epoch 244, Train Loss: 0.8872, Validation Loss: 1.4102, Validation F1: 0.9007\n",
      "Epoch 245, Train Loss: 0.8860, Validation Loss: 1.3600, Validation F1: 0.8981\n",
      "Epoch 246, Train Loss: 0.8948, Validation Loss: 1.4583, Validation F1: 0.8870\n",
      "Epoch 247, Train Loss: 0.8897, Validation Loss: 1.4130, Validation F1: 0.8935\n",
      "Epoch 248, Train Loss: 0.8853, Validation Loss: 1.4281, Validation F1: 0.8931\n",
      "Epoch 249, Train Loss: 0.8871, Validation Loss: 1.4197, Validation F1: 0.8997\n",
      "Epoch 250, Train Loss: 0.8846, Validation Loss: 1.3966, Validation F1: 0.8932\n",
      "Epoch 251, Train Loss: 0.8872, Validation Loss: 1.4010, Validation F1: 0.8961\n",
      "Epoch 252, Train Loss: 0.8830, Validation Loss: 1.3612, Validation F1: 0.8994\n",
      "Epoch 253, Train Loss: 0.8819, Validation Loss: 1.4827, Validation F1: 0.8926\n",
      "Epoch 254, Train Loss: 0.8879, Validation Loss: 1.4025, Validation F1: 0.8978\n",
      "Epoch 255, Train Loss: 0.8822, Validation Loss: 1.3712, Validation F1: 0.8928\n",
      "Epoch 256, Train Loss: 0.8831, Validation Loss: 1.3646, Validation F1: 0.8995\n",
      "Epoch 257, Train Loss: 0.8874, Validation Loss: 1.3170, Validation F1: 0.9006\n",
      "Epoch 258, Train Loss: 0.8780, Validation Loss: 1.4741, Validation F1: 0.8974\n",
      "Epoch 259, Train Loss: 0.8831, Validation Loss: 1.3888, Validation F1: 0.8835\n",
      "Epoch 260, Train Loss: 0.8838, Validation Loss: 1.4340, Validation F1: 0.8939\n",
      "Epoch 261, Train Loss: 0.8847, Validation Loss: 1.4137, Validation F1: 0.8941\n",
      "Epoch 262, Train Loss: 0.8966, Validation Loss: 1.4233, Validation F1: 0.8962\n",
      "Epoch 263, Train Loss: 0.8849, Validation Loss: 1.4223, Validation F1: 0.8960\n",
      "Epoch 264, Train Loss: 0.8842, Validation Loss: 1.3902, Validation F1: 0.8939\n",
      "Epoch 265, Train Loss: 0.9063, Validation Loss: 1.4359, Validation F1: 0.8986\n",
      "Epoch 266, Train Loss: 0.8855, Validation Loss: 1.3901, Validation F1: 0.8973\n",
      "Epoch 267, Train Loss: 0.8779, Validation Loss: 1.4027, Validation F1: 0.8899\n",
      "Epoch 268, Train Loss: 0.8818, Validation Loss: 1.4361, Validation F1: 0.8898\n",
      "Epoch 269, Train Loss: 0.8814, Validation Loss: 1.3701, Validation F1: 0.8911\n",
      "Epoch 270, Train Loss: 0.8817, Validation Loss: 1.4407, Validation F1: 0.9021\n",
      "Epoch 271, Train Loss: 0.8798, Validation Loss: 1.4015, Validation F1: 0.8959\n",
      "Epoch 272, Train Loss: 0.8786, Validation Loss: 1.4351, Validation F1: 0.8897\n",
      "Epoch 273, Train Loss: 0.8826, Validation Loss: 1.4756, Validation F1: 0.8801\n",
      "Epoch 274, Train Loss: 0.8801, Validation Loss: 1.3339, Validation F1: 0.8964\n",
      "Epoch 275, Train Loss: 0.8786, Validation Loss: 1.3827, Validation F1: 0.8926\n",
      "Epoch 276, Train Loss: 0.8780, Validation Loss: 1.4286, Validation F1: 0.8911\n",
      "Epoch 277, Train Loss: 0.8842, Validation Loss: 1.5059, Validation F1: 0.8911\n",
      "Epoch 278, Train Loss: 0.8777, Validation Loss: 1.4448, Validation F1: 0.8940\n",
      "Epoch 279, Train Loss: 0.8807, Validation Loss: 1.4289, Validation F1: 0.9013\n",
      "Epoch 280, Train Loss: 0.8732, Validation Loss: 1.3684, Validation F1: 0.8981\n",
      "Epoch 281, Train Loss: 0.8757, Validation Loss: 1.3484, Validation F1: 0.8951\n",
      "Epoch 282, Train Loss: 0.8813, Validation Loss: 1.4325, Validation F1: 0.8932\n",
      "Epoch 283, Train Loss: 0.8749, Validation Loss: 1.4416, Validation F1: 0.9024\n",
      "Epoch 284, Train Loss: 0.8765, Validation Loss: 1.3791, Validation F1: 0.8944\n",
      "Epoch 285, Train Loss: 0.8897, Validation Loss: 1.4606, Validation F1: 0.8863\n",
      "Epoch 286, Train Loss: 0.8758, Validation Loss: 1.4946, Validation F1: 0.8994\n",
      "Epoch 287, Train Loss: 0.8796, Validation Loss: 1.4611, Validation F1: 0.8933\n",
      "Epoch 288, Train Loss: 0.8740, Validation Loss: 1.3617, Validation F1: 0.8931\n",
      "Epoch 289, Train Loss: 0.8747, Validation Loss: 1.3745, Validation F1: 0.9009\n",
      "Epoch 290, Train Loss: 0.8796, Validation Loss: 1.3630, Validation F1: 0.8877\n",
      "Epoch 291, Train Loss: 0.8782, Validation Loss: 1.3965, Validation F1: 0.8961\n",
      "Epoch 292, Train Loss: 0.8797, Validation Loss: 1.3839, Validation F1: 0.9009\n",
      "Epoch 293, Train Loss: 0.8798, Validation Loss: 1.3831, Validation F1: 0.9018\n",
      "Epoch 294, Train Loss: 0.8796, Validation Loss: 1.4579, Validation F1: 0.8895\n",
      "Epoch 295, Train Loss: 0.8763, Validation Loss: 1.4011, Validation F1: 0.8961\n",
      "Epoch 296, Train Loss: 0.8741, Validation Loss: 1.5003, Validation F1: 0.8855\n",
      "Epoch 297, Train Loss: 0.8751, Validation Loss: 1.4432, Validation F1: 0.8855\n",
      "Epoch 298, Train Loss: 0.9002, Validation Loss: 1.4441, Validation F1: 0.9000\n",
      "Epoch 299, Train Loss: 0.8714, Validation Loss: 1.4873, Validation F1: 0.9014\n",
      "Epoch 300, Train Loss: 0.8764, Validation Loss: 1.4776, Validation F1: 0.8875\n",
      "Epoch 301, Train Loss: 0.8796, Validation Loss: 1.4182, Validation F1: 0.8979\n",
      "Epoch 302, Train Loss: 0.8770, Validation Loss: 1.3932, Validation F1: 0.8866\n",
      "Epoch 303, Train Loss: 0.8686, Validation Loss: 1.3782, Validation F1: 0.8922\n",
      "Epoch 304, Train Loss: 0.8813, Validation Loss: 1.4695, Validation F1: 0.8898\n",
      "Epoch 305, Train Loss: 0.8734, Validation Loss: 1.4108, Validation F1: 0.8884\n",
      "Epoch 306, Train Loss: 0.8769, Validation Loss: 1.4695, Validation F1: 0.8929\n",
      "Epoch 307, Train Loss: 0.8772, Validation Loss: 1.4144, Validation F1: 0.8907\n",
      "Epoch 308, Train Loss: 0.8735, Validation Loss: 1.3846, Validation F1: 0.8953\n",
      "Epoch 309, Train Loss: 0.8715, Validation Loss: 1.4154, Validation F1: 0.8927\n",
      "Epoch 310, Train Loss: 0.8722, Validation Loss: 1.3784, Validation F1: 0.8954\n",
      "Epoch 311, Train Loss: 0.8760, Validation Loss: 1.4210, Validation F1: 0.8929\n",
      "Epoch 312, Train Loss: 0.8789, Validation Loss: 1.3793, Validation F1: 0.8905\n",
      "Epoch 313, Train Loss: 0.8823, Validation Loss: 1.3787, Validation F1: 0.8937\n",
      "Epoch 314, Train Loss: 0.8683, Validation Loss: 1.3841, Validation F1: 0.8976\n",
      "Epoch 315, Train Loss: 0.8751, Validation Loss: 1.4399, Validation F1: 0.8894\n",
      "Epoch 316, Train Loss: 0.8730, Validation Loss: 1.4550, Validation F1: 0.8932\n",
      "Epoch 317, Train Loss: 0.8734, Validation Loss: 1.4036, Validation F1: 0.8895\n",
      "Epoch 318, Train Loss: 0.8657, Validation Loss: 1.4522, Validation F1: 0.8967\n",
      "Epoch 319, Train Loss: 0.8768, Validation Loss: 1.4449, Validation F1: 0.9017\n",
      "Epoch 320, Train Loss: 0.8681, Validation Loss: 1.4795, Validation F1: 0.8962\n",
      "Epoch 321, Train Loss: 0.8691, Validation Loss: 1.4920, Validation F1: 0.8888\n",
      "Epoch 322, Train Loss: 0.8713, Validation Loss: 1.5220, Validation F1: 0.8907\n",
      "Epoch 323, Train Loss: 0.8674, Validation Loss: 1.5591, Validation F1: 0.8823\n",
      "Epoch 324, Train Loss: 0.8655, Validation Loss: 1.4882, Validation F1: 0.8913\n",
      "Epoch 325, Train Loss: 0.8707, Validation Loss: 1.4297, Validation F1: 0.8942\n",
      "Epoch 326, Train Loss: 0.8710, Validation Loss: 1.5068, Validation F1: 0.9022\n",
      "Epoch 327, Train Loss: 0.8662, Validation Loss: 1.4851, Validation F1: 0.8901\n",
      "Epoch 328, Train Loss: 0.8662, Validation Loss: 1.4675, Validation F1: 0.8912\n",
      "Epoch 329, Train Loss: 0.8686, Validation Loss: 1.4282, Validation F1: 0.9015\n",
      "Epoch 330, Train Loss: 0.8659, Validation Loss: 1.5252, Validation F1: 0.8992\n",
      "Epoch 331, Train Loss: 0.8701, Validation Loss: 1.5127, Validation F1: 0.9018\n",
      "Epoch 332, Train Loss: 0.8651, Validation Loss: 1.5077, Validation F1: 0.9025\n",
      "Epoch 333, Train Loss: 0.8713, Validation Loss: 1.4764, Validation F1: 0.9021\n",
      "Epoch 334, Train Loss: 0.8689, Validation Loss: 1.5458, Validation F1: 0.8914\n",
      "Epoch 335, Train Loss: 0.8678, Validation Loss: 1.4636, Validation F1: 0.8902\n",
      "Epoch 336, Train Loss: 0.8660, Validation Loss: 1.4345, Validation F1: 0.8887\n",
      "Epoch 337, Train Loss: 0.8668, Validation Loss: 1.4374, Validation F1: 0.8892\n",
      "Epoch 338, Train Loss: 0.8668, Validation Loss: 1.4668, Validation F1: 0.8889\n",
      "Epoch 339, Train Loss: 0.8686, Validation Loss: 1.4367, Validation F1: 0.8940\n",
      "Epoch 340, Train Loss: 0.8639, Validation Loss: 1.3844, Validation F1: 0.8969\n",
      "Epoch 341, Train Loss: 0.8658, Validation Loss: 1.4380, Validation F1: 0.9030\n",
      "Epoch 342, Train Loss: 0.8668, Validation Loss: 1.4575, Validation F1: 0.8964\n",
      "Epoch 343, Train Loss: 0.8708, Validation Loss: 1.3581, Validation F1: 0.8967\n",
      "Epoch 344, Train Loss: 0.8634, Validation Loss: 1.4495, Validation F1: 0.8955\n",
      "Epoch 345, Train Loss: 0.8729, Validation Loss: 1.4484, Validation F1: 0.8863\n",
      "Epoch 346, Train Loss: 0.8643, Validation Loss: 1.4214, Validation F1: 0.8934\n",
      "Epoch 347, Train Loss: 0.8734, Validation Loss: 1.4431, Validation F1: 0.8940\n",
      "Epoch 348, Train Loss: 0.8685, Validation Loss: 1.3863, Validation F1: 0.8966\n",
      "Epoch 349, Train Loss: 0.8695, Validation Loss: 1.4223, Validation F1: 0.9020\n",
      "Epoch 350, Train Loss: 0.8660, Validation Loss: 1.4706, Validation F1: 0.8969\n",
      "Epoch 351, Train Loss: 0.8632, Validation Loss: 1.3897, Validation F1: 0.8990\n",
      "Epoch 352, Train Loss: 0.8629, Validation Loss: 1.4095, Validation F1: 0.8940\n",
      "Epoch 353, Train Loss: 0.8685, Validation Loss: 1.4758, Validation F1: 0.8940\n",
      "Epoch 354, Train Loss: 0.8634, Validation Loss: 1.5278, Validation F1: 0.8966\n",
      "Epoch 355, Train Loss: 0.8628, Validation Loss: 1.4561, Validation F1: 0.8848\n",
      "Epoch 356, Train Loss: 0.8653, Validation Loss: 1.4235, Validation F1: 0.8981\n",
      "Epoch 357, Train Loss: 0.8776, Validation Loss: 1.4548, Validation F1: 0.8988\n",
      "Epoch 358, Train Loss: 0.8651, Validation Loss: 1.4720, Validation F1: 0.8964\n",
      "Epoch 359, Train Loss: 0.8627, Validation Loss: 1.3981, Validation F1: 0.9011\n",
      "Epoch 360, Train Loss: 0.8657, Validation Loss: 1.4090, Validation F1: 0.8990\n",
      "Epoch 361, Train Loss: 0.8688, Validation Loss: 1.4469, Validation F1: 0.8960\n",
      "Epoch 362, Train Loss: 0.8712, Validation Loss: 1.4645, Validation F1: 0.8944\n",
      "Epoch 363, Train Loss: 0.8641, Validation Loss: 1.3587, Validation F1: 0.8939\n",
      "Epoch 364, Train Loss: 0.8651, Validation Loss: 1.4006, Validation F1: 0.8999\n",
      "Epoch 365, Train Loss: 0.8601, Validation Loss: 1.4918, Validation F1: 0.8906\n",
      "Epoch 366, Train Loss: 0.8643, Validation Loss: 1.4154, Validation F1: 0.8973\n",
      "Epoch 367, Train Loss: 0.8640, Validation Loss: 1.5166, Validation F1: 0.8945\n",
      "Epoch 368, Train Loss: 0.8683, Validation Loss: 1.4420, Validation F1: 0.8899\n",
      "Epoch 369, Train Loss: 0.8622, Validation Loss: 1.4625, Validation F1: 0.8901\n",
      "Epoch 370, Train Loss: 0.8609, Validation Loss: 1.4287, Validation F1: 0.8811\n",
      "Epoch 371, Train Loss: 0.8620, Validation Loss: 1.4984, Validation F1: 0.8812\n",
      "Epoch 372, Train Loss: 0.8592, Validation Loss: 1.5295, Validation F1: 0.8816\n",
      "Epoch 373, Train Loss: 0.8644, Validation Loss: 1.4163, Validation F1: 0.8908\n",
      "Epoch 374, Train Loss: 0.8645, Validation Loss: 1.3945, Validation F1: 0.8933\n",
      "Epoch 375, Train Loss: 0.8592, Validation Loss: 1.4436, Validation F1: 0.8957\n",
      "Epoch 376, Train Loss: 0.8705, Validation Loss: 1.4768, Validation F1: 0.8961\n",
      "Epoch 377, Train Loss: 0.8599, Validation Loss: 1.4526, Validation F1: 0.8911\n",
      "Epoch 378, Train Loss: 0.8603, Validation Loss: 1.5980, Validation F1: 0.8926\n",
      "Epoch 379, Train Loss: 0.8584, Validation Loss: 1.3493, Validation F1: 0.8922\n",
      "Epoch 380, Train Loss: 0.8595, Validation Loss: 1.4681, Validation F1: 0.8861\n",
      "Epoch 381, Train Loss: 0.8583, Validation Loss: 1.4721, Validation F1: 0.8958\n",
      "Epoch 382, Train Loss: 0.8610, Validation Loss: 1.4450, Validation F1: 0.8999\n",
      "Epoch 383, Train Loss: 0.8601, Validation Loss: 1.4360, Validation F1: 0.8984\n",
      "Epoch 384, Train Loss: 0.8585, Validation Loss: 1.5007, Validation F1: 0.9021\n",
      "Epoch 385, Train Loss: 0.8572, Validation Loss: 1.4790, Validation F1: 0.8994\n",
      "Epoch 386, Train Loss: 0.8601, Validation Loss: 1.4927, Validation F1: 0.8974\n",
      "Epoch 387, Train Loss: 0.8557, Validation Loss: 1.4664, Validation F1: 0.8947\n",
      "Epoch 388, Train Loss: 0.8559, Validation Loss: 1.5491, Validation F1: 0.8961\n",
      "Epoch 389, Train Loss: 0.8602, Validation Loss: 1.5714, Validation F1: 0.8875\n",
      "Epoch 390, Train Loss: 0.8610, Validation Loss: 1.4766, Validation F1: 0.8995\n",
      "Epoch 391, Train Loss: 0.8611, Validation Loss: 1.4515, Validation F1: 0.8896\n",
      "Epoch 392, Train Loss: 0.8573, Validation Loss: 1.4117, Validation F1: 0.8991\n",
      "Epoch 393, Train Loss: 0.8591, Validation Loss: 1.4558, Validation F1: 0.8982\n",
      "Epoch 394, Train Loss: 0.8577, Validation Loss: 1.4913, Validation F1: 0.9010\n",
      "Epoch 395, Train Loss: 0.8568, Validation Loss: 1.4351, Validation F1: 0.9024\n",
      "Epoch 396, Train Loss: 0.8593, Validation Loss: 1.5295, Validation F1: 0.8890\n",
      "Epoch 397, Train Loss: 0.8548, Validation Loss: 1.4781, Validation F1: 0.8889\n",
      "Epoch 398, Train Loss: 0.8561, Validation Loss: 1.4906, Validation F1: 0.8858\n",
      "Epoch 399, Train Loss: 0.8576, Validation Loss: 1.4440, Validation F1: 0.8911\n",
      "Epoch 400, Train Loss: 0.8544, Validation Loss: 1.5287, Validation F1: 0.8945\n",
      "Epoch 401, Train Loss: 0.8541, Validation Loss: 1.4152, Validation F1: 0.8966\n",
      "Epoch 402, Train Loss: 0.8568, Validation Loss: 1.4010, Validation F1: 0.9023\n",
      "Epoch 403, Train Loss: 0.8575, Validation Loss: 1.4779, Validation F1: 0.8887\n",
      "Epoch 404, Train Loss: 0.8542, Validation Loss: 1.4445, Validation F1: 0.8876\n",
      "Epoch 405, Train Loss: 0.8566, Validation Loss: 1.4285, Validation F1: 0.9000\n",
      "Epoch 406, Train Loss: 0.8564, Validation Loss: 1.4112, Validation F1: 0.8931\n",
      "Epoch 407, Train Loss: 0.8552, Validation Loss: 1.4353, Validation F1: 0.8977\n",
      "Epoch 408, Train Loss: 0.8526, Validation Loss: 1.5148, Validation F1: 0.8931\n",
      "Epoch 409, Train Loss: 0.8541, Validation Loss: 1.5026, Validation F1: 0.8988\n",
      "Epoch 410, Train Loss: 0.8606, Validation Loss: 1.4666, Validation F1: 0.8904\n",
      "Epoch 411, Train Loss: 0.8534, Validation Loss: 1.5146, Validation F1: 0.8967\n",
      "Epoch 412, Train Loss: 0.8526, Validation Loss: 1.4539, Validation F1: 0.8857\n",
      "Epoch 413, Train Loss: 0.8544, Validation Loss: 1.5003, Validation F1: 0.8889\n",
      "Epoch 414, Train Loss: 0.8577, Validation Loss: 1.4149, Validation F1: 0.8909\n",
      "Epoch 415, Train Loss: 0.8583, Validation Loss: 1.4210, Validation F1: 0.9028\n",
      "Epoch 416, Train Loss: 0.8506, Validation Loss: 1.4722, Validation F1: 0.8891\n",
      "Epoch 417, Train Loss: 0.8519, Validation Loss: 1.4971, Validation F1: 0.8944\n",
      "Epoch 418, Train Loss: 0.8515, Validation Loss: 1.5228, Validation F1: 0.8955\n",
      "Epoch 419, Train Loss: 0.8504, Validation Loss: 1.5189, Validation F1: 0.8890\n",
      "Epoch 420, Train Loss: 0.8575, Validation Loss: 1.5754, Validation F1: 0.8896\n",
      "Epoch 421, Train Loss: 0.8548, Validation Loss: 1.5140, Validation F1: 0.8820\n",
      "Epoch 422, Train Loss: 0.8507, Validation Loss: 1.4361, Validation F1: 0.8947\n",
      "Epoch 423, Train Loss: 0.8497, Validation Loss: 1.4677, Validation F1: 0.8926\n",
      "Epoch 424, Train Loss: 0.8511, Validation Loss: 1.5274, Validation F1: 0.8966\n",
      "Epoch 425, Train Loss: 0.8512, Validation Loss: 1.4407, Validation F1: 0.8961\n",
      "Epoch 426, Train Loss: 0.8515, Validation Loss: 1.4937, Validation F1: 0.8910\n",
      "Epoch 427, Train Loss: 0.8534, Validation Loss: 1.4426, Validation F1: 0.8946\n",
      "Epoch 428, Train Loss: 0.8487, Validation Loss: 1.4990, Validation F1: 0.8951\n",
      "Epoch 429, Train Loss: 0.8474, Validation Loss: 1.5253, Validation F1: 0.8827\n",
      "Epoch 430, Train Loss: 0.8526, Validation Loss: 1.5692, Validation F1: 0.8907\n",
      "Epoch 431, Train Loss: 0.8565, Validation Loss: 1.5025, Validation F1: 0.8874\n",
      "Epoch 432, Train Loss: 0.8490, Validation Loss: 1.4830, Validation F1: 0.9008\n",
      "Epoch 433, Train Loss: 0.8535, Validation Loss: 1.4853, Validation F1: 0.8916\n",
      "Epoch 434, Train Loss: 0.8510, Validation Loss: 1.4182, Validation F1: 0.8949\n",
      "Epoch 435, Train Loss: 0.8505, Validation Loss: 1.4460, Validation F1: 0.8937\n",
      "Epoch 436, Train Loss: 0.8517, Validation Loss: 1.4367, Validation F1: 0.8934\n",
      "Epoch 437, Train Loss: 0.8493, Validation Loss: 1.4983, Validation F1: 0.9009\n",
      "Epoch 438, Train Loss: 0.8531, Validation Loss: 1.4845, Validation F1: 0.8966\n",
      "Epoch 439, Train Loss: 0.8481, Validation Loss: 1.5076, Validation F1: 0.8921\n",
      "Epoch 440, Train Loss: 0.8499, Validation Loss: 1.4995, Validation F1: 0.8985\n",
      "Epoch 441, Train Loss: 0.8469, Validation Loss: 1.5348, Validation F1: 0.8929\n",
      "Epoch 442, Train Loss: 0.8471, Validation Loss: 1.5329, Validation F1: 0.8942\n",
      "Epoch 443, Train Loss: 0.8498, Validation Loss: 1.5139, Validation F1: 0.8872\n",
      "Epoch 444, Train Loss: 0.8486, Validation Loss: 1.4860, Validation F1: 0.8979\n",
      "Epoch 445, Train Loss: 0.8494, Validation Loss: 1.5439, Validation F1: 0.8904\n",
      "Epoch 446, Train Loss: 0.8478, Validation Loss: 1.4692, Validation F1: 0.8876\n",
      "Epoch 447, Train Loss: 0.8561, Validation Loss: 1.4993, Validation F1: 0.8976\n",
      "Epoch 448, Train Loss: 0.8468, Validation Loss: 1.4788, Validation F1: 0.8926\n",
      "Epoch 449, Train Loss: 0.8505, Validation Loss: 1.4788, Validation F1: 0.8866\n",
      "Epoch 450, Train Loss: 0.8470, Validation Loss: 1.5323, Validation F1: 0.8831\n",
      "Epoch 451, Train Loss: 0.8466, Validation Loss: 1.4834, Validation F1: 0.8860\n",
      "Epoch 452, Train Loss: 0.8477, Validation Loss: 1.5975, Validation F1: 0.8898\n",
      "Epoch 453, Train Loss: 0.8477, Validation Loss: 1.5904, Validation F1: 0.8866\n",
      "Epoch 454, Train Loss: 0.8508, Validation Loss: 1.6051, Validation F1: 0.8862\n",
      "Epoch 455, Train Loss: 0.8469, Validation Loss: 1.4832, Validation F1: 0.8902\n",
      "Epoch 456, Train Loss: 0.8475, Validation Loss: 1.4996, Validation F1: 0.9016\n",
      "Epoch 457, Train Loss: 0.8500, Validation Loss: 1.4748, Validation F1: 0.8907\n",
      "Epoch 458, Train Loss: 0.8544, Validation Loss: 1.5873, Validation F1: 0.8957\n",
      "Epoch 459, Train Loss: 0.8502, Validation Loss: 1.4442, Validation F1: 0.9005\n",
      "Epoch 460, Train Loss: 0.8537, Validation Loss: 1.4714, Validation F1: 0.8962\n",
      "Epoch 461, Train Loss: 0.8484, Validation Loss: 1.4360, Validation F1: 0.8947\n",
      "Epoch 462, Train Loss: 0.8491, Validation Loss: 1.4980, Validation F1: 0.8942\n",
      "Epoch 463, Train Loss: 0.8603, Validation Loss: 1.5043, Validation F1: 0.8859\n",
      "Epoch 464, Train Loss: 0.8459, Validation Loss: 1.4809, Validation F1: 0.8971\n",
      "Epoch 465, Train Loss: 0.8469, Validation Loss: 1.3676, Validation F1: 0.8942\n",
      "Epoch 466, Train Loss: 0.8498, Validation Loss: 1.4275, Validation F1: 0.8947\n",
      "Epoch 467, Train Loss: 0.8485, Validation Loss: 1.4499, Validation F1: 0.9016\n",
      "Epoch 468, Train Loss: 0.8493, Validation Loss: 1.5956, Validation F1: 0.8846\n",
      "Epoch 469, Train Loss: 0.8439, Validation Loss: 1.4960, Validation F1: 0.8878\n",
      "Epoch 470, Train Loss: 0.8488, Validation Loss: 1.3791, Validation F1: 0.8954\n",
      "Epoch 471, Train Loss: 0.8530, Validation Loss: 1.5271, Validation F1: 0.8860\n",
      "Epoch 472, Train Loss: 0.8488, Validation Loss: 1.4579, Validation F1: 0.8984\n",
      "Epoch 473, Train Loss: 0.8500, Validation Loss: 1.4581, Validation F1: 0.8900\n",
      "Epoch 474, Train Loss: 0.8462, Validation Loss: 1.4948, Validation F1: 0.8862\n",
      "Epoch 475, Train Loss: 0.8488, Validation Loss: 1.5057, Validation F1: 0.8887\n",
      "Epoch 476, Train Loss: 0.8446, Validation Loss: 1.4051, Validation F1: 0.8914\n",
      "Epoch 477, Train Loss: 0.8456, Validation Loss: 1.4619, Validation F1: 0.8837\n",
      "Epoch 478, Train Loss: 0.8462, Validation Loss: 1.4419, Validation F1: 0.8941\n",
      "Epoch 479, Train Loss: 0.8465, Validation Loss: 1.4502, Validation F1: 0.8937\n",
      "Epoch 480, Train Loss: 0.8472, Validation Loss: 1.5178, Validation F1: 0.8834\n",
      "Epoch 481, Train Loss: 0.8444, Validation Loss: 1.4557, Validation F1: 0.8890\n",
      "Epoch 482, Train Loss: 0.8461, Validation Loss: 1.4108, Validation F1: 0.8836\n",
      "Epoch 483, Train Loss: 0.8548, Validation Loss: 1.4890, Validation F1: 0.8889\n",
      "Epoch 484, Train Loss: 0.8482, Validation Loss: 1.4630, Validation F1: 0.8865\n",
      "Epoch 485, Train Loss: 0.8453, Validation Loss: 1.4864, Validation F1: 0.8889\n",
      "Epoch 486, Train Loss: 0.8438, Validation Loss: 1.4903, Validation F1: 0.8831\n",
      "Epoch 487, Train Loss: 0.8461, Validation Loss: 1.4865, Validation F1: 0.8903\n",
      "Epoch 488 Saved best model. Best F1: 0.9059982980057355\n",
      "Epoch 488, Train Loss: 0.8467, Validation Loss: 1.4847, Validation F1: 0.9060\n",
      "Epoch 489, Train Loss: 0.8455, Validation Loss: 1.4929, Validation F1: 0.8946\n",
      "Epoch 490, Train Loss: 0.8459, Validation Loss: 1.5625, Validation F1: 0.8998\n",
      "Epoch 491, Train Loss: 0.8429, Validation Loss: 1.4566, Validation F1: 0.8901\n",
      "Epoch 492, Train Loss: 0.8419, Validation Loss: 1.5223, Validation F1: 0.8890\n",
      "Epoch 493, Train Loss: 0.8480, Validation Loss: 1.4710, Validation F1: 0.8923\n",
      "Epoch 494, Train Loss: 0.8453, Validation Loss: 1.5873, Validation F1: 0.8885\n",
      "Epoch 495, Train Loss: 0.8448, Validation Loss: 1.4370, Validation F1: 0.8928\n",
      "Epoch 496, Train Loss: 0.8468, Validation Loss: 1.3965, Validation F1: 0.8840\n",
      "Epoch 497, Train Loss: 0.8490, Validation Loss: 1.4767, Validation F1: 0.8857\n",
      "Epoch 498, Train Loss: 0.8405, Validation Loss: 1.5049, Validation F1: 0.8833\n",
      "Epoch 499, Train Loss: 0.8436, Validation Loss: 1.5407, Validation F1: 0.8876\n",
      "Epoch 500, Train Loss: 0.8420, Validation Loss: 1.4532, Validation F1: 0.8948\n",
      "Epoch 501, Train Loss: 0.8412, Validation Loss: 1.5029, Validation F1: 0.8874\n",
      "Epoch 502, Train Loss: 0.8418, Validation Loss: 1.5443, Validation F1: 0.8800\n",
      "Epoch 503, Train Loss: 0.8486, Validation Loss: 1.4520, Validation F1: 0.8922\n",
      "Epoch 504, Train Loss: 0.8395, Validation Loss: 1.4935, Validation F1: 0.8916\n",
      "Epoch 505, Train Loss: 0.8394, Validation Loss: 1.4275, Validation F1: 0.8961\n",
      "Epoch 506, Train Loss: 0.8447, Validation Loss: 1.4402, Validation F1: 0.8982\n",
      "Epoch 507, Train Loss: 0.8446, Validation Loss: 1.4662, Validation F1: 0.8963\n",
      "Epoch 508, Train Loss: 0.8410, Validation Loss: 1.4867, Validation F1: 0.8844\n",
      "Epoch 509, Train Loss: 0.8407, Validation Loss: 1.5411, Validation F1: 0.8864\n",
      "Epoch 510, Train Loss: 0.8391, Validation Loss: 1.5267, Validation F1: 0.8931\n",
      "Epoch 511, Train Loss: 0.8411, Validation Loss: 1.4851, Validation F1: 0.8960\n",
      "Epoch 512, Train Loss: 0.8426, Validation Loss: 1.4817, Validation F1: 0.9000\n",
      "Epoch 513, Train Loss: 0.8450, Validation Loss: 1.5318, Validation F1: 0.8840\n",
      "Epoch 514, Train Loss: 0.8400, Validation Loss: 1.5192, Validation F1: 0.8858\n",
      "Epoch 515, Train Loss: 0.8396, Validation Loss: 1.5287, Validation F1: 0.8894\n",
      "Epoch 516, Train Loss: 0.8413, Validation Loss: 1.4310, Validation F1: 0.8925\n",
      "Epoch 517, Train Loss: 0.8391, Validation Loss: 1.4361, Validation F1: 0.9018\n",
      "Epoch 518, Train Loss: 0.8390, Validation Loss: 1.4367, Validation F1: 0.8947\n",
      "Epoch 519, Train Loss: 0.8423, Validation Loss: 1.4723, Validation F1: 0.8960\n",
      "Epoch 520, Train Loss: 0.8531, Validation Loss: 1.4256, Validation F1: 0.8892\n",
      "Epoch 521, Train Loss: 0.8450, Validation Loss: 1.4812, Validation F1: 0.8874\n",
      "Epoch 522, Train Loss: 0.8426, Validation Loss: 1.4501, Validation F1: 0.9029\n",
      "Epoch 523, Train Loss: 0.8386, Validation Loss: 1.5545, Validation F1: 0.9019\n",
      "Epoch 524, Train Loss: 0.8469, Validation Loss: 1.5286, Validation F1: 0.8939\n",
      "Epoch 525, Train Loss: 0.8431, Validation Loss: 1.4407, Validation F1: 0.8984\n",
      "Epoch 526, Train Loss: 0.8454, Validation Loss: 1.4827, Validation F1: 0.8941\n",
      "Epoch 527, Train Loss: 0.8437, Validation Loss: 1.4299, Validation F1: 0.8991\n",
      "Epoch 528, Train Loss: 0.8441, Validation Loss: 1.5552, Validation F1: 0.8921\n",
      "Epoch 529, Train Loss: 0.8396, Validation Loss: 1.4400, Validation F1: 0.8897\n",
      "Epoch 530, Train Loss: 0.8422, Validation Loss: 1.5102, Validation F1: 0.9023\n",
      "Epoch 531, Train Loss: 0.8454, Validation Loss: 1.4965, Validation F1: 0.9040\n",
      "Epoch 532, Train Loss: 0.8419, Validation Loss: 1.4204, Validation F1: 0.9000\n",
      "Epoch 533, Train Loss: 0.8412, Validation Loss: 1.4995, Validation F1: 0.8990\n",
      "Epoch 534, Train Loss: 0.8415, Validation Loss: 1.4016, Validation F1: 0.8993\n",
      "Epoch 535, Train Loss: 0.8375, Validation Loss: 1.4822, Validation F1: 0.9019\n",
      "Epoch 536, Train Loss: 0.8473, Validation Loss: 1.3833, Validation F1: 0.8947\n",
      "Epoch 537, Train Loss: 0.8464, Validation Loss: 1.4585, Validation F1: 0.8926\n",
      "Epoch 538, Train Loss: 0.8410, Validation Loss: 1.4483, Validation F1: 0.9016\n",
      "Epoch 539, Train Loss: 0.8378, Validation Loss: 1.5071, Validation F1: 0.8919\n",
      "Epoch 540, Train Loss: 0.8452, Validation Loss: 1.4130, Validation F1: 0.9035\n",
      "Epoch 541, Train Loss: 0.8404, Validation Loss: 1.3945, Validation F1: 0.8905\n",
      "Epoch 542, Train Loss: 0.8443, Validation Loss: 1.4128, Validation F1: 0.8958\n",
      "Epoch 543, Train Loss: 0.8401, Validation Loss: 1.4453, Validation F1: 0.8908\n",
      "Epoch 544, Train Loss: 0.8376, Validation Loss: 1.3289, Validation F1: 0.8963\n",
      "Epoch 545, Train Loss: 0.8386, Validation Loss: 1.4644, Validation F1: 0.8945\n",
      "Epoch 546, Train Loss: 0.8384, Validation Loss: 1.3964, Validation F1: 0.8928\n",
      "Epoch 547, Train Loss: 0.8395, Validation Loss: 1.4820, Validation F1: 0.8947\n",
      "Epoch 548, Train Loss: 0.8435, Validation Loss: 1.4480, Validation F1: 0.8923\n",
      "Epoch 549, Train Loss: 0.8380, Validation Loss: 1.4719, Validation F1: 0.8955\n",
      "Epoch 550, Train Loss: 0.8381, Validation Loss: 1.5682, Validation F1: 0.8942\n",
      "Epoch 551, Train Loss: 0.8392, Validation Loss: 1.4355, Validation F1: 0.8946\n",
      "Epoch 552, Train Loss: 0.8362, Validation Loss: 1.4710, Validation F1: 0.8990\n",
      "Epoch 553, Train Loss: 0.8408, Validation Loss: 1.5408, Validation F1: 0.8929\n",
      "Epoch 554, Train Loss: 0.8423, Validation Loss: 1.3570, Validation F1: 0.9017\n",
      "Epoch 555, Train Loss: 0.8379, Validation Loss: 1.4771, Validation F1: 0.8989\n",
      "Epoch 556, Train Loss: 0.8368, Validation Loss: 1.3968, Validation F1: 0.8932\n",
      "Epoch 557, Train Loss: 0.8389, Validation Loss: 1.4166, Validation F1: 0.8973\n",
      "Epoch 558, Train Loss: 0.8343, Validation Loss: 1.5875, Validation F1: 0.8892\n",
      "Epoch 559, Train Loss: 0.8400, Validation Loss: 1.5375, Validation F1: 0.8859\n",
      "Epoch 560, Train Loss: 0.8375, Validation Loss: 1.5790, Validation F1: 0.8862\n",
      "Epoch 561, Train Loss: 0.8342, Validation Loss: 1.5178, Validation F1: 0.8880\n",
      "Epoch 562, Train Loss: 0.8357, Validation Loss: 1.4816, Validation F1: 0.8871\n",
      "Epoch 563, Train Loss: 0.8366, Validation Loss: 1.5240, Validation F1: 0.8812\n",
      "Epoch 564, Train Loss: 0.8366, Validation Loss: 1.4922, Validation F1: 0.8927\n",
      "Epoch 565, Train Loss: 0.8395, Validation Loss: 1.4723, Validation F1: 0.9004\n",
      "Epoch 566, Train Loss: 0.8346, Validation Loss: 1.4930, Validation F1: 0.8947\n",
      "Epoch 567, Train Loss: 0.8368, Validation Loss: 1.4112, Validation F1: 0.9027\n",
      "Epoch 568, Train Loss: 0.8378, Validation Loss: 1.3911, Validation F1: 0.9007\n",
      "Epoch 569, Train Loss: 0.8367, Validation Loss: 1.5055, Validation F1: 0.8886\n",
      "Epoch 570, Train Loss: 0.8346, Validation Loss: 1.4365, Validation F1: 0.8868\n",
      "Epoch 571, Train Loss: 0.8348, Validation Loss: 1.4852, Validation F1: 0.8894\n",
      "Epoch 572, Train Loss: 0.8327, Validation Loss: 1.4977, Validation F1: 0.8904\n",
      "Epoch 573, Train Loss: 0.8352, Validation Loss: 1.4086, Validation F1: 0.8998\n",
      "Epoch 574, Train Loss: 0.8341, Validation Loss: 1.5418, Validation F1: 0.8899\n",
      "Epoch 575, Train Loss: 0.8350, Validation Loss: 1.4461, Validation F1: 0.8964\n",
      "Epoch 576, Train Loss: 0.8350, Validation Loss: 1.4822, Validation F1: 0.8924\n",
      "Epoch 577, Train Loss: 0.8326, Validation Loss: 1.3953, Validation F1: 0.8930\n",
      "Epoch 578, Train Loss: 0.8365, Validation Loss: 1.5132, Validation F1: 0.8923\n",
      "Epoch 579, Train Loss: 0.8337, Validation Loss: 1.3796, Validation F1: 0.8908\n",
      "Epoch 580, Train Loss: 0.8327, Validation Loss: 1.4601, Validation F1: 0.8911\n",
      "Epoch 581, Train Loss: 0.8419, Validation Loss: 1.5130, Validation F1: 0.8882\n",
      "Epoch 582, Train Loss: 0.8320, Validation Loss: 1.4185, Validation F1: 0.8863\n",
      "Epoch 583, Train Loss: 0.8356, Validation Loss: 1.3980, Validation F1: 0.8873\n",
      "Epoch 584, Train Loss: 0.8380, Validation Loss: 1.5143, Validation F1: 0.8912\n",
      "Epoch 585, Train Loss: 0.8349, Validation Loss: 1.4685, Validation F1: 0.8902\n",
      "Epoch 586, Train Loss: 0.8365, Validation Loss: 1.4405, Validation F1: 0.9006\n",
      "Epoch 587, Train Loss: 0.8343, Validation Loss: 1.6110, Validation F1: 0.8897\n",
      "Epoch 588, Train Loss: 0.8343, Validation Loss: 1.4454, Validation F1: 0.9056\n",
      "Epoch 589, Train Loss: 0.8344, Validation Loss: 1.4822, Validation F1: 0.9030\n",
      "Epoch 590, Train Loss: 0.8334, Validation Loss: 1.4986, Validation F1: 0.9043\n",
      "Epoch 591, Train Loss: 0.8312, Validation Loss: 1.4411, Validation F1: 0.9016\n",
      "Epoch 592, Train Loss: 0.8335, Validation Loss: 1.4981, Validation F1: 0.8941\n",
      "Epoch 593, Train Loss: 0.8336, Validation Loss: 1.5004, Validation F1: 0.8940\n",
      "Epoch 594, Train Loss: 0.8363, Validation Loss: 1.5065, Validation F1: 0.8940\n",
      "Epoch 595, Train Loss: 0.8356, Validation Loss: 1.5690, Validation F1: 0.8863\n",
      "Epoch 596, Train Loss: 0.8341, Validation Loss: 1.4921, Validation F1: 0.8883\n",
      "Epoch 597, Train Loss: 0.8338, Validation Loss: 1.5337, Validation F1: 0.8812\n",
      "Epoch 598, Train Loss: 0.8292, Validation Loss: 1.5171, Validation F1: 0.8923\n",
      "Epoch 599, Train Loss: 0.8332, Validation Loss: 1.4851, Validation F1: 0.8947\n",
      "Epoch 600, Train Loss: 0.8337, Validation Loss: 1.4878, Validation F1: 0.8980\n",
      "Epoch 601, Train Loss: 0.8368, Validation Loss: 1.4772, Validation F1: 0.8967\n",
      "Epoch 602, Train Loss: 0.8327, Validation Loss: 1.5138, Validation F1: 0.8995\n",
      "Epoch 603, Train Loss: 0.8318, Validation Loss: 1.5384, Validation F1: 0.8930\n",
      "Epoch 604 Saved best model. Best F1: 0.9088408627665526\n",
      "Epoch 604, Train Loss: 0.8410, Validation Loss: 1.5078, Validation F1: 0.9088\n",
      "Epoch 605, Train Loss: 0.8295, Validation Loss: 1.4627, Validation F1: 0.8980\n",
      "Epoch 606, Train Loss: 0.8466, Validation Loss: 1.4957, Validation F1: 0.8991\n",
      "Epoch 607, Train Loss: 0.8548, Validation Loss: 1.4975, Validation F1: 0.8982\n",
      "Epoch 608, Train Loss: 0.8558, Validation Loss: 1.4453, Validation F1: 0.9031\n",
      "Epoch 609, Train Loss: 0.8465, Validation Loss: 1.4560, Validation F1: 0.8930\n",
      "Epoch 610, Train Loss: 0.8497, Validation Loss: 1.4582, Validation F1: 0.9028\n",
      "Epoch 611, Train Loss: 0.8436, Validation Loss: 1.5015, Validation F1: 0.8961\n",
      "Epoch 612, Train Loss: 0.8504, Validation Loss: 1.5580, Validation F1: 0.8819\n",
      "Epoch 613, Train Loss: 0.8443, Validation Loss: 1.5482, Validation F1: 0.8887\n",
      "Epoch 614, Train Loss: 0.8483, Validation Loss: 1.5262, Validation F1: 0.8817\n",
      "Epoch 615, Train Loss: 0.8453, Validation Loss: 1.4525, Validation F1: 0.8897\n",
      "Epoch 616, Train Loss: 0.8452, Validation Loss: 1.4840, Validation F1: 0.8986\n",
      "Epoch 617, Train Loss: 0.8477, Validation Loss: 1.4346, Validation F1: 0.8941\n",
      "Epoch 618, Train Loss: 0.8436, Validation Loss: 1.4836, Validation F1: 0.9028\n",
      "Epoch 619, Train Loss: 0.8464, Validation Loss: 1.4288, Validation F1: 0.8838\n",
      "Epoch 620, Train Loss: 0.8422, Validation Loss: 1.3845, Validation F1: 0.8857\n",
      "Epoch 621, Train Loss: 0.8445, Validation Loss: 1.4928, Validation F1: 0.8879\n",
      "Epoch 622, Train Loss: 0.8418, Validation Loss: 1.3845, Validation F1: 0.8946\n",
      "Epoch 623, Train Loss: 0.8416, Validation Loss: 1.3998, Validation F1: 0.8928\n",
      "Epoch 624, Train Loss: 0.8419, Validation Loss: 1.3719, Validation F1: 0.8875\n",
      "Epoch 625, Train Loss: 0.8399, Validation Loss: 1.4336, Validation F1: 0.9031\n",
      "Epoch 626, Train Loss: 0.8401, Validation Loss: 1.5002, Validation F1: 0.8916\n",
      "Epoch 627, Train Loss: 0.8403, Validation Loss: 1.5639, Validation F1: 0.8838\n",
      "Epoch 628, Train Loss: 0.8418, Validation Loss: 1.5839, Validation F1: 0.8898\n",
      "Epoch 629, Train Loss: 0.8375, Validation Loss: 1.4323, Validation F1: 0.9023\n",
      "Epoch 630, Train Loss: 0.8396, Validation Loss: 1.4512, Validation F1: 0.9026\n",
      "Epoch 631, Train Loss: 0.8399, Validation Loss: 1.5333, Validation F1: 0.8891\n",
      "Epoch 632, Train Loss: 0.8396, Validation Loss: 1.4166, Validation F1: 0.8917\n",
      "Epoch 633, Train Loss: 0.8389, Validation Loss: 1.4356, Validation F1: 0.8937\n",
      "Epoch 634, Train Loss: 0.8385, Validation Loss: 1.4674, Validation F1: 0.8911\n",
      "Epoch 635, Train Loss: 0.8404, Validation Loss: 1.5160, Validation F1: 0.8937\n",
      "Epoch 636, Train Loss: 0.8388, Validation Loss: 1.4245, Validation F1: 0.8978\n",
      "Epoch 637, Train Loss: 0.8403, Validation Loss: 1.4755, Validation F1: 0.8980\n",
      "Epoch 638, Train Loss: 0.8388, Validation Loss: 1.5481, Validation F1: 0.8893\n",
      "Epoch 639, Train Loss: 0.8372, Validation Loss: 1.4722, Validation F1: 0.8839\n",
      "Epoch 640, Train Loss: 0.8358, Validation Loss: 1.5426, Validation F1: 0.8956\n",
      "Epoch 641, Train Loss: 0.8366, Validation Loss: 1.4200, Validation F1: 0.9007\n",
      "Epoch 642, Train Loss: 0.8351, Validation Loss: 1.4859, Validation F1: 0.8992\n",
      "Epoch 643, Train Loss: 0.8382, Validation Loss: 1.4284, Validation F1: 0.8896\n",
      "Epoch 644, Train Loss: 0.8346, Validation Loss: 1.4921, Validation F1: 0.8871\n",
      "Epoch 645, Train Loss: 0.8381, Validation Loss: 1.5393, Validation F1: 0.8896\n",
      "Epoch 646, Train Loss: 0.8402, Validation Loss: 1.5003, Validation F1: 0.9001\n",
      "Epoch 647, Train Loss: 0.8349, Validation Loss: 1.4341, Validation F1: 0.8992\n",
      "Epoch 648, Train Loss: 0.8378, Validation Loss: 1.4512, Validation F1: 0.8980\n",
      "Epoch 649, Train Loss: 0.8393, Validation Loss: 1.4653, Validation F1: 0.8986\n",
      "Epoch 650, Train Loss: 0.8349, Validation Loss: 1.4505, Validation F1: 0.8994\n",
      "Epoch 651, Train Loss: 0.8344, Validation Loss: 1.5478, Validation F1: 0.8870\n",
      "Epoch 652, Train Loss: 0.8405, Validation Loss: 1.4953, Validation F1: 0.8993\n",
      "Epoch 653, Train Loss: 0.8349, Validation Loss: 1.4938, Validation F1: 0.8992\n",
      "Epoch 654, Train Loss: 0.8364, Validation Loss: 1.5540, Validation F1: 0.9040\n",
      "Epoch 655, Train Loss: 0.8381, Validation Loss: 1.4671, Validation F1: 0.8950\n",
      "Epoch 656, Train Loss: 0.8367, Validation Loss: 1.5994, Validation F1: 0.8869\n",
      "Epoch 657, Train Loss: 0.8348, Validation Loss: 1.4684, Validation F1: 0.8905\n",
      "Epoch 658, Train Loss: 0.8354, Validation Loss: 1.4936, Validation F1: 0.8837\n",
      "Epoch 659, Train Loss: 0.8356, Validation Loss: 1.5174, Validation F1: 0.8851\n",
      "Epoch 660, Train Loss: 0.8360, Validation Loss: 1.4884, Validation F1: 0.8997\n",
      "Epoch 661, Train Loss: 0.8338, Validation Loss: 1.5161, Validation F1: 0.8899\n",
      "Epoch 662, Train Loss: 0.8349, Validation Loss: 1.4596, Validation F1: 0.9023\n",
      "Epoch 663, Train Loss: 0.8343, Validation Loss: 1.5622, Validation F1: 0.8892\n",
      "Epoch 664, Train Loss: 0.8326, Validation Loss: 1.5318, Validation F1: 0.8862\n",
      "Epoch 665, Train Loss: 0.8337, Validation Loss: 1.5436, Validation F1: 0.8897\n",
      "Epoch 666, Train Loss: 0.8330, Validation Loss: 1.5501, Validation F1: 0.8876\n",
      "Epoch 667, Train Loss: 0.8347, Validation Loss: 1.4887, Validation F1: 0.8912\n",
      "Epoch 668, Train Loss: 0.8352, Validation Loss: 1.5441, Validation F1: 0.8894\n",
      "Epoch 669, Train Loss: 0.8318, Validation Loss: 1.4881, Validation F1: 0.8949\n",
      "Epoch 670, Train Loss: 0.8353, Validation Loss: 1.5378, Validation F1: 0.8881\n",
      "Epoch 671, Train Loss: 0.8353, Validation Loss: 1.5239, Validation F1: 0.8919\n",
      "Epoch 672, Train Loss: 0.8332, Validation Loss: 1.5024, Validation F1: 0.8926\n",
      "Epoch 673, Train Loss: 0.8341, Validation Loss: 1.4696, Validation F1: 0.8937\n",
      "Epoch 674, Train Loss: 0.8332, Validation Loss: 1.5426, Validation F1: 0.8968\n",
      "Epoch 675, Train Loss: 0.8325, Validation Loss: 1.4675, Validation F1: 0.8855\n",
      "Epoch 676, Train Loss: 0.8309, Validation Loss: 1.4309, Validation F1: 0.8923\n",
      "Epoch 677, Train Loss: 0.8338, Validation Loss: 1.4835, Validation F1: 0.8917\n",
      "Epoch 678, Train Loss: 0.8306, Validation Loss: 1.4803, Validation F1: 0.9001\n",
      "Epoch 679, Train Loss: 0.8315, Validation Loss: 1.5054, Validation F1: 0.9025\n",
      "Epoch 680, Train Loss: 0.8327, Validation Loss: 1.4607, Validation F1: 0.9003\n",
      "Epoch 681, Train Loss: 0.8290, Validation Loss: 1.5450, Validation F1: 0.8977\n",
      "Epoch 682, Train Loss: 0.8289, Validation Loss: 1.4824, Validation F1: 0.8898\n",
      "Epoch 683, Train Loss: 0.8294, Validation Loss: 1.5636, Validation F1: 0.8900\n",
      "Epoch 684, Train Loss: 0.8277, Validation Loss: 1.5533, Validation F1: 0.8855\n",
      "Epoch 685, Train Loss: 0.8308, Validation Loss: 1.5707, Validation F1: 0.8890\n",
      "Epoch 686, Train Loss: 0.8321, Validation Loss: 1.5252, Validation F1: 0.8924\n",
      "Epoch 687, Train Loss: 0.8320, Validation Loss: 1.5759, Validation F1: 0.8885\n",
      "Epoch 688, Train Loss: 0.8302, Validation Loss: 1.5661, Validation F1: 0.8865\n",
      "Epoch 689, Train Loss: 0.8276, Validation Loss: 1.4844, Validation F1: 0.8876\n",
      "Epoch 690, Train Loss: 0.8303, Validation Loss: 1.5055, Validation F1: 0.8932\n",
      "Epoch 691, Train Loss: 0.8281, Validation Loss: 1.5373, Validation F1: 0.8880\n",
      "Epoch 692, Train Loss: 0.8289, Validation Loss: 1.5442, Validation F1: 0.8972\n",
      "Epoch 693, Train Loss: 0.8314, Validation Loss: 1.5362, Validation F1: 0.8926\n",
      "Epoch 694, Train Loss: 0.8305, Validation Loss: 1.6243, Validation F1: 0.8929\n",
      "Epoch 695, Train Loss: 0.8307, Validation Loss: 1.5187, Validation F1: 0.9038\n",
      "Epoch 696, Train Loss: 0.8300, Validation Loss: 1.4591, Validation F1: 0.8949\n",
      "Epoch 697, Train Loss: 0.8310, Validation Loss: 1.4976, Validation F1: 0.8885\n",
      "Epoch 698, Train Loss: 0.8296, Validation Loss: 1.5383, Validation F1: 0.8898\n",
      "Epoch 699, Train Loss: 0.8306, Validation Loss: 1.4783, Validation F1: 0.8920\n",
      "Epoch 700, Train Loss: 0.8303, Validation Loss: 1.4520, Validation F1: 0.8994\n",
      "Epoch 701, Train Loss: 0.8297, Validation Loss: 1.4878, Validation F1: 0.8922\n",
      "Epoch 702, Train Loss: 0.8286, Validation Loss: 1.4397, Validation F1: 0.8970\n",
      "Epoch 703, Train Loss: 0.8287, Validation Loss: 1.4647, Validation F1: 0.8945\n",
      "Epoch 704, Train Loss: 0.8321, Validation Loss: 1.4261, Validation F1: 0.8950\n",
      "Epoch 705, Train Loss: 0.8280, Validation Loss: 1.5582, Validation F1: 0.8893\n",
      "Epoch 706, Train Loss: 0.8278, Validation Loss: 1.4174, Validation F1: 0.8958\n",
      "Epoch 707, Train Loss: 0.8309, Validation Loss: 1.4598, Validation F1: 0.8918\n",
      "Epoch 708, Train Loss: 0.8340, Validation Loss: 1.5305, Validation F1: 0.8938\n",
      "Epoch 709, Train Loss: 0.8280, Validation Loss: 1.5571, Validation F1: 0.8918\n",
      "Epoch 710, Train Loss: 0.8278, Validation Loss: 1.5322, Validation F1: 0.8911\n",
      "Epoch 711, Train Loss: 0.8267, Validation Loss: 1.5411, Validation F1: 0.8919\n",
      "Epoch 712, Train Loss: 0.8287, Validation Loss: 1.4929, Validation F1: 0.9027\n",
      "Epoch 713, Train Loss: 0.8275, Validation Loss: 1.4912, Validation F1: 0.8950\n",
      "Epoch 714, Train Loss: 0.8292, Validation Loss: 1.5275, Validation F1: 0.8923\n",
      "Epoch 715, Train Loss: 0.8262, Validation Loss: 1.5020, Validation F1: 0.8954\n",
      "Epoch 716, Train Loss: 0.8262, Validation Loss: 1.4775, Validation F1: 0.8903\n",
      "Epoch 717, Train Loss: 0.8277, Validation Loss: 1.5469, Validation F1: 0.8993\n",
      "Epoch 718, Train Loss: 0.8321, Validation Loss: 1.5453, Validation F1: 0.8881\n",
      "Epoch 719, Train Loss: 0.8322, Validation Loss: 1.4382, Validation F1: 0.8926\n",
      "Epoch 720, Train Loss: 0.8249, Validation Loss: 1.5265, Validation F1: 0.8868\n",
      "Epoch 721, Train Loss: 0.8276, Validation Loss: 1.3843, Validation F1: 0.8878\n",
      "Epoch 722, Train Loss: 0.8303, Validation Loss: 1.4853, Validation F1: 0.8990\n",
      "Epoch 723, Train Loss: 0.8324, Validation Loss: 1.5632, Validation F1: 0.8854\n",
      "Epoch 724, Train Loss: 0.8310, Validation Loss: 1.4721, Validation F1: 0.8882\n",
      "Epoch 725, Train Loss: 0.8238, Validation Loss: 1.5351, Validation F1: 0.8911\n",
      "Epoch 726, Train Loss: 0.8301, Validation Loss: 1.4440, Validation F1: 0.8878\n",
      "Epoch 727, Train Loss: 0.8265, Validation Loss: 1.5520, Validation F1: 0.8899\n",
      "Epoch 728, Train Loss: 0.8290, Validation Loss: 1.5084, Validation F1: 0.8920\n",
      "Epoch 729, Train Loss: 0.8288, Validation Loss: 1.5062, Validation F1: 0.8921\n",
      "Epoch 730, Train Loss: 0.8307, Validation Loss: 1.4517, Validation F1: 0.8889\n",
      "Epoch 731, Train Loss: 0.8286, Validation Loss: 1.6301, Validation F1: 0.8897\n",
      "Epoch 732, Train Loss: 0.8290, Validation Loss: 1.4766, Validation F1: 0.8957\n",
      "Epoch 733, Train Loss: 0.8304, Validation Loss: 1.5362, Validation F1: 0.8899\n",
      "Epoch 734, Train Loss: 0.8265, Validation Loss: 1.5219, Validation F1: 0.8908\n",
      "Epoch 735, Train Loss: 0.8333, Validation Loss: 1.4556, Validation F1: 0.8958\n",
      "Epoch 736, Train Loss: 0.8307, Validation Loss: 1.5410, Validation F1: 0.8880\n",
      "Epoch 737, Train Loss: 0.8279, Validation Loss: 1.5432, Validation F1: 0.8985\n",
      "Epoch 738, Train Loss: 0.8295, Validation Loss: 1.5012, Validation F1: 0.8907\n",
      "Epoch 739, Train Loss: 0.8264, Validation Loss: 1.5457, Validation F1: 0.8867\n",
      "Epoch 740, Train Loss: 0.8311, Validation Loss: 1.4226, Validation F1: 0.9008\n",
      "Epoch 741, Train Loss: 0.8319, Validation Loss: 1.5941, Validation F1: 0.8904\n",
      "Epoch 742, Train Loss: 0.8345, Validation Loss: 1.6784, Validation F1: 0.8806\n",
      "Epoch 743, Train Loss: 0.8319, Validation Loss: 1.6038, Validation F1: 0.8850\n",
      "Epoch 744, Train Loss: 0.8291, Validation Loss: 1.5015, Validation F1: 0.8947\n",
      "Epoch 745, Train Loss: 0.8286, Validation Loss: 1.5118, Validation F1: 0.9043\n",
      "Epoch 746, Train Loss: 0.8308, Validation Loss: 1.5718, Validation F1: 0.9036\n",
      "Epoch 747, Train Loss: 0.8303, Validation Loss: 1.5316, Validation F1: 0.9004\n",
      "Epoch 748, Train Loss: 0.8261, Validation Loss: 1.5380, Validation F1: 0.8926\n",
      "Epoch 749, Train Loss: 0.8301, Validation Loss: 1.4653, Validation F1: 0.8847\n",
      "Epoch 750, Train Loss: 0.8278, Validation Loss: 1.5043, Validation F1: 0.8888\n",
      "Epoch 751, Train Loss: 0.8271, Validation Loss: 1.5002, Validation F1: 0.8894\n",
      "Epoch 752, Train Loss: 0.8280, Validation Loss: 1.4204, Validation F1: 0.8929\n",
      "Epoch 753, Train Loss: 0.8294, Validation Loss: 1.5792, Validation F1: 0.8846\n",
      "Epoch 754, Train Loss: 0.8240, Validation Loss: 1.4893, Validation F1: 0.8814\n",
      "Epoch 755, Train Loss: 0.8265, Validation Loss: 1.4986, Validation F1: 0.8833\n",
      "Epoch 756, Train Loss: 0.8241, Validation Loss: 1.4695, Validation F1: 0.8908\n",
      "Epoch 757, Train Loss: 0.8262, Validation Loss: 1.5138, Validation F1: 0.8905\n",
      "Epoch 758, Train Loss: 0.8298, Validation Loss: 1.4223, Validation F1: 0.8997\n",
      "Epoch 759, Train Loss: 0.8232, Validation Loss: 1.4727, Validation F1: 0.8900\n",
      "Epoch 760, Train Loss: 0.8241, Validation Loss: 1.4753, Validation F1: 0.8900\n",
      "Epoch 761, Train Loss: 0.8259, Validation Loss: 1.5221, Validation F1: 0.8899\n",
      "Epoch 762, Train Loss: 0.8265, Validation Loss: 1.4756, Validation F1: 0.8899\n",
      "Epoch 763, Train Loss: 0.8255, Validation Loss: 1.4237, Validation F1: 0.8891\n",
      "Epoch 764, Train Loss: 0.8253, Validation Loss: 1.5492, Validation F1: 0.8931\n",
      "Epoch 765, Train Loss: 0.8247, Validation Loss: 1.4238, Validation F1: 0.8950\n",
      "Epoch 766, Train Loss: 0.8264, Validation Loss: 1.5783, Validation F1: 0.8865\n",
      "Epoch 767, Train Loss: 0.8232, Validation Loss: 1.4839, Validation F1: 0.8967\n",
      "Epoch 768, Train Loss: 0.8221, Validation Loss: 1.4520, Validation F1: 0.8987\n",
      "Epoch 769, Train Loss: 0.8257, Validation Loss: 1.4413, Validation F1: 0.8971\n",
      "Epoch 770, Train Loss: 0.8212, Validation Loss: 1.4918, Validation F1: 0.8975\n",
      "Epoch 771, Train Loss: 0.8229, Validation Loss: 1.5299, Validation F1: 0.8922\n",
      "Epoch 772, Train Loss: 0.8242, Validation Loss: 1.5094, Validation F1: 0.8888\n",
      "Epoch 773, Train Loss: 0.8220, Validation Loss: 1.4552, Validation F1: 0.8989\n",
      "Epoch 774, Train Loss: 0.8229, Validation Loss: 1.6057, Validation F1: 0.8842\n",
      "Epoch 775, Train Loss: 0.8247, Validation Loss: 1.5129, Validation F1: 0.8882\n",
      "Epoch 776, Train Loss: 0.8219, Validation Loss: 1.5025, Validation F1: 0.8881\n",
      "Epoch 777, Train Loss: 0.8243, Validation Loss: 1.4469, Validation F1: 0.8903\n",
      "Epoch 778, Train Loss: 0.8273, Validation Loss: 1.5375, Validation F1: 0.8989\n",
      "Epoch 779, Train Loss: 0.8213, Validation Loss: 1.4652, Validation F1: 0.8972\n",
      "Epoch 780, Train Loss: 0.8322, Validation Loss: 1.5731, Validation F1: 0.8889\n",
      "Epoch 781, Train Loss: 0.8333, Validation Loss: 1.4964, Validation F1: 0.8901\n",
      "Epoch 782, Train Loss: 0.8336, Validation Loss: 1.6302, Validation F1: 0.8901\n",
      "Epoch 783, Train Loss: 0.8365, Validation Loss: 1.4809, Validation F1: 0.9008\n",
      "Epoch 784, Train Loss: 0.8272, Validation Loss: 1.4664, Validation F1: 0.8944\n",
      "Epoch 785, Train Loss: 0.8326, Validation Loss: 1.4772, Validation F1: 0.8936\n",
      "Epoch 786, Train Loss: 0.8325, Validation Loss: 1.5855, Validation F1: 0.8907\n",
      "Epoch 787, Train Loss: 0.8284, Validation Loss: 1.4938, Validation F1: 0.8867\n",
      "Epoch 788, Train Loss: 0.8301, Validation Loss: 1.4486, Validation F1: 0.8990\n",
      "Epoch 789, Train Loss: 0.8292, Validation Loss: 1.5697, Validation F1: 0.9011\n",
      "Epoch 790, Train Loss: 0.8267, Validation Loss: 1.4995, Validation F1: 0.9000\n",
      "Epoch 791, Train Loss: 0.8279, Validation Loss: 1.4803, Validation F1: 0.9001\n",
      "Epoch 792, Train Loss: 0.8259, Validation Loss: 1.5394, Validation F1: 0.8944\n",
      "Epoch 793, Train Loss: 0.8273, Validation Loss: 1.4988, Validation F1: 0.8904\n",
      "Epoch 794, Train Loss: 0.8259, Validation Loss: 1.5056, Validation F1: 0.8926\n",
      "Epoch 795, Train Loss: 0.8265, Validation Loss: 1.4351, Validation F1: 0.8981\n",
      "Epoch 796, Train Loss: 0.8264, Validation Loss: 1.5303, Validation F1: 0.8836\n",
      "Epoch 797, Train Loss: 0.8270, Validation Loss: 1.4682, Validation F1: 0.8887\n",
      "Epoch 798, Train Loss: 0.8244, Validation Loss: 1.5186, Validation F1: 0.8883\n",
      "Epoch 799, Train Loss: 0.8264, Validation Loss: 1.5251, Validation F1: 0.8823\n",
      "Epoch 800, Train Loss: 0.8264, Validation Loss: 1.4953, Validation F1: 0.8867\n",
      "Epoch 801, Train Loss: 0.8229, Validation Loss: 1.4936, Validation F1: 0.8881\n",
      "Epoch 802, Train Loss: 0.8232, Validation Loss: 1.4753, Validation F1: 0.8967\n",
      "Epoch 803, Train Loss: 0.8263, Validation Loss: 1.4455, Validation F1: 0.8945\n",
      "Epoch 804, Train Loss: 0.8261, Validation Loss: 1.4249, Validation F1: 0.8909\n",
      "Epoch 805, Train Loss: 0.8219, Validation Loss: 1.4444, Validation F1: 0.8878\n",
      "Epoch 806, Train Loss: 0.8262, Validation Loss: 1.4744, Validation F1: 0.9010\n",
      "Epoch 807, Train Loss: 0.8258, Validation Loss: 1.5634, Validation F1: 0.8913\n",
      "Epoch 808, Train Loss: 0.8239, Validation Loss: 1.5280, Validation F1: 0.8984\n",
      "Epoch 809, Train Loss: 0.8245, Validation Loss: 1.5129, Validation F1: 0.8844\n",
      "Epoch 810, Train Loss: 0.8223, Validation Loss: 1.4796, Validation F1: 0.8888\n",
      "Epoch 811, Train Loss: 0.8237, Validation Loss: 1.4306, Validation F1: 0.8921\n",
      "Epoch 812, Train Loss: 0.8216, Validation Loss: 1.5165, Validation F1: 0.8889\n",
      "Epoch 813, Train Loss: 0.8220, Validation Loss: 1.5029, Validation F1: 0.8881\n",
      "Epoch 814, Train Loss: 0.8225, Validation Loss: 1.4587, Validation F1: 0.8890\n",
      "Epoch 815, Train Loss: 0.8239, Validation Loss: 1.4220, Validation F1: 0.8859\n",
      "Epoch 816, Train Loss: 0.8196, Validation Loss: 1.4849, Validation F1: 0.8846\n",
      "Epoch 817, Train Loss: 0.8203, Validation Loss: 1.4440, Validation F1: 0.8926\n",
      "Epoch 818, Train Loss: 0.8226, Validation Loss: 1.6022, Validation F1: 0.8883\n",
      "Epoch 819, Train Loss: 0.8208, Validation Loss: 1.5742, Validation F1: 0.8931\n",
      "Epoch 820, Train Loss: 0.8210, Validation Loss: 1.4923, Validation F1: 0.8930\n",
      "Epoch 821, Train Loss: 0.8223, Validation Loss: 1.6117, Validation F1: 0.8922\n",
      "Epoch 822, Train Loss: 0.8206, Validation Loss: 1.6440, Validation F1: 0.8865\n",
      "Epoch 823, Train Loss: 0.8219, Validation Loss: 1.5013, Validation F1: 0.8827\n",
      "Epoch 824, Train Loss: 0.8206, Validation Loss: 1.5634, Validation F1: 0.8865\n",
      "Epoch 825, Train Loss: 0.8241, Validation Loss: 1.4513, Validation F1: 0.8889\n",
      "Epoch 826, Train Loss: 0.8218, Validation Loss: 1.4911, Validation F1: 0.8932\n",
      "Epoch 827, Train Loss: 0.8215, Validation Loss: 1.6026, Validation F1: 0.8889\n",
      "Epoch 828, Train Loss: 0.8225, Validation Loss: 1.5110, Validation F1: 0.8914\n",
      "Epoch 829, Train Loss: 0.8207, Validation Loss: 1.4422, Validation F1: 0.8883\n",
      "Epoch 830, Train Loss: 0.8230, Validation Loss: 1.5275, Validation F1: 0.8901\n",
      "Epoch 831, Train Loss: 0.8224, Validation Loss: 1.5092, Validation F1: 0.8886\n",
      "Epoch 832, Train Loss: 0.8217, Validation Loss: 1.4943, Validation F1: 0.9059\n",
      "Epoch 833, Train Loss: 0.8242, Validation Loss: 1.4943, Validation F1: 0.8901\n",
      "Epoch 834, Train Loss: 0.8212, Validation Loss: 1.6388, Validation F1: 0.8864\n",
      "Epoch 835, Train Loss: 0.8209, Validation Loss: 1.4459, Validation F1: 0.8902\n",
      "Epoch 836, Train Loss: 0.8228, Validation Loss: 1.5031, Validation F1: 0.9000\n",
      "Epoch 837, Train Loss: 0.8196, Validation Loss: 1.4965, Validation F1: 0.8933\n",
      "Epoch 838, Train Loss: 0.8216, Validation Loss: 1.5374, Validation F1: 0.8922\n",
      "Epoch 839, Train Loss: 0.8237, Validation Loss: 1.5140, Validation F1: 0.8855\n",
      "Epoch 840, Train Loss: 0.8246, Validation Loss: 1.6339, Validation F1: 0.8822\n",
      "Epoch 841, Train Loss: 0.8234, Validation Loss: 1.4583, Validation F1: 0.8878\n",
      "Epoch 842, Train Loss: 0.8215, Validation Loss: 1.5286, Validation F1: 0.8843\n",
      "Epoch 843, Train Loss: 0.8209, Validation Loss: 1.5428, Validation F1: 0.8928\n",
      "Epoch 844, Train Loss: 0.8214, Validation Loss: 1.5551, Validation F1: 0.8938\n",
      "Epoch 845, Train Loss: 0.8219, Validation Loss: 1.4571, Validation F1: 0.9020\n",
      "Epoch 846, Train Loss: 0.8192, Validation Loss: 1.5095, Validation F1: 0.8918\n",
      "Epoch 847, Train Loss: 0.8207, Validation Loss: 1.5571, Validation F1: 0.8914\n",
      "Epoch 848, Train Loss: 0.8198, Validation Loss: 1.5089, Validation F1: 0.8866\n",
      "Epoch 849, Train Loss: 0.8203, Validation Loss: 1.5872, Validation F1: 0.8926\n",
      "Epoch 850, Train Loss: 0.8209, Validation Loss: 1.5954, Validation F1: 0.8897\n",
      "Epoch 851, Train Loss: 0.8202, Validation Loss: 1.5467, Validation F1: 0.8901\n",
      "Epoch 852, Train Loss: 0.8189, Validation Loss: 1.5667, Validation F1: 0.8857\n",
      "Epoch 853, Train Loss: 0.8205, Validation Loss: 1.5761, Validation F1: 0.8873\n",
      "Epoch 854, Train Loss: 0.8214, Validation Loss: 1.5273, Validation F1: 0.8880\n",
      "Epoch 855, Train Loss: 0.8208, Validation Loss: 1.5528, Validation F1: 0.8907\n",
      "Epoch 856, Train Loss: 0.8212, Validation Loss: 1.4980, Validation F1: 0.8902\n",
      "Epoch 857, Train Loss: 0.8209, Validation Loss: 1.5291, Validation F1: 0.8906\n",
      "Epoch 858, Train Loss: 0.8209, Validation Loss: 1.5641, Validation F1: 0.8912\n",
      "Epoch 859, Train Loss: 0.8212, Validation Loss: 1.6618, Validation F1: 0.8841\n",
      "Epoch 860, Train Loss: 0.8182, Validation Loss: 1.6008, Validation F1: 0.8863\n",
      "Epoch 861, Train Loss: 0.8204, Validation Loss: 1.4954, Validation F1: 0.8868\n",
      "Epoch 862, Train Loss: 0.8202, Validation Loss: 1.5622, Validation F1: 0.8863\n",
      "Epoch 863, Train Loss: 0.8189, Validation Loss: 1.5357, Validation F1: 0.8860\n",
      "Epoch 864, Train Loss: 0.8180, Validation Loss: 1.4968, Validation F1: 0.8968\n",
      "Epoch 865, Train Loss: 0.8201, Validation Loss: 1.6597, Validation F1: 0.8873\n",
      "Epoch 866, Train Loss: 0.8202, Validation Loss: 1.4466, Validation F1: 0.8910\n",
      "Epoch 867, Train Loss: 0.8196, Validation Loss: 1.5509, Validation F1: 0.8842\n",
      "Epoch 868, Train Loss: 0.8205, Validation Loss: 1.5423, Validation F1: 0.8879\n",
      "Epoch 869, Train Loss: 0.8197, Validation Loss: 1.5934, Validation F1: 0.8897\n",
      "Epoch 870, Train Loss: 0.8203, Validation Loss: 1.4162, Validation F1: 0.8915\n",
      "Epoch 871, Train Loss: 0.8192, Validation Loss: 1.5973, Validation F1: 0.8849\n",
      "Epoch 872, Train Loss: 0.8198, Validation Loss: 1.5391, Validation F1: 0.8901\n",
      "Epoch 873, Train Loss: 0.8186, Validation Loss: 1.6108, Validation F1: 0.8855\n",
      "Epoch 874, Train Loss: 0.8205, Validation Loss: 1.6611, Validation F1: 0.8849\n",
      "Epoch 875, Train Loss: 0.8211, Validation Loss: 1.6202, Validation F1: 0.8866\n",
      "Epoch 876, Train Loss: 0.8171, Validation Loss: 1.4598, Validation F1: 0.8982\n",
      "Epoch 877, Train Loss: 0.8176, Validation Loss: 1.5508, Validation F1: 0.8979\n",
      "Epoch 878, Train Loss: 0.8198, Validation Loss: 1.5474, Validation F1: 0.8949\n",
      "Epoch 879, Train Loss: 0.8196, Validation Loss: 1.5359, Validation F1: 0.8892\n",
      "Epoch 880, Train Loss: 0.8188, Validation Loss: 1.5822, Validation F1: 0.8876\n",
      "Epoch 881, Train Loss: 0.8198, Validation Loss: 1.5287, Validation F1: 0.8870\n",
      "Epoch 882, Train Loss: 0.8187, Validation Loss: 1.5317, Validation F1: 0.8877\n",
      "Epoch 883, Train Loss: 0.8206, Validation Loss: 1.5238, Validation F1: 0.9006\n",
      "Epoch 884, Train Loss: 0.8218, Validation Loss: 1.4368, Validation F1: 0.8881\n",
      "Epoch 885, Train Loss: 0.8202, Validation Loss: 1.5288, Validation F1: 0.8917\n",
      "Epoch 886, Train Loss: 0.8205, Validation Loss: 1.4943, Validation F1: 0.8883\n",
      "Epoch 887, Train Loss: 0.8205, Validation Loss: 1.5163, Validation F1: 0.8916\n",
      "Epoch 888, Train Loss: 0.8184, Validation Loss: 1.5535, Validation F1: 0.8877\n",
      "Epoch 889, Train Loss: 0.8205, Validation Loss: 1.5730, Validation F1: 0.8866\n",
      "Epoch 890, Train Loss: 0.8217, Validation Loss: 1.5163, Validation F1: 0.8868\n",
      "Epoch 891, Train Loss: 0.8185, Validation Loss: 1.4653, Validation F1: 0.8907\n",
      "Epoch 892, Train Loss: 0.8183, Validation Loss: 1.5063, Validation F1: 0.8896\n",
      "Epoch 893, Train Loss: 0.8182, Validation Loss: 1.4682, Validation F1: 0.8956\n",
      "Epoch 894, Train Loss: 0.8184, Validation Loss: 1.5430, Validation F1: 0.9023\n",
      "Epoch 895, Train Loss: 0.8189, Validation Loss: 1.4518, Validation F1: 0.8972\n",
      "Epoch 896, Train Loss: 0.8192, Validation Loss: 1.5192, Validation F1: 0.8955\n",
      "Epoch 897, Train Loss: 0.8188, Validation Loss: 1.4661, Validation F1: 0.8962\n",
      "Epoch 898, Train Loss: 0.8188, Validation Loss: 1.4656, Validation F1: 0.8880\n",
      "Epoch 899, Train Loss: 0.8181, Validation Loss: 1.4468, Validation F1: 0.8948\n",
      "Epoch 900, Train Loss: 0.8174, Validation Loss: 1.5202, Validation F1: 0.8946\n",
      "Epoch 901, Train Loss: 0.8186, Validation Loss: 1.5542, Validation F1: 0.8868\n",
      "Epoch 902, Train Loss: 0.8207, Validation Loss: 1.5442, Validation F1: 0.8881\n",
      "Epoch 903, Train Loss: 0.8169, Validation Loss: 1.3644, Validation F1: 0.9010\n",
      "Epoch 904, Train Loss: 0.8185, Validation Loss: 1.5504, Validation F1: 0.8887\n",
      "Epoch 905, Train Loss: 0.8194, Validation Loss: 1.5423, Validation F1: 0.9009\n",
      "Epoch 906, Train Loss: 0.8171, Validation Loss: 1.5463, Validation F1: 0.8929\n",
      "Epoch 907, Train Loss: 0.8184, Validation Loss: 1.5274, Validation F1: 0.8938\n",
      "Epoch 908, Train Loss: 0.8199, Validation Loss: 1.6145, Validation F1: 0.8879\n",
      "Epoch 909, Train Loss: 0.8210, Validation Loss: 1.4516, Validation F1: 0.8842\n",
      "Epoch 910, Train Loss: 0.8167, Validation Loss: 1.5959, Validation F1: 0.8884\n",
      "Epoch 911, Train Loss: 0.8204, Validation Loss: 1.6493, Validation F1: 0.8817\n",
      "Epoch 912, Train Loss: 0.8195, Validation Loss: 1.4899, Validation F1: 0.8871\n",
      "Epoch 913, Train Loss: 0.8180, Validation Loss: 1.5771, Validation F1: 0.8915\n",
      "Epoch 914, Train Loss: 0.8183, Validation Loss: 1.7082, Validation F1: 0.8863\n",
      "Epoch 915, Train Loss: 0.8193, Validation Loss: 1.5055, Validation F1: 0.8920\n",
      "Epoch 916, Train Loss: 0.8190, Validation Loss: 1.5217, Validation F1: 0.8869\n",
      "Epoch 917, Train Loss: 0.8176, Validation Loss: 1.5748, Validation F1: 0.8868\n",
      "Epoch 918, Train Loss: 0.8170, Validation Loss: 1.6001, Validation F1: 0.8904\n",
      "Epoch 919, Train Loss: 0.8158, Validation Loss: 1.7110, Validation F1: 0.8863\n",
      "Epoch 920, Train Loss: 0.8183, Validation Loss: 1.5465, Validation F1: 0.8878\n",
      "Epoch 921, Train Loss: 0.8177, Validation Loss: 1.5807, Validation F1: 0.8823\n",
      "Epoch 922, Train Loss: 0.8167, Validation Loss: 1.5023, Validation F1: 0.8847\n",
      "Epoch 923, Train Loss: 0.8155, Validation Loss: 1.5488, Validation F1: 0.8874\n",
      "Epoch 924, Train Loss: 0.8171, Validation Loss: 1.5857, Validation F1: 0.8884\n",
      "Epoch 925, Train Loss: 0.8180, Validation Loss: 1.5576, Validation F1: 0.8885\n",
      "Epoch 926, Train Loss: 0.8153, Validation Loss: 1.4582, Validation F1: 0.8850\n",
      "Epoch 927, Train Loss: 0.8172, Validation Loss: 1.4980, Validation F1: 0.8890\n",
      "Epoch 928, Train Loss: 0.8180, Validation Loss: 1.5296, Validation F1: 0.8830\n",
      "Epoch 929, Train Loss: 0.8168, Validation Loss: 1.4732, Validation F1: 0.8940\n",
      "Epoch 930, Train Loss: 0.8168, Validation Loss: 1.5806, Validation F1: 0.8845\n",
      "Epoch 931, Train Loss: 0.8152, Validation Loss: 1.4927, Validation F1: 0.8842\n",
      "Epoch 932, Train Loss: 0.8163, Validation Loss: 1.4951, Validation F1: 0.8850\n",
      "Epoch 933, Train Loss: 0.8148, Validation Loss: 1.5686, Validation F1: 0.8868\n",
      "Epoch 934, Train Loss: 0.8149, Validation Loss: 1.5683, Validation F1: 0.8884\n",
      "Epoch 935, Train Loss: 0.8161, Validation Loss: 1.6169, Validation F1: 0.8819\n",
      "Epoch 936, Train Loss: 0.8161, Validation Loss: 1.5708, Validation F1: 0.8916\n",
      "Epoch 937, Train Loss: 0.8161, Validation Loss: 1.6598, Validation F1: 0.8874\n",
      "Epoch 938, Train Loss: 0.8171, Validation Loss: 1.4537, Validation F1: 0.8919\n",
      "Epoch 939, Train Loss: 0.8176, Validation Loss: 1.6086, Validation F1: 0.9049\n",
      "Epoch 940, Train Loss: 0.8155, Validation Loss: 1.6069, Validation F1: 0.8923\n",
      "Epoch 941, Train Loss: 0.8186, Validation Loss: 1.5565, Validation F1: 0.8999\n",
      "Epoch 942, Train Loss: 0.8182, Validation Loss: 1.5623, Validation F1: 0.8897\n",
      "Epoch 943, Train Loss: 0.8185, Validation Loss: 1.5533, Validation F1: 0.8894\n",
      "Epoch 944, Train Loss: 0.8150, Validation Loss: 1.5711, Validation F1: 0.8865\n",
      "Epoch 945, Train Loss: 0.8173, Validation Loss: 1.5474, Validation F1: 0.8846\n",
      "Epoch 946, Train Loss: 0.8157, Validation Loss: 1.4968, Validation F1: 0.8950\n",
      "Epoch 947, Train Loss: 0.8147, Validation Loss: 1.5003, Validation F1: 0.8928\n",
      "Epoch 948, Train Loss: 0.8157, Validation Loss: 1.5742, Validation F1: 0.8952\n",
      "Epoch 949, Train Loss: 0.8144, Validation Loss: 1.5053, Validation F1: 0.8970\n",
      "Epoch 950, Train Loss: 0.8157, Validation Loss: 1.4842, Validation F1: 0.8883\n",
      "Epoch 951, Train Loss: 0.8158, Validation Loss: 1.6004, Validation F1: 0.8827\n",
      "Epoch 952, Train Loss: 0.8179, Validation Loss: 1.6118, Validation F1: 0.8867\n",
      "Epoch 953, Train Loss: 0.8150, Validation Loss: 1.4959, Validation F1: 0.8892\n",
      "Epoch 954, Train Loss: 0.8154, Validation Loss: 1.5431, Validation F1: 0.8902\n",
      "Epoch 955, Train Loss: 0.8150, Validation Loss: 1.4754, Validation F1: 0.8955\n",
      "Epoch 956, Train Loss: 0.8201, Validation Loss: 1.5934, Validation F1: 0.8833\n",
      "Epoch 957, Train Loss: 0.8194, Validation Loss: 1.5357, Validation F1: 0.8842\n",
      "Epoch 958, Train Loss: 0.8191, Validation Loss: 1.4562, Validation F1: 0.8866\n",
      "Epoch 959, Train Loss: 0.8136, Validation Loss: 1.5751, Validation F1: 0.9010\n",
      "Epoch 960, Train Loss: 0.8192, Validation Loss: 1.5579, Validation F1: 0.8952\n",
      "Epoch 961, Train Loss: 0.8155, Validation Loss: 1.5821, Validation F1: 0.8906\n",
      "Epoch 962, Train Loss: 0.8169, Validation Loss: 1.6985, Validation F1: 0.8902\n",
      "Epoch 963, Train Loss: 0.8173, Validation Loss: 1.4862, Validation F1: 0.8826\n",
      "Epoch 964, Train Loss: 0.8149, Validation Loss: 1.5643, Validation F1: 0.8863\n",
      "Epoch 965, Train Loss: 0.8162, Validation Loss: 1.5791, Validation F1: 0.8903\n",
      "Epoch 966, Train Loss: 0.8146, Validation Loss: 1.5603, Validation F1: 0.8861\n",
      "Epoch 967, Train Loss: 0.8147, Validation Loss: 1.5377, Validation F1: 0.8866\n",
      "Epoch 968, Train Loss: 0.8150, Validation Loss: 1.5029, Validation F1: 0.8870\n",
      "Epoch 969, Train Loss: 0.8154, Validation Loss: 1.5159, Validation F1: 0.8880\n",
      "Epoch 970, Train Loss: 0.8165, Validation Loss: 1.4706, Validation F1: 0.8971\n",
      "Epoch 971, Train Loss: 0.8162, Validation Loss: 1.6044, Validation F1: 0.8904\n",
      "Epoch 972, Train Loss: 0.8155, Validation Loss: 1.4991, Validation F1: 0.8884\n",
      "Epoch 973, Train Loss: 0.8139, Validation Loss: 1.5400, Validation F1: 0.8829\n",
      "Epoch 974, Train Loss: 0.8172, Validation Loss: 1.5649, Validation F1: 0.8903\n",
      "Epoch 975, Train Loss: 0.8159, Validation Loss: 1.5667, Validation F1: 0.8919\n",
      "Epoch 976, Train Loss: 0.8134, Validation Loss: 1.6320, Validation F1: 0.8891\n",
      "Epoch 977, Train Loss: 0.8163, Validation Loss: 1.6583, Validation F1: 0.8888\n",
      "Epoch 978, Train Loss: 0.8179, Validation Loss: 1.5450, Validation F1: 0.8827\n",
      "Epoch 979, Train Loss: 0.8164, Validation Loss: 1.5147, Validation F1: 0.8839\n",
      "Epoch 980, Train Loss: 0.8166, Validation Loss: 1.5266, Validation F1: 0.8883\n",
      "Epoch 981, Train Loss: 0.8146, Validation Loss: 1.5391, Validation F1: 0.8908\n",
      "Epoch 982, Train Loss: 0.8160, Validation Loss: 1.5848, Validation F1: 0.8914\n",
      "Epoch 983, Train Loss: 0.8203, Validation Loss: 1.5599, Validation F1: 0.8898\n",
      "Epoch 984, Train Loss: 0.8150, Validation Loss: 1.4925, Validation F1: 0.8906\n",
      "Epoch 985, Train Loss: 0.8184, Validation Loss: 1.5425, Validation F1: 0.8848\n",
      "Epoch 986, Train Loss: 0.8184, Validation Loss: 1.4980, Validation F1: 0.8887\n",
      "Epoch 987, Train Loss: 0.8141, Validation Loss: 1.6105, Validation F1: 0.8901\n",
      "Epoch 988, Train Loss: 0.8217, Validation Loss: 1.6029, Validation F1: 0.8877\n",
      "Epoch 989, Train Loss: 0.8228, Validation Loss: 1.5829, Validation F1: 0.8900\n",
      "Epoch 990, Train Loss: 0.8153, Validation Loss: 1.5204, Validation F1: 0.8931\n",
      "Epoch 991, Train Loss: 0.8257, Validation Loss: 1.6039, Validation F1: 0.8901\n",
      "Epoch 992, Train Loss: 0.8241, Validation Loss: 1.6052, Validation F1: 0.8888\n",
      "Epoch 993, Train Loss: 0.8184, Validation Loss: 1.6149, Validation F1: 0.8871\n",
      "Epoch 994, Train Loss: 0.8330, Validation Loss: 1.5177, Validation F1: 0.8882\n",
      "Epoch 995, Train Loss: 0.8274, Validation Loss: 1.6183, Validation F1: 0.8870\n",
      "Epoch 996, Train Loss: 0.8302, Validation Loss: 1.5691, Validation F1: 0.8847\n",
      "Epoch 997, Train Loss: 0.8319, Validation Loss: 1.5184, Validation F1: 0.8893\n",
      "Epoch 998, Train Loss: 0.8245, Validation Loss: 1.5920, Validation F1: 0.8872\n",
      "Epoch 999, Train Loss: 0.8253, Validation Loss: 1.5763, Validation F1: 0.8871\n",
      "Model training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract the best parameters from the grid search\n",
    "best_hidden_dim = 256  # Replace with the best hidden_dim found\n",
    "best_learning_rate = 0.001  # Replace with the best learning_rate found\n",
    "best_dropout = 0.3  # Replace with the best dropout found\n",
    "\n",
    "# Initialize the model with the best parameters\n",
    "model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                   edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                   hidden_channels=best_hidden_dim,\n",
    "                   dropout=best_dropout,\n",
    "                   out_channels=num_classes).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Compute class weights for the training dataset\n",
    "labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.unique(labels),\n",
    "                                                  y=labels)\n",
    "\n",
    "# Normalize class weights\n",
    "class_weights = th.FloatTensor(class_weights).to(device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# Move the graph data to the device\n",
    "G_pyg_train = G_pyg_train.to(device)\n",
    "G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "best_f1 = 0\n",
    "\n",
    "# Load checkpoint if exists\n",
    "start_epoch = 0\n",
    "epochs = 1000\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = th.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "    print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_f1_history = []\n",
    "saved_model_epochs = []\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    out = model(G_pyg_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(out, G_pyg_train.edge_label)\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        out = model(G_pyg_val)\n",
    "        loss = criterion(out, G_pyg_val.edge_label)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "\n",
    "    if epoch % 10 == 0 or val_f1 > best_f1:\n",
    "        # Save checkpoint\n",
    "        th.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_f1': best_f1\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1  # Update the best F1 score for this fold\n",
    "        best_model_state = model.state_dict()\n",
    "        th.save(best_model_state, best_model_path)\n",
    "        print(f\"Epoch {epoch} Saved best model. Best F1:\", best_f1)\n",
    "        saved_model_epochs.append(epoch)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_f1_history.append(val_f1)\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Model training completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df26e7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_process(train_loss_history, val_loss_history, val_f1_history, saved_model_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8368fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 52521\n",
      "Shape of node in G_pyg: torch.Size([52521, 51])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 51])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "857f271a-612b-4cd6-a85a-e4236dec9d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_combined_ports/best_model_all_raw_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "class_map ['Analysis' 'Backdoor' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic'\n",
      " 'Normal' 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[  208     0     0    73     2    62     0     0     0     0    57]\n",
      " [  116    17     1    98     2    19     0     0     0    10     6]\n",
      " [    0     3     1     0     0    73     0     0     0     1     2]\n",
      " [ 1095    25     0   642   142   220    12     2     1    63   251]\n",
      " [ 1546    90     2   727  1991   467    23     5    12   206  1610]\n",
      " [  198     5     0    90    62  2940     0    95     0    68   179]\n",
      " [  172    19     0   117   165    47 31450     2     3    41   306]\n",
      " [   16     8     0     0    19   360     0 32818     1    15    44]\n",
      " [  214     1     0   102     4    24     4     0  1431    14   304]\n",
      " [    0     1     0     0     3     0     0     0     1   218     4]\n",
      " [    4     0     0     0     0     0     0     0     0     1    21]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0583    0.5174    0.1048       402\n",
      "      Backdoor     0.1006    0.0632    0.0776       269\n",
      "     Backdoors     0.2500    0.0125    0.0238        80\n",
      "           DoS     0.3472    0.2617    0.2985      2453\n",
      "      Exploits     0.8331    0.2981    0.4391      6679\n",
      "       Fuzzers     0.6980    0.8084    0.7491      3637\n",
      "       Generic     0.9988    0.9730    0.9857     32322\n",
      "        Normal     0.9968    0.9861    0.9914     33281\n",
      "Reconnaissance     0.9876    0.6821    0.8069      2098\n",
      "     Shellcode     0.3422    0.9604    0.5046       227\n",
      "         Worms     0.0075    0.8077    0.0149        26\n",
      "\n",
      "      accuracy                         0.8805     81474\n",
      "     macro avg     0.5109    0.5791    0.4542     81474\n",
      "  weighted avg     0.9406    0.8805    0.8974     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def eval(G_pyg_test, adversarial=False):\n",
    "\n",
    "    G_pyg_test = G_pyg_test.to(device)\n",
    "    G_pyg_test.edge_label = G_pyg_test.edge_label.to(device)\n",
    "    G_pyg_test.edge_attr = G_pyg_test.edge_attr.to(device)\n",
    "\n",
    "    best_model = EGraphSAGE(node_in_channels=G_pyg_test.num_node_features, \n",
    "                       edge_in_channels=G_pyg_test.num_edge_features,\n",
    "                       hidden_channels=best_hidden_dim, \n",
    "                       out_channels=num_classes).to(device)\n",
    "\n",
    "    print(\"Loading model from\", best_model_path)\n",
    "    best_model.load_state_dict(th.load(best_model_path))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    best_model.eval()\n",
    "\n",
    "    print(\"inference start\")\n",
    "    with th.no_grad():\n",
    "            \n",
    "        try:\n",
    "            out = best_model(G_pyg_test)\n",
    "            \n",
    "        except Exception as forward_error:\n",
    "            print(f\"Error during forward/backward pass at {forward_error}\")\n",
    "\n",
    "    print(\"inference done\")\n",
    "\n",
    "    # test_accuracy = compute_accuracy(out, G_pyg_test.edge_label)\n",
    "    # print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    \n",
    "    pred_labels = out.argmax(dim=1).cpu()\n",
    "    all_test_labels = G_pyg_test.edge_label.cpu()\n",
    "\n",
    "    if adversarial:\n",
    "\n",
    "        # Create a boolean mask where the label is NOT equal to the adversarial class\n",
    "        adversarial_mask = all_test_labels == ADVERSARIAL_CLASS_LABEL\n",
    "\n",
    "        # Print the class that the adversarial samples are classified as\n",
    "        cm_adversarial = confusion_matrix(all_test_labels[adversarial_mask], pred_labels[adversarial_mask], labels=range(len(class_map) + 1))\n",
    "        print(\"Adversarial confusion matrix:\", cm_adversarial)\n",
    "\n",
    "        # Apply the mask to both labels and predictions\n",
    "        all_test_labels = all_test_labels[~adversarial_mask]\n",
    "        pred_labels = pred_labels[~adversarial_mask]\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"class_map\", class_map)\n",
    "    # Generate a report\n",
    "    cm = confusion_matrix(all_test_labels, pred_labels, labels=range(len(class_map)))\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    report = classification_report(all_test_labels, pred_labels, target_names=class_map, digits=4)\n",
    "    print(report)\n",
    "\n",
    "    \n",
    "eval(G_pyg_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a352c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def confusion_matrix_plot(cm, all_test_labels, pred_labels):\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_map, yticklabels=class_map)\n",
    "#     plt.xlabel(\"Predicted\")\n",
    "#     plt.ylabel(\"True\")\n",
    "#     plt.title(\"Confusion Matrix\")\n",
    "\n",
    "#     # Compute metrics\n",
    "#     accuracy = accuracy_score(all_test_labels, pred_labels)\n",
    "#     precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_test_labels, pred_labels, average='macro', zero_division=0)\n",
    "#     precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(all_test_labels, pred_labels, average='weighted', zero_division=0)\n",
    "\n",
    "\n",
    "#     metrics_text = (\n",
    "#         f\"Accuracy: {accuracy:.4f}\\n\"\n",
    "#         f\"Macro Precision: {precision_macro:.4f}\\n\"\n",
    "#         f\"Macro Recall: {recall_macro:.4f}\\n\"\n",
    "#         f\"Macro F1: {f1_macro:.4f}\\n\"\n",
    "#         f\"Weighted Precision: {precision_weighted:.4f}\\n\"\n",
    "#         f\"Weighted Recall: {recall_weighted:.4f}\\n\"\n",
    "#         f\"Weighted F1: {f1_weighted:.4f}\"\n",
    "#     )\n",
    "\n",
    "#     # Position: bottom left corner of plot area\n",
    "#     plt.gcf().text(0.02, 0.02, metrics_text, fontsize=12, va='bottom', ha='left',\n",
    "#                 bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bcb5e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_traffic_to_attacker(graph, ratio=0.1, num_injected_nodes=1, is_attack=False):\n",
    "    edge_index = graph.edge_index.clone()\n",
    "    edge_attr = graph.edge_attr.clone()\n",
    "    edge_label = graph.edge_label.clone()\n",
    "    x = graph.x.clone()\n",
    "\n",
    "    num_edges = edge_index.size(1)\n",
    "    feature_dim = graph.x.size(1)\n",
    "\n",
    "    # 1. Identify attacker nodes\n",
    "    attacker_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "    attacker_nodes = th.unique(edge_index[:, attacker_edges])\n",
    "    if attacker_nodes.numel() == 0:\n",
    "        raise ValueError(\"No attacker nodes found.\")\n",
    "\n",
    "    # 2. Sample benign edge feature pool\n",
    "    if is_attack:\n",
    "        attack_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[attack_edges]\n",
    "    else:\n",
    "        benign_edges = (edge_label == BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[benign_edges]\n",
    "\n",
    "    # 3. Inject new nodes\n",
    "    original_num_nodes = x.size(0)\n",
    "\n",
    "    new_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "    x = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "    # 4. Inject edges from injected nodes to attacker nodes\n",
    "    num_to_inject = max(1, int(ratio * num_edges))\n",
    "    new_edges = []\n",
    "    new_attrs = []\n",
    "    new_labels = []\n",
    "\n",
    "    \n",
    "    for _ in range(num_to_inject):\n",
    "        src = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\n",
    "        dst = attacker_nodes[random.randint(0, len(attacker_nodes) - 1)].item()\n",
    "\n",
    "        new_edges.append([src, dst])\n",
    "        attr = inject_edge_attr_pool[random.randint(0, len(inject_edge_attr_pool) - 1)]\n",
    "        new_attrs.append(attr)\n",
    "        new_labels.append(ADVERSARIAL_CLASS_LABEL)\n",
    "\n",
    "    # Create a new empty graph to store the injected edges\n",
    "    new_graph = Data()\n",
    "\n",
    "    # 5. Merge into graph\n",
    "    if new_edges:\n",
    "        new_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "        new_attrs = th.stack(new_attrs)\n",
    "        new_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "        new_graph.edge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "        new_graph.edge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "        new_graph.edge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "        new_graph.x = x\n",
    "\n",
    "        # new_graph.first_injected_node_idx = original_num_nodes # Store injected node indices\n",
    "\n",
    "    return new_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "632c5bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_combined_ports/best_model_all_raw_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  34  129   10  230  677  361 5381  237  253  825   10    0]]\n",
      "class_map ['Analysis' 'Backdoor' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic'\n",
      " 'Normal' 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[    0     0     0   346    56     0     0     0     0     0     0]\n",
      " [    0     2     1   250     5     0     2     2     0     7     0]\n",
      " [    0     1     0    75     2     1     0     0     0     1     0]\n",
      " [    0    13     0  2141   234    13    19     9     7    15     2]\n",
      " [    0    34    13  4180  2194    32    83    68    26    46     3]\n",
      " [    0     6     0  1867   235  1161     1   366     0     1     0]\n",
      " [    0     9     5 10281   266     3 21726    18     2    12     0]\n",
      " [    0     6     0    43   101   266     1 32864     0     0     0]\n",
      " [    0     0     0  1084   835     1     6     0   170     2     0]\n",
      " [    0     6     2   107     8     1    35     9     7    51     1]\n",
      " [    0     0     0    16     8     1     0     1     0     0     0]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0000    0.0000    0.0000       402\n",
      "      Backdoor     0.0260    0.0074    0.0116       269\n",
      "     Backdoors     0.0000    0.0000    0.0000        80\n",
      "           DoS     0.1050    0.8728    0.1875      2453\n",
      "      Exploits     0.5563    0.3285    0.4131      6679\n",
      "       Fuzzers     0.7850    0.3192    0.4539      3637\n",
      "       Generic     0.9933    0.6722    0.8018     32322\n",
      "        Normal     0.9858    0.9875    0.9866     33281\n",
      "Reconnaissance     0.8019    0.0810    0.1472      2098\n",
      "     Shellcode     0.3778    0.2247    0.2818       227\n",
      "         Worms     0.0000    0.0000    0.0000        26\n",
      "\n",
      "      accuracy                         0.7402     81474\n",
      "     macro avg     0.4210    0.3176    0.2985     81474\n",
      "  weighted avg     0.9023    0.7402    0.7855     81474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Inject Attack Traffic to Attacker Nodes\n",
    "G_pyg_test = G_pyg_test.cpu()\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=True)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64c37b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_combined_ports/best_model_all_raw_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   3   39    1   53  125    8    2 7849    4   61    2    0]]\n",
      "class_map ['Analysis' 'Backdoor' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic'\n",
      " 'Normal' 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[    0     0     0   117    56    57     0   172     0     0     0]\n",
      " [    0     9     0   136    51     0     0    73     0     0     0]\n",
      " [    0     2     0     3     0    69     0     6     0     0     0]\n",
      " [    0    18     0   951   458    71     4   939     1    11     0]\n",
      " [    1   127     1  1496  1695   154     8  3158     0    35     4]\n",
      " [    0    85     0   402   188   247     0  2714     0     1     0]\n",
      " [    0    14     0   270   181     0 21499 10349     0     8     1]\n",
      " [    0    12     0     2    36    19     0 33209     0     3     0]\n",
      " [    0   105     0   547   697     0     2   740     0     6     1]\n",
      " [    0    20     1    26     9     0     0   110     1    58     2]\n",
      " [    0     0     0     1     3     1     0    20     0     0     1]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0000    0.0000    0.0000       402\n",
      "      Backdoor     0.0230    0.0335    0.0272       269\n",
      "     Backdoors     0.0000    0.0000    0.0000        80\n",
      "           DoS     0.2407    0.3877    0.2970      2453\n",
      "      Exploits     0.5024    0.2538    0.3372      6679\n",
      "       Fuzzers     0.3997    0.0679    0.1161      3637\n",
      "       Generic     0.9993    0.6652    0.7987     32322\n",
      "        Normal     0.6450    0.9978    0.7835     33281\n",
      "Reconnaissance     0.0000    0.0000    0.0000      2098\n",
      "     Shellcode     0.4754    0.2555    0.3324       227\n",
      "         Worms     0.1111    0.0385    0.0571        26\n",
      "\n",
      "      accuracy                         0.7078     81474\n",
      "     macro avg     0.3088    0.2454    0.2499     81474\n",
      "  weighted avg     0.7276    0.7078    0.6797     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inject BENIGN Traffic to Attacker Nodes\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=False)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bf0e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_random_nodes(graph, ratio=0.1, num_injected_nodes=1):\n",
    "\tedge_index = graph.edge_index.clone()\n",
    "\tedge_attr = graph.edge_attr.clone()\n",
    "\tedge_label = graph.edge_label.clone()\n",
    "\tx = graph.x.clone()\n",
    "\n",
    "\tnum_edges = edge_index.size(1)\n",
    "\tfeature_dim = graph.x.size(1)\n",
    "\n",
    "\t# 1. Inject new nodes\n",
    "\toriginal_num_nodes = x.size(0)\n",
    "\tnew_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "\tx = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "\t# 2. Inject random edges\n",
    "\tnum_to_inject = max(1, int(ratio * num_edges))\n",
    "\tnew_edges = []\n",
    "\tnew_attrs = []\n",
    "\tnew_labels = []\n",
    "\n",
    "\tfor _ in range(num_to_inject):\n",
    "\t\tsrc = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\t\tdst = random.randint(0, original_num_nodes - 1)  # to existing nodes\n",
    "\n",
    "\t\tnew_edges.append([src, dst])\n",
    "\t\tattr = edge_attr[random.randint(0, len(edge_attr) - 1)]  # Randomly sample edge attributes\n",
    "\t\tnew_attrs.append(attr)\n",
    "\t\tnew_labels.append(ADVERSARIAL_CLASS_LABEL)  # Assign benign class label to new edges\n",
    "\n",
    "\t# 3. Merge into graph\n",
    "\tif new_edges:\n",
    "\t\tnew_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "\t\tnew_attrs = th.stack(new_attrs)\n",
    "\t\tnew_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "\t\tedge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "\t\tedge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "\t\tedge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "\n",
    "\t# Create a new graph with the injected nodes and edges\n",
    "\tnew_graph = Data(\n",
    "\t\tedge_index=edge_index,\n",
    "\t\tedge_attr=edge_attr,\n",
    "\t\tedge_label=edge_label,\n",
    "\t\tx=x\n",
    "\t)\n",
    "\n",
    "\treturn new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "abb63898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_combined_ports/best_model_all_raw_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  18   93    8   74  368  166 2206 4546   36  614   18    0]]\n",
      "class_map ['Analysis' 'Backdoor' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic'\n",
      " 'Normal' 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[  178     0     1    57    92    62     0     0     0     0    12]\n",
      " [  118    13     1    40    65    21     3     0     0     8     0]\n",
      " [    0     2     1     0     0    74     0     0     0     1     2]\n",
      " [ 1098    28     0   413   565   218    16    17     2    36    60]\n",
      " [ 1437   127     5   638  3460   373    29    79    17   124   390]\n",
      " [  174    12     1   286   530  2400     0   198     0    18    18]\n",
      " [  144    20     2  3437   376    43 23110  5063     3    25    99]\n",
      " [    4    17     6    55   103   307     2 32778     3     1     5]\n",
      " [  212     4     1   166   250    24     2     2  1348    10    79]\n",
      " [    0     4     0    15     8     0    11    14     4   167     4]\n",
      " [    0     0     0     3    18     2     0     0     0     0     3]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0529    0.4428    0.0945       402\n",
      "      Backdoor     0.0573    0.0483    0.0524       269\n",
      "     Backdoors     0.0556    0.0125    0.0204        80\n",
      "           DoS     0.0808    0.1684    0.1092      2453\n",
      "      Exploits     0.6329    0.5180    0.5697      6679\n",
      "       Fuzzers     0.6810    0.6599    0.6703      3637\n",
      "       Generic     0.9973    0.7150    0.8329     32322\n",
      "        Normal     0.8592    0.9849    0.9177     33281\n",
      "Reconnaissance     0.9789    0.6425    0.7758      2098\n",
      "     Shellcode     0.4282    0.7357    0.5413       227\n",
      "         Worms     0.0045    0.1154    0.0086        26\n",
      "\n",
      "      accuracy                         0.7839     81474\n",
      "     macro avg     0.4390    0.4585    0.4175     81474\n",
      "  weighted avg     0.8582    0.7839    0.8074     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inject Random Nodes in the graph\n",
    "injected_graph = inject_random_nodes(G_pyg_test, 0.1, num_injected_nodes=1)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
