{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb9e1b2-1c5f-49e3-82b4-3bfe52cabad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "=====Experiment=====\n",
    "Dataset: UNSW-NB15 dataset\n",
    "\n",
    "Training with whole graph\n",
    "Downsampled 90% normal traffic randomly\n",
    "Split train and test randomly\n",
    "\n",
    "Combined IP and Port features\n",
    "'''\n",
    "\n",
    "from torch_geometric.utils import from_networkx, add_self_loops, degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "# import dgl.function as fn\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import socket\n",
    "import struct\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from Datasets.UNSW_NB15.UNSW_NB15_config import UNSW_NB15_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ef09a-d405-43b8-971e-fe9e6a592c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3511114/533513547.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "Normal            221876\n",
      "Generic           215481\n",
      "Exploits           44525\n",
      "Fuzzers            24246\n",
      "DoS                16353\n",
      "Reconnaissance     13987\n",
      "Analysis            2677\n",
      "Backdoors           2329\n",
      "Shellcode           1511\n",
      "Worms                174\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    321283\n",
      "0    221876\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "csv_file_name = \"all_downsampled\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(project_root, \"Datasets\", f\"UNSW_NB15/All/{csv_file_name}.csv\"))\n",
    "\n",
    "DATASET_NAME = \"UNSW_NB15\"\n",
    "EXPERIMENT_NAME = \"whole_graph_combined_ports_final\"\n",
    "\n",
    "SOURCE_IP_COL_NAME = UNSW_NB15_Config.SOURCE_IP_COL_NAME\n",
    "DESTINATION_IP_COL_NAME = UNSW_NB15_Config.DESTINATION_IP_COL_NAME\n",
    "SOURCE_PORT_COL_NAME = UNSW_NB15_Config.SOURCE_PORT_COL_NAME\n",
    "DESTINATION_PORT_COL_NAME = UNSW_NB15_Config.DESTINATION_PORT_COL_NAME\n",
    "\n",
    "ATTACK_CLASS_COL_NAME = UNSW_NB15_Config.ATTACK_CLASS_COL_NAME\n",
    "IS_ATTACK_COL_NAME = UNSW_NB15_Config.IS_ATTACK_COL_NAME\n",
    "\n",
    "BENIGN_CLASS_NAME = UNSW_NB15_Config.BENIGN_CLASS_NAME\n",
    "\n",
    "TIME_COLS = UNSW_NB15_Config.TIME_COL_NAMES\n",
    "\n",
    "print(data[ATTACK_CLASS_COL_NAME].value_counts())\n",
    "print(data[IS_ATTACK_COL_NAME].value_counts())\n",
    "\n",
    "MULTICLASS = True\n",
    "\n",
    "if MULTICLASS:\n",
    "    label_col = ATTACK_CLASS_COL_NAME\n",
    "    data.drop(columns=[IS_ATTACK_COL_NAME], inplace=True)\n",
    "else:\n",
    "    label_col = IS_ATTACK_COL_NAME\n",
    "    data.drop(columns=[ATTACK_CLASS_COL_NAME], inplace=True)\n",
    "\n",
    "saves_path = os.path.join(project_root, \"Models/E_GraphSAGE/logs\", DATASET_NAME, EXPERIMENT_NAME)\n",
    "\n",
    "checkpoint_path = os.path.join(saves_path, f\"checkpoints_{csv_file_name}.pth\")\n",
    "best_model_path = os.path.join(saves_path, f\"best_model_{csv_file_name}.pth\")\n",
    "\n",
    "os.makedirs(os.path.dirname(saves_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a1af1-1d3d-4179-9628-7c2ec551ce0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srcip</th>\n",
       "      <th>sport</th>\n",
       "      <th>dstip</th>\n",
       "      <th>dsport</th>\n",
       "      <th>state</th>\n",
       "      <th>dur</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>source_file_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.40.85.1_0</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0.0.5_0</td>\n",
       "      <td>0</td>\n",
       "      <td>INT</td>\n",
       "      <td>50.004341</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.166.0.6_0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.4_0</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>132</td>\n",
       "      <td>164</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175.45.176.0_0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.16_0</td>\n",
       "      <td>80</td>\n",
       "      <td>FIN</td>\n",
       "      <td>2.390390</td>\n",
       "      <td>1362</td>\n",
       "      <td>268</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Reconnaissance</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.166.0.3_0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.8_0</td>\n",
       "      <td>25</td>\n",
       "      <td>FIN</td>\n",
       "      <td>34.077175</td>\n",
       "      <td>37358</td>\n",
       "      <td>3380</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.40.170.2_0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.40.170.2_0</td>\n",
       "      <td>0</td>\n",
       "      <td>INT</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543154</th>\n",
       "      <td>175.45.176.1_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.11_3</td>\n",
       "      <td>6071</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.291164</td>\n",
       "      <td>732</td>\n",
       "      <td>468</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Generic</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543155</th>\n",
       "      <td>175.45.176.3_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.16_3</td>\n",
       "      <td>2140</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.011751</td>\n",
       "      <td>76</td>\n",
       "      <td>132</td>\n",
       "      <td>254</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Backdoors</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543156</th>\n",
       "      <td>59.166.0.2_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.4_3</td>\n",
       "      <td>53</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>146</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Normal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543157</th>\n",
       "      <td>175.45.176.1_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.11_3</td>\n",
       "      <td>5250</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.176514</td>\n",
       "      <td>10778</td>\n",
       "      <td>268</td>\n",
       "      <td>254</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Generic</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543158</th>\n",
       "      <td>59.166.0.2_3</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.2_3</td>\n",
       "      <td>8406</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.049598</td>\n",
       "      <td>2646</td>\n",
       "      <td>25564</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Normal</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543159 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 srcip  sport             dstip dsport state        dur  \\\n",
       "0         10.40.85.1_0      0       224.0.0.5_0      0   INT  50.004341   \n",
       "1         59.166.0.6_0      0   149.171.126.4_0     53   CON   0.001134   \n",
       "2       175.45.176.0_0      0  149.171.126.16_0     80   FIN   2.390390   \n",
       "3         59.166.0.3_0      0   149.171.126.8_0     25   FIN  34.077175   \n",
       "4        10.40.170.2_0      0     10.40.170.2_0      0   INT   0.000000   \n",
       "...                ...    ...               ...    ...   ...        ...   \n",
       "543154  175.45.176.1_3      0  149.171.126.11_3   6071   FIN   0.291164   \n",
       "543155  175.45.176.3_3      0  149.171.126.16_3   2140   CON   0.011751   \n",
       "543156    59.166.0.2_3      0   149.171.126.4_3     53   CON   0.002410   \n",
       "543157  175.45.176.1_3      0  149.171.126.11_3   5250   FIN   0.176514   \n",
       "543158    59.166.0.2_3      0   149.171.126.2_3   8406   FIN   0.049598   \n",
       "\n",
       "        sbytes  dbytes  sttl  dttl  ...  ct_ftp_cmd  ct_srv_src  ct_srv_dst  \\\n",
       "0          384       0     1     0  ...         0.0           2           4   \n",
       "1          132     164    31    29  ...         0.0          12           7   \n",
       "2         1362     268   254   252  ...         0.0           5           2   \n",
       "3        37358    3380    31    29  ...         0.0           1           1   \n",
       "4           46       0     0     0  ...         0.0           2           2   \n",
       "...        ...     ...   ...   ...  ...         ...         ...         ...   \n",
       "543154     732     468   254   252  ...         NaN           1           1   \n",
       "543155      76     132   254    60  ...         NaN           1           1   \n",
       "543156     146     178    31    29  ...         NaN           3           5   \n",
       "543157   10778     268   254   252  ...         NaN           1           1   \n",
       "543158    2646   25564    31    29  ...         NaN           6           2   \n",
       "\n",
       "        ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  \\\n",
       "0                4           2                 2                 4   \n",
       "1                1           2                 2                 1   \n",
       "2                2           1                 1                 1   \n",
       "3               12          10                 1                 1   \n",
       "4                2           2                 2                 2   \n",
       "...            ...         ...               ...               ...   \n",
       "543154           1           1                 1                 1   \n",
       "543155           1           1                 1                 1   \n",
       "543156           3           2                 2                 2   \n",
       "543157           1           1                 1                 1   \n",
       "543158           4           7                 1                 1   \n",
       "\n",
       "        ct_dst_src_ltm      attack_cat  source_file_id  \n",
       "0                    2          Normal               0  \n",
       "1                    1          Normal               0  \n",
       "2                    1  Reconnaissance               0  \n",
       "3                    2          Normal               0  \n",
       "4                    2          Normal               0  \n",
       "...                ...             ...             ...  \n",
       "543154               2         Generic               3  \n",
       "543155               1       Backdoors               3  \n",
       "543156               4          Normal               3  \n",
       "543157               2         Generic               3  \n",
       "543158               3          Normal               3  \n",
       "\n",
       "[543159 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns=UNSW_NB15_Config.DROP_COLS,inplace=True)\n",
    "data.drop(columns=UNSW_NB15_Config.TIME_COL_NAMES)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c690c-86a4-49f7-aa9c-58f94529547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME].apply(str)\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME].apply(str)\n",
    "\n",
    "# # Combine Port and IP\n",
    "data[SOURCE_PORT_COL_NAME] = data[SOURCE_PORT_COL_NAME].apply(str)\n",
    "data[DESTINATION_PORT_COL_NAME] = data[DESTINATION_PORT_COL_NAME].apply(str)\n",
    "\n",
    "data[SOURCE_IP_COL_NAME] = data[SOURCE_IP_COL_NAME] + ':' + data[SOURCE_PORT_COL_NAME]\n",
    "data[DESTINATION_IP_COL_NAME] = data[DESTINATION_IP_COL_NAME] + ':' + data[DESTINATION_PORT_COL_NAME]\n",
    "data.drop(columns=[SOURCE_PORT_COL_NAME,DESTINATION_PORT_COL_NAME],inplace=True)\n",
    "\n",
    "data = pd.get_dummies(data, columns = UNSW_NB15_Config.CATEGORICAL_COLS) # One Hot Encoding for categorical data\n",
    "converted_categorical_cols = [col for col in data.columns if col.startswith(tuple(UNSW_NB15_Config.CATEGORICAL_COLS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5651ef5b-0a9d-4641-aad7-5738092c46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                    srcip                  dstip state        dur  sbytes  \\\n",
      "0         10.40.85.1_0:0          224.0.0.5_0:0   INT  50.004341     384   \n",
      "1         59.166.0.6_0:0     149.171.126.4_0:53   CON   0.001134     132   \n",
      "2       175.45.176.0_0:0    149.171.126.16_0:80   FIN   2.390390    1362   \n",
      "3         59.166.0.3_0:0     149.171.126.8_0:25   FIN  34.077175   37358   \n",
      "4        10.40.170.2_0:0        10.40.170.2_0:0   INT   0.000000      46   \n",
      "...                  ...                    ...   ...        ...     ...   \n",
      "543154  175.45.176.1_3:0  149.171.126.11_3:6071   FIN   0.291164     732   \n",
      "543155  175.45.176.3_3:0  149.171.126.16_3:2140   CON   0.011751      76   \n",
      "543156    59.166.0.2_3:0     149.171.126.4_3:53   CON   0.002410     146   \n",
      "543157  175.45.176.1_3:0  149.171.126.11_3:5250   FIN   0.176514   10778   \n",
      "543158    59.166.0.2_3:0   149.171.126.2_3:8406   FIN   0.049598    2646   \n",
      "\n",
      "        dbytes  sttl  dttl  sloss  dloss  ...  ct_ftp_cmd  ct_srv_src  \\\n",
      "0            0     1     0      0      0  ...         0.0           2   \n",
      "1          164    31    29      0      0  ...         0.0          12   \n",
      "2          268   254   252      6      1  ...         0.0           5   \n",
      "3         3380    31    29     18      8  ...         0.0           1   \n",
      "4            0     0     0      0      0  ...         0.0           2   \n",
      "...        ...   ...   ...    ...    ...  ...         ...         ...   \n",
      "543154     468   254   252      3      2  ...         NaN           1   \n",
      "543155     132   254    60      0      0  ...         NaN           1   \n",
      "543156     178    31    29      0      0  ...         NaN           3   \n",
      "543157     268   254   252      5      1  ...         NaN           1   \n",
      "543158   25564    31    29      7     15  ...         NaN           6   \n",
      "\n",
      "        ct_srv_dst  ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  \\\n",
      "0                4           4           2                 2   \n",
      "1                7           1           2                 2   \n",
      "2                2           2           1                 1   \n",
      "3                1          12          10                 1   \n",
      "4                2           2           2                 2   \n",
      "...            ...         ...         ...               ...   \n",
      "543154           1           1           1                 1   \n",
      "543155           1           1           1                 1   \n",
      "543156           5           3           2                 2   \n",
      "543157           1           1           1                 1   \n",
      "543158           2           4           7                 1   \n",
      "\n",
      "        ct_dst_sport_ltm  ct_dst_src_ltm      attack_cat  source_file_id  \n",
      "0                      4               2          Normal               0  \n",
      "1                      1               1          Normal               0  \n",
      "2                      1               1  Reconnaissance               0  \n",
      "3                      1               2          Normal               0  \n",
      "4                      2               2          Normal               0  \n",
      "...                  ...             ...             ...             ...  \n",
      "543154                 1               2         Generic               3  \n",
      "543155                 1               1       Backdoors               3  \n",
      "543156                 2               4          Normal               3  \n",
      "543157                 1               2         Generic               3  \n",
      "543158                 1               3          Normal               3  \n",
      "\n",
      "[543159 rows x 45 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d96115-31f9-48cb-b3e6-7853d2d253cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                    srcip                  dstip        dur  sbytes  dbytes  \\\n",
      "0         10.40.85.1_0:0          224.0.0.5_0:0  50.004341     384       0   \n",
      "1         59.166.0.6_0:0     149.171.126.4_0:53   0.001134     132     164   \n",
      "2       175.45.176.0_0:0    149.171.126.16_0:80   2.390390    1362     268   \n",
      "3         59.166.0.3_0:0     149.171.126.8_0:25  34.077175   37358    3380   \n",
      "4        10.40.170.2_0:0        10.40.170.2_0:0   0.000000      46       0   \n",
      "...                  ...                    ...        ...     ...     ...   \n",
      "543154  175.45.176.1_3:0  149.171.126.11_3:6071   0.291164     732     468   \n",
      "543155  175.45.176.3_3:0  149.171.126.16_3:2140   0.011751      76     132   \n",
      "543156    59.166.0.2_3:0     149.171.126.4_3:53   0.002410     146     178   \n",
      "543157  175.45.176.1_3:0  149.171.126.11_3:5250   0.176514   10778     268   \n",
      "543158    59.166.0.2_3:0   149.171.126.2_3:8406   0.049598    2646   25564   \n",
      "\n",
      "        sttl  dttl  sloss  dloss          Sload  ...  state_ECR  state_FIN  \\\n",
      "0          1     0      0      0      51.195557  ...      False      False   \n",
      "1         31    29      0      0  465608.468800  ...      False      False   \n",
      "2        254   252      6      1    4233.619141  ...      False       True   \n",
      "3         31    29     18      8    8601.652344  ...      False       True   \n",
      "4          0     0      0      0       0.000000  ...      False      False   \n",
      "...      ...   ...    ...    ...            ...  ...        ...        ...   \n",
      "543154   254   252      3      2   18436.343750  ...      False       True   \n",
      "543155   254    60      0      0   25870.138670  ...      False      False   \n",
      "543156    31    29      0      0  242323.656300  ...      False      False   \n",
      "543157   254   252      5      1  457980.656300  ...      False       True   \n",
      "543158    31    29      7     15  416629.687500  ...      False       True   \n",
      "\n",
      "        state_INT  state_MAS  state_PAR  state_REQ  state_RST  state_TST  \\\n",
      "0            True      False      False      False      False      False   \n",
      "1           False      False      False      False      False      False   \n",
      "2           False      False      False      False      False      False   \n",
      "3           False      False      False      False      False      False   \n",
      "4            True      False      False      False      False      False   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "543154      False      False      False      False      False      False   \n",
      "543155      False      False      False      False      False      False   \n",
      "543156      False      False      False      False      False      False   \n",
      "543157      False      False      False      False      False      False   \n",
      "543158      False      False      False      False      False      False   \n",
      "\n",
      "        state_TXD  state_URH  \n",
      "0           False      False  \n",
      "1           False      False  \n",
      "2           False      False  \n",
      "3           False      False  \n",
      "4           False      False  \n",
      "...           ...        ...  \n",
      "543154      False      False  \n",
      "543155      False      False  \n",
      "543156      False      False  \n",
      "543157      False      False  \n",
      "543158      False      False  \n",
      "\n",
      "[543159 rows x 58 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "data.drop(columns=['index'],inplace=True)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1df5c4c-70a2-4566-ae5e-ee3dcc6037a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 dur        sbytes        dbytes           sttl  \\\n",
      "count  543159.000000  5.431590e+05  5.431590e+05  543159.000000   \n",
      "mean        0.703562  5.129376e+03  1.912066e+04     157.223966   \n",
      "std        12.635598  1.202304e+05  1.382834e+05     108.429349   \n",
      "min         0.000000  0.000000e+00  0.000000e+00       0.000000   \n",
      "25%         0.000007  1.140000e+02  0.000000e+00      31.000000   \n",
      "50%         0.000010  2.000000e+02  0.000000e+00     254.000000   \n",
      "75%         0.070875  1.580000e+03  1.936000e+03     254.000000   \n",
      "max      8760.776367  1.435577e+07  1.465753e+07     255.000000   \n",
      "\n",
      "                dttl          sloss          dloss         Sload  \\\n",
      "count  543159.000000  543159.000000  543159.000000  5.431590e+05   \n",
      "mean       38.847354       3.789714       8.637535  6.901181e+07   \n",
      "std        77.059190      45.614073      49.869719  1.425974e+08   \n",
      "min         0.000000       0.000000       0.000000  0.000000e+00   \n",
      "25%         0.000000       0.000000       0.000000  3.760815e+05   \n",
      "50%         0.000000       0.000000       0.000000  4.560000e+07   \n",
      "75%        29.000000       3.000000       4.000000  8.888889e+07   \n",
      "max       254.000000    5319.000000    5507.000000  5.988000e+09   \n",
      "\n",
      "              Dload          Spkts  ...  ct_flw_http_mthd   is_ftp_login  \\\n",
      "count  5.431590e+05  543159.000000  ...     543159.000000  543159.000000   \n",
      "mean   1.145602e+06      20.260456  ...          0.089263       0.011459   \n",
      "std    3.125320e+06     101.785929  ...          0.568852       0.109870   \n",
      "min    0.000000e+00       0.000000  ...          0.000000       0.000000   \n",
      "25%    0.000000e+00       2.000000  ...          0.000000       0.000000   \n",
      "50%    0.000000e+00       2.000000  ...          0.000000       0.000000   \n",
      "75%    4.080209e+05      14.000000  ...          0.000000       0.000000   \n",
      "max    2.248756e+07   10646.000000  ...         36.000000       4.000000   \n",
      "\n",
      "          ct_ftp_cmd     ct_srv_src     ct_srv_dst     ct_dst_ltm  \\\n",
      "count  543159.000000  543159.000000  543159.000000  543159.000000   \n",
      "mean        0.007661      15.025361      14.853214      10.321932   \n",
      "std         0.091356      14.239878      14.314732      10.996982   \n",
      "min         0.000000       1.000000       1.000000       1.000000   \n",
      "25%         0.000000       3.000000       3.000000       2.000000   \n",
      "50%         0.000000       9.000000       8.000000       5.000000   \n",
      "75%         0.000000      26.000000      26.000000      17.000000   \n",
      "max         4.000000      67.000000      67.000000      67.000000   \n",
      "\n",
      "          ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \n",
      "count  543159.000000     543159.000000     543159.000000   543159.000000  \n",
      "mean       10.848566          9.357573          7.219855       13.786578  \n",
      "std        10.976383         11.399195          8.074346       14.983005  \n",
      "min         1.000000          1.000000          1.000000        1.000000  \n",
      "25%         2.000000          1.000000          1.000000        1.000000  \n",
      "50%         6.000000          2.000000          2.000000        5.000000  \n",
      "75%        17.000000         17.000000         15.000000       26.000000  \n",
      "max        67.000000         67.000000         60.000000       67.000000  \n",
      "\n",
      "[8 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "cols_to_norm = UNSW_NB15_Config.COLS_TO_NORM\n",
    "print(data[cols_to_norm].describe()) # Check if there's any too large value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea95177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All other columns processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_numeric_issues(df, cols_to_norm):\n",
    "    for col in cols_to_norm:\n",
    "        try:\n",
    "            # Try to coerce to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Try to clip the column\n",
    "            df[col] = df[col].clip(lower=-1e9, upper=1e9)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Column '{col}' failed with error: {e}\")\n",
    "            print(f\"  - Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "            print(f\"  - Data type: {df[col].dtype}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n✅ All other columns processed successfully.\")\n",
    "\n",
    "check_numeric_issues(data, UNSW_NB15_Config.COLS_TO_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37256006-abc1-44aa-8e74-46d05dc6ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols_to_norm] = scaler.fit_transform(data[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c6e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "Attack label mapping: {'Analysis': 0, 'Backdoors': 1, 'DoS': 2, 'Exploits': 3, 'Fuzzers': 4, 'Generic': 5, 'Normal': 6, 'Reconnaissance': 7, 'Shellcode': 8, 'Worms': 9}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_classes = 2\n",
    "class_map = [0, 1]\n",
    "if MULTICLASS:\n",
    "    le = LabelEncoder()\n",
    "    attack_labels = le.fit_transform(data[ATTACK_CLASS_COL_NAME])\n",
    "    class_map = le.classes_\n",
    "    print(class_map)\n",
    "    print(\"Attack label mapping:\", dict(zip(class_map, range(len(class_map)))))\n",
    "    data[ATTACK_CLASS_COL_NAME] = attack_labels\n",
    "    num_classes = len(class_map)\n",
    "    class_dict = {le.inverse_transform([i])[0]: i for i in range(len(le.classes_))}\n",
    "\n",
    "BENIGN_CLASS_LABEL = le.transform([BENIGN_CLASS_NAME])[0] if MULTICLASS else 0\n",
    "ADVERSARIAL_CLASS_LABEL = len(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f4cdd-2716-431f-af50-b34cc3d2d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'Sload', 'Dload', 'Spkts', 'Dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'Sjit', 'Djit', 'Sintpkt', 'Dintpkt', 'tcprtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'state_ACC', 'state_CLO', 'state_CON', 'state_ECO', 'state_ECR', 'state_FIN', 'state_INT', 'state_MAS', 'state_PAR', 'state_REQ', 'state_RST', 'state_TST', 'state_TXD', 'state_URH']\n",
      "Number of training samples: 461685\n",
      "attack_cat\n",
      "6    188595\n",
      "5    183159\n",
      "3     37846\n",
      "4     20609\n",
      "2     13900\n",
      "7     11889\n",
      "0      2275\n",
      "1      1980\n",
      "8      1284\n",
      "9       148\n",
      "Name: count, dtype: int64\n",
      "Number of test samples: 81474\n",
      "attack_cat\n",
      "6    33281\n",
      "5    32322\n",
      "3     6679\n",
      "4     3637\n",
      "2     2453\n",
      "7     2098\n",
      "0      402\n",
      "1      349\n",
      "8      227\n",
      "9       26\n",
      "Name: count, dtype: int64\n",
      "                   srcip                  dstip       dur    sbytes    dbytes  \\\n",
      "35798   175.45.176.0_0:0     149.171.126.11_0:0  0.024440 -0.037756 -0.136334   \n",
      "454500    59.166.0.2_3:0  149.171.126.1_3:39482 -0.052447 -0.010009  0.271236   \n",
      "23187   175.45.176.0_0:0     149.171.126.18_0:0 -0.017379 -0.033264 -0.136334   \n",
      "489755  175.45.176.3_3:0    149.171.126.15_3:53 -0.055680 -0.041715 -0.138272   \n",
      "134305    59.166.0.1_1:0     149.171.126.4_1:25 -0.004966  0.269172 -0.113829   \n",
      "\n",
      "            sttl      dttl     sloss     dloss     Sload     Dload     Spkts  \\\n",
      "35798   0.892527  2.766092 -0.039236 -0.153150 -0.554662 -0.365989 -0.100804   \n",
      "454500 -1.164114 -0.127790  0.070379  0.368209 -0.548462  3.128362  0.449370   \n",
      "23187   0.892527  2.766092 -0.039236 -0.153150 -0.554559 -0.365371 -0.100804   \n",
      "489755  0.892527 -0.504124 -0.083082 -0.173202 -0.018194 -0.366555 -0.179401   \n",
      "134305 -1.164114 -0.127790  0.311533 -0.012784 -0.550916 -0.353373  0.311827   \n",
      "\n",
      "           Dpkts      swin      dwin     stcpb     dtcpb   smeansz   dmeansz  \\\n",
      "35798  -0.158623  1.303104  1.304850  0.950180 -0.462170 -0.329716 -0.366711   \n",
      "454500  0.427016  1.303104  1.304850  0.274400  0.303777 -0.329716  2.437416   \n",
      "23187  -0.158623  1.303104  1.304850  1.484216  1.484299  0.017606 -0.366711   \n",
      "489755 -0.215298 -0.767401 -0.766374 -0.720896 -0.720905 -0.342580 -0.526845   \n",
      "134305  0.181426  1.303104  1.304850  1.484216 -0.715505  3.928197 -0.242162   \n",
      "\n",
      "        trans_depth  res_bdy_len      Sjit      Djit       Stime       Ltime  \\\n",
      "35798     -0.134792     -0.05343  0.153264 -0.076895  1421933745  1421933746   \n",
      "454500    -0.134792     -0.05343 -0.080902 -0.175606  1424240533  1424240533   \n",
      "23187     -0.134792     -0.05343  0.012423 -0.150894  1421931706  1421931707   \n",
      "489755    -0.134792     -0.05343 -0.080902 -0.189417  1424245264  1424245264   \n",
      "134305    -0.134792     -0.05343 -0.043454 -0.177189  1424222454  1424222454   \n",
      "\n",
      "         Sintpkt   Dintpkt    tcprtt    synack    ackdat  is_sm_ips_ports  \\\n",
      "35798  -0.008638  0.119318  3.717742  3.612803  3.416380        -0.028049   \n",
      "454500 -0.060137 -0.054933 -0.278494 -0.242636 -0.288018        -0.028049   \n",
      "23187  -0.036975  0.019956  2.899294  2.974987  2.483664        -0.028049   \n",
      "489755 -0.060450 -0.055530 -0.289971 -0.258558 -0.293097        -0.028049   \n",
      "134305 -0.053968 -0.040295 -0.277104 -0.239905 -0.288324        -0.028049   \n",
      "\n",
      "        ct_state_ttl  ct_flw_http_mthd  is_ftp_login  ct_ftp_cmd  ct_srv_src  \\\n",
      "35798      -0.098678         -0.156918     -0.104295   -0.083856   -0.633809   \n",
      "454500     -1.123769         -0.156918     -0.104295   -0.083856   -0.774260   \n",
      "23187      -0.098678         -0.156918     -0.104295   -0.083856   -0.984936   \n",
      "489755      0.926413         -0.156918     -0.104295   -0.083856    0.489797   \n",
      "134305     -1.123769         -0.156918     -0.104295   -0.083856   -0.984936   \n",
      "\n",
      "        ct_srv_dst  ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  \\\n",
      "35798    -0.618469   -0.756748   -0.806147         -0.733173   \n",
      "454500   -0.688327   -0.574879   -0.715042         -0.733173   \n",
      "23187    -0.967760   -0.483945   -0.715042         -0.733173   \n",
      "489755    0.499261    1.061935    1.015949          1.109064   \n",
      "134305   -0.967760   -0.483945   -0.441727         -0.733173   \n",
      "\n",
      "        ct_dst_sport_ltm  ct_dst_src_ltm  attack_cat  source_file_id  \\\n",
      "35798          -0.770324       -0.786664           4               0   \n",
      "454500         -0.770324       -0.786664           6               3   \n",
      "23187          -0.770324       -0.853406           3               0   \n",
      "489755          1.830509        0.548183           5               3   \n",
      "134305         -0.770324       -0.786664           6               1   \n",
      "\n",
      "        state_ACC  state_CLO  state_CON  state_ECO  state_ECR  state_FIN  \\\n",
      "35798       False      False      False      False      False       True   \n",
      "454500      False      False      False      False      False       True   \n",
      "23187       False      False      False      False      False       True   \n",
      "489755      False      False      False      False      False      False   \n",
      "134305      False      False      False      False      False       True   \n",
      "\n",
      "        state_INT  state_MAS  state_PAR  state_REQ  state_RST  state_TST  \\\n",
      "35798       False      False      False      False      False      False   \n",
      "454500      False      False      False      False      False      False   \n",
      "23187       False      False      False      False      False      False   \n",
      "489755       True      False      False      False      False      False   \n",
      "134305      False      False      False      False      False      False   \n",
      "\n",
      "        state_TXD  state_URH  \\\n",
      "35798       False      False   \n",
      "454500      False      False   \n",
      "23187       False      False   \n",
      "489755      False      False   \n",
      "134305      False      False   \n",
      "\n",
      "                                                        h  \n",
      "35798   [0.024439518249307186, -0.03775569116632746, -...  \n",
      "454500  [-0.05244682114681552, -0.01000892950063232, 0...  \n",
      "23187   [-0.01737919714689172, -0.03326430888231206, -...  \n",
      "489755  [-0.05568046603287485, -0.04171476147594104, -...  \n",
      "134305  [-0.004966318747785316, 0.269172066323925, -0....  \n"
     ]
    }
   ],
   "source": [
    "# 70% train, 15% validation, 15% test\n",
    "train_full_df, test_df = train_test_split(\n",
    "     data, test_size=0.15, random_state=42, stratify=data[label_col])\n",
    "\n",
    "\n",
    "feature_cols = UNSW_NB15_Config.COLS_TO_NORM + converted_categorical_cols\n",
    "\n",
    "print('Feature Columns:', feature_cols)\n",
    "\n",
    "train_full_df['h'] = train_full_df[ feature_cols ].values.tolist()\n",
    "test_df['h'] = test_df[ feature_cols ].values.tolist()\n",
    "\n",
    "y_train = train_full_df[label_col]\n",
    "y_test = test_df[label_col]\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_full_df))\n",
    "print(y_train.value_counts())\n",
    "print(\"Number of test samples:\", len(test_df))\n",
    "print(y_test.value_counts())\n",
    "\n",
    "print(train_full_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f83bd73-531f-42a0-9f73-e1fd98dce1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df, source_ip_col, destination_ip_col, edge_attr, create_using=nx.MultiDiGraph(), **kwargs):\n",
    "    G_nx = nx.from_pandas_edgelist(df, source_ip_col, destination_ip_col, edge_attr, create_using=create_using, **kwargs)\n",
    "    G_pyg = from_networkx(G_nx)\n",
    "\n",
    "    num_nodes = G_pyg.num_nodes\n",
    "    num_edges = G_pyg.num_edges\n",
    "\n",
    "    G_pyg.x = th.ones(num_nodes, len(df['h'].iloc[0])) \n",
    "\n",
    "    edge_attr_list = []\n",
    "    edge_label_list = []\n",
    "\n",
    "    for u, v, key, data in G_nx.edges(keys=True, data=True):\n",
    "        edge_attr_list.append(data['h']) \n",
    "        edge_label_list.append(data[label_col]) \n",
    "\n",
    "    G_pyg.edge_attr = th.tensor(edge_attr_list, dtype=th.float32)\n",
    "    G_pyg.edge_label = th.tensor(edge_label_list, dtype=th.long)\n",
    "\n",
    "    print(\"Number of edges in G_pyg:\", num_edges)\n",
    "    print(\"Number of node in G_pyg:\", num_nodes)\n",
    "    print(\"Shape of node in G_pyg:\", G_pyg.x.shape)\n",
    "    print(\"Shape of edge attr in G_pyg:\", G_pyg.edge_attr.shape)\n",
    "    print(\"Shape of edge label in G_pyg:\", G_pyg.edge_label.shape)\n",
    "\n",
    "    return G_nx, G_pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8f9cdb-1316-461a-a927-8d67d90d6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 9773\n",
      "Shape of node in G_pyg: torch.Size([9773, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41795339-6036-468f-9b9d-2bb68d78ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGELayerPyG(MessagePassing):\n",
    "    def __init__(self, in_channels, edge_dim, out_channels, activation=F.relu):\n",
    "        super().__init__(aggr='mean')  # mean aggregation\n",
    "        self.W_msg = nn.Linear(in_channels + edge_dim, out_channels)\n",
    "        self.W_apply = nn.Linear(in_channels + out_channels, out_channels)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: [num_nodes, in_channels]\n",
    "        # edge_attr: [num_edges, edge_dim]\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j: features of source nodes (neighbours)\n",
    "        msg_input = th.cat([x_j, edge_attr], dim=1)\n",
    "        return self.W_msg(msg_input)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out: [num_nodes, out_channels]\n",
    "        combined = th.cat([x, aggr_out], dim=1)\n",
    "        out = self.W_apply(combined)\n",
    "        return self.activation(out)\n",
    "    \n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.lin = nn.Linear(in_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, data, z):\n",
    "        row, col = data.edge_index\n",
    "        # Concatenate the features of source and target nodes for each edge\n",
    "        edge_feat = th.cat([z[row], z[col]], dim=1)\n",
    "        return self.lin(edge_feat)\n",
    "\n",
    "class EGraphSAGE(nn.Module):\n",
    "    def __init__(self, node_in_channels, edge_in_channels, hidden_channels, out_channels, dropout=0.2):\n",
    "        super(EGraphSAGE, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGELayerPyG(node_in_channels, edge_in_channels, hidden_channels)\n",
    "        self.conv2 = SAGELayerPyG(hidden_channels, edge_in_channels, hidden_channels)\n",
    "        self.mlp_predictor = MLPPredictor(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return self.mlp_predictor(data, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bca25fef-29d9-40cf-8910-16b24d530693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = th.device(\"cuda:0\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cccdc850-b98d-4836-b82b-67aa4b9e1e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89157faf-e24b-49d6-9c90-6f71dae515b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d37f0-713b-4abc-8d7a-3e768ae9a2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 307790\n",
      "Number of node in G_pyg: 32941\n",
      "Shape of node in G_pyg: torch.Size([32941, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([307790, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([307790])\n",
      "Number of edges in G_pyg: 153895\n",
      "Number of node in G_pyg: 17455\n",
      "Shape of node in G_pyg: torch.Size([17455, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([153895, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([153895])\n",
      "Number of edges in G_pyg: 307790\n",
      "Number of node in G_pyg: 32777\n",
      "Shape of node in G_pyg: torch.Size([32777, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([307790, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([307790])\n",
      "Number of edges in G_pyg: 153895\n",
      "Number of node in G_pyg: 17575\n",
      "Shape of node in G_pyg: torch.Size([17575, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([153895, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([153895])\n",
      "Number of edges in G_pyg: 307790\n",
      "Number of node in G_pyg: 32912\n",
      "Shape of node in G_pyg: torch.Size([32912, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([307790, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([307790])\n",
      "Number of edges in G_pyg: 153895\n",
      "Number of node in G_pyg: 17452\n",
      "Shape of node in G_pyg: torch.Size([17452, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([153895, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([153895])\n",
      "Testing with learning rate: 0.0005, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1044, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5561, Validation Loss: 2.4055, Validation F1: 0.1044\n",
      "Best F1 Score at epoch 1: 0.2897, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.3396, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.4151, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.4463, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.7263, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.7289, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7534, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.7606, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.7720, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.7849, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8016, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8254, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8330, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8502, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8551, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 27: 0.8561, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8658, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8693, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 39: 0.8716, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 40: 0.8748, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 51: 0.8774, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 56: 0.8780, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 57: 0.8806, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 70: 0.8811, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 71: 0.8831, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 96: 0.8848, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0162, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4383, Validation Loss: 2.3188, Validation F1: 0.0162\n",
      "Best F1 Score at epoch 1: 0.3357, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.4868, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.5688, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.6237, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.6521, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.6741, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.6917, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7003, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.7216, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.7478, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.7654, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.7749, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.7821, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.7834, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.7952, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8052, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8096, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8153, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8300, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8471, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 32: 0.8626, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8629, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 36: 0.8676, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 40: 0.8708, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 42: 0.8764, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 46: 0.8782, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 48: 0.8793, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 59: 0.8841, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 82: 0.8843, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 91: 0.8846, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.0060, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.6431, Validation Loss: 2.4350, Validation F1: 0.0060\n",
      "Best F1 Score at epoch 1: 0.1209, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.3474, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.5061, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.5906, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.6523, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.6854, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.7217, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7390, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.7573, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.7656, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.7755, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.7856, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.7945, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8014, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8032, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8166, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8400, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 23: 0.8444, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8542, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8580, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8674, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 30: 0.8736, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 39: 0.8762, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 41: 0.8776, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 43: 0.8793, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 46: 0.8795, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 56: 0.8801, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 58: 0.8814, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 59: 0.8825, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Best F1 Score at epoch 80: 0.8855, Parameters: lr=0.0005, hidden_dim128, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.0005, 128): {'folds': [0.8848306437670744, 0.8846195701333323, 0.8855187321443875], 'avg_f1': 0.8849896486815981}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.0005, hidden_dim 128: 0.8850\n",
      "Testing with learning rate: 0.0005, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3232, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.3142, Validation Loss: 2.1914, Validation F1: 0.3232\n",
      "Best F1 Score at epoch 1: 0.6036, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.6623, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7319, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7454, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.7471, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.7599, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.7645, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.7815, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.7859, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.7890, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8063, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8098, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8115, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8245, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8363, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8451, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8518, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8539, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8615, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 29: 0.8651, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.8707, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8755, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 41: 0.8761, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 45: 0.8762, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 47: 0.8798, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 58: 0.8821, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 70: 0.8841, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2871, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4983, Validation Loss: 2.3746, Validation F1: 0.2871\n",
      "Best F1 Score at epoch 1: 0.4804, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.6088, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.6830, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.6965, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7286, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7336, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.7448, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.7589, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.7682, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.7701, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.7886, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8086, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8097, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8117, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8151, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8254, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8281, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 33: 0.8450, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 35: 0.8545, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8563, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 37: 0.8572, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 38: 0.8619, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 41: 0.8654, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 42: 0.8665, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8683, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 45: 0.8701, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 49: 0.8702, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 50: 0.8715, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 52: 0.8726, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 55: 0.8758, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 64: 0.8771, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 66: 0.8778, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 68: 0.8791, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 76: 0.8804, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 78: 0.8831, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 94: 0.8838, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2251, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.6663, Validation Loss: 2.4768, Validation F1: 0.2251\n",
      "Best F1 Score at epoch 1: 0.2734, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.3040, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.5718, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.6603, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.6686, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.7253, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.7308, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.7383, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.7434, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.7627, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.7778, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.7827, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.7852, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8084, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8103, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8240, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 29: 0.8396, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 30: 0.8447, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.8487, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8495, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 34: 0.8561, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 38: 0.8678, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 39: 0.8704, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8708, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 46: 0.8715, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 47: 0.8716, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 50: 0.8731, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 51: 0.8752, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 53: 0.8761, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 55: 0.8775, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 60: 0.8801, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 79: 0.8803, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 84: 0.8806, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 85: 0.8825, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Best F1 Score at epoch 99: 0.8838, Parameters: lr=0.0005, hidden_dim128, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.0005, 128): {'folds': [0.8848306437670744, 0.8846195701333323, 0.8855187321443875], 'avg_f1': 0.8849896486815981}, (0.3, 0.0005, 128): {'folds': [0.8840680382989046, 0.8838043291890669, 0.883767964926719], 'avg_f1': 0.8838801108048968}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.0005, hidden_dim 128: 0.8839\n",
      "Testing with learning rate: 0.0005, hidden_dim: 128\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2272, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4900, Validation Loss: 2.3589, Validation F1: 0.2272\n",
      "Best F1 Score at epoch 2: 0.2904, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.5719, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.6607, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7472, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7616, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.7828, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7974, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8050, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8053, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8088, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8236, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8292, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8316, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8363, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8450, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8543, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 32: 0.8579, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8618, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 37: 0.8647, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 39: 0.8680, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 40: 0.8704, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 47: 0.8714, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 52: 0.8736, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 54: 0.8749, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 59: 0.8773, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 65: 0.8782, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 75: 0.8791, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 87: 0.8815, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 88: 0.8825, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 91: 0.8826, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2194, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.6668, Validation Loss: 2.4358, Validation F1: 0.2194\n",
      "Best F1 Score at epoch 1: 0.2202, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.5357, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.6062, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.6194, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.6720, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.6920, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.7111, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7487, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.7518, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.7627, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.7665, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.7994, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8010, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8096, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.8107, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8232, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8368, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.8463, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.8482, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8579, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 32: 0.8581, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8632, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 39: 0.8672, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 42: 0.8683, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 44: 0.8748, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 63: 0.8755, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 64: 0.8788, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 80: 0.8804, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 91: 0.8838, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.2307, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.6742, Validation Loss: 2.5218, Validation F1: 0.2307\n",
      "Best F1 Score at epoch 1: 0.3419, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.4231, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.4333, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.4863, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.5803, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.6670, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.6786, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.7188, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7205, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.7389, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.7510, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.7557, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.7705, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.7706, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 23: 0.7767, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.7929, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8037, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8082, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8192, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.8210, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 30: 0.8340, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 31: 0.8623, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 43: 0.8660, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 46: 0.8710, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 59: 0.8717, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 61: 0.8722, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 65: 0.8730, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 66: 0.8750, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 69: 0.8756, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 73: 0.8766, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 75: 0.8767, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 77: 0.8780, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Best F1 Score at epoch 86: 0.8803, Parameters: lr=0.0005, hidden_dim128, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.0005, 128): {'folds': [0.8848306437670744, 0.8846195701333323, 0.8855187321443875], 'avg_f1': 0.8849896486815981}, (0.3, 0.0005, 128): {'folds': [0.8840680382989046, 0.8838043291890669, 0.883767964926719], 'avg_f1': 0.8838801108048968}, (0.4, 0.0005, 128): {'folds': [0.8825877049288328, 0.8837779258594431, 0.8802541329714935], 'avg_f1': 0.8822065879199231}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.0005, hidden_dim 128: 0.8822\n",
      "Testing with learning rate: 0.0005, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6794, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.3569, Validation Loss: 2.1778, Validation F1: 0.6794\n",
      "Best F1 Score at epoch 1: 0.6976, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7229, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7363, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.7479, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.7801, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7893, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8111, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8301, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8365, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8639, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8653, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8693, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8702, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8738, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 22: 0.8765, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8769, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 27: 0.8786, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 34: 0.8806, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8809, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 40: 0.8824, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 54: 0.8829, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 55: 0.8860, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 76: 0.8861, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 79: 0.8875, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 81: 0.8877, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 87: 0.8882, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 95: 0.8889, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.1624, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.4522, Validation Loss: 2.2728, Validation F1: 0.1624\n",
      "Best F1 Score at epoch 1: 0.3470, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.6885, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7194, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.7628, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.7813, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.7813, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.7834, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.7877, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8181, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8248, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8450, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8546, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 15: 0.8581, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8649, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8679, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8700, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8706, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8719, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 25: 0.8745, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8771, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8779, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 37: 0.8803, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 40: 0.8820, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 49: 0.8850, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 54: 0.8857, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 63: 0.8882, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 68: 0.8887, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5195, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.5420, Validation Loss: 2.1948, Validation F1: 0.5195\n",
      "Best F1 Score at epoch 1: 0.6785, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.6800, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7483, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.7709, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8078, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8080, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8117, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8193, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8250, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8500, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8510, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8631, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8742, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 21: 0.8746, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8751, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 28: 0.8767, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 31: 0.8790, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 34: 0.8818, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 56: 0.8824, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 57: 0.8838, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 78: 0.8854, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 87: 0.8855, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 95: 0.8856, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Best F1 Score at epoch 96: 0.8869, Parameters: lr=0.0005, hidden_dim256, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.0005, 128): {'folds': [0.8848306437670744, 0.8846195701333323, 0.8855187321443875], 'avg_f1': 0.8849896486815981}, (0.3, 0.0005, 128): {'folds': [0.8840680382989046, 0.8838043291890669, 0.883767964926719], 'avg_f1': 0.8838801108048968}, (0.4, 0.0005, 128): {'folds': [0.8825877049288328, 0.8837779258594431, 0.8802541329714935], 'avg_f1': 0.8822065879199231}, (0.2, 0.0005, 256): {'folds': [0.8888746848400695, 0.8886817963664612, 0.8868714539367958], 'avg_f1': 0.8881426450477754}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.0005, hidden_dim 256: 0.8881\n",
      "Testing with learning rate: 0.0005, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3221, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4182, Validation Loss: 2.2134, Validation F1: 0.3221\n",
      "Best F1 Score at epoch 1: 0.5147, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7371, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7881, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7906, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8051, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8259, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8490, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8494, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8598, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8682, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8712, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8718, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8757, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8786, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 39: 0.8809, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 43: 0.8809, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 48: 0.8825, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 59: 0.8835, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 63: 0.8840, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 64: 0.8851, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 75: 0.8865, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 84: 0.8873, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 93: 0.8874, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 95: 0.8878, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 96: 0.8879, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5906, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4275, Validation Loss: 2.2088, Validation F1: 0.5906\n",
      "Best F1 Score at epoch 1: 0.6880, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7352, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7557, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.7640, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7821, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.7995, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8227, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8345, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8405, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8541, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 18: 0.8669, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8735, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8742, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 29: 0.8780, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.8809, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8812, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 38: 0.8830, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 52: 0.8847, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 62: 0.8849, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 68: 0.8862, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 78: 0.8865, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 86: 0.8884, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5603, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.3751, Validation Loss: 2.2002, Validation F1: 0.5603\n",
      "Best F1 Score at epoch 1: 0.7247, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7616, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7790, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.7815, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.7832, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8003, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8183, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.8310, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8498, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8600, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 17: 0.8638, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8673, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8704, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 21: 0.8722, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8731, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8738, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 24: 0.8756, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8772, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8789, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 35: 0.8830, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8838, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 67: 0.8840, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 68: 0.8859, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Best F1 Score at epoch 74: 0.8871, Parameters: lr=0.0005, hidden_dim256, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.0005, 128): {'folds': [0.8848306437670744, 0.8846195701333323, 0.8855187321443875], 'avg_f1': 0.8849896486815981}, (0.3, 0.0005, 128): {'folds': [0.8840680382989046, 0.8838043291890669, 0.883767964926719], 'avg_f1': 0.8838801108048968}, (0.4, 0.0005, 128): {'folds': [0.8825877049288328, 0.8837779258594431, 0.8802541329714935], 'avg_f1': 0.8822065879199231}, (0.2, 0.0005, 256): {'folds': [0.8888746848400695, 0.8886817963664612, 0.8868714539367958], 'avg_f1': 0.8881426450477754}, (0.3, 0.0005, 256): {'folds': [0.8878510571095638, 0.8884353175779859, 0.887090307199621], 'avg_f1': 0.8877922272957236}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.0005, hidden_dim 256: 0.8878\n",
      "Testing with learning rate: 0.0005, hidden_dim: 256\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4915, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.3599, Validation Loss: 2.1646, Validation F1: 0.4915\n",
      "Best F1 Score at epoch 1: 0.6422, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.6773, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7560, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7777, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.7798, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.7972, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8093, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8336, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8391, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8589, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8624, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8658, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.8674, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 22: 0.8674, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8718, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8746, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8781, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 38: 0.8795, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 40: 0.8814, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 51: 0.8846, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 52: 0.8848, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 59: 0.8849, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 72: 0.8864, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 73: 0.8864, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 74: 0.8864, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 95: 0.8870, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 97: 0.8891, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.4535, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.3686, Validation Loss: 2.2120, Validation F1: 0.4535\n",
      "Best F1 Score at epoch 1: 0.6405, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.6635, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7259, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7377, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7511, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7516, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.7555, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.7624, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.7818, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.7949, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.7992, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8284, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 15: 0.8333, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8545, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8555, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8591, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8623, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.8693, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 21: 0.8712, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8751, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 29: 0.8769, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 34: 0.8791, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 35: 0.8801, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 36: 0.8827, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 47: 0.8829, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 61: 0.8829, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 62: 0.8839, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 63: 0.8861, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 93: 0.8876, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.3756, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.3225, Validation Loss: 2.1880, Validation F1: 0.3756\n",
      "Best F1 Score at epoch 1: 0.6926, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7188, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7682, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7787, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.7837, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8065, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8264, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8305, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8481, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8481, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8630, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8706, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8732, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 36: 0.8733, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 39: 0.8790, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 43: 0.8802, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 58: 0.8814, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 69: 0.8815, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 78: 0.8833, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 87: 0.8847, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 96: 0.8852, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Best F1 Score at epoch 97: 0.8855, Parameters: lr=0.0005, hidden_dim256, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.0005, 128): {'folds': [0.8848306437670744, 0.8846195701333323, 0.8855187321443875], 'avg_f1': 0.8849896486815981}, (0.3, 0.0005, 128): {'folds': [0.8840680382989046, 0.8838043291890669, 0.883767964926719], 'avg_f1': 0.8838801108048968}, (0.4, 0.0005, 128): {'folds': [0.8825877049288328, 0.8837779258594431, 0.8802541329714935], 'avg_f1': 0.8822065879199231}, (0.2, 0.0005, 256): {'folds': [0.8888746848400695, 0.8886817963664612, 0.8868714539367958], 'avg_f1': 0.8881426450477754}, (0.3, 0.0005, 256): {'folds': [0.8878510571095638, 0.8884353175779859, 0.887090307199621], 'avg_f1': 0.8877922272957236}, (0.4, 0.0005, 256): {'folds': [0.8891367497550894, 0.8875781566251034, 0.8854534887630489], 'avg_f1': 0.8873894650477472}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.0005, hidden_dim 256: 0.8874\n",
      "Testing with learning rate: 0.0005, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6899, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.3691, Validation Loss: 2.1196, Validation F1: 0.6899\n",
      "Best F1 Score at epoch 1: 0.7315, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7555, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.7896, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.8143, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8317, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8446, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8656, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8696, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8702, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 11: 0.8730, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8799, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 17: 0.8835, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 29: 0.8836, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 35: 0.8865, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 41: 0.8869, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 47: 0.8869, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 48: 0.8886, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 60: 0.8894, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 73: 0.8897, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.7379, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.3650, Validation Loss: 2.0622, Validation F1: 0.7379\n",
      "Best F1 Score at epoch 1: 0.7637, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7919, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 3: 0.8007, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 4: 0.8193, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8309, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8410, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8536, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8698, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 10: 0.8705, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 12: 0.8765, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 13: 0.8773, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 16: 0.8780, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 18: 0.8795, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 19: 0.8812, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 24: 0.8820, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 33: 0.8821, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 34: 0.8846, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 37: 0.8847, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 38: 0.8857, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 40: 0.8859, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 47: 0.8870, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 51: 0.8881, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 61: 0.8900, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 70: 0.8901, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 76: 0.8901, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 87: 0.8903, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5004, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Epoch 0, Train Loss: 2.3549, Validation Loss: 2.0859, Validation F1: 0.5004\n",
      "Best F1 Score at epoch 1: 0.7604, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 2: 0.7773, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 5: 0.8001, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 6: 0.8362, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 7: 0.8497, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 8: 0.8644, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 9: 0.8683, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 14: 0.8746, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 20: 0.8822, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 26: 0.8823, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 32: 0.8850, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 51: 0.8866, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 69: 0.8874, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 78: 0.8878, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 85: 0.8882, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 88: 0.8891, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Best F1 Score at epoch 98: 0.8895, Parameters: lr=0.0005, hidden_dim512, drop_out=0.2\n",
      "Current Results:  {(0.2, 0.0005, 128): {'folds': [0.8848306437670744, 0.8846195701333323, 0.8855187321443875], 'avg_f1': 0.8849896486815981}, (0.3, 0.0005, 128): {'folds': [0.8840680382989046, 0.8838043291890669, 0.883767964926719], 'avg_f1': 0.8838801108048968}, (0.4, 0.0005, 128): {'folds': [0.8825877049288328, 0.8837779258594431, 0.8802541329714935], 'avg_f1': 0.8822065879199231}, (0.2, 0.0005, 256): {'folds': [0.8888746848400695, 0.8886817963664612, 0.8868714539367958], 'avg_f1': 0.8881426450477754}, (0.3, 0.0005, 256): {'folds': [0.8878510571095638, 0.8884353175779859, 0.887090307199621], 'avg_f1': 0.8877922272957236}, (0.4, 0.0005, 256): {'folds': [0.8891367497550894, 0.8875781566251034, 0.8854534887630489], 'avg_f1': 0.8873894650477472}, (0.2, 0.0005, 512): {'folds': [0.8896743718636538, 0.8903419710609674, 0.8895407828019174], 'avg_f1': 0.8898523752421795}}\n",
      "Average F1 Score for dropout 0.2 learning rate 0.0005, hidden_dim 512: 0.8899\n",
      "Testing with learning rate: 0.0005, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6345, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4373, Validation Loss: 2.1249, Validation F1: 0.6345\n",
      "Best F1 Score at epoch 1: 0.6863, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7286, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7552, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 4: 0.8080, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8272, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8424, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8549, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 10: 0.8661, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.8675, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8690, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8728, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 15: 0.8743, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8761, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 20: 0.8791, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8794, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 25: 0.8803, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 31: 0.8820, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 32: 0.8845, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 47: 0.8851, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 48: 0.8859, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 54: 0.8878, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 60: 0.8885, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 68: 0.8919, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.7343, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.4013, Validation Loss: 2.1159, Validation F1: 0.7343\n",
      "Best F1 Score at epoch 1: 0.7489, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.8022, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.8233, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 5: 0.8233, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8296, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8467, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8616, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 9: 0.8749, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 16: 0.8768, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 22: 0.8783, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 23: 0.8816, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 27: 0.8824, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 28: 0.8828, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 36: 0.8863, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 52: 0.8879, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 64: 0.8885, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 65: 0.8887, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 72: 0.8899, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 73: 0.8903, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6075, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Epoch 0, Train Loss: 2.3892, Validation Loss: 2.1092, Validation F1: 0.6075\n",
      "Best F1 Score at epoch 1: 0.7291, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 2: 0.7528, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 3: 0.7871, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 6: 0.8271, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 7: 0.8431, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 8: 0.8631, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 11: 0.8675, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 12: 0.8685, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 13: 0.8772, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 14: 0.8781, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 19: 0.8807, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 26: 0.8880, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Best F1 Score at epoch 97: 0.8882, Parameters: lr=0.0005, hidden_dim512, drop_out=0.3\n",
      "Current Results:  {(0.2, 0.0005, 128): {'folds': [0.8848306437670744, 0.8846195701333323, 0.8855187321443875], 'avg_f1': 0.8849896486815981}, (0.3, 0.0005, 128): {'folds': [0.8840680382989046, 0.8838043291890669, 0.883767964926719], 'avg_f1': 0.8838801108048968}, (0.4, 0.0005, 128): {'folds': [0.8825877049288328, 0.8837779258594431, 0.8802541329714935], 'avg_f1': 0.8822065879199231}, (0.2, 0.0005, 256): {'folds': [0.8888746848400695, 0.8886817963664612, 0.8868714539367958], 'avg_f1': 0.8881426450477754}, (0.3, 0.0005, 256): {'folds': [0.8878510571095638, 0.8884353175779859, 0.887090307199621], 'avg_f1': 0.8877922272957236}, (0.4, 0.0005, 256): {'folds': [0.8891367497550894, 0.8875781566251034, 0.8854534887630489], 'avg_f1': 0.8873894650477472}, (0.2, 0.0005, 512): {'folds': [0.8896743718636538, 0.8903419710609674, 0.8895407828019174], 'avg_f1': 0.8898523752421795}, (0.3, 0.0005, 512): {'folds': [0.8919188465406471, 0.8902836950411127, 0.8882183960349204], 'avg_f1': 0.8901403125388935}}\n",
      "Average F1 Score for dropout 0.3 learning rate 0.0005, hidden_dim 512: 0.8901\n",
      "Testing with learning rate: 0.0005, hidden_dim: 512\n",
      "Fold 1\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3217e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6818, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4042, Validation Loss: 2.1390, Validation F1: 0.6818\n",
      "Best F1 Score at epoch 1: 0.7476, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7930, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7965, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8040, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8244, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8477, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8556, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8576, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8687, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 13: 0.8752, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 14: 0.8784, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8814, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8824, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8834, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 49: 0.8867, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 50: 0.8905, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Fold 2\n",
      "Class weights: tensor([2.0303e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1090e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.6935, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4152, Validation Loss: 2.1467, Validation F1: 0.6935\n",
      "Best F1 Score at epoch 1: 0.7333, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7437, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7469, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.7489, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.7727, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 6: 0.8126, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8197, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8444, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8524, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 10: 0.8597, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8687, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 12: 0.8728, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 19: 0.8737, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 20: 0.8773, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 24: 0.8773, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 25: 0.8797, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 26: 0.8816, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 27: 0.8819, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 28: 0.8848, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 38: 0.8857, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 50: 0.8874, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 74: 0.8890, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 91: 0.8902, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 98: 0.8908, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Fold 3\n",
      "Class weights: tensor([2.0289e+01, 2.3317e+01, 3.3214e+00, 1.2199e+00, 2.2401e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8833e+00, 3.5957e+01, 3.1407e+02], device='cuda:0')\n",
      "Best F1 Score at epoch 0: 0.5843, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Epoch 0, Train Loss: 2.4463, Validation Loss: 2.1400, Validation F1: 0.5843\n",
      "Best F1 Score at epoch 1: 0.6990, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 2: 0.7417, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 3: 0.7611, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 4: 0.8039, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 5: 0.8274, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 7: 0.8365, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 8: 0.8458, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 9: 0.8668, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 11: 0.8678, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 16: 0.8721, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 17: 0.8811, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 18: 0.8853, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 54: 0.8856, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 68: 0.8858, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 75: 0.8874, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 76: 0.8884, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 92: 0.8884, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Best F1 Score at epoch 97: 0.8888, Parameters: lr=0.0005, hidden_dim512, drop_out=0.4\n",
      "Current Results:  {(0.2, 0.0005, 128): {'folds': [0.8848306437670744, 0.8846195701333323, 0.8855187321443875], 'avg_f1': 0.8849896486815981}, (0.3, 0.0005, 128): {'folds': [0.8840680382989046, 0.8838043291890669, 0.883767964926719], 'avg_f1': 0.8838801108048968}, (0.4, 0.0005, 128): {'folds': [0.8825877049288328, 0.8837779258594431, 0.8802541329714935], 'avg_f1': 0.8822065879199231}, (0.2, 0.0005, 256): {'folds': [0.8888746848400695, 0.8886817963664612, 0.8868714539367958], 'avg_f1': 0.8881426450477754}, (0.3, 0.0005, 256): {'folds': [0.8878510571095638, 0.8884353175779859, 0.887090307199621], 'avg_f1': 0.8877922272957236}, (0.4, 0.0005, 256): {'folds': [0.8891367497550894, 0.8875781566251034, 0.8854534887630489], 'avg_f1': 0.8873894650477472}, (0.2, 0.0005, 512): {'folds': [0.8896743718636538, 0.8903419710609674, 0.8895407828019174], 'avg_f1': 0.8898523752421795}, (0.3, 0.0005, 512): {'folds': [0.8919188465406471, 0.8902836950411127, 0.8882183960349204], 'avg_f1': 0.8901403125388935}, (0.4, 0.0005, 512): {'folds': [0.8905191604941721, 0.8908145373222903, 0.8887557378994857], 'avg_f1': 0.890029811905316}}\n",
      "Average F1 Score for dropout 0.4 learning rate 0.0005, hidden_dim 512: 0.8900\n",
      "Best Parameters: {'learning_rate': 0.0005, 'hidden_dim': 512, 'drop_out': 0.3}, Best F1 Score: 0.8901\n",
      "All results: {(0.2, 0.0005, 128): {'folds': [0.8848306437670744, 0.8846195701333323, 0.8855187321443875], 'avg_f1': 0.8849896486815981}, (0.3, 0.0005, 128): {'folds': [0.8840680382989046, 0.8838043291890669, 0.883767964926719], 'avg_f1': 0.8838801108048968}, (0.4, 0.0005, 128): {'folds': [0.8825877049288328, 0.8837779258594431, 0.8802541329714935], 'avg_f1': 0.8822065879199231}, (0.2, 0.0005, 256): {'folds': [0.8888746848400695, 0.8886817963664612, 0.8868714539367958], 'avg_f1': 0.8881426450477754}, (0.3, 0.0005, 256): {'folds': [0.8878510571095638, 0.8884353175779859, 0.887090307199621], 'avg_f1': 0.8877922272957236}, (0.4, 0.0005, 256): {'folds': [0.8891367497550894, 0.8875781566251034, 0.8854534887630489], 'avg_f1': 0.8873894650477472}, (0.2, 0.0005, 512): {'folds': [0.8896743718636538, 0.8903419710609674, 0.8895407828019174], 'avg_f1': 0.8898523752421795}, (0.3, 0.0005, 512): {'folds': [0.8919188465406471, 0.8902836950411127, 0.8882183960349204], 'avg_f1': 0.8901403125388935}, (0.4, 0.0005, 512): {'folds': [0.8905191604941721, 0.8908145373222903, 0.8887557378994857], 'avg_f1': 0.890029811905316}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def grid_search(data, epochs, learning_rates, hidden_dims, drop_outs):\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_params = {}\n",
    "    best_f1 = 0\n",
    "\n",
    "    # Precompute the train and validation graphs for all folds\n",
    "    folds = []\n",
    "    for train_idx, val_idx in skf.split(data, data[label_col]):\n",
    "        train_df = data.iloc[train_idx]\n",
    "        val_df = data.iloc[val_idx]\n",
    "\n",
    "        G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "        G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "\n",
    "        G_pyg_train = G_pyg_train.to(device)\n",
    "        G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "        G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "        G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "        G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "        G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "        folds.append((G_pyg_train, G_pyg_val))\n",
    "\n",
    "    params_results = {}\n",
    "    for lr in learning_rates:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for drop_out in drop_outs:\n",
    "                print(f\"Testing with learning rate: {lr}, hidden_dim: {hidden_dim}, drop_out: {drop_out}\")\n",
    "                fold_f1_scores = []\n",
    "\n",
    "                for fold, (G_pyg_train, G_pyg_val) in enumerate(folds):\n",
    "                    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "                    model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                                    edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                                    hidden_channels=hidden_dim,\n",
    "                                    dropout=drop_out,\n",
    "                                    out_channels=num_classes).to(device)\n",
    "\n",
    "                    model.apply(init_weights)\n",
    "\n",
    "                    labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "                    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                                    classes=np.unique(labels),\n",
    "                                                                    y=labels)\n",
    "\n",
    "                    # Normalize to stabilize training\n",
    "                    class_weights = th.FloatTensor(class_weights).to(device)\n",
    "                    print(\"Class weights:\", class_weights)\n",
    "\n",
    "                    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "                    optimizer = th.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    best_epoch_f1 = 0  # Track the best F1 score for this fold\n",
    "\n",
    "                    for epoch in range(epochs):\n",
    "                        train_loss = 0\n",
    "                        val_loss = 0\n",
    "\n",
    "                        try:\n",
    "                            model.train()\n",
    "                            out = model(G_pyg_train)\n",
    "                            \n",
    "                            optimizer.zero_grad()\n",
    "                            loss = criterion(out, G_pyg_train.edge_label)\n",
    "                            train_loss = loss.item()\n",
    "\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                            model.eval()\n",
    "                            with th.no_grad():\n",
    "                                out = model(G_pyg_val)\n",
    "                                loss = criterion(out, G_pyg_val.edge_label)\n",
    "                                val_loss = loss.item()\n",
    "\n",
    "                            val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "\n",
    "                            if val_f1 > best_epoch_f1:\n",
    "                                best_epoch_f1 = val_f1  # Update the best F1 score for this fold\n",
    "                                print(f\"Best F1 Score at epoch {epoch}: {best_epoch_f1:.4f}, Parameters: lr={lr}, hidden_dim{hidden_dim}, drop_out={drop_out}\")\n",
    "\n",
    "                            if epoch % 100 == 0:\n",
    "                                print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred at epoch {epoch}: {str(e)}\")\n",
    "                            break\n",
    "\n",
    "                    fold_f1_scores.append(best_epoch_f1)  # Append the best F1 score for this fold\n",
    "                \n",
    "                avg_f1 = sum(fold_f1_scores) / len(fold_f1_scores)\n",
    "                params_results[(drop_out, lr, hidden_dim)] = {'folds': fold_f1_scores, 'avg_f1': avg_f1}\n",
    "                print(\"Current Results: \", params_results)\n",
    "                print(f\"Average F1 Score for dropout {drop_out}, learning rate {lr}, hidden_dim {hidden_dim}: {avg_f1:.4f}\")\n",
    "\n",
    "                if avg_f1 > best_f1:\n",
    "                    best_f1 = avg_f1\n",
    "                    best_params = {'learning_rate': lr, 'hidden_dim': hidden_dim, 'drop_out': drop_out}\n",
    "\n",
    "        print(f\"Best Parameters: {best_params}, Best F1 Score: {best_f1:.4f}\")\n",
    "        print(\"All results:\", params_results)\n",
    "\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "hidden_dims = [128, 256, 512]\n",
    "drop_outs = [0.2, 0.3, 0.4]\n",
    "\n",
    "grid_search(train_full_df, epochs=100, learning_rates=learning_rates, hidden_dims=hidden_dims, drop_outs=drop_outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b2fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 392432\n",
      "Number of node in G_pyg: 40942\n",
      "Shape of node in G_pyg: torch.Size([40942, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([392432, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([392432])\n",
      "Number of edges in G_pyg: 69253\n",
      "Number of node in G_pyg: 8688\n",
      "Shape of node in G_pyg: torch.Size([8688, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([69253, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([69253])\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "     train_full_df, test_size=0.15, random_state=42, stratify=train_full_df[label_col])\n",
    "\n",
    "G_nx_train, G_pyg_train = create_graph(train_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n",
    "G_nx_val, G_pyg_val = create_graph(val_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([2.0291e+01, 2.3317e+01, 3.3215e+00, 1.2199e+00, 2.2403e+00, 2.5207e-01,\n",
      "        2.4480e-01, 3.8832e+00, 3.5970e+01, 3.1145e+02], device='cuda:0')\n",
      "Resumed training from epoch 3461\n",
      "Epoch 3461, Train Loss: 0.7647, Validation Loss: 1.1410, Validation F1: 0.9047\n",
      "Epoch 3462, Train Loss: 0.7644, Validation Loss: 1.1407, Validation F1: 0.9014\n",
      "Epoch 3463, Train Loss: 0.7641, Validation Loss: 1.1979, Validation F1: 0.8986\n",
      "Epoch 3464, Train Loss: 0.7644, Validation Loss: 1.2094, Validation F1: 0.9038\n",
      "Epoch 3465, Train Loss: 0.7652, Validation Loss: 1.1949, Validation F1: 0.9048\n",
      "Epoch 3466, Train Loss: 0.7641, Validation Loss: 1.1853, Validation F1: 0.9041\n",
      "Epoch 3467, Train Loss: 0.7633, Validation Loss: 1.1875, Validation F1: 0.9022\n",
      "Epoch 3468, Train Loss: 0.7640, Validation Loss: 1.1403, Validation F1: 0.9038\n",
      "Epoch 3469, Train Loss: 0.7647, Validation Loss: 1.1508, Validation F1: 0.9041\n",
      "Epoch 3470, Train Loss: 0.7642, Validation Loss: 1.2001, Validation F1: 0.9072\n",
      "Epoch 3471, Train Loss: 0.7639, Validation Loss: 1.1631, Validation F1: 0.9068\n",
      "Epoch 3472, Train Loss: 0.7640, Validation Loss: 1.1858, Validation F1: 0.9049\n",
      "Epoch 3473, Train Loss: 0.7645, Validation Loss: 1.1920, Validation F1: 0.9020\n",
      "Epoch 3474, Train Loss: 0.7636, Validation Loss: 1.1688, Validation F1: 0.9010\n",
      "Epoch 3475, Train Loss: 0.7636, Validation Loss: 1.1784, Validation F1: 0.9038\n",
      "Epoch 3476, Train Loss: 0.7636, Validation Loss: 1.1754, Validation F1: 0.9028\n",
      "Epoch 3477, Train Loss: 0.7653, Validation Loss: 1.1814, Validation F1: 0.9063\n",
      "Epoch 3478, Train Loss: 0.7644, Validation Loss: 1.1303, Validation F1: 0.9039\n",
      "Epoch 3479, Train Loss: 0.7637, Validation Loss: 1.1844, Validation F1: 0.9023\n",
      "Epoch 3480, Train Loss: 0.7646, Validation Loss: 1.2104, Validation F1: 0.9024\n",
      "Epoch 3481, Train Loss: 0.7632, Validation Loss: 1.1692, Validation F1: 0.9023\n",
      "Epoch 3482, Train Loss: 0.7644, Validation Loss: 1.1879, Validation F1: 0.9052\n",
      "Epoch 3483, Train Loss: 0.7652, Validation Loss: 1.2091, Validation F1: 0.9033\n",
      "Epoch 3484, Train Loss: 0.7643, Validation Loss: 1.2159, Validation F1: 0.9055\n",
      "Epoch 3485, Train Loss: 0.7648, Validation Loss: 1.1679, Validation F1: 0.9013\n",
      "Epoch 3486, Train Loss: 0.7646, Validation Loss: 1.1792, Validation F1: 0.9019\n",
      "Epoch 3487, Train Loss: 0.7659, Validation Loss: 1.1800, Validation F1: 0.8991\n",
      "Epoch 3488, Train Loss: 0.7640, Validation Loss: 1.2385, Validation F1: 0.9033\n",
      "Epoch 3489, Train Loss: 0.7647, Validation Loss: 1.1844, Validation F1: 0.9055\n",
      "Epoch 3490, Train Loss: 0.7643, Validation Loss: 1.1760, Validation F1: 0.9044\n",
      "Epoch 3491, Train Loss: 0.7643, Validation Loss: 1.1813, Validation F1: 0.9038\n",
      "Epoch 3492, Train Loss: 0.7635, Validation Loss: 1.1937, Validation F1: 0.9067\n",
      "Epoch 3493, Train Loss: 0.7630, Validation Loss: 1.1580, Validation F1: 0.9042\n",
      "Epoch 3494, Train Loss: 0.7639, Validation Loss: 1.2070, Validation F1: 0.9038\n",
      "Epoch 3495, Train Loss: 0.7646, Validation Loss: 1.1606, Validation F1: 0.9026\n",
      "Epoch 3496, Train Loss: 0.7635, Validation Loss: 1.2060, Validation F1: 0.9061\n",
      "Epoch 3497, Train Loss: 0.7652, Validation Loss: 1.2015, Validation F1: 0.9047\n",
      "Epoch 3498, Train Loss: 0.7644, Validation Loss: 1.2000, Validation F1: 0.9010\n",
      "Epoch 3499, Train Loss: 0.7651, Validation Loss: 1.2088, Validation F1: 0.9027\n",
      "Epoch 3500, Train Loss: 0.7650, Validation Loss: 1.1920, Validation F1: 0.9067\n",
      "Epoch 3501, Train Loss: 0.7661, Validation Loss: 1.1829, Validation F1: 0.9058\n",
      "Epoch 3502, Train Loss: 0.7660, Validation Loss: 1.1759, Validation F1: 0.9012\n",
      "Epoch 3503, Train Loss: 0.7650, Validation Loss: 1.1732, Validation F1: 0.9015\n",
      "Epoch 3504, Train Loss: 0.7638, Validation Loss: 1.1763, Validation F1: 0.9039\n",
      "Epoch 3505, Train Loss: 0.7647, Validation Loss: 1.1738, Validation F1: 0.9050\n",
      "Epoch 3506, Train Loss: 0.7649, Validation Loss: 1.2305, Validation F1: 0.9050\n",
      "Epoch 3507, Train Loss: 0.7646, Validation Loss: 1.1886, Validation F1: 0.9042\n",
      "Epoch 3508, Train Loss: 0.7642, Validation Loss: 1.2082, Validation F1: 0.9034\n",
      "Epoch 3509, Train Loss: 0.7639, Validation Loss: 1.1753, Validation F1: 0.9057\n",
      "Epoch 3510, Train Loss: 0.7661, Validation Loss: 1.2132, Validation F1: 0.9052\n",
      "Epoch 3511, Train Loss: 0.7641, Validation Loss: 1.2332, Validation F1: 0.9036\n",
      "Epoch 3512, Train Loss: 0.7625, Validation Loss: 1.2093, Validation F1: 0.9020\n",
      "Epoch 3513, Train Loss: 0.7638, Validation Loss: 1.2101, Validation F1: 0.9013\n",
      "Epoch 3514, Train Loss: 0.7642, Validation Loss: 1.1633, Validation F1: 0.9049\n",
      "Epoch 3515, Train Loss: 0.7644, Validation Loss: 1.1987, Validation F1: 0.9044\n",
      "Epoch 3516, Train Loss: 0.7633, Validation Loss: 1.2305, Validation F1: 0.9059\n",
      "Epoch 3517, Train Loss: 0.7645, Validation Loss: 1.1750, Validation F1: 0.9026\n",
      "Epoch 3518, Train Loss: 0.7639, Validation Loss: 1.1546, Validation F1: 0.9030\n",
      "Epoch 3519, Train Loss: 0.7640, Validation Loss: 1.1788, Validation F1: 0.9027\n",
      "Epoch 3520, Train Loss: 0.7636, Validation Loss: 1.1338, Validation F1: 0.9038\n",
      "Epoch 3521, Train Loss: 0.7638, Validation Loss: 1.1498, Validation F1: 0.9026\n",
      "Epoch 3522, Train Loss: 0.7638, Validation Loss: 1.2188, Validation F1: 0.9039\n",
      "Epoch 3523, Train Loss: 0.7633, Validation Loss: 1.1549, Validation F1: 0.9021\n",
      "Epoch 3524, Train Loss: 0.7634, Validation Loss: 1.2311, Validation F1: 0.9013\n",
      "Epoch 3525, Train Loss: 0.7632, Validation Loss: 1.2029, Validation F1: 0.9023\n",
      "Epoch 3526, Train Loss: 0.7631, Validation Loss: 1.1811, Validation F1: 0.9053\n",
      "Epoch 3527, Train Loss: 0.7639, Validation Loss: 1.1707, Validation F1: 0.9066\n",
      "Epoch 3528, Train Loss: 0.7633, Validation Loss: 1.1972, Validation F1: 0.9044\n",
      "Epoch 3529, Train Loss: 0.7629, Validation Loss: 1.1951, Validation F1: 0.8990\n",
      "Epoch 3530, Train Loss: 0.7642, Validation Loss: 1.2151, Validation F1: 0.8946\n",
      "Epoch 3531, Train Loss: 0.7636, Validation Loss: 1.2319, Validation F1: 0.9025\n",
      "Epoch 3532, Train Loss: 0.7638, Validation Loss: 1.2142, Validation F1: 0.9067\n",
      "Epoch 3533, Train Loss: 0.7643, Validation Loss: 1.1623, Validation F1: 0.9067\n",
      "Epoch 3534, Train Loss: 0.7635, Validation Loss: 1.1558, Validation F1: 0.9019\n",
      "Epoch 3535, Train Loss: 0.7634, Validation Loss: 1.1859, Validation F1: 0.9012\n",
      "Epoch 3536, Train Loss: 0.7632, Validation Loss: 1.2045, Validation F1: 0.9031\n",
      "Epoch 3537, Train Loss: 0.7638, Validation Loss: 1.1667, Validation F1: 0.9025\n",
      "Epoch 3538, Train Loss: 0.7632, Validation Loss: 1.1895, Validation F1: 0.9036\n",
      "Epoch 3539, Train Loss: 0.7635, Validation Loss: 1.1900, Validation F1: 0.9058\n",
      "Epoch 3540, Train Loss: 0.7632, Validation Loss: 1.1781, Validation F1: 0.9038\n",
      "Epoch 3541, Train Loss: 0.7623, Validation Loss: 1.1748, Validation F1: 0.9025\n",
      "Epoch 3542, Train Loss: 0.7629, Validation Loss: 1.1979, Validation F1: 0.9001\n",
      "Epoch 3543, Train Loss: 0.7629, Validation Loss: 1.2156, Validation F1: 0.9017\n",
      "Epoch 3544, Train Loss: 0.7620, Validation Loss: 1.2007, Validation F1: 0.9052\n",
      "Epoch 3545, Train Loss: 0.7633, Validation Loss: 1.2168, Validation F1: 0.9054\n",
      "Epoch 3546, Train Loss: 0.7628, Validation Loss: 1.1694, Validation F1: 0.9032\n",
      "Epoch 3547, Train Loss: 0.7626, Validation Loss: 1.1736, Validation F1: 0.9005\n",
      "Epoch 3548, Train Loss: 0.7626, Validation Loss: 1.1817, Validation F1: 0.9031\n",
      "Epoch 3549, Train Loss: 0.7624, Validation Loss: 1.1606, Validation F1: 0.9022\n",
      "Epoch 3550, Train Loss: 0.7623, Validation Loss: 1.2145, Validation F1: 0.9045\n",
      "Epoch 3551, Train Loss: 0.7630, Validation Loss: 1.1710, Validation F1: 0.9039\n",
      "Epoch 3552, Train Loss: 0.7621, Validation Loss: 1.1669, Validation F1: 0.9030\n",
      "Epoch 3553, Train Loss: 0.7636, Validation Loss: 1.2020, Validation F1: 0.9020\n",
      "Epoch 3554, Train Loss: 0.7641, Validation Loss: 1.1576, Validation F1: 0.9030\n",
      "Epoch 3555, Train Loss: 0.7617, Validation Loss: 1.1823, Validation F1: 0.9026\n",
      "Epoch 3556, Train Loss: 0.7630, Validation Loss: 1.1886, Validation F1: 0.9026\n",
      "Epoch 3557, Train Loss: 0.7633, Validation Loss: 1.1865, Validation F1: 0.9025\n",
      "Epoch 3558, Train Loss: 0.7624, Validation Loss: 1.1352, Validation F1: 0.9032\n",
      "Epoch 3559, Train Loss: 0.7633, Validation Loss: 1.1882, Validation F1: 0.9008\n",
      "Epoch 3560, Train Loss: 0.7628, Validation Loss: 1.2120, Validation F1: 0.9005\n",
      "Epoch 3561, Train Loss: 0.7629, Validation Loss: 1.2025, Validation F1: 0.9039\n",
      "Epoch 3562, Train Loss: 0.7623, Validation Loss: 1.1693, Validation F1: 0.9034\n",
      "Epoch 3563, Train Loss: 0.7635, Validation Loss: 1.1865, Validation F1: 0.9046\n",
      "Epoch 3564, Train Loss: 0.7636, Validation Loss: 1.1906, Validation F1: 0.9028\n",
      "Epoch 3565, Train Loss: 0.7625, Validation Loss: 1.1550, Validation F1: 0.9028\n",
      "Epoch 3566, Train Loss: 0.7621, Validation Loss: 1.1903, Validation F1: 0.9033\n",
      "Epoch 3567, Train Loss: 0.7635, Validation Loss: 1.2139, Validation F1: 0.9018\n",
      "Epoch 3568, Train Loss: 0.7624, Validation Loss: 1.2103, Validation F1: 0.9025\n",
      "Epoch 3569, Train Loss: 0.7631, Validation Loss: 1.1782, Validation F1: 0.9046\n",
      "Epoch 3570, Train Loss: 0.7624, Validation Loss: 1.1795, Validation F1: 0.9027\n",
      "Epoch 3571, Train Loss: 0.7627, Validation Loss: 1.1755, Validation F1: 0.9038\n",
      "Epoch 3572, Train Loss: 0.7621, Validation Loss: 1.1992, Validation F1: 0.9032\n",
      "Epoch 3573, Train Loss: 0.7629, Validation Loss: 1.2133, Validation F1: 0.9021\n",
      "Epoch 3574, Train Loss: 0.7626, Validation Loss: 1.2133, Validation F1: 0.9017\n",
      "Epoch 3575, Train Loss: 0.7661, Validation Loss: 1.1947, Validation F1: 0.9042\n",
      "Epoch 3576, Train Loss: 0.7652, Validation Loss: 1.1817, Validation F1: 0.9029\n",
      "Epoch 3577, Train Loss: 0.7657, Validation Loss: 1.1760, Validation F1: 0.9016\n",
      "Epoch 3578, Train Loss: 0.7656, Validation Loss: 1.1862, Validation F1: 0.9032\n",
      "Epoch 3579, Train Loss: 0.7675, Validation Loss: 1.1987, Validation F1: 0.9035\n",
      "Epoch 3580, Train Loss: 0.7685, Validation Loss: 1.1937, Validation F1: 0.9049\n",
      "Epoch 3581, Train Loss: 0.7657, Validation Loss: 1.2253, Validation F1: 0.9039\n",
      "Epoch 3582, Train Loss: 0.7663, Validation Loss: 1.2225, Validation F1: 0.9023\n",
      "Epoch 3583, Train Loss: 0.7664, Validation Loss: 1.2359, Validation F1: 0.9015\n",
      "Epoch 3584, Train Loss: 0.7670, Validation Loss: 1.1690, Validation F1: 0.9039\n",
      "Epoch 3585, Train Loss: 0.7647, Validation Loss: 1.2068, Validation F1: 0.9061\n",
      "Epoch 3586, Train Loss: 0.7666, Validation Loss: 1.1509, Validation F1: 0.9031\n",
      "Epoch 3587, Train Loss: 0.7665, Validation Loss: 1.1709, Validation F1: 0.9048\n",
      "Epoch 3588, Train Loss: 0.7646, Validation Loss: 1.2081, Validation F1: 0.9037\n",
      "Epoch 3589, Train Loss: 0.7670, Validation Loss: 1.1695, Validation F1: 0.9021\n",
      "Epoch 3590, Train Loss: 0.7660, Validation Loss: 1.1635, Validation F1: 0.8983\n",
      "Epoch 3591, Train Loss: 0.7664, Validation Loss: 1.2042, Validation F1: 0.9018\n",
      "Epoch 3592, Train Loss: 0.7662, Validation Loss: 1.1739, Validation F1: 0.9054\n",
      "Epoch 3593, Train Loss: 0.7645, Validation Loss: 1.1882, Validation F1: 0.9071\n",
      "Epoch 3594, Train Loss: 0.7677, Validation Loss: 1.2118, Validation F1: 0.9061\n",
      "Epoch 3595, Train Loss: 0.7644, Validation Loss: 1.1744, Validation F1: 0.9024\n",
      "Epoch 3596, Train Loss: 0.7677, Validation Loss: 1.1853, Validation F1: 0.9002\n",
      "Epoch 3597, Train Loss: 0.7674, Validation Loss: 1.1797, Validation F1: 0.8980\n",
      "Epoch 3598, Train Loss: 0.7677, Validation Loss: 1.1933, Validation F1: 0.9043\n",
      "Epoch 3599, Train Loss: 0.7667, Validation Loss: 1.2170, Validation F1: 0.9068\n",
      "Epoch 3600, Train Loss: 0.7657, Validation Loss: 1.1776, Validation F1: 0.9047\n",
      "Epoch 3601, Train Loss: 0.7666, Validation Loss: 1.1743, Validation F1: 0.9048\n",
      "Epoch 3602, Train Loss: 0.7672, Validation Loss: 1.1592, Validation F1: 0.8965\n",
      "Epoch 3603, Train Loss: 0.7690, Validation Loss: 1.1695, Validation F1: 0.8990\n",
      "Epoch 3604, Train Loss: 0.7683, Validation Loss: 1.2050, Validation F1: 0.9011\n",
      "Epoch 3605, Train Loss: 0.7696, Validation Loss: 1.2016, Validation F1: 0.9070\n",
      "Epoch 3606, Train Loss: 0.7663, Validation Loss: 1.1800, Validation F1: 0.9054\n",
      "Epoch 3607, Train Loss: 0.7687, Validation Loss: 1.2085, Validation F1: 0.9056\n",
      "Epoch 3608, Train Loss: 0.7684, Validation Loss: 1.1673, Validation F1: 0.9022\n",
      "Epoch 3609, Train Loss: 0.7669, Validation Loss: 1.1633, Validation F1: 0.8994\n",
      "Epoch 3610, Train Loss: 0.7671, Validation Loss: 1.1826, Validation F1: 0.9049\n",
      "Epoch 3611, Train Loss: 0.7648, Validation Loss: 1.1978, Validation F1: 0.9041\n",
      "Epoch 3612, Train Loss: 0.7657, Validation Loss: 1.1757, Validation F1: 0.9027\n",
      "Epoch 3613, Train Loss: 0.7674, Validation Loss: 1.1660, Validation F1: 0.9010\n",
      "Epoch 3614, Train Loss: 0.7655, Validation Loss: 1.1812, Validation F1: 0.8989\n",
      "Epoch 3615, Train Loss: 0.7654, Validation Loss: 1.1644, Validation F1: 0.9026\n",
      "Epoch 3616, Train Loss: 0.7649, Validation Loss: 1.1839, Validation F1: 0.9039\n",
      "Epoch 3617, Train Loss: 0.7651, Validation Loss: 1.2086, Validation F1: 0.9044\n",
      "Epoch 3618, Train Loss: 0.7648, Validation Loss: 1.1928, Validation F1: 0.9069\n",
      "Epoch 3619, Train Loss: 0.7657, Validation Loss: 1.1738, Validation F1: 0.9056\n",
      "Epoch 3620, Train Loss: 0.7647, Validation Loss: 1.1788, Validation F1: 0.9014\n",
      "Epoch 3621, Train Loss: 0.7639, Validation Loss: 1.2213, Validation F1: 0.8994\n",
      "Epoch 3622, Train Loss: 0.7647, Validation Loss: 1.1643, Validation F1: 0.9035\n",
      "Epoch 3623, Train Loss: 0.7637, Validation Loss: 1.1560, Validation F1: 0.9030\n",
      "Epoch 3624, Train Loss: 0.7639, Validation Loss: 1.2230, Validation F1: 0.9033\n",
      "Epoch 3625, Train Loss: 0.7647, Validation Loss: 1.2031, Validation F1: 0.9034\n",
      "Epoch 3626, Train Loss: 0.7645, Validation Loss: 1.1948, Validation F1: 0.9039\n",
      "Epoch 3627, Train Loss: 0.7639, Validation Loss: 1.1752, Validation F1: 0.9064\n",
      "Epoch 3628, Train Loss: 0.7649, Validation Loss: 1.1693, Validation F1: 0.9031\n",
      "Epoch 3629, Train Loss: 0.7644, Validation Loss: 1.2052, Validation F1: 0.9014\n",
      "Epoch 3630, Train Loss: 0.7628, Validation Loss: 1.2035, Validation F1: 0.9022\n",
      "Epoch 3631, Train Loss: 0.7631, Validation Loss: 1.1761, Validation F1: 0.9035\n",
      "Epoch 3632, Train Loss: 0.7654, Validation Loss: 1.1541, Validation F1: 0.9048\n",
      "Epoch 3633, Train Loss: 0.7649, Validation Loss: 1.2001, Validation F1: 0.9010\n",
      "Epoch 3634, Train Loss: 0.7648, Validation Loss: 1.1914, Validation F1: 0.9029\n",
      "Epoch 3635, Train Loss: 0.7632, Validation Loss: 1.1828, Validation F1: 0.9020\n",
      "Epoch 3636, Train Loss: 0.7641, Validation Loss: 1.2375, Validation F1: 0.9042\n",
      "Epoch 3637, Train Loss: 0.7640, Validation Loss: 1.2011, Validation F1: 0.9018\n",
      "Epoch 3638, Train Loss: 0.7637, Validation Loss: 1.1633, Validation F1: 0.9041\n",
      "Epoch 3639, Train Loss: 0.7642, Validation Loss: 1.2032, Validation F1: 0.9058\n",
      "Epoch 3640, Train Loss: 0.7640, Validation Loss: 1.1925, Validation F1: 0.9035\n",
      "Epoch 3641, Train Loss: 0.7629, Validation Loss: 1.1164, Validation F1: 0.9037\n",
      "Epoch 3642, Train Loss: 0.7649, Validation Loss: 1.1983, Validation F1: 0.9039\n",
      "Epoch 3643, Train Loss: 0.7639, Validation Loss: 1.1815, Validation F1: 0.9044\n",
      "Epoch 3644, Train Loss: 0.7631, Validation Loss: 1.1793, Validation F1: 0.9011\n",
      "Epoch 3645, Train Loss: 0.7644, Validation Loss: 1.1888, Validation F1: 0.9030\n",
      "Epoch 3646, Train Loss: 0.7634, Validation Loss: 1.2240, Validation F1: 0.9018\n",
      "Epoch 3647, Train Loss: 0.7637, Validation Loss: 1.2137, Validation F1: 0.9006\n",
      "Epoch 3648, Train Loss: 0.7634, Validation Loss: 1.2100, Validation F1: 0.9027\n",
      "Epoch 3649, Train Loss: 0.7630, Validation Loss: 1.1665, Validation F1: 0.9058\n",
      "Epoch 3650, Train Loss: 0.7635, Validation Loss: 1.2281, Validation F1: 0.9061\n",
      "Epoch 3651, Train Loss: 0.7628, Validation Loss: 1.1682, Validation F1: 0.9032\n",
      "Epoch 3652, Train Loss: 0.7632, Validation Loss: 1.1402, Validation F1: 0.9026\n",
      "Epoch 3653, Train Loss: 0.7642, Validation Loss: 1.1884, Validation F1: 0.9018\n",
      "Epoch 3654, Train Loss: 0.7648, Validation Loss: 1.1956, Validation F1: 0.8993\n",
      "Epoch 3655, Train Loss: 0.7639, Validation Loss: 1.2083, Validation F1: 0.9061\n",
      "Epoch 3656, Train Loss: 0.7629, Validation Loss: 1.2243, Validation F1: 0.9064\n",
      "Epoch 3657, Train Loss: 0.7645, Validation Loss: 1.2200, Validation F1: 0.9052\n",
      "Epoch 3658, Train Loss: 0.7642, Validation Loss: 1.2104, Validation F1: 0.9066\n",
      "Epoch 3659, Train Loss: 0.7663, Validation Loss: 1.2021, Validation F1: 0.9025\n",
      "Epoch 3660, Train Loss: 0.7642, Validation Loss: 1.1924, Validation F1: 0.9021\n",
      "Epoch 3661, Train Loss: 0.7656, Validation Loss: 1.1643, Validation F1: 0.9006\n",
      "Epoch 3662, Train Loss: 0.7664, Validation Loss: 1.2114, Validation F1: 0.9053\n",
      "Epoch 3663, Train Loss: 0.7640, Validation Loss: 1.2017, Validation F1: 0.9049\n",
      "Epoch 3664, Train Loss: 0.7643, Validation Loss: 1.1973, Validation F1: 0.9027\n",
      "Epoch 3665, Train Loss: 0.7691, Validation Loss: 1.1654, Validation F1: 0.9028\n",
      "Epoch 3666, Train Loss: 0.7633, Validation Loss: 1.2064, Validation F1: 0.9039\n",
      "Epoch 3667 Saved best model. Best F1: 0.9081229700418486\n",
      "Epoch 3667, Train Loss: 0.7644, Validation Loss: 1.2103, Validation F1: 0.9081\n",
      "Epoch 3668, Train Loss: 0.7651, Validation Loss: 1.1864, Validation F1: 0.9049\n",
      "Epoch 3669, Train Loss: 0.7643, Validation Loss: 1.2258, Validation F1: 0.9044\n",
      "Epoch 3670, Train Loss: 0.7647, Validation Loss: 1.1872, Validation F1: 0.9037\n",
      "Epoch 3671, Train Loss: 0.7648, Validation Loss: 1.1864, Validation F1: 0.9018\n",
      "Epoch 3672, Train Loss: 0.7641, Validation Loss: 1.2267, Validation F1: 0.9017\n",
      "Epoch 3673, Train Loss: 0.7642, Validation Loss: 1.1387, Validation F1: 0.9024\n",
      "Epoch 3674, Train Loss: 0.7663, Validation Loss: 1.1964, Validation F1: 0.9027\n",
      "Epoch 3675, Train Loss: 0.7641, Validation Loss: 1.1810, Validation F1: 0.9031\n",
      "Epoch 3676, Train Loss: 0.7634, Validation Loss: 1.1888, Validation F1: 0.9042\n",
      "Epoch 3677, Train Loss: 0.7658, Validation Loss: 1.2200, Validation F1: 0.9014\n",
      "Epoch 3678, Train Loss: 0.7666, Validation Loss: 1.2153, Validation F1: 0.9013\n",
      "Epoch 3679, Train Loss: 0.7643, Validation Loss: 1.1834, Validation F1: 0.9031\n",
      "Epoch 3680, Train Loss: 0.7647, Validation Loss: 1.2177, Validation F1: 0.9050\n",
      "Epoch 3681, Train Loss: 0.7640, Validation Loss: 1.2042, Validation F1: 0.9004\n",
      "Epoch 3682, Train Loss: 0.7640, Validation Loss: 1.1921, Validation F1: 0.8988\n",
      "Epoch 3683, Train Loss: 0.7639, Validation Loss: 1.1848, Validation F1: 0.9029\n",
      "Epoch 3684, Train Loss: 0.7644, Validation Loss: 1.1742, Validation F1: 0.9026\n",
      "Epoch 3685, Train Loss: 0.7650, Validation Loss: 1.1890, Validation F1: 0.9061\n",
      "Epoch 3686, Train Loss: 0.7630, Validation Loss: 1.1897, Validation F1: 0.9037\n",
      "Epoch 3687, Train Loss: 0.7663, Validation Loss: 1.2108, Validation F1: 0.9007\n",
      "Epoch 3688, Train Loss: 0.7657, Validation Loss: 1.1982, Validation F1: 0.9016\n",
      "Epoch 3689, Train Loss: 0.7658, Validation Loss: 1.1843, Validation F1: 0.9029\n",
      "Epoch 3690, Train Loss: 0.7667, Validation Loss: 1.2182, Validation F1: 0.9035\n",
      "Epoch 3691, Train Loss: 0.7652, Validation Loss: 1.1869, Validation F1: 0.9038\n",
      "Epoch 3692, Train Loss: 0.7663, Validation Loss: 1.2258, Validation F1: 0.9051\n",
      "Epoch 3693, Train Loss: 0.7646, Validation Loss: 1.2106, Validation F1: 0.9075\n",
      "Epoch 3694, Train Loss: 0.7665, Validation Loss: 1.1829, Validation F1: 0.9039\n",
      "Epoch 3695, Train Loss: 0.7639, Validation Loss: 1.2228, Validation F1: 0.9021\n",
      "Epoch 3696, Train Loss: 0.7654, Validation Loss: 1.2040, Validation F1: 0.9024\n",
      "Epoch 3697, Train Loss: 0.7645, Validation Loss: 1.1628, Validation F1: 0.9011\n",
      "Epoch 3698, Train Loss: 0.7644, Validation Loss: 1.1636, Validation F1: 0.9026\n",
      "Epoch 3699, Train Loss: 0.7651, Validation Loss: 1.2015, Validation F1: 0.9065\n",
      "Epoch 3700, Train Loss: 0.7652, Validation Loss: 1.2156, Validation F1: 0.9056\n",
      "Epoch 3701, Train Loss: 0.7640, Validation Loss: 1.2279, Validation F1: 0.9026\n",
      "Epoch 3702, Train Loss: 0.7643, Validation Loss: 1.1684, Validation F1: 0.9022\n",
      "Epoch 3703, Train Loss: 0.7636, Validation Loss: 1.2498, Validation F1: 0.9029\n",
      "Epoch 3704, Train Loss: 0.7633, Validation Loss: 1.2389, Validation F1: 0.9037\n",
      "Epoch 3705, Train Loss: 0.7626, Validation Loss: 1.2096, Validation F1: 0.9025\n",
      "Epoch 3706, Train Loss: 0.7629, Validation Loss: 1.2098, Validation F1: 0.9063\n",
      "Epoch 3707, Train Loss: 0.7631, Validation Loss: 1.2025, Validation F1: 0.9051\n",
      "Epoch 3708, Train Loss: 0.7637, Validation Loss: 1.2032, Validation F1: 0.9017\n",
      "Epoch 3709, Train Loss: 0.7619, Validation Loss: 1.2116, Validation F1: 0.9024\n",
      "Epoch 3710, Train Loss: 0.7626, Validation Loss: 1.1974, Validation F1: 0.9012\n",
      "Epoch 3711, Train Loss: 0.7617, Validation Loss: 1.2316, Validation F1: 0.9040\n",
      "Epoch 3712, Train Loss: 0.7625, Validation Loss: 1.2083, Validation F1: 0.9007\n",
      "Epoch 3713, Train Loss: 0.7626, Validation Loss: 1.2330, Validation F1: 0.9018\n",
      "Epoch 3714, Train Loss: 0.7625, Validation Loss: 1.2002, Validation F1: 0.9041\n",
      "Epoch 3715, Train Loss: 0.7620, Validation Loss: 1.1777, Validation F1: 0.9054\n",
      "Epoch 3716, Train Loss: 0.7624, Validation Loss: 1.1619, Validation F1: 0.9023\n",
      "Epoch 3717, Train Loss: 0.7619, Validation Loss: 1.2283, Validation F1: 0.9018\n",
      "Epoch 3718, Train Loss: 0.7631, Validation Loss: 1.2098, Validation F1: 0.9031\n",
      "Epoch 3719, Train Loss: 0.7618, Validation Loss: 1.1698, Validation F1: 0.9027\n",
      "Epoch 3720, Train Loss: 0.7628, Validation Loss: 1.2042, Validation F1: 0.9066\n",
      "Epoch 3721, Train Loss: 0.7621, Validation Loss: 1.2169, Validation F1: 0.9051\n",
      "Epoch 3722, Train Loss: 0.7619, Validation Loss: 1.2010, Validation F1: 0.9010\n",
      "Epoch 3723, Train Loss: 0.7619, Validation Loss: 1.1975, Validation F1: 0.9024\n",
      "Epoch 3724, Train Loss: 0.7624, Validation Loss: 1.1898, Validation F1: 0.9034\n",
      "Epoch 3725, Train Loss: 0.7616, Validation Loss: 1.2046, Validation F1: 0.9037\n",
      "Epoch 3726, Train Loss: 0.7628, Validation Loss: 1.1916, Validation F1: 0.9031\n",
      "Epoch 3727, Train Loss: 0.7620, Validation Loss: 1.2159, Validation F1: 0.9026\n",
      "Epoch 3728, Train Loss: 0.7618, Validation Loss: 1.1614, Validation F1: 0.9023\n",
      "Epoch 3729, Train Loss: 0.7610, Validation Loss: 1.2268, Validation F1: 0.9063\n",
      "Epoch 3730, Train Loss: 0.7616, Validation Loss: 1.1823, Validation F1: 0.9039\n",
      "Epoch 3731, Train Loss: 0.7627, Validation Loss: 1.1781, Validation F1: 0.9067\n",
      "Epoch 3732, Train Loss: 0.7615, Validation Loss: 1.1945, Validation F1: 0.9036\n",
      "Epoch 3733, Train Loss: 0.7618, Validation Loss: 1.1714, Validation F1: 0.9028\n",
      "Epoch 3734, Train Loss: 0.7618, Validation Loss: 1.2146, Validation F1: 0.9012\n",
      "Epoch 3735, Train Loss: 0.7638, Validation Loss: 1.2201, Validation F1: 0.9027\n",
      "Epoch 3736, Train Loss: 0.7624, Validation Loss: 1.2071, Validation F1: 0.9026\n",
      "Epoch 3737, Train Loss: 0.7617, Validation Loss: 1.2102, Validation F1: 0.9035\n",
      "Epoch 3738, Train Loss: 0.7615, Validation Loss: 1.2007, Validation F1: 0.9068\n",
      "Epoch 3739, Train Loss: 0.7618, Validation Loss: 1.2232, Validation F1: 0.9033\n",
      "Epoch 3740, Train Loss: 0.7626, Validation Loss: 1.2257, Validation F1: 0.9061\n",
      "Epoch 3741, Train Loss: 0.7625, Validation Loss: 1.1786, Validation F1: 0.9022\n",
      "Epoch 3742, Train Loss: 0.7622, Validation Loss: 1.2201, Validation F1: 0.9025\n",
      "Epoch 3743, Train Loss: 0.7630, Validation Loss: 1.2026, Validation F1: 0.8986\n",
      "Epoch 3744, Train Loss: 0.7638, Validation Loss: 1.1963, Validation F1: 0.9039\n",
      "Epoch 3745, Train Loss: 0.7632, Validation Loss: 1.1882, Validation F1: 0.9069\n",
      "Epoch 3746, Train Loss: 0.7632, Validation Loss: 1.2154, Validation F1: 0.9013\n",
      "Epoch 3747, Train Loss: 0.7637, Validation Loss: 1.1817, Validation F1: 0.9017\n",
      "Epoch 3748, Train Loss: 0.7633, Validation Loss: 1.1999, Validation F1: 0.9028\n",
      "Epoch 3749, Train Loss: 0.7625, Validation Loss: 1.2260, Validation F1: 0.9046\n",
      "Epoch 3750, Train Loss: 0.7625, Validation Loss: 1.2261, Validation F1: 0.9059\n",
      "Epoch 3751, Train Loss: 0.7626, Validation Loss: 1.2286, Validation F1: 0.9008\n",
      "Epoch 3752, Train Loss: 0.7619, Validation Loss: 1.2326, Validation F1: 0.8991\n",
      "Epoch 3753, Train Loss: 0.7627, Validation Loss: 1.1769, Validation F1: 0.9018\n",
      "Epoch 3754, Train Loss: 0.7634, Validation Loss: 1.2040, Validation F1: 0.9028\n",
      "Epoch 3755, Train Loss: 0.7620, Validation Loss: 1.1698, Validation F1: 0.9029\n",
      "Epoch 3756, Train Loss: 0.7620, Validation Loss: 1.2267, Validation F1: 0.9038\n",
      "Epoch 3757, Train Loss: 0.7633, Validation Loss: 1.1864, Validation F1: 0.9015\n",
      "Epoch 3758, Train Loss: 0.7619, Validation Loss: 1.2474, Validation F1: 0.9046\n",
      "Epoch 3759, Train Loss: 0.7622, Validation Loss: 1.2150, Validation F1: 0.9038\n",
      "Epoch 3760, Train Loss: 0.7614, Validation Loss: 1.2341, Validation F1: 0.9042\n",
      "Epoch 3761, Train Loss: 0.7637, Validation Loss: 1.2268, Validation F1: 0.9041\n",
      "Epoch 3762, Train Loss: 0.7618, Validation Loss: 1.2238, Validation F1: 0.9031\n",
      "Epoch 3763, Train Loss: 0.7611, Validation Loss: 1.1921, Validation F1: 0.8994\n",
      "Epoch 3764, Train Loss: 0.7625, Validation Loss: 1.2000, Validation F1: 0.9011\n",
      "Epoch 3765, Train Loss: 0.7615, Validation Loss: 1.2227, Validation F1: 0.9059\n",
      "Epoch 3766, Train Loss: 0.7632, Validation Loss: 1.1957, Validation F1: 0.9052\n",
      "Epoch 3767, Train Loss: 0.7619, Validation Loss: 1.1851, Validation F1: 0.9038\n",
      "Epoch 3768, Train Loss: 0.7618, Validation Loss: 1.2201, Validation F1: 0.9015\n",
      "Epoch 3769, Train Loss: 0.7620, Validation Loss: 1.1675, Validation F1: 0.9030\n",
      "Epoch 3770, Train Loss: 0.7617, Validation Loss: 1.2560, Validation F1: 0.9021\n",
      "Epoch 3771, Train Loss: 0.7636, Validation Loss: 1.2215, Validation F1: 0.9062\n",
      "Epoch 3772, Train Loss: 0.7619, Validation Loss: 1.1437, Validation F1: 0.9067\n",
      "Epoch 3773, Train Loss: 0.7612, Validation Loss: 1.2232, Validation F1: 0.9017\n",
      "Epoch 3774, Train Loss: 0.7628, Validation Loss: 1.2227, Validation F1: 0.9026\n",
      "Epoch 3775, Train Loss: 0.7615, Validation Loss: 1.1990, Validation F1: 0.9035\n",
      "Epoch 3776, Train Loss: 0.7619, Validation Loss: 1.1854, Validation F1: 0.9015\n",
      "Epoch 3777, Train Loss: 0.7616, Validation Loss: 1.1887, Validation F1: 0.9028\n",
      "Epoch 3778, Train Loss: 0.7612, Validation Loss: 1.1916, Validation F1: 0.9060\n",
      "Epoch 3779, Train Loss: 0.7621, Validation Loss: 1.2178, Validation F1: 0.9022\n",
      "Epoch 3780, Train Loss: 0.7612, Validation Loss: 1.2268, Validation F1: 0.9054\n",
      "Epoch 3781, Train Loss: 0.7651, Validation Loss: 1.1920, Validation F1: 0.9041\n",
      "Epoch 3782, Train Loss: 0.7634, Validation Loss: 1.2374, Validation F1: 0.9003\n",
      "Epoch 3783, Train Loss: 0.7676, Validation Loss: 1.1860, Validation F1: 0.9021\n",
      "Epoch 3784, Train Loss: 0.7664, Validation Loss: 1.2052, Validation F1: 0.9060\n",
      "Epoch 3785, Train Loss: 0.7644, Validation Loss: 1.2060, Validation F1: 0.9052\n",
      "Epoch 3786, Train Loss: 0.7658, Validation Loss: 1.2368, Validation F1: 0.9061\n",
      "Epoch 3787, Train Loss: 0.7656, Validation Loss: 1.2259, Validation F1: 0.9016\n",
      "Epoch 3788, Train Loss: 0.7634, Validation Loss: 1.2318, Validation F1: 0.9001\n",
      "Epoch 3789, Train Loss: 0.7654, Validation Loss: 1.2292, Validation F1: 0.8977\n",
      "Epoch 3790, Train Loss: 0.7682, Validation Loss: 1.1620, Validation F1: 0.9025\n",
      "Epoch 3791, Train Loss: 0.7643, Validation Loss: 1.2020, Validation F1: 0.9022\n",
      "Epoch 3792, Train Loss: 0.7664, Validation Loss: 1.1853, Validation F1: 0.9048\n",
      "Epoch 3793, Train Loss: 0.7675, Validation Loss: 1.1773, Validation F1: 0.9041\n",
      "Epoch 3794, Train Loss: 0.7682, Validation Loss: 1.1442, Validation F1: 0.9037\n",
      "Epoch 3795, Train Loss: 0.7648, Validation Loss: 1.1830, Validation F1: 0.9030\n",
      "Epoch 3796, Train Loss: 0.7662, Validation Loss: 1.2123, Validation F1: 0.9032\n",
      "Epoch 3797, Train Loss: 0.7671, Validation Loss: 1.1896, Validation F1: 0.9028\n",
      "Epoch 3798, Train Loss: 0.7649, Validation Loss: 1.2061, Validation F1: 0.9057\n",
      "Epoch 3799, Train Loss: 0.7680, Validation Loss: 1.2189, Validation F1: 0.9043\n",
      "Epoch 3800, Train Loss: 0.7659, Validation Loss: 1.1926, Validation F1: 0.9025\n",
      "Epoch 3801, Train Loss: 0.7646, Validation Loss: 1.1883, Validation F1: 0.9031\n",
      "Epoch 3802, Train Loss: 0.7667, Validation Loss: 1.1731, Validation F1: 0.9025\n",
      "Epoch 3803, Train Loss: 0.7637, Validation Loss: 1.1931, Validation F1: 0.8997\n",
      "Epoch 3804, Train Loss: 0.7635, Validation Loss: 1.1470, Validation F1: 0.9023\n",
      "Epoch 3805, Train Loss: 0.7644, Validation Loss: 1.1999, Validation F1: 0.9037\n",
      "Epoch 3806, Train Loss: 0.7645, Validation Loss: 1.1651, Validation F1: 0.9039\n",
      "Epoch 3807, Train Loss: 0.7630, Validation Loss: 1.1726, Validation F1: 0.9033\n",
      "Epoch 3808, Train Loss: 0.7631, Validation Loss: 1.1826, Validation F1: 0.9045\n",
      "Epoch 3809, Train Loss: 0.7646, Validation Loss: 1.1836, Validation F1: 0.9027\n",
      "Epoch 3810, Train Loss: 0.7629, Validation Loss: 1.2088, Validation F1: 0.9039\n",
      "Epoch 3811, Train Loss: 0.7635, Validation Loss: 1.2097, Validation F1: 0.9040\n",
      "Epoch 3812, Train Loss: 0.7636, Validation Loss: 1.1646, Validation F1: 0.9027\n",
      "Epoch 3813, Train Loss: 0.7638, Validation Loss: 1.1781, Validation F1: 0.9049\n",
      "Epoch 3814, Train Loss: 0.7628, Validation Loss: 1.2325, Validation F1: 0.9028\n",
      "Epoch 3815, Train Loss: 0.7618, Validation Loss: 1.1858, Validation F1: 0.9028\n",
      "Epoch 3816, Train Loss: 0.7632, Validation Loss: 1.2323, Validation F1: 0.9026\n",
      "Epoch 3817, Train Loss: 0.7627, Validation Loss: 1.1872, Validation F1: 0.9040\n",
      "Epoch 3818, Train Loss: 0.7642, Validation Loss: 1.2002, Validation F1: 0.9030\n",
      "Epoch 3819, Train Loss: 0.7636, Validation Loss: 1.1695, Validation F1: 0.9049\n",
      "Epoch 3820, Train Loss: 0.7623, Validation Loss: 1.2583, Validation F1: 0.9019\n",
      "Epoch 3821, Train Loss: 0.7660, Validation Loss: 1.2249, Validation F1: 0.9015\n",
      "Epoch 3822, Train Loss: 0.7634, Validation Loss: 1.2062, Validation F1: 0.9025\n",
      "Epoch 3823, Train Loss: 0.7644, Validation Loss: 1.1694, Validation F1: 0.9022\n",
      "Epoch 3824, Train Loss: 0.7644, Validation Loss: 1.1756, Validation F1: 0.9008\n",
      "Epoch 3825, Train Loss: 0.7645, Validation Loss: 1.1741, Validation F1: 0.9010\n",
      "Epoch 3826, Train Loss: 0.7650, Validation Loss: 1.1752, Validation F1: 0.9025\n",
      "Epoch 3827, Train Loss: 0.7626, Validation Loss: 1.2115, Validation F1: 0.9037\n",
      "Epoch 3828, Train Loss: 0.7633, Validation Loss: 1.2053, Validation F1: 0.9024\n",
      "Epoch 3829, Train Loss: 0.7636, Validation Loss: 1.1694, Validation F1: 0.9018\n",
      "Epoch 3830, Train Loss: 0.7632, Validation Loss: 1.1982, Validation F1: 0.9018\n",
      "Epoch 3831, Train Loss: 0.7638, Validation Loss: 1.2129, Validation F1: 0.9037\n",
      "Epoch 3832, Train Loss: 0.7645, Validation Loss: 1.1888, Validation F1: 0.9043\n",
      "Epoch 3833, Train Loss: 0.7628, Validation Loss: 1.2137, Validation F1: 0.9032\n",
      "Epoch 3834, Train Loss: 0.7642, Validation Loss: 1.2060, Validation F1: 0.8991\n",
      "Epoch 3835, Train Loss: 0.7631, Validation Loss: 1.1774, Validation F1: 0.9041\n",
      "Epoch 3836, Train Loss: 0.7635, Validation Loss: 1.2526, Validation F1: 0.9062\n",
      "Epoch 3837, Train Loss: 0.7622, Validation Loss: 1.1955, Validation F1: 0.9042\n",
      "Epoch 3838, Train Loss: 0.7622, Validation Loss: 1.2181, Validation F1: 0.9020\n",
      "Epoch 3839, Train Loss: 0.7623, Validation Loss: 1.2379, Validation F1: 0.8987\n",
      "Epoch 3840, Train Loss: 0.7618, Validation Loss: 1.2171, Validation F1: 0.8998\n",
      "Epoch 3841, Train Loss: 0.7611, Validation Loss: 1.2252, Validation F1: 0.9045\n",
      "Epoch 3842, Train Loss: 0.7625, Validation Loss: 1.2223, Validation F1: 0.9044\n",
      "Epoch 3843, Train Loss: 0.7629, Validation Loss: 1.1875, Validation F1: 0.9030\n",
      "Epoch 3844, Train Loss: 0.7625, Validation Loss: 1.1648, Validation F1: 0.9039\n",
      "Epoch 3845, Train Loss: 0.7620, Validation Loss: 1.2061, Validation F1: 0.9032\n",
      "Epoch 3846, Train Loss: 0.7631, Validation Loss: 1.2122, Validation F1: 0.9044\n",
      "Epoch 3847, Train Loss: 0.7615, Validation Loss: 1.2084, Validation F1: 0.9031\n",
      "Epoch 3848, Train Loss: 0.7625, Validation Loss: 1.1743, Validation F1: 0.9033\n",
      "Epoch 3849, Train Loss: 0.7625, Validation Loss: 1.2268, Validation F1: 0.9058\n",
      "Epoch 3850, Train Loss: 0.7614, Validation Loss: 1.2020, Validation F1: 0.9072\n",
      "Epoch 3851, Train Loss: 0.7624, Validation Loss: 1.1943, Validation F1: 0.9024\n",
      "Epoch 3852, Train Loss: 0.7616, Validation Loss: 1.2249, Validation F1: 0.9021\n",
      "Epoch 3853, Train Loss: 0.7629, Validation Loss: 1.2489, Validation F1: 0.9027\n",
      "Epoch 3854, Train Loss: 0.7622, Validation Loss: 1.2150, Validation F1: 0.9038\n",
      "Epoch 3855, Train Loss: 0.7614, Validation Loss: 1.2415, Validation F1: 0.9030\n",
      "Epoch 3856, Train Loss: 0.7619, Validation Loss: 1.2351, Validation F1: 0.9024\n",
      "Epoch 3857, Train Loss: 0.7615, Validation Loss: 1.1693, Validation F1: 0.9011\n",
      "Epoch 3858, Train Loss: 0.7624, Validation Loss: 1.2301, Validation F1: 0.9004\n",
      "Epoch 3859, Train Loss: 0.7618, Validation Loss: 1.1855, Validation F1: 0.9044\n",
      "Epoch 3860, Train Loss: 0.7616, Validation Loss: 1.1624, Validation F1: 0.9057\n",
      "Epoch 3861, Train Loss: 0.7620, Validation Loss: 1.1952, Validation F1: 0.9042\n",
      "Epoch 3862, Train Loss: 0.7612, Validation Loss: 1.1655, Validation F1: 0.9032\n",
      "Epoch 3863, Train Loss: 0.7623, Validation Loss: 1.2276, Validation F1: 0.9022\n",
      "Epoch 3864, Train Loss: 0.7619, Validation Loss: 1.2214, Validation F1: 0.9021\n",
      "Epoch 3865, Train Loss: 0.7618, Validation Loss: 1.2508, Validation F1: 0.9052\n",
      "Epoch 3866, Train Loss: 0.7612, Validation Loss: 1.2012, Validation F1: 0.9061\n",
      "Epoch 3867, Train Loss: 0.7620, Validation Loss: 1.2466, Validation F1: 0.9051\n",
      "Epoch 3868, Train Loss: 0.7618, Validation Loss: 1.2083, Validation F1: 0.9003\n",
      "Epoch 3869, Train Loss: 0.7618, Validation Loss: 1.2246, Validation F1: 0.8999\n",
      "Epoch 3870, Train Loss: 0.7621, Validation Loss: 1.1914, Validation F1: 0.9033\n",
      "Epoch 3871, Train Loss: 0.7616, Validation Loss: 1.2086, Validation F1: 0.9048\n",
      "Epoch 3872, Train Loss: 0.7624, Validation Loss: 1.1953, Validation F1: 0.9059\n",
      "Epoch 3873, Train Loss: 0.7613, Validation Loss: 1.2554, Validation F1: 0.9041\n",
      "Epoch 3874, Train Loss: 0.7626, Validation Loss: 1.2017, Validation F1: 0.9032\n",
      "Epoch 3875, Train Loss: 0.7623, Validation Loss: 1.2151, Validation F1: 0.9011\n",
      "Epoch 3876, Train Loss: 0.7620, Validation Loss: 1.1999, Validation F1: 0.9029\n",
      "Epoch 3877, Train Loss: 0.7625, Validation Loss: 1.2033, Validation F1: 0.9030\n",
      "Epoch 3878, Train Loss: 0.7609, Validation Loss: 1.2196, Validation F1: 0.9074\n",
      "Epoch 3879, Train Loss: 0.7625, Validation Loss: 1.1544, Validation F1: 0.9058\n",
      "Epoch 3880, Train Loss: 0.7612, Validation Loss: 1.2138, Validation F1: 0.8992\n",
      "Epoch 3881, Train Loss: 0.7625, Validation Loss: 1.2058, Validation F1: 0.9020\n",
      "Epoch 3882, Train Loss: 0.7613, Validation Loss: 1.1876, Validation F1: 0.9038\n",
      "Epoch 3883, Train Loss: 0.7616, Validation Loss: 1.2050, Validation F1: 0.9047\n",
      "Epoch 3884, Train Loss: 0.7628, Validation Loss: 1.1906, Validation F1: 0.9041\n",
      "Epoch 3885, Train Loss: 0.7628, Validation Loss: 1.2088, Validation F1: 0.9018\n",
      "Epoch 3886, Train Loss: 0.7629, Validation Loss: 1.1957, Validation F1: 0.9023\n",
      "Epoch 3887, Train Loss: 0.7637, Validation Loss: 1.2293, Validation F1: 0.9013\n",
      "Epoch 3888, Train Loss: 0.7645, Validation Loss: 1.2275, Validation F1: 0.9011\n",
      "Epoch 3889, Train Loss: 0.7623, Validation Loss: 1.2172, Validation F1: 0.9020\n",
      "Epoch 3890, Train Loss: 0.7646, Validation Loss: 1.2347, Validation F1: 0.9019\n",
      "Epoch 3891, Train Loss: 0.7644, Validation Loss: 1.1604, Validation F1: 0.9017\n",
      "Epoch 3892, Train Loss: 0.7644, Validation Loss: 1.1850, Validation F1: 0.9032\n",
      "Epoch 3893, Train Loss: 0.7633, Validation Loss: 1.2317, Validation F1: 0.9020\n",
      "Epoch 3894, Train Loss: 0.7629, Validation Loss: 1.1907, Validation F1: 0.9020\n",
      "Epoch 3895, Train Loss: 0.7631, Validation Loss: 1.2215, Validation F1: 0.9041\n",
      "Epoch 3896, Train Loss: 0.7620, Validation Loss: 1.1768, Validation F1: 0.9038\n",
      "Epoch 3897, Train Loss: 0.7656, Validation Loss: 1.1903, Validation F1: 0.9017\n",
      "Epoch 3898, Train Loss: 0.7632, Validation Loss: 1.2006, Validation F1: 0.9024\n",
      "Epoch 3899, Train Loss: 0.7632, Validation Loss: 1.1979, Validation F1: 0.9052\n",
      "Epoch 3900, Train Loss: 0.7630, Validation Loss: 1.2272, Validation F1: 0.9063\n",
      "Epoch 3901, Train Loss: 0.7615, Validation Loss: 1.1765, Validation F1: 0.9050\n",
      "Epoch 3902, Train Loss: 0.7625, Validation Loss: 1.2233, Validation F1: 0.9021\n",
      "Epoch 3903, Train Loss: 0.7618, Validation Loss: 1.2334, Validation F1: 0.9007\n",
      "Epoch 3904, Train Loss: 0.7614, Validation Loss: 1.2627, Validation F1: 0.9008\n",
      "Epoch 3905, Train Loss: 0.7628, Validation Loss: 1.2119, Validation F1: 0.9042\n",
      "Epoch 3906, Train Loss: 0.7629, Validation Loss: 1.2011, Validation F1: 0.9053\n",
      "Epoch 3907, Train Loss: 0.7625, Validation Loss: 1.1720, Validation F1: 0.9063\n",
      "Epoch 3908, Train Loss: 0.7631, Validation Loss: 1.2621, Validation F1: 0.9023\n",
      "Epoch 3909, Train Loss: 0.7616, Validation Loss: 1.1884, Validation F1: 0.9003\n",
      "Epoch 3910, Train Loss: 0.7638, Validation Loss: 1.2641, Validation F1: 0.9055\n",
      "Epoch 3911, Train Loss: 0.7617, Validation Loss: 1.2377, Validation F1: 0.9059\n",
      "Epoch 3912, Train Loss: 0.7638, Validation Loss: 1.2060, Validation F1: 0.9051\n",
      "Epoch 3913, Train Loss: 0.7627, Validation Loss: 1.2164, Validation F1: 0.9026\n",
      "Epoch 3914, Train Loss: 0.7623, Validation Loss: 1.2158, Validation F1: 0.9022\n",
      "Epoch 3915, Train Loss: 0.7631, Validation Loss: 1.2421, Validation F1: 0.9020\n",
      "Epoch 3916, Train Loss: 0.7611, Validation Loss: 1.1888, Validation F1: 0.9035\n",
      "Epoch 3917, Train Loss: 0.7614, Validation Loss: 1.2347, Validation F1: 0.9062\n",
      "Epoch 3918, Train Loss: 0.7619, Validation Loss: 1.2044, Validation F1: 0.9061\n",
      "Epoch 3919, Train Loss: 0.7611, Validation Loss: 1.2175, Validation F1: 0.9003\n",
      "Epoch 3920, Train Loss: 0.7613, Validation Loss: 1.2297, Validation F1: 0.8985\n",
      "Epoch 3921, Train Loss: 0.7611, Validation Loss: 1.2331, Validation F1: 0.9031\n",
      "Epoch 3922, Train Loss: 0.7603, Validation Loss: 1.2205, Validation F1: 0.9060\n",
      "Epoch 3923, Train Loss: 0.7618, Validation Loss: 1.1727, Validation F1: 0.9040\n",
      "Epoch 3924, Train Loss: 0.7608, Validation Loss: 1.2220, Validation F1: 0.8991\n",
      "Epoch 3925, Train Loss: 0.7609, Validation Loss: 1.2262, Validation F1: 0.9024\n",
      "Epoch 3926, Train Loss: 0.7612, Validation Loss: 1.2302, Validation F1: 0.9015\n",
      "Epoch 3927, Train Loss: 0.7608, Validation Loss: 1.1830, Validation F1: 0.9047\n",
      "Epoch 3928, Train Loss: 0.7610, Validation Loss: 1.2146, Validation F1: 0.9031\n",
      "Epoch 3929, Train Loss: 0.7612, Validation Loss: 1.2311, Validation F1: 0.9019\n",
      "Epoch 3930, Train Loss: 0.7617, Validation Loss: 1.1841, Validation F1: 0.9014\n",
      "Epoch 3931, Train Loss: 0.7610, Validation Loss: 1.2484, Validation F1: 0.9035\n",
      "Epoch 3932, Train Loss: 0.7623, Validation Loss: 1.2484, Validation F1: 0.9040\n",
      "Epoch 3933, Train Loss: 0.7612, Validation Loss: 1.2470, Validation F1: 0.9040\n",
      "Epoch 3934, Train Loss: 0.7614, Validation Loss: 1.1834, Validation F1: 0.9045\n",
      "Epoch 3935, Train Loss: 0.7629, Validation Loss: 1.1891, Validation F1: 0.9035\n",
      "Epoch 3936, Train Loss: 0.7611, Validation Loss: 1.2435, Validation F1: 0.9008\n",
      "Epoch 3937, Train Loss: 0.7613, Validation Loss: 1.1830, Validation F1: 0.9040\n",
      "Epoch 3938, Train Loss: 0.7628, Validation Loss: 1.1985, Validation F1: 0.9052\n",
      "Epoch 3939, Train Loss: 0.7612, Validation Loss: 1.2366, Validation F1: 0.9023\n",
      "Epoch 3940, Train Loss: 0.7619, Validation Loss: 1.2361, Validation F1: 0.9019\n",
      "Epoch 3941, Train Loss: 0.7619, Validation Loss: 1.2205, Validation F1: 0.9027\n",
      "Epoch 3942, Train Loss: 0.7621, Validation Loss: 1.1321, Validation F1: 0.9015\n",
      "Epoch 3943, Train Loss: 0.7616, Validation Loss: 1.2095, Validation F1: 0.9035\n",
      "Epoch 3944, Train Loss: 0.7615, Validation Loss: 1.2052, Validation F1: 0.9044\n",
      "Epoch 3945, Train Loss: 0.7623, Validation Loss: 1.2054, Validation F1: 0.9058\n",
      "Epoch 3946, Train Loss: 0.7621, Validation Loss: 1.2123, Validation F1: 0.9025\n",
      "Epoch 3947, Train Loss: 0.7621, Validation Loss: 1.1511, Validation F1: 0.9031\n",
      "Epoch 3948, Train Loss: 0.7616, Validation Loss: 1.1792, Validation F1: 0.9025\n",
      "Epoch 3949, Train Loss: 0.7611, Validation Loss: 1.2065, Validation F1: 0.9022\n",
      "Epoch 3950, Train Loss: 0.7615, Validation Loss: 1.2150, Validation F1: 0.9035\n",
      "Epoch 3951, Train Loss: 0.7612, Validation Loss: 1.2277, Validation F1: 0.9049\n",
      "Epoch 3952, Train Loss: 0.7609, Validation Loss: 1.2150, Validation F1: 0.9019\n",
      "Epoch 3953, Train Loss: 0.7611, Validation Loss: 1.2109, Validation F1: 0.9019\n",
      "Epoch 3954, Train Loss: 0.7612, Validation Loss: 1.2355, Validation F1: 0.9033\n",
      "Epoch 3955, Train Loss: 0.7611, Validation Loss: 1.2295, Validation F1: 0.9049\n",
      "Epoch 3956, Train Loss: 0.7607, Validation Loss: 1.2423, Validation F1: 0.9039\n",
      "Epoch 3957, Train Loss: 0.7601, Validation Loss: 1.2220, Validation F1: 0.9023\n",
      "Epoch 3958, Train Loss: 0.7619, Validation Loss: 1.2381, Validation F1: 0.9019\n",
      "Epoch 3959, Train Loss: 0.7615, Validation Loss: 1.2430, Validation F1: 0.9036\n",
      "Epoch 3960, Train Loss: 0.7616, Validation Loss: 1.2486, Validation F1: 0.9038\n",
      "Epoch 3961, Train Loss: 0.7628, Validation Loss: 1.2073, Validation F1: 0.9030\n",
      "Epoch 3962, Train Loss: 0.7610, Validation Loss: 1.2270, Validation F1: 0.9029\n",
      "Epoch 3963, Train Loss: 0.7622, Validation Loss: 1.2385, Validation F1: 0.9031\n",
      "Epoch 3964, Train Loss: 0.7614, Validation Loss: 1.2565, Validation F1: 0.9061\n",
      "Epoch 3965, Train Loss: 0.7620, Validation Loss: 1.1772, Validation F1: 0.9045\n",
      "Epoch 3966, Train Loss: 0.7608, Validation Loss: 1.1821, Validation F1: 0.9053\n",
      "Epoch 3967, Train Loss: 0.7625, Validation Loss: 1.2511, Validation F1: 0.9039\n",
      "Epoch 3968, Train Loss: 0.7620, Validation Loss: 1.1972, Validation F1: 0.9019\n",
      "Epoch 3969, Train Loss: 0.7640, Validation Loss: 1.2301, Validation F1: 0.9040\n",
      "Epoch 3970, Train Loss: 0.7648, Validation Loss: 1.2520, Validation F1: 0.9021\n",
      "Epoch 3971, Train Loss: 0.7623, Validation Loss: 1.2257, Validation F1: 0.9034\n",
      "Epoch 3972, Train Loss: 0.7642, Validation Loss: 1.2301, Validation F1: 0.9044\n",
      "Epoch 3973, Train Loss: 0.7622, Validation Loss: 1.2166, Validation F1: 0.9047\n",
      "Epoch 3974, Train Loss: 0.7623, Validation Loss: 1.2358, Validation F1: 0.9018\n",
      "Epoch 3975, Train Loss: 0.7636, Validation Loss: 1.1941, Validation F1: 0.9009\n",
      "Epoch 3976, Train Loss: 0.7619, Validation Loss: 1.2403, Validation F1: 0.9026\n",
      "Epoch 3977, Train Loss: 0.7634, Validation Loss: 1.2199, Validation F1: 0.9023\n",
      "Epoch 3978, Train Loss: 0.7608, Validation Loss: 1.2247, Validation F1: 0.9033\n",
      "Epoch 3979, Train Loss: 0.7610, Validation Loss: 1.2489, Validation F1: 0.9068\n",
      "Epoch 3980, Train Loss: 0.7621, Validation Loss: 1.1970, Validation F1: 0.9073\n",
      "Epoch 3981, Train Loss: 0.7609, Validation Loss: 1.1860, Validation F1: 0.9059\n",
      "Epoch 3982, Train Loss: 0.7613, Validation Loss: 1.2413, Validation F1: 0.9036\n",
      "Epoch 3983, Train Loss: 0.7606, Validation Loss: 1.2489, Validation F1: 0.9036\n",
      "Epoch 3984, Train Loss: 0.7603, Validation Loss: 1.2077, Validation F1: 0.9016\n",
      "Epoch 3985, Train Loss: 0.7610, Validation Loss: 1.2556, Validation F1: 0.9012\n",
      "Epoch 3986, Train Loss: 0.7613, Validation Loss: 1.2403, Validation F1: 0.9019\n",
      "Epoch 3987, Train Loss: 0.7611, Validation Loss: 1.2071, Validation F1: 0.9050\n",
      "Epoch 3988, Train Loss: 0.7606, Validation Loss: 1.2084, Validation F1: 0.9062\n",
      "Epoch 3989, Train Loss: 0.7608, Validation Loss: 1.2567, Validation F1: 0.9047\n",
      "Epoch 3990, Train Loss: 0.7614, Validation Loss: 1.2211, Validation F1: 0.9058\n",
      "Epoch 3991, Train Loss: 0.7614, Validation Loss: 1.2362, Validation F1: 0.9035\n",
      "Epoch 3992, Train Loss: 0.7611, Validation Loss: 1.2373, Validation F1: 0.9023\n",
      "Epoch 3993, Train Loss: 0.7605, Validation Loss: 1.2469, Validation F1: 0.9035\n",
      "Epoch 3994, Train Loss: 0.7621, Validation Loss: 1.2212, Validation F1: 0.9057\n",
      "Epoch 3995, Train Loss: 0.7623, Validation Loss: 1.2640, Validation F1: 0.9054\n",
      "Epoch 3996, Train Loss: 0.7616, Validation Loss: 1.2240, Validation F1: 0.9037\n",
      "Epoch 3997, Train Loss: 0.7607, Validation Loss: 1.2498, Validation F1: 0.9018\n",
      "Epoch 3998, Train Loss: 0.7625, Validation Loss: 1.2220, Validation F1: 0.9031\n",
      "Epoch 3999, Train Loss: 0.7607, Validation Loss: 1.1747, Validation F1: 0.9050\n",
      "Epoch 4000, Train Loss: 0.7615, Validation Loss: 1.1898, Validation F1: 0.9064\n",
      "Epoch 4001, Train Loss: 0.7604, Validation Loss: 1.2796, Validation F1: 0.9059\n",
      "Epoch 4002, Train Loss: 0.7622, Validation Loss: 1.2496, Validation F1: 0.9055\n",
      "Epoch 4003, Train Loss: 0.7632, Validation Loss: 1.1943, Validation F1: 0.9015\n",
      "Epoch 4004, Train Loss: 0.7615, Validation Loss: 1.2748, Validation F1: 0.9035\n",
      "Epoch 4005, Train Loss: 0.7633, Validation Loss: 1.2820, Validation F1: 0.9018\n",
      "Epoch 4006, Train Loss: 0.7616, Validation Loss: 1.2304, Validation F1: 0.9031\n",
      "Epoch 4007, Train Loss: 0.7622, Validation Loss: 1.2032, Validation F1: 0.9057\n",
      "Epoch 4008, Train Loss: 0.7601, Validation Loss: 1.2401, Validation F1: 0.9051\n",
      "Epoch 4009, Train Loss: 0.7623, Validation Loss: 1.2440, Validation F1: 0.9036\n",
      "Epoch 4010, Train Loss: 0.7610, Validation Loss: 1.2207, Validation F1: 0.9031\n",
      "Epoch 4011, Train Loss: 0.7620, Validation Loss: 1.2635, Validation F1: 0.9028\n",
      "Epoch 4012, Train Loss: 0.7630, Validation Loss: 1.1822, Validation F1: 0.9038\n",
      "Epoch 4013, Train Loss: 0.7625, Validation Loss: 1.1954, Validation F1: 0.9014\n",
      "Epoch 4014, Train Loss: 0.7626, Validation Loss: 1.2103, Validation F1: 0.9033\n",
      "Epoch 4015, Train Loss: 0.7623, Validation Loss: 1.2347, Validation F1: 0.9008\n",
      "Epoch 4016, Train Loss: 0.7627, Validation Loss: 1.2467, Validation F1: 0.9045\n",
      "Epoch 4017, Train Loss: 0.7628, Validation Loss: 1.2734, Validation F1: 0.9071\n",
      "Epoch 4018, Train Loss: 0.7611, Validation Loss: 1.2159, Validation F1: 0.9057\n",
      "Epoch 4019, Train Loss: 0.7630, Validation Loss: 1.2364, Validation F1: 0.9018\n",
      "Epoch 4020, Train Loss: 0.7611, Validation Loss: 1.2401, Validation F1: 0.9005\n",
      "Epoch 4021, Train Loss: 0.7615, Validation Loss: 1.2473, Validation F1: 0.9033\n",
      "Epoch 4022, Train Loss: 0.7632, Validation Loss: 1.2573, Validation F1: 0.9069\n",
      "Epoch 4023, Train Loss: 0.7628, Validation Loss: 1.2000, Validation F1: 0.9058\n",
      "Epoch 4024, Train Loss: 0.7622, Validation Loss: 1.2323, Validation F1: 0.9048\n",
      "Epoch 4025, Train Loss: 0.7619, Validation Loss: 1.1988, Validation F1: 0.9026\n",
      "Epoch 4026, Train Loss: 0.7615, Validation Loss: 1.2435, Validation F1: 0.9016\n",
      "Epoch 4027, Train Loss: 0.7639, Validation Loss: 1.2262, Validation F1: 0.9069\n",
      "Epoch 4028, Train Loss: 0.7617, Validation Loss: 1.2240, Validation F1: 0.9071\n",
      "Epoch 4029, Train Loss: 0.7615, Validation Loss: 1.2255, Validation F1: 0.9025\n",
      "Epoch 4030, Train Loss: 0.7625, Validation Loss: 1.2199, Validation F1: 0.9029\n",
      "Epoch 4031, Train Loss: 0.7613, Validation Loss: 1.2438, Validation F1: 0.9037\n",
      "Epoch 4032, Train Loss: 0.7632, Validation Loss: 1.2600, Validation F1: 0.9042\n",
      "Epoch 4033, Train Loss: 0.7627, Validation Loss: 1.2277, Validation F1: 0.9054\n",
      "Epoch 4034, Train Loss: 0.7631, Validation Loss: 1.2341, Validation F1: 0.9053\n",
      "Epoch 4035, Train Loss: 0.7651, Validation Loss: 1.2361, Validation F1: 0.9021\n",
      "Epoch 4036, Train Loss: 0.7616, Validation Loss: 1.2535, Validation F1: 0.9024\n",
      "Epoch 4037, Train Loss: 0.7667, Validation Loss: 1.2192, Validation F1: 0.9038\n",
      "Epoch 4038, Train Loss: 0.7646, Validation Loss: 1.1867, Validation F1: 0.9040\n",
      "Epoch 4039, Train Loss: 0.7640, Validation Loss: 1.2299, Validation F1: 0.9035\n",
      "Epoch 4040, Train Loss: 0.7656, Validation Loss: 1.1995, Validation F1: 0.9028\n",
      "Epoch 4041, Train Loss: 0.7615, Validation Loss: 1.2211, Validation F1: 0.9007\n",
      "Epoch 4042, Train Loss: 0.7646, Validation Loss: 1.2242, Validation F1: 0.9021\n",
      "Epoch 4043, Train Loss: 0.7632, Validation Loss: 1.1730, Validation F1: 0.9035\n",
      "Epoch 4044, Train Loss: 0.7634, Validation Loss: 1.2078, Validation F1: 0.9067\n",
      "Epoch 4045, Train Loss: 0.7667, Validation Loss: 1.2221, Validation F1: 0.9045\n",
      "Epoch 4046, Train Loss: 0.7656, Validation Loss: 1.1505, Validation F1: 0.9055\n",
      "Epoch 4047, Train Loss: 0.7641, Validation Loss: 1.2087, Validation F1: 0.9030\n",
      "Epoch 4048, Train Loss: 0.7696, Validation Loss: 1.2533, Validation F1: 0.8986\n",
      "Epoch 4049, Train Loss: 0.7836, Validation Loss: 1.1961, Validation F1: 0.9035\n",
      "Epoch 4050, Train Loss: 0.7755, Validation Loss: 1.1922, Validation F1: 0.9036\n",
      "Epoch 4051, Train Loss: 0.7759, Validation Loss: 1.2368, Validation F1: 0.9039\n",
      "Epoch 4052, Train Loss: 0.7750, Validation Loss: 1.2283, Validation F1: 0.9028\n",
      "Epoch 4053, Train Loss: 0.7716, Validation Loss: 1.2042, Validation F1: 0.8990\n",
      "Epoch 4054, Train Loss: 0.7736, Validation Loss: 1.2264, Validation F1: 0.9017\n",
      "Epoch 4055, Train Loss: 0.7688, Validation Loss: 1.2264, Validation F1: 0.9031\n",
      "Epoch 4056, Train Loss: 0.7726, Validation Loss: 1.2399, Validation F1: 0.9034\n",
      "Epoch 4057, Train Loss: 0.7754, Validation Loss: 1.2585, Validation F1: 0.9005\n",
      "Epoch 4058, Train Loss: 0.7713, Validation Loss: 1.2425, Validation F1: 0.9004\n",
      "Epoch 4059, Train Loss: 0.7704, Validation Loss: 1.2713, Validation F1: 0.9047\n",
      "Epoch 4060, Train Loss: 0.7730, Validation Loss: 1.2303, Validation F1: 0.9017\n",
      "Epoch 4061, Train Loss: 0.7706, Validation Loss: 1.2147, Validation F1: 0.9019\n",
      "Epoch 4062, Train Loss: 0.7744, Validation Loss: 1.1882, Validation F1: 0.9042\n",
      "Epoch 4063, Train Loss: 0.7787, Validation Loss: 1.2351, Validation F1: 0.8990\n",
      "Epoch 4064, Train Loss: 0.7687, Validation Loss: 1.2482, Validation F1: 0.8995\n",
      "Epoch 4065, Train Loss: 0.7746, Validation Loss: 1.2447, Validation F1: 0.9023\n",
      "Epoch 4066, Train Loss: 0.7768, Validation Loss: 1.2265, Validation F1: 0.9059\n",
      "Epoch 4067, Train Loss: 0.7714, Validation Loss: 1.2076, Validation F1: 0.9066\n",
      "Epoch 4068, Train Loss: 0.7767, Validation Loss: 1.2071, Validation F1: 0.9039\n",
      "Epoch 4069, Train Loss: 0.7737, Validation Loss: 1.2088, Validation F1: 0.8941\n",
      "Epoch 4070, Train Loss: 0.7673, Validation Loss: 1.1932, Validation F1: 0.8961\n",
      "Epoch 4071, Train Loss: 0.7732, Validation Loss: 1.1958, Validation F1: 0.9004\n",
      "Epoch 4072, Train Loss: 0.7731, Validation Loss: 1.2159, Validation F1: 0.9044\n",
      "Epoch 4073, Train Loss: 0.7739, Validation Loss: 1.1693, Validation F1: 0.9063\n",
      "Epoch 4074, Train Loss: 0.7723, Validation Loss: 1.2053, Validation F1: 0.9066\n",
      "Epoch 4075, Train Loss: 0.7727, Validation Loss: 1.1947, Validation F1: 0.9016\n",
      "Epoch 4076, Train Loss: 0.7703, Validation Loss: 1.2351, Validation F1: 0.9049\n",
      "Epoch 4077, Train Loss: 0.7674, Validation Loss: 1.2231, Validation F1: 0.9014\n",
      "Epoch 4078, Train Loss: 0.7690, Validation Loss: 1.2165, Validation F1: 0.9037\n",
      "Epoch 4079, Train Loss: 0.7695, Validation Loss: 1.2691, Validation F1: 0.9028\n",
      "Epoch 4080, Train Loss: 0.7687, Validation Loss: 1.2012, Validation F1: 0.9036\n",
      "Epoch 4081, Train Loss: 0.7671, Validation Loss: 1.2286, Validation F1: 0.9034\n",
      "Epoch 4082, Train Loss: 0.7696, Validation Loss: 1.2314, Validation F1: 0.8993\n",
      "Epoch 4083, Train Loss: 0.7693, Validation Loss: 1.2344, Validation F1: 0.9026\n",
      "Epoch 4084, Train Loss: 0.7666, Validation Loss: 1.1945, Validation F1: 0.9059\n",
      "Epoch 4085, Train Loss: 0.7714, Validation Loss: 1.2225, Validation F1: 0.9057\n",
      "Epoch 4086, Train Loss: 0.7684, Validation Loss: 1.1846, Validation F1: 0.9053\n",
      "Epoch 4087, Train Loss: 0.7688, Validation Loss: 1.2116, Validation F1: 0.9026\n",
      "Epoch 4088, Train Loss: 0.7674, Validation Loss: 1.2363, Validation F1: 0.9018\n",
      "Epoch 4089, Train Loss: 0.7664, Validation Loss: 1.2041, Validation F1: 0.9029\n",
      "Epoch 4090, Train Loss: 0.7667, Validation Loss: 1.2382, Validation F1: 0.9048\n",
      "Epoch 4091, Train Loss: 0.7670, Validation Loss: 1.2252, Validation F1: 0.9048\n",
      "Epoch 4092, Train Loss: 0.7655, Validation Loss: 1.2161, Validation F1: 0.8991\n",
      "Epoch 4093, Train Loss: 0.7655, Validation Loss: 1.2110, Validation F1: 0.9027\n",
      "Epoch 4094, Train Loss: 0.7659, Validation Loss: 1.2440, Validation F1: 0.9019\n",
      "Epoch 4095, Train Loss: 0.7646, Validation Loss: 1.2012, Validation F1: 0.9045\n",
      "Epoch 4096, Train Loss: 0.7662, Validation Loss: 1.2203, Validation F1: 0.9010\n",
      "Epoch 4097, Train Loss: 0.7651, Validation Loss: 1.2660, Validation F1: 0.9011\n",
      "Epoch 4098, Train Loss: 0.7648, Validation Loss: 1.1886, Validation F1: 0.9040\n",
      "Epoch 4099, Train Loss: 0.7635, Validation Loss: 1.2158, Validation F1: 0.9028\n",
      "Epoch 4100, Train Loss: 0.7648, Validation Loss: 1.2391, Validation F1: 0.9019\n",
      "Epoch 4101, Train Loss: 0.7648, Validation Loss: 1.2402, Validation F1: 0.9019\n",
      "Epoch 4102, Train Loss: 0.7649, Validation Loss: 1.1962, Validation F1: 0.9002\n",
      "Epoch 4103, Train Loss: 0.7637, Validation Loss: 1.2064, Validation F1: 0.9045\n",
      "Epoch 4104, Train Loss: 0.7635, Validation Loss: 1.2422, Validation F1: 0.9077\n",
      "Epoch 4105, Train Loss: 0.7639, Validation Loss: 1.2925, Validation F1: 0.9063\n",
      "Epoch 4106, Train Loss: 0.7625, Validation Loss: 1.1688, Validation F1: 0.9027\n",
      "Epoch 4107, Train Loss: 0.7642, Validation Loss: 1.2398, Validation F1: 0.8940\n",
      "Epoch 4108, Train Loss: 0.7654, Validation Loss: 1.2293, Validation F1: 0.8936\n",
      "Epoch 4109, Train Loss: 0.7648, Validation Loss: 1.2036, Validation F1: 0.9034\n",
      "Epoch 4110, Train Loss: 0.7640, Validation Loss: 1.2082, Validation F1: 0.9058\n",
      "Epoch 4111, Train Loss: 0.7632, Validation Loss: 1.2261, Validation F1: 0.9059\n",
      "Epoch 4112, Train Loss: 0.7628, Validation Loss: 1.2341, Validation F1: 0.9027\n",
      "Epoch 4113, Train Loss: 0.7635, Validation Loss: 1.2263, Validation F1: 0.8942\n",
      "Epoch 4114, Train Loss: 0.7624, Validation Loss: 1.2481, Validation F1: 0.9019\n",
      "Epoch 4115, Train Loss: 0.7645, Validation Loss: 1.2578, Validation F1: 0.9030\n",
      "Epoch 4116, Train Loss: 0.7625, Validation Loss: 1.2207, Validation F1: 0.9045\n",
      "Epoch 4117, Train Loss: 0.7624, Validation Loss: 1.2156, Validation F1: 0.9013\n",
      "Epoch 4118, Train Loss: 0.7629, Validation Loss: 1.2227, Validation F1: 0.9024\n",
      "Epoch 4119, Train Loss: 0.7633, Validation Loss: 1.1948, Validation F1: 0.9041\n",
      "Epoch 4120, Train Loss: 0.7628, Validation Loss: 1.2191, Validation F1: 0.9024\n",
      "Epoch 4121, Train Loss: 0.7623, Validation Loss: 1.2258, Validation F1: 0.9019\n",
      "Epoch 4122, Train Loss: 0.7630, Validation Loss: 1.2115, Validation F1: 0.9041\n",
      "Epoch 4123, Train Loss: 0.7621, Validation Loss: 1.2193, Validation F1: 0.9038\n",
      "Epoch 4124, Train Loss: 0.7620, Validation Loss: 1.2300, Validation F1: 0.9032\n",
      "Epoch 4125, Train Loss: 0.7618, Validation Loss: 1.2531, Validation F1: 0.9037\n",
      "Epoch 4126, Train Loss: 0.7622, Validation Loss: 1.2190, Validation F1: 0.9025\n",
      "Epoch 4127, Train Loss: 0.7620, Validation Loss: 1.2402, Validation F1: 0.9012\n",
      "Epoch 4128, Train Loss: 0.7627, Validation Loss: 1.2239, Validation F1: 0.9055\n",
      "Epoch 4129, Train Loss: 0.7617, Validation Loss: 1.1981, Validation F1: 0.9060\n",
      "Epoch 4130, Train Loss: 0.7621, Validation Loss: 1.2572, Validation F1: 0.9054\n",
      "Epoch 4131, Train Loss: 0.7623, Validation Loss: 1.2033, Validation F1: 0.9021\n",
      "Epoch 4132, Train Loss: 0.7619, Validation Loss: 1.2484, Validation F1: 0.8935\n",
      "Epoch 4133, Train Loss: 0.7615, Validation Loss: 1.2193, Validation F1: 0.8980\n",
      "Epoch 4134, Train Loss: 0.7623, Validation Loss: 1.2128, Validation F1: 0.9049\n",
      "Epoch 4135, Train Loss: 0.7626, Validation Loss: 1.2372, Validation F1: 0.9043\n",
      "Epoch 4136, Train Loss: 0.7638, Validation Loss: 1.2437, Validation F1: 0.9035\n",
      "Epoch 4137, Train Loss: 0.7617, Validation Loss: 1.2473, Validation F1: 0.9038\n",
      "Epoch 4138, Train Loss: 0.7615, Validation Loss: 1.2277, Validation F1: 0.9032\n",
      "Epoch 4139, Train Loss: 0.7618, Validation Loss: 1.1967, Validation F1: 0.9032\n",
      "Epoch 4140, Train Loss: 0.7629, Validation Loss: 1.2434, Validation F1: 0.9028\n",
      "Epoch 4141, Train Loss: 0.7621, Validation Loss: 1.2247, Validation F1: 0.9046\n",
      "Epoch 4142, Train Loss: 0.7612, Validation Loss: 1.2646, Validation F1: 0.9033\n",
      "Epoch 4143, Train Loss: 0.7619, Validation Loss: 1.2194, Validation F1: 0.9060\n",
      "Epoch 4144, Train Loss: 0.7618, Validation Loss: 1.2462, Validation F1: 0.9036\n",
      "Epoch 4145, Train Loss: 0.7622, Validation Loss: 1.2121, Validation F1: 0.9043\n",
      "Epoch 4146, Train Loss: 0.7612, Validation Loss: 1.2093, Validation F1: 0.9024\n",
      "Epoch 4147, Train Loss: 0.7620, Validation Loss: 1.2438, Validation F1: 0.9021\n",
      "Epoch 4148, Train Loss: 0.7621, Validation Loss: 1.2039, Validation F1: 0.9019\n",
      "Epoch 4149, Train Loss: 0.7612, Validation Loss: 1.2000, Validation F1: 0.9035\n",
      "Epoch 4150, Train Loss: 0.7611, Validation Loss: 1.2479, Validation F1: 0.9058\n",
      "Epoch 4151, Train Loss: 0.7608, Validation Loss: 1.2100, Validation F1: 0.9033\n",
      "Epoch 4152, Train Loss: 0.7608, Validation Loss: 1.2144, Validation F1: 0.9029\n",
      "Epoch 4153, Train Loss: 0.7613, Validation Loss: 1.2329, Validation F1: 0.9040\n",
      "Epoch 4154, Train Loss: 0.7615, Validation Loss: 1.2177, Validation F1: 0.9030\n",
      "Epoch 4155, Train Loss: 0.7613, Validation Loss: 1.2358, Validation F1: 0.9015\n",
      "Epoch 4156, Train Loss: 0.7611, Validation Loss: 1.2478, Validation F1: 0.9042\n",
      "Epoch 4157, Train Loss: 0.7613, Validation Loss: 1.2367, Validation F1: 0.9049\n",
      "Epoch 4158, Train Loss: 0.7608, Validation Loss: 1.2439, Validation F1: 0.9028\n",
      "Epoch 4159, Train Loss: 0.7619, Validation Loss: 1.2359, Validation F1: 0.9014\n",
      "Epoch 4160, Train Loss: 0.7610, Validation Loss: 1.2258, Validation F1: 0.9011\n",
      "Epoch 4161, Train Loss: 0.7604, Validation Loss: 1.2433, Validation F1: 0.9011\n",
      "Epoch 4162, Train Loss: 0.7608, Validation Loss: 1.2380, Validation F1: 0.9044\n",
      "Epoch 4163, Train Loss: 0.7612, Validation Loss: 1.1898, Validation F1: 0.9046\n",
      "Epoch 4164, Train Loss: 0.7620, Validation Loss: 1.2746, Validation F1: 0.9027\n",
      "Epoch 4165, Train Loss: 0.7614, Validation Loss: 1.2518, Validation F1: 0.9023\n",
      "Epoch 4166, Train Loss: 0.7606, Validation Loss: 1.1879, Validation F1: 0.9034\n",
      "Epoch 4167, Train Loss: 0.7621, Validation Loss: 1.2518, Validation F1: 0.9043\n",
      "Epoch 4168, Train Loss: 0.7605, Validation Loss: 1.2017, Validation F1: 0.9046\n",
      "Epoch 4169, Train Loss: 0.7611, Validation Loss: 1.2020, Validation F1: 0.9026\n",
      "Epoch 4170, Train Loss: 0.7613, Validation Loss: 1.2431, Validation F1: 0.9046\n",
      "Epoch 4171, Train Loss: 0.7611, Validation Loss: 1.2295, Validation F1: 0.9051\n",
      "Epoch 4172, Train Loss: 0.7602, Validation Loss: 1.2626, Validation F1: 0.9041\n",
      "Epoch 4173, Train Loss: 0.7601, Validation Loss: 1.2093, Validation F1: 0.9005\n",
      "Epoch 4174, Train Loss: 0.7625, Validation Loss: 1.2245, Validation F1: 0.8984\n",
      "Epoch 4175, Train Loss: 0.7612, Validation Loss: 1.2653, Validation F1: 0.9030\n",
      "Epoch 4176, Train Loss: 0.7607, Validation Loss: 1.1712, Validation F1: 0.9046\n",
      "Epoch 4177, Train Loss: 0.7617, Validation Loss: 1.2377, Validation F1: 0.9072\n",
      "Epoch 4178, Train Loss: 0.7607, Validation Loss: 1.2315, Validation F1: 0.9063\n",
      "Epoch 4179, Train Loss: 0.7627, Validation Loss: 1.2208, Validation F1: 0.9012\n",
      "Epoch 4180, Train Loss: 0.7610, Validation Loss: 1.2541, Validation F1: 0.9033\n",
      "Epoch 4181, Train Loss: 0.7611, Validation Loss: 1.2582, Validation F1: 0.9031\n",
      "Epoch 4182, Train Loss: 0.7614, Validation Loss: 1.2611, Validation F1: 0.9054\n",
      "Epoch 4183, Train Loss: 0.7615, Validation Loss: 1.2466, Validation F1: 0.9033\n",
      "Epoch 4184, Train Loss: 0.7608, Validation Loss: 1.2152, Validation F1: 0.8988\n",
      "Epoch 4185, Train Loss: 0.7617, Validation Loss: 1.1911, Validation F1: 0.9010\n",
      "Epoch 4186, Train Loss: 0.7612, Validation Loss: 1.2145, Validation F1: 0.9042\n",
      "Epoch 4187, Train Loss: 0.7619, Validation Loss: 1.2207, Validation F1: 0.9059\n",
      "Epoch 4188, Train Loss: 0.7614, Validation Loss: 1.2351, Validation F1: 0.9016\n",
      "Epoch 4189, Train Loss: 0.7615, Validation Loss: 1.2178, Validation F1: 0.8999\n",
      "Epoch 4190, Train Loss: 0.7610, Validation Loss: 1.2206, Validation F1: 0.9033\n",
      "Epoch 4191, Train Loss: 0.7623, Validation Loss: 1.2386, Validation F1: 0.9056\n",
      "Epoch 4192, Train Loss: 0.7616, Validation Loss: 1.2089, Validation F1: 0.9043\n",
      "Epoch 4193, Train Loss: 0.7617, Validation Loss: 1.2715, Validation F1: 0.9046\n",
      "Epoch 4194, Train Loss: 0.7642, Validation Loss: 1.2617, Validation F1: 0.9032\n",
      "Epoch 4195, Train Loss: 0.7646, Validation Loss: 1.2559, Validation F1: 0.9012\n",
      "Epoch 4196, Train Loss: 0.7637, Validation Loss: 1.2454, Validation F1: 0.9051\n",
      "Epoch 4197, Train Loss: 0.7661, Validation Loss: 1.2216, Validation F1: 0.9016\n",
      "Epoch 4198, Train Loss: 0.7626, Validation Loss: 1.2390, Validation F1: 0.9010\n",
      "Epoch 4199, Train Loss: 0.7650, Validation Loss: 1.2157, Validation F1: 0.9032\n",
      "Epoch 4200, Train Loss: 0.7629, Validation Loss: 1.2230, Validation F1: 0.9059\n",
      "Epoch 4201, Train Loss: 0.7644, Validation Loss: 1.2571, Validation F1: 0.9056\n",
      "Epoch 4202, Train Loss: 0.7635, Validation Loss: 1.2675, Validation F1: 0.9045\n",
      "Epoch 4203, Train Loss: 0.7635, Validation Loss: 1.2023, Validation F1: 0.9024\n",
      "Epoch 4204, Train Loss: 0.7650, Validation Loss: 1.2244, Validation F1: 0.9019\n",
      "Epoch 4205, Train Loss: 0.7622, Validation Loss: 1.2661, Validation F1: 0.9007\n",
      "Epoch 4206, Train Loss: 0.7647, Validation Loss: 1.2145, Validation F1: 0.9010\n",
      "Epoch 4207, Train Loss: 0.7622, Validation Loss: 1.2367, Validation F1: 0.9021\n",
      "Epoch 4208, Train Loss: 0.7633, Validation Loss: 1.1977, Validation F1: 0.9023\n",
      "Epoch 4209, Train Loss: 0.7630, Validation Loss: 1.2800, Validation F1: 0.8948\n",
      "Epoch 4210, Train Loss: 0.7623, Validation Loss: 1.2236, Validation F1: 0.9008\n",
      "Epoch 4211, Train Loss: 0.7625, Validation Loss: 1.2485, Validation F1: 0.9031\n",
      "Epoch 4212, Train Loss: 0.7617, Validation Loss: 1.2133, Validation F1: 0.9067\n",
      "Epoch 4213, Train Loss: 0.7613, Validation Loss: 1.2155, Validation F1: 0.9031\n",
      "Epoch 4214, Train Loss: 0.7603, Validation Loss: 1.2432, Validation F1: 0.8986\n",
      "Epoch 4215, Train Loss: 0.7629, Validation Loss: 1.2124, Validation F1: 0.9017\n",
      "Epoch 4216, Train Loss: 0.7610, Validation Loss: 1.2103, Validation F1: 0.9032\n",
      "Epoch 4217, Train Loss: 0.7616, Validation Loss: 1.1947, Validation F1: 0.9041\n",
      "Epoch 4218, Train Loss: 0.7620, Validation Loss: 1.2020, Validation F1: 0.9030\n",
      "Epoch 4219, Train Loss: 0.7615, Validation Loss: 1.2437, Validation F1: 0.9006\n",
      "Epoch 4220, Train Loss: 0.7619, Validation Loss: 1.2476, Validation F1: 0.8999\n",
      "Epoch 4221, Train Loss: 0.7616, Validation Loss: 1.2469, Validation F1: 0.9074\n",
      "Epoch 4222, Train Loss: 0.7608, Validation Loss: 1.2478, Validation F1: 0.9072\n",
      "Epoch 4223, Train Loss: 0.7614, Validation Loss: 1.2470, Validation F1: 0.9055\n",
      "Epoch 4224, Train Loss: 0.7614, Validation Loss: 1.2448, Validation F1: 0.9013\n",
      "Epoch 4225, Train Loss: 0.7609, Validation Loss: 1.2287, Validation F1: 0.9032\n",
      "Epoch 4226, Train Loss: 0.7603, Validation Loss: 1.2250, Validation F1: 0.9025\n",
      "Epoch 4227, Train Loss: 0.7601, Validation Loss: 1.1986, Validation F1: 0.9024\n",
      "Epoch 4228, Train Loss: 0.7603, Validation Loss: 1.2521, Validation F1: 0.9041\n",
      "Epoch 4229, Train Loss: 0.7603, Validation Loss: 1.2526, Validation F1: 0.9045\n",
      "Epoch 4230, Train Loss: 0.7597, Validation Loss: 1.2282, Validation F1: 0.9027\n",
      "Epoch 4231, Train Loss: 0.7609, Validation Loss: 1.2346, Validation F1: 0.9018\n",
      "Epoch 4232, Train Loss: 0.7603, Validation Loss: 1.2411, Validation F1: 0.9040\n",
      "Epoch 4233, Train Loss: 0.7604, Validation Loss: 1.2351, Validation F1: 0.9071\n",
      "Epoch 4234, Train Loss: 0.7613, Validation Loss: 1.2528, Validation F1: 0.9036\n",
      "Epoch 4235, Train Loss: 0.7605, Validation Loss: 1.2347, Validation F1: 0.9012\n",
      "Epoch 4236, Train Loss: 0.7603, Validation Loss: 1.2191, Validation F1: 0.9032\n",
      "Epoch 4237, Train Loss: 0.7601, Validation Loss: 1.2381, Validation F1: 0.9031\n",
      "Epoch 4238, Train Loss: 0.7609, Validation Loss: 1.2727, Validation F1: 0.9027\n",
      "Epoch 4239, Train Loss: 0.7609, Validation Loss: 1.2188, Validation F1: 0.9029\n",
      "Epoch 4240, Train Loss: 0.7604, Validation Loss: 1.2005, Validation F1: 0.9032\n",
      "Epoch 4241, Train Loss: 0.7626, Validation Loss: 1.2703, Validation F1: 0.8981\n",
      "Epoch 4242, Train Loss: 0.7615, Validation Loss: 1.1998, Validation F1: 0.9004\n",
      "Epoch 4243, Train Loss: 0.7607, Validation Loss: 1.2541, Validation F1: 0.9036\n",
      "Epoch 4244, Train Loss: 0.7618, Validation Loss: 1.2291, Validation F1: 0.9074\n",
      "Epoch 4245, Train Loss: 0.7620, Validation Loss: 1.2305, Validation F1: 0.9067\n",
      "Epoch 4246, Train Loss: 0.7613, Validation Loss: 1.2502, Validation F1: 0.9015\n",
      "Epoch 4247, Train Loss: 0.7610, Validation Loss: 1.2317, Validation F1: 0.8985\n",
      "Epoch 4248, Train Loss: 0.7612, Validation Loss: 1.2602, Validation F1: 0.9025\n",
      "Epoch 4249, Train Loss: 0.7604, Validation Loss: 1.2453, Validation F1: 0.9035\n",
      "Epoch 4250, Train Loss: 0.7603, Validation Loss: 1.2682, Validation F1: 0.9041\n",
      "Epoch 4251, Train Loss: 0.7602, Validation Loss: 1.2355, Validation F1: 0.9039\n",
      "Epoch 4252, Train Loss: 0.7607, Validation Loss: 1.2190, Validation F1: 0.9064\n",
      "Epoch 4253, Train Loss: 0.7595, Validation Loss: 1.1887, Validation F1: 0.9037\n",
      "Epoch 4254, Train Loss: 0.7598, Validation Loss: 1.2505, Validation F1: 0.9055\n",
      "Epoch 4255, Train Loss: 0.7613, Validation Loss: 1.2123, Validation F1: 0.9025\n",
      "Epoch 4256, Train Loss: 0.7601, Validation Loss: 1.2619, Validation F1: 0.9009\n",
      "Epoch 4257, Train Loss: 0.7601, Validation Loss: 1.2939, Validation F1: 0.9043\n",
      "Epoch 4258, Train Loss: 0.7598, Validation Loss: 1.2335, Validation F1: 0.9004\n",
      "Epoch 4259, Train Loss: 0.7599, Validation Loss: 1.2433, Validation F1: 0.9033\n",
      "Epoch 4260, Train Loss: 0.7603, Validation Loss: 1.2785, Validation F1: 0.9024\n",
      "Epoch 4261, Train Loss: 0.7606, Validation Loss: 1.2688, Validation F1: 0.9059\n",
      "Epoch 4262, Train Loss: 0.7603, Validation Loss: 1.2380, Validation F1: 0.9037\n",
      "Epoch 4263, Train Loss: 0.7600, Validation Loss: 1.2521, Validation F1: 0.9031\n",
      "Epoch 4264, Train Loss: 0.7602, Validation Loss: 1.2025, Validation F1: 0.9001\n",
      "Epoch 4265, Train Loss: 0.7599, Validation Loss: 1.2431, Validation F1: 0.8992\n",
      "Epoch 4266, Train Loss: 0.7606, Validation Loss: 1.2976, Validation F1: 0.9040\n",
      "Epoch 4267, Train Loss: 0.7603, Validation Loss: 1.2703, Validation F1: 0.9063\n",
      "Epoch 4268, Train Loss: 0.7598, Validation Loss: 1.2307, Validation F1: 0.9051\n",
      "Epoch 4269, Train Loss: 0.7597, Validation Loss: 1.2594, Validation F1: 0.9030\n",
      "Epoch 4270, Train Loss: 0.7608, Validation Loss: 1.2990, Validation F1: 0.9031\n",
      "Epoch 4271, Train Loss: 0.7604, Validation Loss: 1.3278, Validation F1: 0.9038\n",
      "Epoch 4272, Train Loss: 0.7601, Validation Loss: 1.2550, Validation F1: 0.9062\n",
      "Epoch 4273, Train Loss: 0.7595, Validation Loss: 1.2243, Validation F1: 0.9040\n",
      "Epoch 4274, Train Loss: 0.7600, Validation Loss: 1.2450, Validation F1: 0.9028\n",
      "Epoch 4275, Train Loss: 0.7601, Validation Loss: 1.2960, Validation F1: 0.8994\n",
      "Epoch 4276, Train Loss: 0.7606, Validation Loss: 1.2007, Validation F1: 0.9033\n",
      "Epoch 4277, Train Loss: 0.7604, Validation Loss: 1.3104, Validation F1: 0.9046\n",
      "Epoch 4278, Train Loss: 0.7592, Validation Loss: 1.2686, Validation F1: 0.9034\n",
      "Epoch 4279, Train Loss: 0.7606, Validation Loss: 1.2364, Validation F1: 0.9019\n",
      "Epoch 4280, Train Loss: 0.7602, Validation Loss: 1.2417, Validation F1: 0.9031\n",
      "Epoch 4281, Train Loss: 0.7603, Validation Loss: 1.2282, Validation F1: 0.9030\n",
      "Epoch 4282, Train Loss: 0.7597, Validation Loss: 1.2435, Validation F1: 0.9056\n",
      "Epoch 4283, Train Loss: 0.7608, Validation Loss: 1.2051, Validation F1: 0.9032\n",
      "Epoch 4284, Train Loss: 0.7598, Validation Loss: 1.2404, Validation F1: 0.9007\n",
      "Epoch 4285, Train Loss: 0.7607, Validation Loss: 1.2524, Validation F1: 0.9013\n",
      "Epoch 4286, Train Loss: 0.7604, Validation Loss: 1.2065, Validation F1: 0.9039\n",
      "Epoch 4287, Train Loss: 0.7599, Validation Loss: 1.2520, Validation F1: 0.9056\n",
      "Epoch 4288, Train Loss: 0.7606, Validation Loss: 1.1946, Validation F1: 0.9064\n",
      "Epoch 4289, Train Loss: 0.7597, Validation Loss: 1.1968, Validation F1: 0.9009\n",
      "Epoch 4290, Train Loss: 0.7627, Validation Loss: 1.2727, Validation F1: 0.8996\n",
      "Epoch 4291, Train Loss: 0.7603, Validation Loss: 1.1882, Validation F1: 0.9028\n",
      "Epoch 4292, Train Loss: 0.7595, Validation Loss: 1.2010, Validation F1: 0.9027\n",
      "Epoch 4293, Train Loss: 0.7600, Validation Loss: 1.2309, Validation F1: 0.9062\n",
      "Epoch 4294, Train Loss: 0.7597, Validation Loss: 1.2630, Validation F1: 0.9059\n",
      "Epoch 4295, Train Loss: 0.7598, Validation Loss: 1.2692, Validation F1: 0.9042\n",
      "Epoch 4296, Train Loss: 0.7601, Validation Loss: 1.2858, Validation F1: 0.8992\n",
      "Epoch 4297, Train Loss: 0.7606, Validation Loss: 1.2904, Validation F1: 0.9057\n",
      "Epoch 4298, Train Loss: 0.7614, Validation Loss: 1.2581, Validation F1: 0.9033\n",
      "Epoch 4299, Train Loss: 0.7617, Validation Loss: 1.2651, Validation F1: 0.9030\n",
      "Epoch 4300, Train Loss: 0.7611, Validation Loss: 1.2152, Validation F1: 0.9017\n",
      "Epoch 4301, Train Loss: 0.7605, Validation Loss: 1.2538, Validation F1: 0.9015\n",
      "Epoch 4302, Train Loss: 0.7606, Validation Loss: 1.2605, Validation F1: 0.9049\n",
      "Epoch 4303, Train Loss: 0.7611, Validation Loss: 1.2221, Validation F1: 0.9044\n",
      "Epoch 4304, Train Loss: 0.7608, Validation Loss: 1.2427, Validation F1: 0.9049\n",
      "Epoch 4305, Train Loss: 0.7615, Validation Loss: 1.2567, Validation F1: 0.9011\n",
      "Epoch 4306, Train Loss: 0.7599, Validation Loss: 1.2498, Validation F1: 0.8998\n",
      "Epoch 4307, Train Loss: 0.7603, Validation Loss: 1.1871, Validation F1: 0.9064\n",
      "Epoch 4308, Train Loss: 0.7605, Validation Loss: 1.2397, Validation F1: 0.9056\n",
      "Epoch 4309, Train Loss: 0.7600, Validation Loss: 1.2725, Validation F1: 0.9016\n",
      "Epoch 4310, Train Loss: 0.7597, Validation Loss: 1.2267, Validation F1: 0.9016\n",
      "Epoch 4311, Train Loss: 0.7599, Validation Loss: 1.2281, Validation F1: 0.8992\n",
      "Epoch 4312, Train Loss: 0.7604, Validation Loss: 1.2175, Validation F1: 0.9016\n",
      "Epoch 4313, Train Loss: 0.7597, Validation Loss: 1.2319, Validation F1: 0.9037\n",
      "Epoch 4314, Train Loss: 0.7587, Validation Loss: 1.2551, Validation F1: 0.9041\n",
      "Epoch 4315, Train Loss: 0.7595, Validation Loss: 1.2331, Validation F1: 0.9044\n",
      "Epoch 4316, Train Loss: 0.7598, Validation Loss: 1.2623, Validation F1: 0.9015\n",
      "Epoch 4317, Train Loss: 0.7602, Validation Loss: 1.2351, Validation F1: 0.9029\n",
      "Epoch 4318, Train Loss: 0.7598, Validation Loss: 1.2589, Validation F1: 0.9034\n",
      "Epoch 4319, Train Loss: 0.7593, Validation Loss: 1.2210, Validation F1: 0.9066\n",
      "Epoch 4320, Train Loss: 0.7607, Validation Loss: 1.2183, Validation F1: 0.9043\n",
      "Epoch 4321, Train Loss: 0.7601, Validation Loss: 1.1928, Validation F1: 0.8989\n",
      "Epoch 4322, Train Loss: 0.7598, Validation Loss: 1.2500, Validation F1: 0.8975\n",
      "Epoch 4323, Train Loss: 0.7612, Validation Loss: 1.2351, Validation F1: 0.9023\n",
      "Epoch 4324, Train Loss: 0.7604, Validation Loss: 1.2429, Validation F1: 0.9065\n",
      "Epoch 4325, Train Loss: 0.7591, Validation Loss: 1.2289, Validation F1: 0.9050\n",
      "Epoch 4326, Train Loss: 0.7607, Validation Loss: 1.2114, Validation F1: 0.9014\n",
      "Epoch 4327, Train Loss: 0.7595, Validation Loss: 1.2667, Validation F1: 0.9017\n",
      "Epoch 4328, Train Loss: 0.7603, Validation Loss: 1.2577, Validation F1: 0.9010\n",
      "Epoch 4329, Train Loss: 0.7601, Validation Loss: 1.2714, Validation F1: 0.9065\n",
      "Epoch 4330, Train Loss: 0.7605, Validation Loss: 1.2490, Validation F1: 0.9043\n",
      "Epoch 4331, Train Loss: 0.7594, Validation Loss: 1.2695, Validation F1: 0.9032\n",
      "Epoch 4332, Train Loss: 0.7595, Validation Loss: 1.2400, Validation F1: 0.8998\n",
      "Epoch 4333, Train Loss: 0.7592, Validation Loss: 1.2429, Validation F1: 0.9038\n",
      "Epoch 4334, Train Loss: 0.7599, Validation Loss: 1.2672, Validation F1: 0.9072\n",
      "Epoch 4335, Train Loss: 0.7602, Validation Loss: 1.2500, Validation F1: 0.9056\n",
      "Epoch 4336, Train Loss: 0.7592, Validation Loss: 1.2599, Validation F1: 0.9016\n",
      "Epoch 4337, Train Loss: 0.7594, Validation Loss: 1.2214, Validation F1: 0.8996\n",
      "Epoch 4338, Train Loss: 0.7605, Validation Loss: 1.2679, Validation F1: 0.9030\n",
      "Epoch 4339, Train Loss: 0.7598, Validation Loss: 1.2571, Validation F1: 0.9042\n",
      "Epoch 4340, Train Loss: 0.7598, Validation Loss: 1.2764, Validation F1: 0.9040\n",
      "Epoch 4341, Train Loss: 0.7588, Validation Loss: 1.2412, Validation F1: 0.9030\n",
      "Epoch 4342, Train Loss: 0.7599, Validation Loss: 1.2163, Validation F1: 0.9018\n",
      "Epoch 4343, Train Loss: 0.7596, Validation Loss: 1.2543, Validation F1: 0.9022\n",
      "Epoch 4344, Train Loss: 0.7599, Validation Loss: 1.2327, Validation F1: 0.9034\n",
      "Epoch 4345, Train Loss: 0.7590, Validation Loss: 1.2581, Validation F1: 0.9021\n",
      "Epoch 4346, Train Loss: 0.7601, Validation Loss: 1.2917, Validation F1: 0.9040\n",
      "Epoch 4347, Train Loss: 0.7599, Validation Loss: 1.2486, Validation F1: 0.9055\n",
      "Epoch 4348, Train Loss: 0.7595, Validation Loss: 1.2297, Validation F1: 0.9026\n",
      "Epoch 4349, Train Loss: 0.7598, Validation Loss: 1.2177, Validation F1: 0.9020\n",
      "Epoch 4350, Train Loss: 0.7592, Validation Loss: 1.2620, Validation F1: 0.9041\n",
      "Epoch 4351, Train Loss: 0.7594, Validation Loss: 1.2302, Validation F1: 0.9025\n",
      "Epoch 4352, Train Loss: 0.7596, Validation Loss: 1.1761, Validation F1: 0.9026\n",
      "Epoch 4353, Train Loss: 0.7593, Validation Loss: 1.1847, Validation F1: 0.9012\n",
      "Epoch 4354, Train Loss: 0.7597, Validation Loss: 1.2593, Validation F1: 0.9022\n",
      "Epoch 4355, Train Loss: 0.7582, Validation Loss: 1.3119, Validation F1: 0.9038\n",
      "Epoch 4356, Train Loss: 0.7592, Validation Loss: 1.2164, Validation F1: 0.9057\n",
      "Epoch 4357, Train Loss: 0.7595, Validation Loss: 1.2543, Validation F1: 0.9047\n",
      "Epoch 4358, Train Loss: 0.7603, Validation Loss: 1.2222, Validation F1: 0.8994\n",
      "Epoch 4359, Train Loss: 0.7593, Validation Loss: 1.2078, Validation F1: 0.9025\n",
      "Epoch 4360, Train Loss: 0.7597, Validation Loss: 1.2328, Validation F1: 0.9030\n",
      "Epoch 4361, Train Loss: 0.7604, Validation Loss: 1.1963, Validation F1: 0.9039\n",
      "Epoch 4362, Train Loss: 0.7592, Validation Loss: 1.2983, Validation F1: 0.9037\n",
      "Epoch 4363, Train Loss: 0.7591, Validation Loss: 1.3448, Validation F1: 0.8989\n",
      "Epoch 4364, Train Loss: 0.7602, Validation Loss: 1.2893, Validation F1: 0.9018\n",
      "Epoch 4365, Train Loss: 0.7597, Validation Loss: 1.2627, Validation F1: 0.9028\n",
      "Epoch 4366, Train Loss: 0.7593, Validation Loss: 1.2687, Validation F1: 0.9032\n",
      "Epoch 4367, Train Loss: 0.7606, Validation Loss: 1.2477, Validation F1: 0.9027\n",
      "Epoch 4368, Train Loss: 0.7597, Validation Loss: 1.2183, Validation F1: 0.9040\n",
      "Epoch 4369, Train Loss: 0.7598, Validation Loss: 1.2184, Validation F1: 0.9045\n",
      "Epoch 4370, Train Loss: 0.7601, Validation Loss: 1.2768, Validation F1: 0.9030\n",
      "Epoch 4371, Train Loss: 0.7592, Validation Loss: 1.2139, Validation F1: 0.9021\n",
      "Epoch 4372, Train Loss: 0.7603, Validation Loss: 1.2013, Validation F1: 0.9037\n",
      "Epoch 4373, Train Loss: 0.7607, Validation Loss: 1.2882, Validation F1: 0.9022\n",
      "Epoch 4374, Train Loss: 0.7598, Validation Loss: 1.3115, Validation F1: 0.9023\n",
      "Epoch 4375, Train Loss: 0.7607, Validation Loss: 1.2274, Validation F1: 0.9027\n",
      "Epoch 4376, Train Loss: 0.7591, Validation Loss: 1.2655, Validation F1: 0.9031\n",
      "Epoch 4377, Train Loss: 0.7604, Validation Loss: 1.2493, Validation F1: 0.9065\n",
      "Epoch 4378, Train Loss: 0.7606, Validation Loss: 1.2558, Validation F1: 0.9018\n",
      "Epoch 4379, Train Loss: 0.7595, Validation Loss: 1.2673, Validation F1: 0.9027\n",
      "Epoch 4380, Train Loss: 0.7597, Validation Loss: 1.2093, Validation F1: 0.9018\n",
      "Epoch 4381, Train Loss: 0.7604, Validation Loss: 1.2496, Validation F1: 0.9042\n",
      "Epoch 4382, Train Loss: 0.7598, Validation Loss: 1.2175, Validation F1: 0.9031\n",
      "Epoch 4383, Train Loss: 0.7598, Validation Loss: 1.2701, Validation F1: 0.9005\n",
      "Epoch 4384, Train Loss: 0.7600, Validation Loss: 1.2431, Validation F1: 0.9029\n",
      "Epoch 4385, Train Loss: 0.7597, Validation Loss: 1.2301, Validation F1: 0.9064\n",
      "Epoch 4386, Train Loss: 0.7606, Validation Loss: 1.2690, Validation F1: 0.9028\n",
      "Epoch 4387, Train Loss: 0.7603, Validation Loss: 1.2724, Validation F1: 0.9018\n",
      "Epoch 4388, Train Loss: 0.7605, Validation Loss: 1.2427, Validation F1: 0.9034\n",
      "Epoch 4389, Train Loss: 0.7624, Validation Loss: 1.3049, Validation F1: 0.9043\n",
      "Epoch 4390, Train Loss: 0.7618, Validation Loss: 1.1821, Validation F1: 0.9052\n",
      "Epoch 4391, Train Loss: 0.7605, Validation Loss: 1.2609, Validation F1: 0.9038\n",
      "Epoch 4392, Train Loss: 0.7618, Validation Loss: 1.2535, Validation F1: 0.9032\n",
      "Epoch 4393, Train Loss: 0.7607, Validation Loss: 1.2646, Validation F1: 0.9020\n",
      "Epoch 4394, Train Loss: 0.7605, Validation Loss: 1.2657, Validation F1: 0.9058\n",
      "Epoch 4395, Train Loss: 0.7604, Validation Loss: 1.2945, Validation F1: 0.9038\n",
      "Epoch 4396, Train Loss: 0.7608, Validation Loss: 1.2351, Validation F1: 0.9012\n",
      "Epoch 4397, Train Loss: 0.7601, Validation Loss: 1.2530, Validation F1: 0.9020\n",
      "Epoch 4398, Train Loss: 0.7607, Validation Loss: 1.2742, Validation F1: 0.9041\n",
      "Epoch 4399, Train Loss: 0.7620, Validation Loss: 1.2645, Validation F1: 0.9031\n",
      "Epoch 4400, Train Loss: 0.7611, Validation Loss: 1.2007, Validation F1: 0.9020\n",
      "Epoch 4401, Train Loss: 0.7600, Validation Loss: 1.2679, Validation F1: 0.9031\n",
      "Epoch 4402, Train Loss: 0.7609, Validation Loss: 1.2474, Validation F1: 0.9017\n",
      "Epoch 4403, Train Loss: 0.7601, Validation Loss: 1.2206, Validation F1: 0.9006\n",
      "Epoch 4404, Train Loss: 0.7612, Validation Loss: 1.2735, Validation F1: 0.9019\n",
      "Epoch 4405, Train Loss: 0.7591, Validation Loss: 1.2968, Validation F1: 0.9036\n",
      "Epoch 4406, Train Loss: 0.7603, Validation Loss: 1.3028, Validation F1: 0.9056\n",
      "Epoch 4407, Train Loss: 0.7625, Validation Loss: 1.2871, Validation F1: 0.9029\n",
      "Epoch 4408, Train Loss: 0.7596, Validation Loss: 1.2743, Validation F1: 0.9028\n",
      "Epoch 4409, Train Loss: 0.7627, Validation Loss: 1.2411, Validation F1: 0.9030\n",
      "Epoch 4410, Train Loss: 0.7609, Validation Loss: 1.2562, Validation F1: 0.9026\n",
      "Epoch 4411, Train Loss: 0.7615, Validation Loss: 1.2627, Validation F1: 0.9037\n",
      "Epoch 4412, Train Loss: 0.7605, Validation Loss: 1.2200, Validation F1: 0.9043\n",
      "Epoch 4413, Train Loss: 0.7606, Validation Loss: 1.2626, Validation F1: 0.9014\n",
      "Epoch 4414, Train Loss: 0.7612, Validation Loss: 1.2455, Validation F1: 0.9026\n",
      "Epoch 4415, Train Loss: 0.7606, Validation Loss: 1.2248, Validation F1: 0.9042\n",
      "Epoch 4416, Train Loss: 0.7616, Validation Loss: 1.2464, Validation F1: 0.9061\n",
      "Epoch 4417, Train Loss: 0.7605, Validation Loss: 1.2542, Validation F1: 0.9031\n",
      "Epoch 4418, Train Loss: 0.7630, Validation Loss: 1.2492, Validation F1: 0.9015\n",
      "Epoch 4419, Train Loss: 0.7613, Validation Loss: 1.2691, Validation F1: 0.9021\n",
      "Epoch 4420, Train Loss: 0.7616, Validation Loss: 1.2737, Validation F1: 0.9026\n",
      "Epoch 4421, Train Loss: 0.7620, Validation Loss: 1.2411, Validation F1: 0.9047\n",
      "Epoch 4422, Train Loss: 0.7621, Validation Loss: 1.2520, Validation F1: 0.9066\n",
      "Epoch 4423, Train Loss: 0.7624, Validation Loss: 1.2291, Validation F1: 0.9018\n",
      "Epoch 4424, Train Loss: 0.7597, Validation Loss: 1.2217, Validation F1: 0.8995\n",
      "Epoch 4425, Train Loss: 0.7608, Validation Loss: 1.2837, Validation F1: 0.9015\n",
      "Epoch 4426, Train Loss: 0.7584, Validation Loss: 1.2651, Validation F1: 0.9062\n",
      "Epoch 4427, Train Loss: 0.7621, Validation Loss: 1.2330, Validation F1: 0.9050\n",
      "Epoch 4428, Train Loss: 0.7630, Validation Loss: 1.2600, Validation F1: 0.9014\n",
      "Epoch 4429, Train Loss: 0.7604, Validation Loss: 1.2420, Validation F1: 0.8991\n",
      "Epoch 4430, Train Loss: 0.7634, Validation Loss: 1.2877, Validation F1: 0.9019\n",
      "Epoch 4431, Train Loss: 0.7611, Validation Loss: 1.2979, Validation F1: 0.9048\n",
      "Epoch 4432, Train Loss: 0.7626, Validation Loss: 1.2315, Validation F1: 0.9062\n",
      "Epoch 4433, Train Loss: 0.7609, Validation Loss: 1.2267, Validation F1: 0.9043\n",
      "Epoch 4434, Train Loss: 0.7604, Validation Loss: 1.2138, Validation F1: 0.9012\n",
      "Epoch 4435, Train Loss: 0.7607, Validation Loss: 1.2872, Validation F1: 0.9015\n",
      "Epoch 4436, Train Loss: 0.7604, Validation Loss: 1.2785, Validation F1: 0.9030\n",
      "Epoch 4437, Train Loss: 0.7604, Validation Loss: 1.2814, Validation F1: 0.9056\n",
      "Epoch 4438, Train Loss: 0.7606, Validation Loss: 1.2781, Validation F1: 0.9051\n",
      "Epoch 4439, Train Loss: 0.7600, Validation Loss: 1.2479, Validation F1: 0.9016\n",
      "Epoch 4440, Train Loss: 0.7591, Validation Loss: 1.2820, Validation F1: 0.9020\n",
      "Epoch 4441, Train Loss: 0.7612, Validation Loss: 1.2689, Validation F1: 0.9058\n",
      "Epoch 4442, Train Loss: 0.7606, Validation Loss: 1.2856, Validation F1: 0.9041\n",
      "Epoch 4443, Train Loss: 0.7601, Validation Loss: 1.2283, Validation F1: 0.9029\n",
      "Epoch 4444, Train Loss: 0.7593, Validation Loss: 1.3179, Validation F1: 0.9018\n",
      "Epoch 4445, Train Loss: 0.7602, Validation Loss: 1.2804, Validation F1: 0.9010\n",
      "Epoch 4446, Train Loss: 0.7607, Validation Loss: 1.2649, Validation F1: 0.9034\n",
      "Epoch 4447, Train Loss: 0.7611, Validation Loss: 1.2886, Validation F1: 0.9063\n",
      "Epoch 4448, Train Loss: 0.7605, Validation Loss: 1.2906, Validation F1: 0.9057\n",
      "Epoch 4449, Train Loss: 0.7600, Validation Loss: 1.2926, Validation F1: 0.9015\n",
      "Epoch 4450, Train Loss: 0.7599, Validation Loss: 1.2463, Validation F1: 0.9027\n",
      "Epoch 4451, Train Loss: 0.7609, Validation Loss: 1.2716, Validation F1: 0.9007\n",
      "Epoch 4452, Train Loss: 0.7585, Validation Loss: 1.2472, Validation F1: 0.9048\n",
      "Epoch 4453, Train Loss: 0.7597, Validation Loss: 1.2759, Validation F1: 0.9047\n",
      "Epoch 4454, Train Loss: 0.7606, Validation Loss: 1.2605, Validation F1: 0.9027\n",
      "Epoch 4455, Train Loss: 0.7616, Validation Loss: 1.2586, Validation F1: 0.9041\n",
      "Epoch 4456, Train Loss: 0.7601, Validation Loss: 1.2780, Validation F1: 0.9039\n",
      "Epoch 4457, Train Loss: 0.7596, Validation Loss: 1.2764, Validation F1: 0.9055\n",
      "Epoch 4458, Train Loss: 0.7601, Validation Loss: 1.2556, Validation F1: 0.9060\n",
      "Epoch 4459, Train Loss: 0.7593, Validation Loss: 1.2472, Validation F1: 0.9043\n",
      "Epoch 4460, Train Loss: 0.7589, Validation Loss: 1.2580, Validation F1: 0.9033\n",
      "Epoch 4461, Train Loss: 0.7602, Validation Loss: 1.2948, Validation F1: 0.9027\n",
      "Epoch 4462, Train Loss: 0.7597, Validation Loss: 1.2633, Validation F1: 0.9024\n",
      "Epoch 4463, Train Loss: 0.7592, Validation Loss: 1.2640, Validation F1: 0.9012\n",
      "Epoch 4464, Train Loss: 0.7604, Validation Loss: 1.2590, Validation F1: 0.9051\n",
      "Epoch 4465, Train Loss: 0.7622, Validation Loss: 1.2575, Validation F1: 0.9028\n",
      "Epoch 4466, Train Loss: 0.7594, Validation Loss: 1.2955, Validation F1: 0.9025\n",
      "Epoch 4467, Train Loss: 0.7603, Validation Loss: 1.2675, Validation F1: 0.9032\n",
      "Epoch 4468, Train Loss: 0.7608, Validation Loss: 1.2762, Validation F1: 0.9060\n",
      "Epoch 4469, Train Loss: 0.7618, Validation Loss: 1.2509, Validation F1: 0.9055\n",
      "Epoch 4470, Train Loss: 0.7620, Validation Loss: 1.2179, Validation F1: 0.9036\n",
      "Epoch 4471, Train Loss: 0.7597, Validation Loss: 1.2364, Validation F1: 0.9051\n",
      "Epoch 4472, Train Loss: 0.7623, Validation Loss: 1.2194, Validation F1: 0.9051\n",
      "Epoch 4473, Train Loss: 0.7618, Validation Loss: 1.2409, Validation F1: 0.9063\n",
      "Epoch 4474, Train Loss: 0.7609, Validation Loss: 1.1850, Validation F1: 0.9059\n",
      "Epoch 4475, Train Loss: 0.7661, Validation Loss: 1.2162, Validation F1: 0.8989\n",
      "Epoch 4476, Train Loss: 0.7612, Validation Loss: 1.3116, Validation F1: 0.8934\n",
      "Epoch 4477, Train Loss: 0.7634, Validation Loss: 1.2694, Validation F1: 0.9030\n",
      "Epoch 4478, Train Loss: 0.7595, Validation Loss: 1.2460, Validation F1: 0.9040\n",
      "Epoch 4479, Train Loss: 0.7619, Validation Loss: 1.2877, Validation F1: 0.9063\n",
      "Epoch 4480, Train Loss: 0.7624, Validation Loss: 1.2525, Validation F1: 0.8991\n",
      "Epoch 4481, Train Loss: 0.7598, Validation Loss: 1.2653, Validation F1: 0.9024\n",
      "Epoch 4482, Train Loss: 0.7607, Validation Loss: 1.3044, Validation F1: 0.9030\n",
      "Epoch 4483, Train Loss: 0.7590, Validation Loss: 1.2270, Validation F1: 0.9042\n",
      "Epoch 4484, Train Loss: 0.7601, Validation Loss: 1.2527, Validation F1: 0.9025\n",
      "Epoch 4485, Train Loss: 0.7588, Validation Loss: 1.2237, Validation F1: 0.9048\n",
      "Epoch 4486, Train Loss: 0.7606, Validation Loss: 1.3036, Validation F1: 0.9016\n",
      "Epoch 4487, Train Loss: 0.7600, Validation Loss: 1.2825, Validation F1: 0.9019\n",
      "Epoch 4488, Train Loss: 0.7617, Validation Loss: 1.2464, Validation F1: 0.9018\n",
      "Epoch 4489, Train Loss: 0.7586, Validation Loss: 1.2691, Validation F1: 0.9033\n",
      "Epoch 4490, Train Loss: 0.7602, Validation Loss: 1.2987, Validation F1: 0.9030\n",
      "Epoch 4491, Train Loss: 0.7594, Validation Loss: 1.2850, Validation F1: 0.9031\n",
      "Epoch 4492, Train Loss: 0.7606, Validation Loss: 1.2909, Validation F1: 0.9065\n",
      "Epoch 4493, Train Loss: 0.7601, Validation Loss: 1.2567, Validation F1: 0.9050\n",
      "Epoch 4494, Train Loss: 0.7602, Validation Loss: 1.2132, Validation F1: 0.9024\n",
      "Epoch 4495, Train Loss: 0.7610, Validation Loss: 1.2674, Validation F1: 0.9010\n",
      "Epoch 4496, Train Loss: 0.7615, Validation Loss: 1.2500, Validation F1: 0.9021\n",
      "Epoch 4497, Train Loss: 0.7601, Validation Loss: 1.2565, Validation F1: 0.9040\n",
      "Epoch 4498, Train Loss: 0.7598, Validation Loss: 1.2806, Validation F1: 0.9021\n",
      "Epoch 4499, Train Loss: 0.7594, Validation Loss: 1.2144, Validation F1: 0.9049\n",
      "Epoch 4500, Train Loss: 0.7596, Validation Loss: 1.2708, Validation F1: 0.9034\n",
      "Epoch 4501, Train Loss: 0.7596, Validation Loss: 1.2650, Validation F1: 0.9013\n",
      "Epoch 4502, Train Loss: 0.7596, Validation Loss: 1.2717, Validation F1: 0.9015\n",
      "Epoch 4503, Train Loss: 0.7590, Validation Loss: 1.2435, Validation F1: 0.9054\n",
      "Epoch 4504, Train Loss: 0.7592, Validation Loss: 1.2866, Validation F1: 0.9044\n",
      "Epoch 4505, Train Loss: 0.7592, Validation Loss: 1.2436, Validation F1: 0.9042\n",
      "Epoch 4506, Train Loss: 0.7600, Validation Loss: 1.2571, Validation F1: 0.9052\n",
      "Epoch 4507, Train Loss: 0.7600, Validation Loss: 1.2883, Validation F1: 0.9010\n",
      "Epoch 4508, Train Loss: 0.7599, Validation Loss: 1.2773, Validation F1: 0.9037\n",
      "Epoch 4509, Train Loss: 0.7588, Validation Loss: 1.2562, Validation F1: 0.9057\n",
      "Epoch 4510, Train Loss: 0.7588, Validation Loss: 1.2225, Validation F1: 0.9068\n",
      "Epoch 4511, Train Loss: 0.7586, Validation Loss: 1.2560, Validation F1: 0.9063\n",
      "Epoch 4512, Train Loss: 0.7587, Validation Loss: 1.2400, Validation F1: 0.9034\n",
      "Epoch 4513, Train Loss: 0.7592, Validation Loss: 1.2457, Validation F1: 0.9024\n",
      "Epoch 4514, Train Loss: 0.7593, Validation Loss: 1.2280, Validation F1: 0.8989\n",
      "Epoch 4515, Train Loss: 0.7585, Validation Loss: 1.2858, Validation F1: 0.9043\n",
      "Epoch 4516, Train Loss: 0.7588, Validation Loss: 1.2480, Validation F1: 0.9032\n",
      "Epoch 4517, Train Loss: 0.7580, Validation Loss: 1.2732, Validation F1: 0.9053\n",
      "Epoch 4518, Train Loss: 0.7589, Validation Loss: 1.2236, Validation F1: 0.9046\n",
      "Epoch 4519, Train Loss: 0.7595, Validation Loss: 1.2747, Validation F1: 0.9042\n",
      "Epoch 4520, Train Loss: 0.7585, Validation Loss: 1.2606, Validation F1: 0.9035\n",
      "Epoch 4521, Train Loss: 0.7584, Validation Loss: 1.2336, Validation F1: 0.9036\n",
      "Epoch 4522, Train Loss: 0.7591, Validation Loss: 1.2406, Validation F1: 0.9019\n",
      "Epoch 4523, Train Loss: 0.7593, Validation Loss: 1.2449, Validation F1: 0.9025\n",
      "Epoch 4524, Train Loss: 0.7584, Validation Loss: 1.2583, Validation F1: 0.9051\n",
      "Epoch 4525, Train Loss: 0.7587, Validation Loss: 1.2968, Validation F1: 0.9059\n",
      "Epoch 4526, Train Loss: 0.7583, Validation Loss: 1.2434, Validation F1: 0.9009\n",
      "Epoch 4527, Train Loss: 0.7583, Validation Loss: 1.2382, Validation F1: 0.9031\n",
      "Epoch 4528, Train Loss: 0.7578, Validation Loss: 1.2402, Validation F1: 0.9021\n",
      "Epoch 4529, Train Loss: 0.7588, Validation Loss: 1.2461, Validation F1: 0.9032\n",
      "Epoch 4530, Train Loss: 0.7589, Validation Loss: 1.2820, Validation F1: 0.9031\n",
      "Epoch 4531, Train Loss: 0.7587, Validation Loss: 1.2531, Validation F1: 0.9039\n",
      "Epoch 4532, Train Loss: 0.7586, Validation Loss: 1.2380, Validation F1: 0.9032\n",
      "Epoch 4533, Train Loss: 0.7593, Validation Loss: 1.2612, Validation F1: 0.9028\n",
      "Epoch 4534, Train Loss: 0.7583, Validation Loss: 1.2305, Validation F1: 0.9036\n",
      "Epoch 4535, Train Loss: 0.7602, Validation Loss: 1.2751, Validation F1: 0.9055\n",
      "Epoch 4536, Train Loss: 0.7591, Validation Loss: 1.2638, Validation F1: 0.9047\n",
      "Epoch 4537, Train Loss: 0.7587, Validation Loss: 1.2845, Validation F1: 0.9041\n",
      "Epoch 4538, Train Loss: 0.7589, Validation Loss: 1.2327, Validation F1: 0.9028\n",
      "Epoch 4539, Train Loss: 0.7592, Validation Loss: 1.2835, Validation F1: 0.9028\n",
      "Epoch 4540, Train Loss: 0.7585, Validation Loss: 1.2604, Validation F1: 0.9040\n",
      "Epoch 4541, Train Loss: 0.7584, Validation Loss: 1.2760, Validation F1: 0.9013\n",
      "Epoch 4542, Train Loss: 0.7585, Validation Loss: 1.2928, Validation F1: 0.9029\n",
      "Epoch 4543, Train Loss: 0.7585, Validation Loss: 1.2986, Validation F1: 0.9025\n",
      "Epoch 4544, Train Loss: 0.7599, Validation Loss: 1.2945, Validation F1: 0.9026\n",
      "Epoch 4545, Train Loss: 0.7587, Validation Loss: 1.2068, Validation F1: 0.9062\n",
      "Epoch 4546, Train Loss: 0.7587, Validation Loss: 1.2705, Validation F1: 0.9037\n",
      "Epoch 4547, Train Loss: 0.7584, Validation Loss: 1.2935, Validation F1: 0.9018\n",
      "Epoch 4548, Train Loss: 0.7584, Validation Loss: 1.2722, Validation F1: 0.9033\n",
      "Epoch 4549, Train Loss: 0.7601, Validation Loss: 1.2896, Validation F1: 0.9050\n",
      "Epoch 4550, Train Loss: 0.7587, Validation Loss: 1.2902, Validation F1: 0.9045\n",
      "Epoch 4551, Train Loss: 0.7581, Validation Loss: 1.2942, Validation F1: 0.9035\n",
      "Epoch 4552, Train Loss: 0.7587, Validation Loss: 1.2756, Validation F1: 0.9039\n",
      "Epoch 4553, Train Loss: 0.7600, Validation Loss: 1.2813, Validation F1: 0.9050\n",
      "Epoch 4554, Train Loss: 0.7587, Validation Loss: 1.2970, Validation F1: 0.9038\n",
      "Epoch 4555, Train Loss: 0.7587, Validation Loss: 1.2812, Validation F1: 0.9019\n",
      "Epoch 4556, Train Loss: 0.7586, Validation Loss: 1.2572, Validation F1: 0.9049\n",
      "Epoch 4557, Train Loss: 0.7590, Validation Loss: 1.2141, Validation F1: 0.9056\n",
      "Epoch 4558, Train Loss: 0.7592, Validation Loss: 1.2302, Validation F1: 0.9064\n",
      "Epoch 4559, Train Loss: 0.7598, Validation Loss: 1.2655, Validation F1: 0.9038\n",
      "Epoch 4560, Train Loss: 0.7593, Validation Loss: 1.3138, Validation F1: 0.9021\n",
      "Epoch 4561, Train Loss: 0.7595, Validation Loss: 1.2594, Validation F1: 0.9031\n",
      "Epoch 4562, Train Loss: 0.7599, Validation Loss: 1.2672, Validation F1: 0.9031\n",
      "Epoch 4563, Train Loss: 0.7591, Validation Loss: 1.2744, Validation F1: 0.9023\n",
      "Epoch 4564, Train Loss: 0.7591, Validation Loss: 1.2915, Validation F1: 0.9051\n",
      "Epoch 4565, Train Loss: 0.7592, Validation Loss: 1.2559, Validation F1: 0.9022\n",
      "Epoch 4566, Train Loss: 0.7595, Validation Loss: 1.1990, Validation F1: 0.9021\n",
      "Epoch 4567, Train Loss: 0.7596, Validation Loss: 1.2265, Validation F1: 0.9041\n",
      "Epoch 4568, Train Loss: 0.7592, Validation Loss: 1.2542, Validation F1: 0.9039\n",
      "Epoch 4569, Train Loss: 0.7597, Validation Loss: 1.2566, Validation F1: 0.9040\n",
      "Epoch 4570, Train Loss: 0.7596, Validation Loss: 1.2630, Validation F1: 0.9033\n",
      "Epoch 4571, Train Loss: 0.7589, Validation Loss: 1.2611, Validation F1: 0.9065\n",
      "Epoch 4572, Train Loss: 0.7590, Validation Loss: 1.2440, Validation F1: 0.9057\n",
      "Epoch 4573, Train Loss: 0.7599, Validation Loss: 1.2850, Validation F1: 0.9027\n",
      "Epoch 4574, Train Loss: 0.7614, Validation Loss: 1.2539, Validation F1: 0.8996\n",
      "Epoch 4575, Train Loss: 0.7591, Validation Loss: 1.2854, Validation F1: 0.9029\n",
      "Epoch 4576, Train Loss: 0.7609, Validation Loss: 1.2802, Validation F1: 0.9030\n",
      "Epoch 4577, Train Loss: 0.7582, Validation Loss: 1.2483, Validation F1: 0.9053\n",
      "Epoch 4578, Train Loss: 0.7594, Validation Loss: 1.2174, Validation F1: 0.9017\n",
      "Epoch 4579, Train Loss: 0.7589, Validation Loss: 1.2249, Validation F1: 0.9015\n",
      "Epoch 4580, Train Loss: 0.7587, Validation Loss: 1.2773, Validation F1: 0.9027\n",
      "Epoch 4581, Train Loss: 0.7600, Validation Loss: 1.2824, Validation F1: 0.9050\n",
      "Epoch 4582, Train Loss: 0.7593, Validation Loss: 1.2470, Validation F1: 0.9065\n",
      "Epoch 4583, Train Loss: 0.7590, Validation Loss: 1.2613, Validation F1: 0.9020\n",
      "Epoch 4584, Train Loss: 0.7599, Validation Loss: 1.2528, Validation F1: 0.8986\n",
      "Epoch 4585, Train Loss: 0.7599, Validation Loss: 1.2396, Validation F1: 0.9023\n",
      "Epoch 4586, Train Loss: 0.7587, Validation Loss: 1.3013, Validation F1: 0.9056\n",
      "Epoch 4587, Train Loss: 0.7588, Validation Loss: 1.2620, Validation F1: 0.9050\n",
      "Epoch 4588, Train Loss: 0.7589, Validation Loss: 1.2521, Validation F1: 0.9017\n",
      "Epoch 4589, Train Loss: 0.7606, Validation Loss: 1.2771, Validation F1: 0.8997\n",
      "Epoch 4590, Train Loss: 0.7586, Validation Loss: 1.3197, Validation F1: 0.9039\n",
      "Epoch 4591, Train Loss: 0.7586, Validation Loss: 1.2621, Validation F1: 0.9056\n",
      "Epoch 4592, Train Loss: 0.7589, Validation Loss: 1.2835, Validation F1: 0.9064\n",
      "Epoch 4593, Train Loss: 0.7590, Validation Loss: 1.2537, Validation F1: 0.9047\n",
      "Epoch 4594, Train Loss: 0.7604, Validation Loss: 1.2732, Validation F1: 0.9020\n",
      "Epoch 4595, Train Loss: 0.7589, Validation Loss: 1.2777, Validation F1: 0.8994\n",
      "Epoch 4596, Train Loss: 0.7590, Validation Loss: 1.2793, Validation F1: 0.9021\n",
      "Epoch 4597, Train Loss: 0.7583, Validation Loss: 1.2696, Validation F1: 0.9072\n",
      "Epoch 4598, Train Loss: 0.7613, Validation Loss: 1.2325, Validation F1: 0.9057\n",
      "Epoch 4599, Train Loss: 0.7636, Validation Loss: 1.2062, Validation F1: 0.9023\n",
      "Epoch 4600, Train Loss: 0.7611, Validation Loss: 1.2809, Validation F1: 0.9071\n",
      "Epoch 4601, Train Loss: 0.7620, Validation Loss: 1.2609, Validation F1: 0.8983\n",
      "Epoch 4602, Train Loss: 0.7610, Validation Loss: 1.2686, Validation F1: 0.9005\n",
      "Epoch 4603, Train Loss: 0.7611, Validation Loss: 1.2942, Validation F1: 0.9009\n",
      "Epoch 4604, Train Loss: 0.7624, Validation Loss: 1.2885, Validation F1: 0.9037\n",
      "Epoch 4605, Train Loss: 0.7604, Validation Loss: 1.2941, Validation F1: 0.9046\n",
      "Epoch 4606, Train Loss: 0.7609, Validation Loss: 1.2764, Validation F1: 0.9073\n",
      "Epoch 4607, Train Loss: 0.7641, Validation Loss: 1.2375, Validation F1: 0.9027\n",
      "Epoch 4608, Train Loss: 0.7613, Validation Loss: 1.2247, Validation F1: 0.8999\n",
      "Epoch 4609, Train Loss: 0.7661, Validation Loss: 1.2483, Validation F1: 0.9031\n",
      "Epoch 4610, Train Loss: 0.7636, Validation Loss: 1.2789, Validation F1: 0.9034\n",
      "Epoch 4611, Train Loss: 0.7640, Validation Loss: 1.3009, Validation F1: 0.9035\n",
      "Epoch 4612, Train Loss: 0.7646, Validation Loss: 1.2260, Validation F1: 0.9020\n",
      "Epoch 4613, Train Loss: 0.7618, Validation Loss: 1.2295, Validation F1: 0.9049\n",
      "Epoch 4614, Train Loss: 0.7617, Validation Loss: 1.2488, Validation F1: 0.9033\n",
      "Epoch 4615, Train Loss: 0.7627, Validation Loss: 1.3260, Validation F1: 0.9015\n",
      "Epoch 4616, Train Loss: 0.7618, Validation Loss: 1.2492, Validation F1: 0.9030\n",
      "Epoch 4617, Train Loss: 0.7624, Validation Loss: 1.3144, Validation F1: 0.9037\n",
      "Epoch 4618, Train Loss: 0.7622, Validation Loss: 1.2939, Validation F1: 0.9049\n",
      "Epoch 4619, Train Loss: 0.7611, Validation Loss: 1.2911, Validation F1: 0.9013\n",
      "Epoch 4620, Train Loss: 0.7625, Validation Loss: 1.2657, Validation F1: 0.9032\n",
      "Epoch 4621, Train Loss: 0.7604, Validation Loss: 1.2890, Validation F1: 0.9016\n",
      "Epoch 4622, Train Loss: 0.7620, Validation Loss: 1.2279, Validation F1: 0.9011\n",
      "Epoch 4623, Train Loss: 0.7599, Validation Loss: 1.2260, Validation F1: 0.9009\n",
      "Epoch 4624, Train Loss: 0.7590, Validation Loss: 1.2227, Validation F1: 0.9019\n",
      "Epoch 4625, Train Loss: 0.7604, Validation Loss: 1.1578, Validation F1: 0.9070\n",
      "Epoch 4626, Train Loss: 0.7601, Validation Loss: 1.2326, Validation F1: 0.9069\n",
      "Epoch 4627, Train Loss: 0.7597, Validation Loss: 1.2802, Validation F1: 0.9047\n",
      "Epoch 4628, Train Loss: 0.7610, Validation Loss: 1.2550, Validation F1: 0.8986\n",
      "Epoch 4629, Train Loss: 0.7603, Validation Loss: 1.2875, Validation F1: 0.8939\n",
      "Epoch 4630, Train Loss: 0.7604, Validation Loss: 1.2949, Validation F1: 0.9026\n",
      "Epoch 4631, Train Loss: 0.7612, Validation Loss: 1.2643, Validation F1: 0.9023\n",
      "Epoch 4632, Train Loss: 0.7595, Validation Loss: 1.2542, Validation F1: 0.9035\n",
      "Epoch 4633, Train Loss: 0.7607, Validation Loss: 1.2611, Validation F1: 0.9061\n",
      "Epoch 4634, Train Loss: 0.7596, Validation Loss: 1.3240, Validation F1: 0.9038\n",
      "Epoch 4635, Train Loss: 0.7594, Validation Loss: 1.2798, Validation F1: 0.9064\n",
      "Epoch 4636, Train Loss: 0.7606, Validation Loss: 1.2618, Validation F1: 0.9056\n",
      "Epoch 4637, Train Loss: 0.7604, Validation Loss: 1.3114, Validation F1: 0.9039\n",
      "Epoch 4638, Train Loss: 0.7603, Validation Loss: 1.2405, Validation F1: 0.8988\n",
      "Epoch 4639, Train Loss: 0.7607, Validation Loss: 1.2521, Validation F1: 0.8982\n",
      "Epoch 4640, Train Loss: 0.7606, Validation Loss: 1.2710, Validation F1: 0.9014\n",
      "Epoch 4641, Train Loss: 0.7594, Validation Loss: 1.2872, Validation F1: 0.9052\n",
      "Epoch 4642, Train Loss: 0.7601, Validation Loss: 1.2189, Validation F1: 0.9056\n",
      "Epoch 4643, Train Loss: 0.7600, Validation Loss: 1.2077, Validation F1: 0.9023\n",
      "Epoch 4644, Train Loss: 0.7602, Validation Loss: 1.2398, Validation F1: 0.9037\n",
      "Epoch 4645, Train Loss: 0.7605, Validation Loss: 1.2225, Validation F1: 0.9021\n",
      "Epoch 4646, Train Loss: 0.7602, Validation Loss: 1.2965, Validation F1: 0.9031\n",
      "Epoch 4647, Train Loss: 0.7604, Validation Loss: 1.2175, Validation F1: 0.9045\n",
      "Epoch 4648, Train Loss: 0.7608, Validation Loss: 1.2630, Validation F1: 0.9063\n",
      "Epoch 4649, Train Loss: 0.7591, Validation Loss: 1.3125, Validation F1: 0.9046\n",
      "Epoch 4650, Train Loss: 0.7596, Validation Loss: 1.2990, Validation F1: 0.9019\n",
      "Epoch 4651, Train Loss: 0.7599, Validation Loss: 1.2750, Validation F1: 0.9029\n",
      "Epoch 4652, Train Loss: 0.7598, Validation Loss: 1.2500, Validation F1: 0.9056\n",
      "Epoch 4653, Train Loss: 0.7590, Validation Loss: 1.2521, Validation F1: 0.9017\n",
      "Epoch 4654, Train Loss: 0.7590, Validation Loss: 1.2225, Validation F1: 0.9058\n",
      "Epoch 4655, Train Loss: 0.7608, Validation Loss: 1.2527, Validation F1: 0.9062\n",
      "Epoch 4656, Train Loss: 0.7611, Validation Loss: 1.2920, Validation F1: 0.9035\n",
      "Epoch 4657, Train Loss: 0.7600, Validation Loss: 1.2595, Validation F1: 0.9022\n",
      "Epoch 4658, Train Loss: 0.7624, Validation Loss: 1.2651, Validation F1: 0.9034\n",
      "Epoch 4659, Train Loss: 0.7607, Validation Loss: 1.2817, Validation F1: 0.9052\n",
      "Epoch 4660, Train Loss: 0.7608, Validation Loss: 1.2306, Validation F1: 0.9049\n",
      "Epoch 4661, Train Loss: 0.7600, Validation Loss: 1.2508, Validation F1: 0.9047\n",
      "Epoch 4662, Train Loss: 0.7627, Validation Loss: 1.2864, Validation F1: 0.9018\n",
      "Epoch 4663, Train Loss: 0.7601, Validation Loss: 1.2464, Validation F1: 0.9017\n",
      "Epoch 4664, Train Loss: 0.7594, Validation Loss: 1.2478, Validation F1: 0.9027\n",
      "Epoch 4665, Train Loss: 0.7631, Validation Loss: 1.2797, Validation F1: 0.9033\n",
      "Epoch 4666, Train Loss: 0.7658, Validation Loss: 1.2587, Validation F1: 0.9055\n",
      "Epoch 4667, Train Loss: 0.7635, Validation Loss: 1.2127, Validation F1: 0.9071\n",
      "Epoch 4668, Train Loss: 0.7634, Validation Loss: 1.2876, Validation F1: 0.9052\n",
      "Epoch 4669, Train Loss: 0.7628, Validation Loss: 1.2615, Validation F1: 0.9046\n",
      "Epoch 4670, Train Loss: 0.7636, Validation Loss: 1.2885, Validation F1: 0.9018\n",
      "Epoch 4671, Train Loss: 0.7630, Validation Loss: 1.2697, Validation F1: 0.8947\n",
      "Epoch 4672, Train Loss: 0.7622, Validation Loss: 1.2814, Validation F1: 0.8981\n",
      "Epoch 4673, Train Loss: 0.7649, Validation Loss: 1.2692, Validation F1: 0.9055\n",
      "Epoch 4674, Train Loss: 0.7626, Validation Loss: 1.2252, Validation F1: 0.9052\n",
      "Epoch 4675, Train Loss: 0.7626, Validation Loss: 1.2607, Validation F1: 0.9034\n",
      "Epoch 4676, Train Loss: 0.7613, Validation Loss: 1.2804, Validation F1: 0.9024\n",
      "Epoch 4677, Train Loss: 0.7638, Validation Loss: 1.2400, Validation F1: 0.8981\n",
      "Epoch 4678, Train Loss: 0.7617, Validation Loss: 1.2527, Validation F1: 0.8957\n",
      "Epoch 4679, Train Loss: 0.7630, Validation Loss: 1.2538, Validation F1: 0.9017\n",
      "Epoch 4680, Train Loss: 0.7607, Validation Loss: 1.2620, Validation F1: 0.9061\n",
      "Epoch 4681, Train Loss: 0.7611, Validation Loss: 1.2965, Validation F1: 0.9057\n",
      "Epoch 4682, Train Loss: 0.7625, Validation Loss: 1.3086, Validation F1: 0.9012\n",
      "Epoch 4683, Train Loss: 0.7596, Validation Loss: 1.3176, Validation F1: 0.9027\n",
      "Epoch 4684, Train Loss: 0.7609, Validation Loss: 1.2401, Validation F1: 0.9040\n",
      "Epoch 4685, Train Loss: 0.7600, Validation Loss: 1.2743, Validation F1: 0.9058\n",
      "Epoch 4686, Train Loss: 0.7601, Validation Loss: 1.2859, Validation F1: 0.9034\n",
      "Epoch 4687, Train Loss: 0.7599, Validation Loss: 1.2539, Validation F1: 0.9030\n",
      "Epoch 4688, Train Loss: 0.7605, Validation Loss: 1.3056, Validation F1: 0.9039\n",
      "Epoch 4689, Train Loss: 0.7610, Validation Loss: 1.2480, Validation F1: 0.9080\n",
      "Epoch 4690, Train Loss: 0.7597, Validation Loss: 1.2774, Validation F1: 0.9035\n",
      "Epoch 4691, Train Loss: 0.7608, Validation Loss: 1.2474, Validation F1: 0.8984\n",
      "Epoch 4692, Train Loss: 0.7594, Validation Loss: 1.2905, Validation F1: 0.8997\n",
      "Epoch 4693, Train Loss: 0.7592, Validation Loss: 1.3213, Validation F1: 0.9035\n",
      "Epoch 4694, Train Loss: 0.7592, Validation Loss: 1.2706, Validation F1: 0.9059\n",
      "Epoch 4695, Train Loss: 0.7595, Validation Loss: 1.2338, Validation F1: 0.9024\n",
      "Epoch 4696, Train Loss: 0.7592, Validation Loss: 1.2991, Validation F1: 0.8996\n",
      "Epoch 4697, Train Loss: 0.7593, Validation Loss: 1.2589, Validation F1: 0.9049\n",
      "Epoch 4698, Train Loss: 0.7605, Validation Loss: 1.2630, Validation F1: 0.9031\n",
      "Epoch 4699, Train Loss: 0.7591, Validation Loss: 1.2005, Validation F1: 0.9031\n",
      "Epoch 4700, Train Loss: 0.7582, Validation Loss: 1.3179, Validation F1: 0.9041\n",
      "Epoch 4701, Train Loss: 0.7600, Validation Loss: 1.2968, Validation F1: 0.9054\n",
      "Epoch 4702, Train Loss: 0.7599, Validation Loss: 1.2067, Validation F1: 0.9067\n",
      "Epoch 4703, Train Loss: 0.7590, Validation Loss: 1.2435, Validation F1: 0.9056\n",
      "Epoch 4704, Train Loss: 0.7589, Validation Loss: 1.2522, Validation F1: 0.9013\n",
      "Epoch 4705, Train Loss: 0.7584, Validation Loss: 1.1879, Validation F1: 0.9022\n",
      "Epoch 4706, Train Loss: 0.7590, Validation Loss: 1.3241, Validation F1: 0.9030\n",
      "Epoch 4707, Train Loss: 0.7588, Validation Loss: 1.2546, Validation F1: 0.9026\n",
      "Epoch 4708, Train Loss: 0.7598, Validation Loss: 1.3180, Validation F1: 0.9041\n",
      "Epoch 4709, Train Loss: 0.7597, Validation Loss: 1.2744, Validation F1: 0.9063\n",
      "Epoch 4710, Train Loss: 0.7579, Validation Loss: 1.2894, Validation F1: 0.9047\n",
      "Epoch 4711, Train Loss: 0.7585, Validation Loss: 1.2668, Validation F1: 0.9038\n",
      "Epoch 4712, Train Loss: 0.7589, Validation Loss: 1.2786, Validation F1: 0.9021\n",
      "Epoch 4713, Train Loss: 0.7589, Validation Loss: 1.2709, Validation F1: 0.9038\n",
      "Epoch 4714, Train Loss: 0.7578, Validation Loss: 1.2621, Validation F1: 0.9040\n",
      "Epoch 4715, Train Loss: 0.7585, Validation Loss: 1.2797, Validation F1: 0.9026\n",
      "Epoch 4716, Train Loss: 0.7584, Validation Loss: 1.2714, Validation F1: 0.9032\n",
      "Epoch 4717, Train Loss: 0.7590, Validation Loss: 1.3107, Validation F1: 0.9021\n",
      "Epoch 4718, Train Loss: 0.7584, Validation Loss: 1.2784, Validation F1: 0.9029\n",
      "Epoch 4719, Train Loss: 0.7576, Validation Loss: 1.3162, Validation F1: 0.9045\n",
      "Epoch 4720, Train Loss: 0.7578, Validation Loss: 1.2830, Validation F1: 0.9038\n",
      "Epoch 4721, Train Loss: 0.7584, Validation Loss: 1.2925, Validation F1: 0.9015\n",
      "Epoch 4722, Train Loss: 0.7582, Validation Loss: 1.2382, Validation F1: 0.9021\n",
      "Epoch 4723, Train Loss: 0.7577, Validation Loss: 1.2918, Validation F1: 0.9037\n",
      "Epoch 4724, Train Loss: 0.7583, Validation Loss: 1.2916, Validation F1: 0.9024\n",
      "Epoch 4725, Train Loss: 0.7579, Validation Loss: 1.2857, Validation F1: 0.9012\n",
      "Epoch 4726, Train Loss: 0.7579, Validation Loss: 1.2372, Validation F1: 0.9032\n",
      "Epoch 4727, Train Loss: 0.7585, Validation Loss: 1.2623, Validation F1: 0.9022\n",
      "Epoch 4728, Train Loss: 0.7582, Validation Loss: 1.2631, Validation F1: 0.9055\n",
      "Epoch 4729, Train Loss: 0.7582, Validation Loss: 1.2977, Validation F1: 0.9046\n",
      "Epoch 4730, Train Loss: 0.7590, Validation Loss: 1.2752, Validation F1: 0.9019\n",
      "Epoch 4731, Train Loss: 0.7590, Validation Loss: 1.2861, Validation F1: 0.9016\n",
      "Epoch 4732, Train Loss: 0.7581, Validation Loss: 1.2821, Validation F1: 0.9033\n",
      "Epoch 4733, Train Loss: 0.7599, Validation Loss: 1.3134, Validation F1: 0.9061\n",
      "Epoch 4734, Train Loss: 0.7593, Validation Loss: 1.2409, Validation F1: 0.9063\n",
      "Epoch 4735, Train Loss: 0.7582, Validation Loss: 1.2813, Validation F1: 0.9025\n",
      "Epoch 4736, Train Loss: 0.7600, Validation Loss: 1.3055, Validation F1: 0.9012\n",
      "Epoch 4737, Train Loss: 0.7613, Validation Loss: 1.3243, Validation F1: 0.9032\n",
      "Epoch 4738, Train Loss: 0.7606, Validation Loss: 1.2770, Validation F1: 0.9039\n",
      "Epoch 4739, Train Loss: 0.7605, Validation Loss: 1.2647, Validation F1: 0.9037\n",
      "Epoch 4740, Train Loss: 0.7590, Validation Loss: 1.3107, Validation F1: 0.9042\n",
      "Epoch 4741, Train Loss: 0.7630, Validation Loss: 1.3025, Validation F1: 0.9026\n",
      "Epoch 4742, Train Loss: 0.7592, Validation Loss: 1.2832, Validation F1: 0.9030\n",
      "Epoch 4743, Train Loss: 0.7644, Validation Loss: 1.2381, Validation F1: 0.9015\n",
      "Epoch 4744, Train Loss: 0.7647, Validation Loss: 1.3102, Validation F1: 0.9048\n",
      "Epoch 4745, Train Loss: 0.7610, Validation Loss: 1.2974, Validation F1: 0.9061\n",
      "Epoch 4746, Train Loss: 0.7693, Validation Loss: 1.3140, Validation F1: 0.9033\n",
      "Epoch 4747, Train Loss: 0.7885, Validation Loss: 1.2671, Validation F1: 0.8991\n",
      "Epoch 4748, Train Loss: 0.7837, Validation Loss: 1.2724, Validation F1: 0.8976\n",
      "Epoch 4749, Train Loss: 0.7736, Validation Loss: 1.2898, Validation F1: 0.9004\n",
      "Epoch 4750, Train Loss: 0.7769, Validation Loss: 1.2315, Validation F1: 0.9057\n",
      "Epoch 4751, Train Loss: 0.7750, Validation Loss: 1.2373, Validation F1: 0.9076\n",
      "Epoch 4752, Train Loss: 0.7729, Validation Loss: 1.1742, Validation F1: 0.9025\n",
      "Epoch 4753, Train Loss: 0.7731, Validation Loss: 1.2752, Validation F1: 0.8944\n",
      "Epoch 4754, Train Loss: 0.7728, Validation Loss: 1.2619, Validation F1: 0.9022\n",
      "Epoch 4755, Train Loss: 0.7701, Validation Loss: 1.2637, Validation F1: 0.9057\n",
      "Epoch 4756, Train Loss: 0.7690, Validation Loss: 1.2857, Validation F1: 0.9025\n",
      "Epoch 4757, Train Loss: 0.7689, Validation Loss: 1.2455, Validation F1: 0.9035\n",
      "Epoch 4758, Train Loss: 0.7666, Validation Loss: 1.2561, Validation F1: 0.8997\n",
      "Epoch 4759, Train Loss: 0.7667, Validation Loss: 1.2489, Validation F1: 0.9020\n",
      "Epoch 4760, Train Loss: 0.7703, Validation Loss: 1.3235, Validation F1: 0.9033\n",
      "Epoch 4761, Train Loss: 0.7658, Validation Loss: 1.2818, Validation F1: 0.9022\n",
      "Epoch 4762, Train Loss: 0.7681, Validation Loss: 1.2482, Validation F1: 0.9057\n",
      "Epoch 4763, Train Loss: 0.7660, Validation Loss: 1.2714, Validation F1: 0.9025\n",
      "Epoch 4764, Train Loss: 0.7670, Validation Loss: 1.2422, Validation F1: 0.9061\n",
      "Epoch 4765, Train Loss: 0.7656, Validation Loss: 1.3298, Validation F1: 0.9013\n",
      "Epoch 4766, Train Loss: 0.7687, Validation Loss: 1.2622, Validation F1: 0.8987\n",
      "Epoch 4767, Train Loss: 0.7627, Validation Loss: 1.2739, Validation F1: 0.8988\n",
      "Epoch 4768, Train Loss: 0.7724, Validation Loss: 1.2127, Validation F1: 0.8998\n",
      "Epoch 4769, Train Loss: 0.7774, Validation Loss: 1.2193, Validation F1: 0.9039\n",
      "Epoch 4770, Train Loss: 0.7773, Validation Loss: 1.2152, Validation F1: 0.9034\n",
      "Epoch 4771, Train Loss: 0.7797, Validation Loss: 1.2560, Validation F1: 0.9008\n",
      "Epoch 4772, Train Loss: 0.7698, Validation Loss: 1.2663, Validation F1: 0.9002\n",
      "Epoch 4773, Train Loss: 0.7745, Validation Loss: 1.2533, Validation F1: 0.9067\n",
      "Epoch 4774, Train Loss: 0.7715, Validation Loss: 1.2865, Validation F1: 0.9058\n",
      "Epoch 4775, Train Loss: 0.7688, Validation Loss: 1.2412, Validation F1: 0.9048\n",
      "Epoch 4776, Train Loss: 0.7677, Validation Loss: 1.2477, Validation F1: 0.9037\n",
      "Epoch 4777, Train Loss: 0.7681, Validation Loss: 1.1964, Validation F1: 0.9010\n",
      "Epoch 4778, Train Loss: 0.7676, Validation Loss: 1.3025, Validation F1: 0.9022\n",
      "Epoch 4779, Train Loss: 0.7680, Validation Loss: 1.2257, Validation F1: 0.9016\n",
      "Epoch 4780, Train Loss: 0.7662, Validation Loss: 1.2664, Validation F1: 0.9043\n",
      "Epoch 4781, Train Loss: 0.7665, Validation Loss: 1.2046, Validation F1: 0.9018\n",
      "Epoch 4782, Train Loss: 0.7655, Validation Loss: 1.2345, Validation F1: 0.9046\n",
      "Epoch 4783, Train Loss: 0.7654, Validation Loss: 1.2352, Validation F1: 0.9041\n",
      "Epoch 4784, Train Loss: 0.7657, Validation Loss: 1.1832, Validation F1: 0.9045\n",
      "Epoch 4785, Train Loss: 0.7640, Validation Loss: 1.2296, Validation F1: 0.9036\n",
      "Epoch 4786, Train Loss: 0.7644, Validation Loss: 1.3028, Validation F1: 0.9040\n",
      "Epoch 4787, Train Loss: 0.7645, Validation Loss: 1.2618, Validation F1: 0.9026\n",
      "Epoch 4788, Train Loss: 0.7646, Validation Loss: 1.2486, Validation F1: 0.9028\n",
      "Epoch 4789, Train Loss: 0.7635, Validation Loss: 1.2849, Validation F1: 0.9063\n",
      "Epoch 4790, Train Loss: 0.7628, Validation Loss: 1.2873, Validation F1: 0.9032\n",
      "Epoch 4791, Train Loss: 0.7628, Validation Loss: 1.2793, Validation F1: 0.9039\n",
      "Epoch 4792, Train Loss: 0.7617, Validation Loss: 1.2492, Validation F1: 0.9023\n",
      "Epoch 4793, Train Loss: 0.7614, Validation Loss: 1.2694, Validation F1: 0.9027\n",
      "Epoch 4794, Train Loss: 0.7627, Validation Loss: 1.2360, Validation F1: 0.9021\n",
      "Epoch 4795, Train Loss: 0.7633, Validation Loss: 1.2192, Validation F1: 0.9022\n",
      "Epoch 4796, Train Loss: 0.7614, Validation Loss: 1.2203, Validation F1: 0.9047\n",
      "Epoch 4797, Train Loss: 0.7617, Validation Loss: 1.2362, Validation F1: 0.9064\n",
      "Epoch 4798, Train Loss: 0.7622, Validation Loss: 1.2305, Validation F1: 0.9030\n",
      "Epoch 4799, Train Loss: 0.7617, Validation Loss: 1.2216, Validation F1: 0.8994\n",
      "Epoch 4800, Train Loss: 0.7633, Validation Loss: 1.2458, Validation F1: 0.9018\n",
      "Epoch 4801, Train Loss: 0.7609, Validation Loss: 1.2398, Validation F1: 0.9062\n",
      "Epoch 4802, Train Loss: 0.7614, Validation Loss: 1.3040, Validation F1: 0.9055\n",
      "Epoch 4803, Train Loss: 0.7612, Validation Loss: 1.2652, Validation F1: 0.9042\n",
      "Epoch 4804, Train Loss: 0.7621, Validation Loss: 1.2543, Validation F1: 0.9027\n",
      "Epoch 4805, Train Loss: 0.7730, Validation Loss: 1.2217, Validation F1: 0.8968\n",
      "Epoch 4806, Train Loss: 0.7693, Validation Loss: 1.2947, Validation F1: 0.8960\n",
      "Epoch 4807, Train Loss: 0.7695, Validation Loss: 1.2900, Validation F1: 0.9046\n",
      "Epoch 4808, Train Loss: 0.7755, Validation Loss: 1.2289, Validation F1: 0.9047\n",
      "Epoch 4809, Train Loss: 0.7735, Validation Loss: 1.2487, Validation F1: 0.9020\n",
      "Epoch 4810, Train Loss: 0.7776, Validation Loss: 1.2481, Validation F1: 0.9025\n",
      "Epoch 4811, Train Loss: 0.7691, Validation Loss: 1.2713, Validation F1: 0.8985\n",
      "Epoch 4812, Train Loss: 0.7747, Validation Loss: 1.2822, Validation F1: 0.9032\n",
      "Epoch 4813, Train Loss: 0.7712, Validation Loss: 1.2921, Validation F1: 0.9031\n",
      "Epoch 4814, Train Loss: 0.7703, Validation Loss: 1.2827, Validation F1: 0.8961\n",
      "Epoch 4815, Train Loss: 0.7793, Validation Loss: 1.2283, Validation F1: 0.8936\n",
      "Epoch 4816, Train Loss: 0.7676, Validation Loss: 1.2490, Validation F1: 0.8961\n",
      "Epoch 4817, Train Loss: 0.7710, Validation Loss: 1.2134, Validation F1: 0.9037\n",
      "Epoch 4818, Train Loss: 0.7704, Validation Loss: 1.2270, Validation F1: 0.9060\n",
      "Epoch 4819, Train Loss: 0.7677, Validation Loss: 1.2605, Validation F1: 0.9056\n",
      "Epoch 4820, Train Loss: 0.7678, Validation Loss: 1.2640, Validation F1: 0.8991\n",
      "Epoch 4821, Train Loss: 0.7665, Validation Loss: 1.2649, Validation F1: 0.8979\n",
      "Epoch 4822, Train Loss: 0.7656, Validation Loss: 1.2826, Validation F1: 0.9008\n",
      "Epoch 4823, Train Loss: 0.7660, Validation Loss: 1.2804, Validation F1: 0.9039\n",
      "Epoch 4824, Train Loss: 0.7648, Validation Loss: 1.2577, Validation F1: 0.9053\n",
      "Epoch 4825, Train Loss: 0.7696, Validation Loss: 1.2507, Validation F1: 0.8982\n",
      "Epoch 4826, Train Loss: 0.7709, Validation Loss: 1.1884, Validation F1: 0.8944\n",
      "Epoch 4827, Train Loss: 0.7793, Validation Loss: 1.2181, Validation F1: 0.9016\n",
      "Epoch 4828, Train Loss: 0.7721, Validation Loss: 1.2601, Validation F1: 0.9053\n",
      "Epoch 4829, Train Loss: 0.7741, Validation Loss: 1.2564, Validation F1: 0.9048\n",
      "Epoch 4830, Train Loss: 0.7735, Validation Loss: 1.2663, Validation F1: 0.9025\n",
      "Epoch 4831, Train Loss: 0.7715, Validation Loss: 1.2304, Validation F1: 0.9046\n",
      "Epoch 4832, Train Loss: 0.7714, Validation Loss: 1.2242, Validation F1: 0.9042\n",
      "Epoch 4833, Train Loss: 0.7712, Validation Loss: 1.2560, Validation F1: 0.9038\n",
      "Epoch 4834, Train Loss: 0.7704, Validation Loss: 1.2055, Validation F1: 0.9003\n",
      "Epoch 4835, Train Loss: 0.7695, Validation Loss: 1.2218, Validation F1: 0.9033\n",
      "Epoch 4836, Train Loss: 0.7680, Validation Loss: 1.2080, Validation F1: 0.9068\n",
      "Epoch 4837 Saved best model. Best F1: 0.9088588255154834\n",
      "Epoch 4837, Train Loss: 0.7680, Validation Loss: 1.2595, Validation F1: 0.9089\n",
      "Epoch 4838, Train Loss: 0.7688, Validation Loss: 1.2786, Validation F1: 0.9056\n",
      "Epoch 4839, Train Loss: 0.7664, Validation Loss: 1.2404, Validation F1: 0.9002\n",
      "Epoch 4840, Train Loss: 0.7677, Validation Loss: 1.2591, Validation F1: 0.8999\n",
      "Epoch 4841, Train Loss: 0.7658, Validation Loss: 1.2761, Validation F1: 0.9048\n",
      "Epoch 4842, Train Loss: 0.7655, Validation Loss: 1.2977, Validation F1: 0.9043\n",
      "Epoch 4843, Train Loss: 0.7647, Validation Loss: 1.2372, Validation F1: 0.9031\n",
      "Epoch 4844, Train Loss: 0.7665, Validation Loss: 1.2893, Validation F1: 0.9012\n",
      "Epoch 4845, Train Loss: 0.7638, Validation Loss: 1.2362, Validation F1: 0.9003\n",
      "Epoch 4846, Train Loss: 0.7627, Validation Loss: 1.2309, Validation F1: 0.9026\n",
      "Epoch 4847, Train Loss: 0.7653, Validation Loss: 1.2486, Validation F1: 0.9009\n",
      "Epoch 4848, Train Loss: 0.7662, Validation Loss: 1.2639, Validation F1: 0.9006\n",
      "Epoch 4849, Train Loss: 0.7641, Validation Loss: 1.2334, Validation F1: 0.9039\n",
      "Epoch 4850, Train Loss: 0.7658, Validation Loss: 1.2411, Validation F1: 0.9069\n",
      "Epoch 4851, Train Loss: 0.7658, Validation Loss: 1.1833, Validation F1: 0.9068\n",
      "Epoch 4852, Train Loss: 0.7669, Validation Loss: 1.2530, Validation F1: 0.9052\n",
      "Epoch 4853, Train Loss: 0.7647, Validation Loss: 1.3078, Validation F1: 0.9026\n",
      "Epoch 4854, Train Loss: 0.7636, Validation Loss: 1.2330, Validation F1: 0.8989\n",
      "Epoch 4855, Train Loss: 0.7638, Validation Loss: 1.2700, Validation F1: 0.8989\n",
      "Epoch 4856, Train Loss: 0.7646, Validation Loss: 1.3203, Validation F1: 0.8993\n",
      "Epoch 4857, Train Loss: 0.7638, Validation Loss: 1.2745, Validation F1: 0.9055\n",
      "Epoch 4858, Train Loss: 0.7682, Validation Loss: 1.2628, Validation F1: 0.9024\n",
      "Epoch 4859, Train Loss: 0.7657, Validation Loss: 1.2096, Validation F1: 0.9001\n",
      "Epoch 4860, Train Loss: 0.7720, Validation Loss: 1.2525, Validation F1: 0.8943\n",
      "Epoch 4861, Train Loss: 0.7694, Validation Loss: 1.2612, Validation F1: 0.9031\n",
      "Epoch 4862, Train Loss: 0.7707, Validation Loss: 1.2632, Validation F1: 0.9042\n",
      "Epoch 4863, Train Loss: 0.7700, Validation Loss: 1.2437, Validation F1: 0.9060\n",
      "Epoch 4864, Train Loss: 0.7689, Validation Loss: 1.2251, Validation F1: 0.9024\n",
      "Epoch 4865, Train Loss: 0.7677, Validation Loss: 1.2879, Validation F1: 0.9005\n",
      "Epoch 4866, Train Loss: 0.7643, Validation Loss: 1.2687, Validation F1: 0.8978\n",
      "Epoch 4867, Train Loss: 0.7659, Validation Loss: 1.3206, Validation F1: 0.9017\n",
      "Epoch 4868, Train Loss: 0.7679, Validation Loss: 1.3085, Validation F1: 0.9062\n",
      "Epoch 4869, Train Loss: 0.7659, Validation Loss: 1.3262, Validation F1: 0.9040\n",
      "Epoch 4870, Train Loss: 0.7754, Validation Loss: 1.2615, Validation F1: 0.8980\n",
      "Epoch 4871, Train Loss: 0.7673, Validation Loss: 1.2080, Validation F1: 0.8943\n",
      "Epoch 4872, Train Loss: 0.7747, Validation Loss: 1.2691, Validation F1: 0.8993\n",
      "Epoch 4873, Train Loss: 0.7698, Validation Loss: 1.2786, Validation F1: 0.9045\n",
      "Epoch 4874, Train Loss: 0.7665, Validation Loss: 1.2764, Validation F1: 0.9058\n",
      "Epoch 4875, Train Loss: 0.7721, Validation Loss: 1.2156, Validation F1: 0.9041\n",
      "Epoch 4876, Train Loss: 0.7696, Validation Loss: 1.2369, Validation F1: 0.9005\n",
      "Epoch 4877, Train Loss: 0.7689, Validation Loss: 1.2801, Validation F1: 0.9013\n",
      "Epoch 4878, Train Loss: 0.7692, Validation Loss: 1.2498, Validation F1: 0.9023\n",
      "Epoch 4879, Train Loss: 0.7671, Validation Loss: 1.2713, Validation F1: 0.9002\n",
      "Epoch 4880, Train Loss: 0.7700, Validation Loss: 1.3091, Validation F1: 0.9036\n",
      "Epoch 4881, Train Loss: 0.7664, Validation Loss: 1.2905, Validation F1: 0.8998\n",
      "Epoch 4882, Train Loss: 0.7658, Validation Loss: 1.2826, Validation F1: 0.9014\n",
      "Epoch 4883, Train Loss: 0.7642, Validation Loss: 1.2360, Validation F1: 0.9050\n",
      "Epoch 4884, Train Loss: 0.7623, Validation Loss: 1.2255, Validation F1: 0.9056\n",
      "Epoch 4885, Train Loss: 0.7714, Validation Loss: 1.2438, Validation F1: 0.9044\n",
      "Epoch 4886, Train Loss: 0.7701, Validation Loss: 1.2522, Validation F1: 0.9006\n",
      "Epoch 4887, Train Loss: 0.7754, Validation Loss: 1.2621, Validation F1: 0.8985\n",
      "Epoch 4888, Train Loss: 0.7652, Validation Loss: 1.2153, Validation F1: 0.9013\n",
      "Epoch 4889, Train Loss: 0.7726, Validation Loss: 1.2541, Validation F1: 0.9035\n",
      "Epoch 4890, Train Loss: 0.7690, Validation Loss: 1.2446, Validation F1: 0.9036\n",
      "Epoch 4891, Train Loss: 0.7694, Validation Loss: 1.2259, Validation F1: 0.9018\n",
      "Epoch 4892, Train Loss: 0.7629, Validation Loss: 1.2274, Validation F1: 0.8950\n",
      "Epoch 4893, Train Loss: 0.7689, Validation Loss: 1.2493, Validation F1: 0.8940\n",
      "Epoch 4894, Train Loss: 0.7654, Validation Loss: 1.2390, Validation F1: 0.8982\n",
      "Epoch 4895, Train Loss: 0.7659, Validation Loss: 1.2061, Validation F1: 0.9043\n",
      "Epoch 4896, Train Loss: 0.7628, Validation Loss: 1.2610, Validation F1: 0.9045\n",
      "Epoch 4897, Train Loss: 0.7645, Validation Loss: 1.2321, Validation F1: 0.9059\n",
      "Epoch 4898, Train Loss: 0.7620, Validation Loss: 1.2985, Validation F1: 0.9020\n",
      "Epoch 4899, Train Loss: 0.7618, Validation Loss: 1.2532, Validation F1: 0.8980\n",
      "Epoch 4900, Train Loss: 0.7613, Validation Loss: 1.2380, Validation F1: 0.9014\n",
      "Epoch 4901, Train Loss: 0.7639, Validation Loss: 1.2179, Validation F1: 0.9055\n",
      "Epoch 4902, Train Loss: 0.7687, Validation Loss: 1.2521, Validation F1: 0.9024\n",
      "Epoch 4903, Train Loss: 0.7651, Validation Loss: 1.3106, Validation F1: 0.9014\n",
      "Epoch 4904, Train Loss: 0.7639, Validation Loss: 1.2356, Validation F1: 0.9022\n",
      "Epoch 4905, Train Loss: 0.7634, Validation Loss: 1.2637, Validation F1: 0.9024\n",
      "Epoch 4906, Train Loss: 0.7635, Validation Loss: 1.3091, Validation F1: 0.9051\n",
      "Epoch 4907, Train Loss: 0.7638, Validation Loss: 1.1963, Validation F1: 0.9083\n",
      "Epoch 4908, Train Loss: 0.7626, Validation Loss: 1.2574, Validation F1: 0.9057\n",
      "Epoch 4909, Train Loss: 0.7650, Validation Loss: 1.2243, Validation F1: 0.8945\n",
      "Epoch 4910, Train Loss: 0.7643, Validation Loss: 1.2543, Validation F1: 0.8934\n",
      "Epoch 4911, Train Loss: 0.7628, Validation Loss: 1.2146, Validation F1: 0.9014\n",
      "Epoch 4912, Train Loss: 0.7617, Validation Loss: 1.2667, Validation F1: 0.9059\n",
      "Epoch 4913, Train Loss: 0.7629, Validation Loss: 1.2901, Validation F1: 0.9073\n",
      "Epoch 4914, Train Loss: 0.7622, Validation Loss: 1.2925, Validation F1: 0.9040\n",
      "Epoch 4915, Train Loss: 0.7611, Validation Loss: 1.2622, Validation F1: 0.9013\n",
      "Epoch 4916, Train Loss: 0.7628, Validation Loss: 1.1850, Validation F1: 0.8985\n",
      "Epoch 4917, Train Loss: 0.7610, Validation Loss: 1.3177, Validation F1: 0.9007\n",
      "Epoch 4918, Train Loss: 0.7603, Validation Loss: 1.2828, Validation F1: 0.9053\n",
      "Epoch 4919, Train Loss: 0.7607, Validation Loss: 1.2747, Validation F1: 0.9026\n",
      "Epoch 4920, Train Loss: 0.7608, Validation Loss: 1.2765, Validation F1: 0.9037\n",
      "Epoch 4921, Train Loss: 0.7650, Validation Loss: 1.2642, Validation F1: 0.8983\n",
      "Epoch 4922, Train Loss: 0.7628, Validation Loss: 1.2478, Validation F1: 0.9011\n",
      "Epoch 4923, Train Loss: 0.7645, Validation Loss: 1.3176, Validation F1: 0.9028\n",
      "Epoch 4924, Train Loss: 0.7647, Validation Loss: 1.3051, Validation F1: 0.9055\n",
      "Epoch 4925, Train Loss: 0.7627, Validation Loss: 1.2677, Validation F1: 0.9062\n",
      "Epoch 4926, Train Loss: 0.7646, Validation Loss: 1.2577, Validation F1: 0.9053\n",
      "Epoch 4927, Train Loss: 0.7613, Validation Loss: 1.2532, Validation F1: 0.9012\n",
      "Epoch 4928, Train Loss: 0.7620, Validation Loss: 1.2612, Validation F1: 0.8959\n",
      "Epoch 4929, Train Loss: 0.7621, Validation Loss: 1.2653, Validation F1: 0.8963\n",
      "Epoch 4930, Train Loss: 0.7620, Validation Loss: 1.2471, Validation F1: 0.9031\n",
      "Epoch 4931, Train Loss: 0.7621, Validation Loss: 1.3102, Validation F1: 0.9054\n",
      "Epoch 4932, Train Loss: 0.7612, Validation Loss: 1.2837, Validation F1: 0.9054\n",
      "Epoch 4933, Train Loss: 0.7614, Validation Loss: 1.2397, Validation F1: 0.9036\n",
      "Epoch 4934, Train Loss: 0.7612, Validation Loss: 1.2636, Validation F1: 0.9025\n",
      "Epoch 4935, Train Loss: 0.7619, Validation Loss: 1.2745, Validation F1: 0.8996\n",
      "Epoch 4936, Train Loss: 0.7613, Validation Loss: 1.2655, Validation F1: 0.8983\n",
      "Epoch 4937, Train Loss: 0.7610, Validation Loss: 1.2796, Validation F1: 0.8981\n",
      "Epoch 4938, Train Loss: 0.7612, Validation Loss: 1.2277, Validation F1: 0.9064\n",
      "Epoch 4939, Train Loss: 0.7619, Validation Loss: 1.3081, Validation F1: 0.9043\n",
      "Epoch 4940, Train Loss: 0.7599, Validation Loss: 1.2954, Validation F1: 0.9030\n",
      "Epoch 4941, Train Loss: 0.7610, Validation Loss: 1.2525, Validation F1: 0.9005\n",
      "Epoch 4942, Train Loss: 0.7600, Validation Loss: 1.2453, Validation F1: 0.9041\n",
      "Epoch 4943, Train Loss: 0.7594, Validation Loss: 1.1999, Validation F1: 0.9044\n",
      "Epoch 4944, Train Loss: 0.7601, Validation Loss: 1.3016, Validation F1: 0.9024\n",
      "Epoch 4945, Train Loss: 0.7602, Validation Loss: 1.2748, Validation F1: 0.9021\n",
      "Epoch 4946, Train Loss: 0.7599, Validation Loss: 1.3112, Validation F1: 0.9030\n",
      "Epoch 4947, Train Loss: 0.7616, Validation Loss: 1.2352, Validation F1: 0.9043\n",
      "Epoch 4948, Train Loss: 0.7591, Validation Loss: 1.3013, Validation F1: 0.9007\n",
      "Epoch 4949, Train Loss: 0.7595, Validation Loss: 1.3016, Validation F1: 0.9028\n",
      "Epoch 4950, Train Loss: 0.7597, Validation Loss: 1.2658, Validation F1: 0.8987\n",
      "Epoch 4951, Train Loss: 0.7626, Validation Loss: 1.2734, Validation F1: 0.8969\n",
      "Epoch 4952, Train Loss: 0.7603, Validation Loss: 1.2730, Validation F1: 0.8998\n",
      "Epoch 4953, Train Loss: 0.7603, Validation Loss: 1.2579, Validation F1: 0.9035\n",
      "Epoch 4954, Train Loss: 0.7606, Validation Loss: 1.2815, Validation F1: 0.9045\n",
      "Epoch 4955, Train Loss: 0.7601, Validation Loss: 1.3021, Validation F1: 0.9006\n",
      "Epoch 4956, Train Loss: 0.7611, Validation Loss: 1.2770, Validation F1: 0.9029\n",
      "Epoch 4957, Train Loss: 0.7617, Validation Loss: 1.3045, Validation F1: 0.9002\n",
      "Epoch 4958, Train Loss: 0.7609, Validation Loss: 1.2969, Validation F1: 0.9006\n",
      "Epoch 4959, Train Loss: 0.7611, Validation Loss: 1.3326, Validation F1: 0.9029\n",
      "Epoch 4960, Train Loss: 0.7607, Validation Loss: 1.2805, Validation F1: 0.9039\n",
      "Epoch 4961, Train Loss: 0.7589, Validation Loss: 1.2978, Validation F1: 0.9051\n",
      "Epoch 4962, Train Loss: 0.7603, Validation Loss: 1.3153, Validation F1: 0.9035\n",
      "Epoch 4963, Train Loss: 0.7601, Validation Loss: 1.2641, Validation F1: 0.9056\n",
      "Epoch 4964, Train Loss: 0.7597, Validation Loss: 1.2798, Validation F1: 0.9046\n",
      "Epoch 4965, Train Loss: 0.7601, Validation Loss: 1.3085, Validation F1: 0.9025\n",
      "Epoch 4966, Train Loss: 0.7606, Validation Loss: 1.2618, Validation F1: 0.8977\n",
      "Epoch 4967, Train Loss: 0.7588, Validation Loss: 1.2767, Validation F1: 0.9016\n",
      "Epoch 4968, Train Loss: 0.7595, Validation Loss: 1.2576, Validation F1: 0.9046\n",
      "Epoch 4969, Train Loss: 0.7590, Validation Loss: 1.3186, Validation F1: 0.9071\n",
      "Epoch 4970, Train Loss: 0.7599, Validation Loss: 1.2827, Validation F1: 0.9070\n",
      "Epoch 4971, Train Loss: 0.7600, Validation Loss: 1.2339, Validation F1: 0.9053\n",
      "Epoch 4972, Train Loss: 0.7593, Validation Loss: 1.3166, Validation F1: 0.9044\n",
      "Epoch 4973, Train Loss: 0.7597, Validation Loss: 1.2430, Validation F1: 0.9030\n",
      "Epoch 4974, Train Loss: 0.7596, Validation Loss: 1.3149, Validation F1: 0.9017\n",
      "Epoch 4975, Train Loss: 0.7592, Validation Loss: 1.3104, Validation F1: 0.9077\n",
      "Epoch 4976, Train Loss: 0.7592, Validation Loss: 1.2681, Validation F1: 0.9050\n",
      "Epoch 4977, Train Loss: 0.7592, Validation Loss: 1.2689, Validation F1: 0.9021\n",
      "Epoch 4978, Train Loss: 0.7596, Validation Loss: 1.2213, Validation F1: 0.9022\n",
      "Epoch 4979, Train Loss: 0.7584, Validation Loss: 1.2232, Validation F1: 0.9043\n",
      "Epoch 4980, Train Loss: 0.7600, Validation Loss: 1.2999, Validation F1: 0.9022\n",
      "Epoch 4981, Train Loss: 0.7584, Validation Loss: 1.2984, Validation F1: 0.9017\n",
      "Epoch 4982, Train Loss: 0.7597, Validation Loss: 1.3181, Validation F1: 0.9022\n",
      "Epoch 4983, Train Loss: 0.7588, Validation Loss: 1.2277, Validation F1: 0.9021\n",
      "Epoch 4984, Train Loss: 0.7604, Validation Loss: 1.2560, Validation F1: 0.9011\n",
      "Epoch 4985, Train Loss: 0.7592, Validation Loss: 1.3046, Validation F1: 0.9026\n",
      "Epoch 4986, Train Loss: 0.7595, Validation Loss: 1.2903, Validation F1: 0.9074\n",
      "Epoch 4987, Train Loss: 0.7595, Validation Loss: 1.2487, Validation F1: 0.9058\n",
      "Epoch 4988, Train Loss: 0.7592, Validation Loss: 1.2425, Validation F1: 0.9019\n",
      "Epoch 4989, Train Loss: 0.7587, Validation Loss: 1.2648, Validation F1: 0.8931\n",
      "Epoch 4990, Train Loss: 0.7605, Validation Loss: 1.2740, Validation F1: 0.8991\n",
      "Epoch 4991, Train Loss: 0.7588, Validation Loss: 1.3093, Validation F1: 0.9053\n",
      "Epoch 4992, Train Loss: 0.7594, Validation Loss: 1.2528, Validation F1: 0.9049\n",
      "Epoch 4993, Train Loss: 0.7596, Validation Loss: 1.3173, Validation F1: 0.9062\n",
      "Epoch 4994, Train Loss: 0.7598, Validation Loss: 1.2859, Validation F1: 0.9047\n",
      "Epoch 4995, Train Loss: 0.7591, Validation Loss: 1.2880, Validation F1: 0.9024\n",
      "Epoch 4996, Train Loss: 0.7588, Validation Loss: 1.3150, Validation F1: 0.8930\n",
      "Epoch 4997, Train Loss: 0.7581, Validation Loss: 1.3186, Validation F1: 0.9033\n",
      "Epoch 4998, Train Loss: 0.7595, Validation Loss: 1.3219, Validation F1: 0.9049\n",
      "Epoch 4999, Train Loss: 0.7580, Validation Loss: 1.2859, Validation F1: 0.9083\n",
      "Model training completed and saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Extract the best parameters from the grid search\n",
    "best_hidden_dim = 256  # Replace with the best hidden_dim found\n",
    "best_learning_rate = 0.001  # Replace with the best learning_rate found\n",
    "best_dropout = 0.3  # Replace with the best dropout found\n",
    "\n",
    "# Initialize the model with the best parameters\n",
    "model = EGraphSAGE(node_in_channels=G_pyg_train.num_node_features,\n",
    "                   edge_in_channels=G_pyg_train.num_edge_features,\n",
    "                   hidden_channels=best_hidden_dim,\n",
    "                   dropout=best_dropout,\n",
    "                   out_channels=num_classes).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Compute class weights for the training dataset\n",
    "labels = G_pyg_train.edge_label.cpu().numpy()\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.unique(labels),\n",
    "                                                  y=labels)\n",
    "\n",
    "# Normalize class weights\n",
    "class_weights = th.FloatTensor(class_weights).to(device)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# Move the graph data to the device\n",
    "G_pyg_train = G_pyg_train.to(device)\n",
    "G_pyg_val = G_pyg_val.to(device)\n",
    "\n",
    "G_pyg_train.edge_label = G_pyg_train.edge_label.to(device)\n",
    "G_pyg_train.edge_attr = G_pyg_train.edge_attr.to(device)\n",
    "\n",
    "G_pyg_val.edge_label = G_pyg_val.edge_label.to(device)\n",
    "G_pyg_val.edge_attr = G_pyg_val.edge_attr.to(device)\n",
    "\n",
    "# ===== Load checkpoint if exists =====\n",
    "best_f1 = 0\n",
    "start_epoch = 0\n",
    "epochs = 5000\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = th.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_f1 = checkpoint['best_f1']\n",
    "    print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "val_f1_history = []\n",
    "saved_model_epochs = []\n",
    "\n",
    "train_loss_history_path = os.path.join(saves_path, 'train_loss_history.pkl')\n",
    "val_loss_history_path = os.path.join(saves_path, 'val_loss_history.pkl')\n",
    "val_f1_history_path = os.path.join(saves_path, 'val_f1_history.pkl')\n",
    "saved_model_epochs_path = os.path.join(saves_path, 'saved_model_epochs.pkl')\n",
    "\n",
    "if os.path.exists(train_loss_history_path) and os.path.exists(val_loss_history_path) and os.path.exists(val_f1_history_path) and os.path.exists(saved_model_epochs_path):\n",
    "    with open(train_loss_history_path, 'rb') as f:\n",
    "        train_loss_history = pickle.load(f)\n",
    "    with open(val_loss_history_path, 'rb') as f:\n",
    "        val_loss_history = pickle.load(f)\n",
    "    with open(val_f1_history_path, 'rb') as f:\n",
    "        val_f1_history = pickle.load(f)\n",
    "    with open(saved_model_epochs_path, 'rb') as f:\n",
    "        saved_model_epochs = pickle.load(f)\n",
    "\n",
    "# ===== Start Training =====\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    out = model(G_pyg_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(out, G_pyg_train.edge_label)\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        out = model(G_pyg_val)\n",
    "        loss = criterion(out, G_pyg_val.edge_label)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    val_f1 = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='weighted')\n",
    "    val_f1_micro = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='micro')\n",
    "    val_f1_macro = f1_score(G_pyg_val.edge_label.cpu(), out.argmax(dim=1).cpu(), average='macro')\n",
    "\n",
    "    if epoch % 10 == 0 or val_f1 > best_f1:\n",
    "        # Save checkpoint\n",
    "        th.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_f1': best_f1\n",
    "        }, checkpoint_path)\n",
    "        with open(train_loss_history_path, 'wb') as f:\n",
    "            pickle.dump(train_loss_history, f)\n",
    "        with open(val_loss_history_path, 'wb') as f:\n",
    "            pickle.dump(val_loss_history, f)\n",
    "        with open(val_f1_history_path, 'wb') as f:\n",
    "            pickle.dump(val_f1_history, f)\n",
    "        with open(saved_model_epochs_path, 'wb') as f:\n",
    "            pickle.dump(saved_model_epochs, f)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1  # Update the best F1 score for this fold\n",
    "        best_model_state = model.state_dict()\n",
    "        th.save(best_model_state, best_model_path)\n",
    "        print(f\"Epoch {epoch} Saved best model. Best F1:\", best_f1)\n",
    "        saved_model_epochs.append(epoch)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}')\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_f1_history.append((val_f1, val_f1_micro, val_f1_macro))\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Model training completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_process(train_losses, val_losses, val_f1, saved_model_epochs):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "    # Plot Train Loss\n",
    "    axs[0].plot(train_losses, label='Train Loss', color='blue')\n",
    "    axs[0].plot(val_losses, label='Validation Loss', color='red')\n",
    "    axs[0].set_ylabel('Train Loss')\n",
    "    axs[0].set_title('Training Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid()\n",
    "\n",
    "    val_f1_weighted_history = []\n",
    "    val_f1_micro_history = []\n",
    "    val_f1_macro_history = []\n",
    "\n",
    "    for val_f1_weighted, val_f1_micro, val_f1_macro in val_f1:\n",
    "        val_f1_weighted_history.append(val_f1_weighted)\n",
    "        val_f1_micro_history.append(val_f1_micro)\n",
    "        val_f1_macro_history.append(val_f1_macro)\n",
    "    \n",
    "    # Plot Validation F1\n",
    "    axs[1].plot(val_f1_weighted_history, label='Validation F1 Weighted', color='green')\n",
    "    axs[1].plot(val_f1_micro_history, label='Validation F1 Micro', color='blue')\n",
    "    axs[1].plot(val_f1_macro_history, label='Validation F1 Macro', color='red')\n",
    "    average_val_f1 = np.mean(val_f1)\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Validation F1')\n",
    "    axs[1].set_title('Validation F1 Score')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid()\n",
    "\n",
    "    print(len(train_losses))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4989\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_process(train_loss_history, val_loss_history, val_f1_history, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8368fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in G_pyg: 81474\n",
      "Number of node in G_pyg: 9773\n",
      "Shape of node in G_pyg: torch.Size([9773, 52])\n",
      "Shape of edge attr in G_pyg: torch.Size([81474, 52])\n",
      "Shape of edge label in G_pyg: torch.Size([81474])\n"
     ]
    }
   ],
   "source": [
    "G_nx_test, G_pyg_test = create_graph(test_df, SOURCE_IP_COL_NAME, DESTINATION_IP_COL_NAME, ['h', label_col], create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f271a-612b-4cd6-a85a-e4236dec9d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_combined_ports_final/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[   49    62   244    23     0     0     0     1     0    23]\n",
      " [    5   102   217     6     4     0     0     2     8     5]\n",
      " [   27   160  1776   208    40     5     0     9    52   176]\n",
      " [  161   377  2258  2433   170    12     3    68   127  1070]\n",
      " [   47   166   259   220  2548     6    42    25   150   174]\n",
      " [   27    29   274   252    23 31472     2    11    67   165]\n",
      " [   21     5     4    63   311     0 32824    14    18    21]\n",
      " [   30    30   278   104    25     1     0  1412    18   200]\n",
      " [    3     2     1     9     5     0     0     4   201     2]\n",
      " [    4     0     0     5     0     0     0     1     0    16]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.1310    0.1219    0.1263       402\n",
      "     Backdoors     0.1093    0.2923    0.1591       349\n",
      "           DoS     0.3344    0.7240    0.4575      2453\n",
      "      Exploits     0.7322    0.3643    0.4865      6679\n",
      "       Fuzzers     0.8151    0.7006    0.7535      3637\n",
      "       Generic     0.9992    0.9737    0.9863     32322\n",
      "        Normal     0.9986    0.9863    0.9924     33281\n",
      "Reconnaissance     0.9127    0.6730    0.7748      2098\n",
      "     Shellcode     0.3136    0.8855    0.4631       227\n",
      "         Worms     0.0086    0.6154    0.0170        26\n",
      "\n",
      "      accuracy                         0.8939     81474\n",
      "     macro avg     0.5355    0.6337    0.5217     81474\n",
      "  weighted avg     0.9363    0.8939    0.9065     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_accuracy(pred, labels):\n",
    "    return (pred.argmax(1) == labels).float().mean().item()\n",
    "\n",
    "def eval(G_pyg_test, adversarial=False):\n",
    "\n",
    "    G_pyg_test = G_pyg_test.to(device)\n",
    "    G_pyg_test.edge_label = G_pyg_test.edge_label.to(device)\n",
    "    G_pyg_test.edge_attr = G_pyg_test.edge_attr.to(device)\n",
    "\n",
    "    best_model = EGraphSAGE(node_in_channels=G_pyg_test.num_node_features, \n",
    "                       edge_in_channels=G_pyg_test.num_edge_features,\n",
    "                       hidden_channels=best_hidden_dim, \n",
    "                       out_channels=num_classes).to(device)\n",
    "\n",
    "    print(\"Loading model from\", best_model_path)\n",
    "    best_model.load_state_dict(th.load(best_model_path))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    best_model.eval()\n",
    "\n",
    "    print(\"inference start\")\n",
    "    with th.no_grad():\n",
    "            \n",
    "        try:\n",
    "            out = best_model(G_pyg_test)\n",
    "            \n",
    "        except Exception as forward_error:\n",
    "            print(f\"Error during forward/backward pass at {forward_error}\")\n",
    "\n",
    "    print(\"inference done\")\n",
    "\n",
    "    pred_labels = out.argmax(dim=1).cpu()\n",
    "    all_test_labels = G_pyg_test.edge_label.cpu()\n",
    "\n",
    "    if adversarial:\n",
    "\n",
    "        # Create a boolean mask where the label is NOT equal to the adversarial class\n",
    "        adversarial_mask = all_test_labels == ADVERSARIAL_CLASS_LABEL\n",
    "\n",
    "        # Print the class that the adversarial samples are classified as\n",
    "        cm_adversarial = confusion_matrix(all_test_labels[adversarial_mask], pred_labels[adversarial_mask], labels=range(len(class_map) + 1))\n",
    "        print(\"Adversarial confusion matrix:\", cm_adversarial)\n",
    "\n",
    "        # Apply the mask to both labels and predictions\n",
    "        all_test_labels = all_test_labels[~adversarial_mask]\n",
    "        pred_labels = pred_labels[~adversarial_mask]\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"class_map\", class_map)\n",
    "    # Generate a report\n",
    "    cm = confusion_matrix(all_test_labels, pred_labels, labels=range(len(class_map)))\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    report = classification_report(all_test_labels, pred_labels, target_names=class_map, digits=4)\n",
    "    print(report)\n",
    "\n",
    "    \n",
    "eval(G_pyg_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_traffic_to_attacker(graph, ratio=0.1, num_injected_nodes=1, is_attack=False):\n",
    "    edge_index = graph.edge_index.clone()\n",
    "    edge_attr = graph.edge_attr.clone()\n",
    "    edge_label = graph.edge_label.clone()\n",
    "    x = graph.x.clone()\n",
    "\n",
    "    num_edges = edge_index.size(1)\n",
    "    feature_dim = graph.x.size(1)\n",
    "\n",
    "    # 1. Identify attacker nodes\n",
    "    attacker_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "    attacker_nodes = th.unique(edge_index[:, attacker_edges])\n",
    "    if attacker_nodes.numel() == 0:\n",
    "        raise ValueError(\"No attacker nodes found.\")\n",
    "\n",
    "    # 2. Sample benign edge feature pool\n",
    "    if is_attack:\n",
    "        attack_edges = (edge_label != BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[attack_edges]\n",
    "    else:\n",
    "        benign_edges = (edge_label == BENIGN_CLASS_LABEL).nonzero(as_tuple=False).squeeze()\n",
    "        inject_edge_attr_pool = edge_attr[benign_edges]\n",
    "\n",
    "    # 3. Inject new nodes\n",
    "    original_num_nodes = x.size(0)\n",
    "\n",
    "    new_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "    x = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "    # 4. Inject edges from injected nodes to attacker nodes\n",
    "    num_to_inject = max(1, int(ratio * num_edges))\n",
    "    new_edges = []\n",
    "    new_attrs = []\n",
    "    new_labels = []\n",
    "\n",
    "    \n",
    "    for _ in range(num_to_inject):\n",
    "        src = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\n",
    "        dst = attacker_nodes[random.randint(0, len(attacker_nodes) - 1)].item()\n",
    "\n",
    "        new_edges.append([src, dst])\n",
    "        attr = inject_edge_attr_pool[random.randint(0, len(inject_edge_attr_pool) - 1)]\n",
    "        new_attrs.append(attr)\n",
    "        new_labels.append(ADVERSARIAL_CLASS_LABEL)\n",
    "\n",
    "    # Create a new empty graph to store the injected edges\n",
    "    new_graph = Data()\n",
    "\n",
    "    # 5. Merge into graph\n",
    "    if new_edges:\n",
    "        new_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "        new_attrs = th.stack(new_attrs)\n",
    "        new_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "        new_graph.edge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "        new_graph.edge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "        new_graph.edge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "        new_graph.x = x\n",
    "\n",
    "        # new_graph.first_injected_node_idx = original_num_nodes # Store injected node indices\n",
    "\n",
    "    return new_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c5bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_combined_ports_final/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0   56  771  995 1251 2314 1085 1546  129    0    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[    0    59   203     2     0     0   138     0     0     0]\n",
      " [    0    60   143     2     1     4   124    15     0     0]\n",
      " [    0    73  1399    31     9     9   905    26     1     0]\n",
      " [    0   168  2754   356    58    58  3080   202     3     0]\n",
      " [    0   179   893    44   535    19  1961     6     0     0]\n",
      " [    0     4 13606  1415   332  4993 11939    33     0     0]\n",
      " [    0     7   126    33    73     3 33039     0     0     0]\n",
      " [    0     5  1045     5     6     3  1029     5     0     0]\n",
      " [    0     5    24    23     5    21    87    62     0     0]\n",
      " [    0     0    14     0     0     0    12     0     0     0]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0000    0.0000    0.0000       402\n",
      "     Backdoors     0.1071    0.1719    0.1320       349\n",
      "           DoS     0.0692    0.5703    0.1235      2453\n",
      "      Exploits     0.1863    0.0533    0.0829      6679\n",
      "       Fuzzers     0.5250    0.1471    0.2298      3637\n",
      "       Generic     0.9771    0.1545    0.2668     32322\n",
      "        Normal     0.6316    0.9927    0.7720     33281\n",
      "Reconnaissance     0.0143    0.0024    0.0041      2098\n",
      "     Shellcode     0.0000    0.0000    0.0000       227\n",
      "         Worms     0.0000    0.0000    0.0000        26\n",
      "\n",
      "      accuracy                         0.4957     81474\n",
      "     macro avg     0.2511    0.2092    0.1611     81474\n",
      "  weighted avg     0.6872    0.4957    0.4426     81474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Inject Attack Traffic to Attacker Nodes\n",
    "G_pyg_test = G_pyg_test.cpu()\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=True)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c37b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_combined_ports_final/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  13   13  159  457  654    4 6787   19   21   20    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[    0    57    16     1   158     0   170     0     0     0]\n",
      " [    2    66     4     1   175     0   100     0     1     0]\n",
      " [   13   113    81    38  1260     0   915     4     0    29]\n",
      " [  106   236   377   434  1865     5  3472     6     1   177]\n",
      " [   16   195    66    26  1054     6  2240     0     0    34]\n",
      " [   18    18  7618    41   527   326   880 22866     1    27]\n",
      " [    0     3     7     5   121     0 33143     0     1     1]\n",
      " [   20    15    36    11   565     0  1413     1     0    37]\n",
      " [    0     0     4     4     4     0   214     0     1     0]\n",
      " [    0     0     3     0     4     0    16     0     0     3]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0000    0.0000    0.0000       402\n",
      "     Backdoors     0.0939    0.1891    0.1255       349\n",
      "           DoS     0.0099    0.0330    0.0152      2453\n",
      "      Exploits     0.7736    0.0650    0.1199      6679\n",
      "       Fuzzers     0.1838    0.2898    0.2250      3637\n",
      "       Generic     0.9674    0.0101    0.0200     32322\n",
      "        Normal     0.7787    0.9959    0.8740     33281\n",
      "Reconnaissance     0.0000    0.0005    0.0001      2098\n",
      "     Shellcode     0.2000    0.0044    0.0086       227\n",
      "         Worms     0.0097    0.1154    0.0180        26\n",
      "\n",
      "      accuracy                         0.4309     81474\n",
      "     macro avg     0.3017    0.1703    0.1406     81474\n",
      "  weighted avg     0.7747    0.4309    0.3858     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inject BENIGN Traffic to Attacker Nodes\n",
    "injected_graph = inject_traffic_to_attacker(G_pyg_test, 0.1, num_injected_nodes=1, is_attack=False)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_random_nodes(graph, ratio=0.1, num_injected_nodes=1):\n",
    "\tedge_index = graph.edge_index.clone()\n",
    "\tedge_attr = graph.edge_attr.clone()\n",
    "\tedge_label = graph.edge_label.clone()\n",
    "\tx = graph.x.clone()\n",
    "\n",
    "\tnum_edges = edge_index.size(1)\n",
    "\tfeature_dim = graph.x.size(1)\n",
    "\n",
    "\t# 1. Inject new nodes\n",
    "\toriginal_num_nodes = x.size(0)\n",
    "\tnew_node_feats = th.ones((num_injected_nodes, feature_dim))\n",
    "\tx = th.cat([x, new_node_feats], dim=0)\n",
    "\n",
    "\t# 2. Inject random edges\n",
    "\tnum_to_inject = max(1, int(ratio * num_edges))\n",
    "\tnew_edges = []\n",
    "\tnew_attrs = []\n",
    "\tnew_labels = []\n",
    "\n",
    "\tfor _ in range(num_to_inject):\n",
    "\t\tsrc = random.randint(original_num_nodes, original_num_nodes + num_injected_nodes - 1)  # from injected nodes\n",
    "\t\tdst = random.randint(0, original_num_nodes - 1)  # to existing nodes\n",
    "\n",
    "\t\tnew_edges.append([src, dst])\n",
    "\t\tattr = edge_attr[random.randint(0, len(edge_attr) - 1)]  # Randomly sample edge attributes\n",
    "\t\tnew_attrs.append(attr)\n",
    "\t\tnew_labels.append(ADVERSARIAL_CLASS_LABEL)  # Assign benign class label to new edges\n",
    "\n",
    "\t# 3. Merge into graph\n",
    "\tif new_edges:\n",
    "\t\tnew_edges = th.tensor(new_edges, dtype=th.long).t().contiguous()\n",
    "\t\tnew_attrs = th.stack(new_attrs)\n",
    "\t\tnew_labels = th.tensor(new_labels, dtype=th.long)\n",
    "\n",
    "\t\tedge_index = th.cat([edge_index, new_edges], dim=1)\n",
    "\t\tedge_attr = th.cat([edge_attr, new_attrs], dim=0)\n",
    "\t\tedge_label = th.cat([edge_label, new_labels], dim=0)\n",
    "\n",
    "\t# Create a new graph with the injected nodes and edges\n",
    "\tnew_graph = Data(\n",
    "\t\tedge_index=edge_index,\n",
    "\t\tedge_attr=edge_attr,\n",
    "\t\tedge_label=edge_label,\n",
    "\t\tx=x\n",
    "\t)\n",
    "\n",
    "\treturn new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb63898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /vol/bitbucket/shc20/FYP/GNN-Adversarial-Attack/Models/E_GraphSAGE/logs/UNSW_NB15/whole_graph_combined_ports_final/best_model_all_downsampled.pth\n",
      "inference start\n",
      "inference done\n",
      "Adversarial confusion matrix: [[   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   3   38  474  925  234  565 5726  112   70    0    0]]\n",
      "class_map ['Analysis' 'Backdoors' 'DoS' 'Exploits' 'Fuzzers' 'Generic' 'Normal'\n",
      " 'Reconnaissance' 'Shellcode' 'Worms']\n",
      "[[    0    59   279    24    37     0     3     0     0     0]\n",
      " [    0    75   185    14    54     4    15     1     1     0]\n",
      " [    0   103  1747   182   298     9    95     9     8     2]\n",
      " [    3   258  3397  1738   580    40   555    75    30     3]\n",
      " [    0   180   763   480  1482    16   654     9    51     2]\n",
      " [    0    11 14264  1551    68  4799    59 11562     5     3]\n",
      " [    0     3   216   287   157    39 32547    21    11     0]\n",
      " [    1    17  1432    72    89     4   300   180     2     1]\n",
      " [    0     4    64    26     6    15    45    12    55     0]\n",
      " [    0     0    19     6     0     0     1     0     0     0]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis     0.0000    0.0000    0.0000       402\n",
      "     Backdoors     0.1056    0.2149    0.1416       349\n",
      "           DoS     0.0781    0.7122    0.1408      2453\n",
      "      Exploits     0.3968    0.2602    0.3143      6679\n",
      "       Fuzzers     0.5348    0.4075    0.4625      3637\n",
      "       Generic     0.9742    0.1485    0.2577     32322\n",
      "        Normal     0.9496    0.9779    0.9636     33281\n",
      "Reconnaissance     0.0152    0.0858    0.0258      2098\n",
      "     Shellcode     0.3374    0.2423    0.2821       227\n",
      "         Worms     0.0000    0.0000    0.0000        26\n",
      "\n",
      "      accuracy                         0.5231     81474\n",
      "     macro avg     0.3392    0.3049    0.2588     81474\n",
      "  weighted avg     0.8349    0.5231    0.5485     81474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inject Random Nodes in the graph\n",
    "injected_graph = inject_random_nodes(G_pyg_test, 0.1, num_injected_nodes=1)\n",
    "eval(injected_graph, adversarial=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
